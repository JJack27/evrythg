Adaptive and Predictive Control
L2: Parameter Estimation

Dr.-Ing. Stefan Sosnowski
Institute for Information-Oriented Control
Technische Universit¨t M¨nchen
a
u
APR Lecture, summer semester 2014

www.itr.ei.tum.de

Content
1. Introduction to Adaptive and Predictive Control
2. Parameter Estimation
3. Self tuning regulators, pole placement
4. Model Reference Adaptive systems
5. Autotuning, Gain Scheduling
6. Practical Issues and Implementation, Applications

Parameter Estimation

2

Parameter Estimation
Key element in adaptive control
Directly used in self tuning regulator
Indirectly used in MRAC
Key element in MPC
Used to reﬁne model and thus prediction

Parameter Estimation

3

Parameter Estimation
Parameter estimation is part of system identiﬁcation
Selection of the model structure
Experiment design (choosing the input signal)
Parameter estimation (simpliﬁed if model is linear in the
unknown parameters)
Validation
In adaptive systems ...
system identiﬁcation is done automatically
parameter estimation must be done online
system input is generated by feedback

Parameter Estimation

4

Parameter Estimation

Parameter Estimation

5

Regression Models

observed variable $y(i)$
regressors $\phi^T$
model parameters $\theta^0$
index (time) $i$
experimental data

-6-

Loss Function for Least Squares Estimation
Objective: Choose parameter set θ such, that the sum of squares
of the diﬀerences between measured (y) and estimated (ϕT θ)
outputs is minimized.
Loss function
V (θ, t) =

1
2

t

(y(i) − ϕT θ)2
i=1

Linear in unknown parameters θ0
Quadratic cost function
⇒ Analytic solution

Parameter Estimation

7

Residuals of the Least Squares Estimation
residuals:

ε(i) = y(i) − y (i) = y(i) − ϕT (i)θ
ˆ

matrix notation:
[ ... ]
_v('140503201135')
Parameter Estimation

8

Theorem for Least Squares Estimation
ˆ
E = Y − Y = Y − Φθ
ˆ
V (θ, t) is minimal for the following choice of parameters θ:
ˆ
ΦT Φθ = ΦT Y
If the matrix ΦT Φ is nonsingular (invertible), the minimum is
unique and is given by
with
_v('140503202223')
-9-

Remarks
Nonsingularity of ΦT Φ ⇒ excitation condition
requires N > n for Φ ∈ RN ×n
Moore Penrose Pseudoinverse: Φ+ = (ΦT Φ)−1 ΦT
Equal weighting of all errors ε(i)
Diﬀerent weighting through generalized inverse:
1
V = ET W E
2
with a symmetric and positive deﬁnite weighting matrix
W = W T > 0.
ˆ
θ = (ΦT W Φ)−1 ΦT W Y
Parameter Estimation

10

Example 1/3: Static System
Regression model: y(i) = b0 + b1 u(i) + b2 u2 (i) + e(i)
with

e(i)

Gaussian noise with zero mean
and standard deviation of 0.1

θT = (b0

b1

b2 )

unknown parameters

ϕT (i) = (1 u(i) u2 (i))

regression variables

Consideration of diﬀerent model structures (not clear a-priori):
model
model
model
model

1:
2:
3:
4:

y
y
y
y

= b0
= b0 + b1 u
= b0 + b1 u + b2 u2
= b0 + b1 u + b2 u2 + b3 u3

Parameter Estimation

11

Example 2/3: Static System
Estimated model parameters and loss functions:

Measured data and estimated model:
y = b0

y = b0 + b1 u

Parameter Estimation

12

Example 3/3: Static System
y = b0 + b1 u + b2 u2

y = b0 + b1 u + b2 u2 + b3 u3

⇒ Choosing a suitable model structure is important!
Too few parameters: ⇒ bad ﬁt
Too many parameters: “overﬁtting”, very good ﬁt for given
data set, but bad ﬁt for slightly diﬀerent data set
⇒ Validation with diﬀerent data set!
Parameter Estimation

13

Geometric Interpretation
Assumption: two parameters θ1 , θ2



 



ε(1)
y(1)
ϕ1 (1)
ϕ2 (1)
E =  . . .  = Y −ϕ1 θ1 −ϕ2 θ2 =  . . . − . . .  θ1 − . . .  θ2
ε(t)
y(t)
ϕ1 (t)
ϕ2 (t)
ˆ
E = Y − Y is minimal if E
is orthogonal to the plane
spanned by ϕ1 , ϕ2
solution is unique if ϕi are
linearly independent
ﬁnd θ1 , θ2 such that Y is
best approximated by a
linear combination of ϕ1 , ϕ2
Parameter Estimation

14

Recursive Computation 1/3

Online estimation: observations are obtained sequentially
⇒ for eﬃcient computation: use estimate for time t − 1 to obtain
estimate at time t (recursive scheme)
ˆ
θ(t − 1)

is the parameter estimate computed at t − 1

ΦT Φ

Assumptions:

is invertible for all t

Parameter Estimation

15

Recursive Computation 2/3
Splitting in current and previous estimate:
t

t−1

ˆ
θ(t) = P (t)

ϕ(i)y(i)

= P (t)

i=1

i=1

t

P

−1

t−1
T

(t) =

ϕ(i)ϕT (i) +ϕ(t)ϕT (t)

ϕ(i)ϕ (i) =
i=1

ϕ(i)y(i) + ϕ(t)y(t)

i=1
P −1 (t−1)

In combination:
t−1

ˆ
ˆ
ˆ
ϕ(i)y(i) = P −1 (t−1)θ(t−1) = P −1 (t)θ(t−1)−ϕ(t)ϕT (t)θ(t−1)
i=1
Parameter Estimation

16

Recursive Computation 3/3
Estimate at time t:
ˆ
ˆ
ˆ
θ(t) = θ(t − 1) − P (t)ϕ(t)ϕT (t)θ(t − 1) + P (t)ϕ(t)y(t)
Simpliﬁed to:

ˆ
ˆ
θ(t) = θ(t − 1) + K(t)ε(t)

with

K(t) = P (t)ϕ(t)
ˆ
ε(t) = y(t) − ϕT (t)θ(t − 1)

Determination of P (t) instead of P −1 (t): Matrix inversion lemma
K(t) = P (t)ϕ(t) = P (t − 1)ϕ(t)(I + ϕT (t)P (t − 1)ϕ(t))−1
Parameter Estimation

17

Recursive Least-Squares Estimation
Summary:
_v('140503203547')
last estimate corrected by the colored term, which is proportional to the error of a prediction computed using the last estimate
K(t) introduces a weighting for the combination of the last estimate and the correction
-18-

Exponential Forgetting 1/2
Pragmatic approaches for handling time-varying parameters:
Abrupt changes in the parameters: resetting
Slowly varying parameters: forgetting factor λ
Reformulate least-squares criterion:
V (θ, t) =
1
2

t

λt−i (y(i) − ϕT (i)θ)2 ,

0<λ≤1

i=1

K(t) = P (t)ϕ(t) = P (t − 1)ϕ(t)(λI + ϕT (t)P (t − 1)ϕ(t))−1
P (t) = (I − K(t)ϕT (t))P (t − 1)/λ

⇒ Lower weighting of older data
Parameter Estimation

19

Exponential Forgetting 2/2
Example: y(t) + ay(t − 1) = bu(t − 1) + e(t) + ce(t − 1)
ˆ
with a = −0.8, b = 0.5, c = 0, P (0) = 100I, θ and e(t) is zero mean white noise (σ = 0.5)

Parameter Estimation

20

Identiﬁcation of Finite-Impulse Response
Models 1/2
LTI dynamic systems: uniquely determined by the impulse response
Model (truncated after n samples):
y(t) = b1 u(t − 1) + b2 u(t − 2) + . . . + bn u(t − n) = ϕT (t − 1)θ

with

θT = (b1

...

bn )

ϕT (t − 1) = (u(t − 1)

...

u(t − n))

⇒ Least-squares formulation

Parameter Estimation

21

Identiﬁcation of Finite-Impulse Response
Models 2/2
Adaptive ﬁlter to predict y:
y = ˆ1 (t − 1)u(t − 1) + . . . + ˆn (t − 1)u(t − n)
ˆ b
b

Parameter Estimation

22

Identiﬁcation of Transfer Function Models 1/2
Model:

A(q)y(t) = B(q)u(t)

with

q forward shift operator

(other notation: z)

A(q) = q n + a1 q n−1 + . . . + an
B(q) = b1 q m−1 + b2 q m−2 + . . . + bm
as diﬀerence equation:
y(t)+a1 y(t−1)+. . .+an y(t−n) = b1 u(t+m−n−1)+. . .+bm u(t−n)
Exp. inputs:

{u(1), u(2), . . . , u(t)}

Exp. observations:

{y(1), y(2), . . . , y(t)}

Parameter Estimation

23

Identiﬁcation of Transfer Function Models 2/2
y(t)+a1 y(t−1)+. . .+an y(t−n) = b1 u(t+m−n−1)+. . .+bm u(t−n)
Parameter vector:


Regression vector:


a1
 . 
 . 
 . 
a 
θ =  n
 b1 
 
 . 
 . 
.
bm



−y(t − 1)
.
.
.











−y(t − n)

ϕ(t − 1) = 
u(t + m − n − 1)




.
.


.
u(t − n)
T (t − 1)θ
Regression model: y(t) = ϕ
Delayed outputs in ϕ: autoregressive model
ΦT = ϕT (n)

. . . ϕT (t − 1)

Parameter Estimation

24

Experimental Conditions
Example: y(t) + ay(t − 1) = bu(t − 1) + e(t) + ce(t − 1)
ˆ
with a = −0.8, b = 0.5, c = 0, P (0) = 100I, θ and e(t) is zero mean white noise (σ = 0.5)

ˆ
θT = (ˆ ˆ and ϕ(t − 1) = (−y(t − 1) u(t − 1))
a b)
Top: unit pulse at t = 50
a converges correctly
ˆ
through excitation by
noise
ˆ insuﬃciently excited
b
Bottom: square wave
(period 100)
a and ˆ converge
ˆ
b
Parameter Estimation

25

Persistant Excitation

u is persistently exciting (PE) of order n if for all t there exist
m, 1 > 0, 2 > 0 such that:
t+m
1I

ϕ(k)ϕT (k) >

>

2I

k=t

for ϕ(k) = (u(k − 1) u(k − 2)

...

u(k − n))

Parameter Estimation

26

Persistant Excitation
Examples:
pulse u(0) = 1, u(t) = 0 for t = 0: not PE for any n
step u(t) = 1 for t > 0, else u(t) = 0: PE of order 1
periodic signal of period n: PE of order n
random signal u(t) = H(z)e(t) with pulse transfer function
H(z) and white noise e(t): PE of any order
Applications:
FIR model of order n can be identiﬁed with input signal which
is PE of order n. Notice that there are n parameters to be
identiﬁed.
Transfer function model can be identiﬁed if input signal is PE
of order deg(A) + deg(B) + 1
Parameter Estimation

27

Identiﬁcation in Closed Loops
Adaptive Control: system identiﬁcation online in closed loops

Figure: Self-tuning regulator loop

Parameter Estimation

28

Identiﬁcation in Closed Loops
Dynamic model:
y(t)+a1 y(t−1)+. . .+an y(t−n) = b1 u(t+m−n−1)+. . .+bm u(t−n)
i.e.


−y(t − 1)
.
.
.

T 









−y(t − n)
T


y(t) = ϕ (t − 1)θ = 

u(t + m − n − 1)


.
.


.


a1
 . 
 . 
 . 
 an 
 
 b1 
 
 . 
 . 
.

u(t − n)

bm

and

 T

−y(n)
ϕ (1)
−y(n + 1)





.

Φ=
.
=
.


.
.

.
ϕT (t)
−y(t − 1)

...
...

...

−y(1)
−y(2)
.
.
.
−y(t − n)

u(m)
u(m + 1)
.
.
.
u(t + m − n − 1)

...
...

...


u(1)
u(2) 


.

.

.
u(t − n)

Parameter Estimation

29

Identiﬁcation in Closed Loops
to be considered for linear feedback laws of low order:
linearly dependent columns of Φ
no unique determination of the model parameters
⇒ loss of identiﬁability



 T
−y(n)
ϕ (1)
−y(n + 1)





.

Φ=
.
=
.


.
.

.
ϕT (t)
−y(t − 1)

...
...

...

−y(1)
−y(2)
.
.
.
−y(t − n)

u(m)
u(m + 1)
.
.
.
u(t + m − n − 1)

...
...

...


u(1)
u(2) 


.

.

.
u(t − n)

Parameter Estimation

30

Identiﬁcation in Closed Loops
Example:

y(t + 1) + ay(t) = bu(t)

u(t) = −ky(t)
add αu(t) = −αky(t) to diﬀerence equation:
y(t + 1) + (a + αk) y(t) = (b − α) u(t)
a
ˆ

ˆ
b

any choice with ˆ = b + (a − a) k leads to the same value of
b
ˆ 1
the loss function
no problem for u(t) = −k1 y(t) − k2 y(t − 1) [higher order feedback]
Parameter Estimation

31

Identiﬁcation in Closed Loops
Example: y(t) + ay(t − 1) = bu(t − 1) + e(t) + ce(t − 1)
ˆ
with a = −0.8, b = 0.5, c = 0, P (0) = 100I, θ and e(t) is zero mean white noise (σ = 0.5)

ˆ
θT = (ˆ ˆ and ϕ(t − 1) = (−y(t − 1) u(t − 1))
a b)
Top: u(t) = −0.2y(t)
convergence to wrong
values
Bottom:
u(t) = −0.32y(t − 1)
correct values
estimated

Parameter Estimation

32

Identiﬁcation in Closed Loops
Space of estimated parameters for P (0) = 0.01I and diﬀerent
initial conditions:
u(t) = −0.2y(t)
identiﬁable subspace
(dashed line):
a + 0.2ˆ + 0.7 = 0
ˆ
b

u(t) = −0.32y(t − 1)
convergence towards
(-0.8,0.5) for all initial
conditions
Parameter Estimation

33

Further Reading / References

Lennart Ljung. System Identiﬁcation. In: Wiley Encyclopedia of Electrical and Electronics Engineering.
John Wiley & Sons, Inc., 2001. isbn: 9780471346081. doi: 10.1002/047134608X.W1046.

34


