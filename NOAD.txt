We give no guarantee for the correctness of these notes.
Lecture Notes
Nonconvex Optimization for Analyzing
Big Data
M. Kleinsteuber, M. Seibert
Technische Universit¨t M¨nchen
Nonconvex Optimization
In order to illustrate the necessity of understanding nonconvex optimization algorithms, we will start by giving some data analysis examples where nonconvex optimization is employed.
Example 1.1 (Principal component analysis (PCA)). In this model the data matrix X
is given by X = L + N . L is low rank, N is Gaussian noise.
Associated Optimization Problem: Given X, ﬁnd L, N via
subject to
rk(L) ≤ k.
Solution: Find the singular value decomposition
with Σ = diag(σ1 , ..., σn ), σ1 > . . . > σn . Then L is given by
with Σk := diag(σ1 , . . . , σk , 0, . . . , 0). Therefore, we have an explicit solution to the
problem in terms of a SVD.
Example 1.2 (Robust PCA). Data model: X = L + S + N , where L is a low rank
matrix, S is sparse, i.e. only a few of the entries of S are unequal to zero, and N is
Gaussian noise. In order to analyze the data X, depending on the application, it is
either meaningful to recover the underlying low-rank structure L, or the sparse matrix
Associated Optimization Problem:
+ λ rk(L)
Here, the 0 -pseudo-norm is used. It is deﬁned as the number of non-zero elements in S,
i.e. S 0 := #{sij | sij = 0, i, j = 1, . . . , n} and λ > 0 is a parameter that weighs between
the rank and the sparsity constraints.
Example 1.3 (Sparse Coding). Data model: We assume that each sample xi of the
data is a linear combination of at most k predeﬁned vectors, called atoms. This reads
as xi = [d1 , . . . , dN ] · αi + ε with αi 0 ≤ k, and ε is Gaussian noise. Here, the matrix
D := [d1 , . . . , dN ] consisting of atoms as its columns is called a dictionary.
If the dictionary is known, the associated optimization problem for ﬁnding the sparse
code is: Given xi ﬁnd its ”sparse code” α via solving
Alternatively, one can consider the problem:
Observation: Given one data model, there is no ultimate corresponding optimization
problem. Rather, there are a multitude of optimization problems to choose from and it
is the ﬁrst task to pick the right one for the application at hand.
Note 1.4. There are many possibilities to choose a dictionary mentioned in the example
of sparse coding. One example for natural images are wavelets. However, for other
images like e.g. Computer Tomographies, such a dictionary might be suboptimal.
Example 1.5 (Learning the dictionary D). Task: Given a class of signals x1 , . . . , xM ,
ﬁnd a dictionary D that is able to represent all images belonging to this class. The
corresponding optimization problem is given by
The problem with this optimization problem is that it is ill-posed, since without further
constraints, the solutions αi may tend to zero, whereas the norm of D will tend to
inﬁnity. This is called the scaling ambiguity. Therefore, in order to regularize the
problem, we demand that the columns of D all have unit norm, i.e. di = 1 for all
i = 1, . . . , N . This means that the dictionary D belongs to the so called product of
spheres, i.e. D ∈ Sn−1 × . . . × Sn−1 with Sn−1 := {x ∈ Rn | x 2 = 1}.
Example 1.6 (Foley Sammon Discriminant Analysis). Given the between class variance
SB (for completeness sake, it is deﬁned as SB := c ni (¯i − x)(¯i − x) ) and the within
class variance SW (= i=1 j=1 (xij − xi )(xij − xi ) ), where c is the number of classes,
ni is the number of samples in class i, xi is the center of class i, and x is the center of
all data samples.
The goal of the Foley Sammon Discriminant Analysis is to ﬁnd a subspace S such that
the between class variance is minimized with respect to the within class variance. In
order to formulate this as an optimization problem, we ﬁrst need to deﬁne the projection
onto a subset S. Let U = [u1 , . . . , uk ] be an orthonormal basis of S, then the coordinates
of the projected vector are given by s = U x.
Therefore, the we can formulate the optimization problem as
tr(U SB U )
tr(U SW U )
subject to U U = Ik ,
where Ik is the k × k identity matrix.
There are much more examples in the ﬁeld data analysis where nonconvex optimization
problems lead to effective solutions. Many of them deal with high dimensional data, and
quite a few of them can be stated in the general form
min f (x)
subject to x ∈ M,
where M is a set that reﬂects some smooth but not necessarily convex constraints.
Examples 1.5 and 1.6 ﬁt into this general framework.
The scope of this lecture is to provide general tools that allows to tackle such problems,
where we consider that f is a smooth function and M deﬁnes a Riemannian manifold.
Throughout the lecture, we will only consider local optimality criteria.
2 Derivatives in ﬁnite dimensional Hilbert spaces

[ _t('140508105357') ]

for all x, y, z ∈ H is called a ﬁnite dimensional Hilbert space.
Example 2.2.
[ _to('140519075502') ]

Inner products allow to deﬁne lengths and angles between vectors in H. For optimization purposes, we often face the problems
• What is a good direction to search for an optimal or at least improved solution?
• How far should we go in this direction, or, otherwise spoken, what is a good length of the considered direction.
 M. Kleinsteuber, M. Seibert
It is thus evident that the scalar product plays an important role in answering these
two questions. Now given some Hilbert space H, it might sometimes be beneﬁcial to
choose another representations, say F , such that angles and lengths in H are preserved
in the alternative representation F . More formally, we require the existence of a mapping
φ : H → F that is linear and bijective and that respects inner products, i.e.
= φ(x), φ(y)
(11)
Such a mapping is an called isometric isomorphism, and in that case F is said to be
isometrically isomorph to H, and vice versa.
Example 2.3. If we consider H := Rm×n with its standard scalar product tr(AB ) and
Rmn with x y, then the mapping
A → vec(A),
(12)
is an isometric isomorphism. Here, vec(A) is stacking the columns of A one below the
other.
Example 2.4. Consider now the Hilbert-space Sym(n) of symmetric (n × n)-matrices
with the scalar product tr(AB ) (wich due to symmetry of A and B is northing else
than tr(AB). The dimension of Sym(n) is N := n(n + 1)/2, so it is isomorphic (as a
vector space) to RN . What would be an appropriate isomorphism and an appropriate
scalar product in RN that turns it into an isometrically isomorphic Hilbertspace?
Linear maps between ﬁnite dimensional Hilbert spaces. Let H and F be ﬁnite dimensional Hilbert spaces and let
$\mathcall{L}(H, F ) := \dots$ (13)
denote the set of all linear maps from H to F . Then L(H, F ) is also a ﬁnite dimensional vector space with dimension
dim L(H, F ) = dim H · dim F.  (14)
Since we need to compare different linear maps in the future, it is good to know that this vector space can be endowed with a norm in a natural way, namely the so-called induced norm
(15)
If not explicitly mentioned otherwise, we will always endow L(H, F ) with this induced norm. We are now ready to deﬁne differentiability of a function between two Hilbertspaces.

[ _to('140519075717') ]
\todo{Does this hold true for vector valued functions? i.e. should there be a 0 vector on the right side. If so what is the product rule defined for matrix valued functions as well?}

The following holds true:
1. The following statements are equivalent:
• The function f is differentiable in x0 .
• There exists an Ax0 ∈ L(H, F ) and rx0 : X → F where rx0 is continuous in
x0 and rx0 (x0 ) = 0 such that
f (x) = f (x0 ) + Ax0 (x − x0 ) + rx0 (x) · x − x0 .
(17)
• There exists an Ax0 ∈ L(H, F ) such that
2. If f is differentiable in x0 , then f is continuous in x0 .
3. If f is differentiable in x0 , then Ax0 is unique.
Proof-Sketch (of 3.) Assume that there exists another linear operator B that can be
used in (18). Subtract this newly obtained equation from (18) and divide by x − x0 .
Given an arbitrary point y with y = 1 construct a sequence of points xn = x0 + n with
n ∈ N\{0}. Together with the equation obtained above this yields limn→∞ (Ax0 −B)y = 0
for all y as above. Therefore, we have Ax0 = B, i.e. the operator is unique.
Deﬁnition 2.6. We say that $\operatorname{f}$ is differentiable in X if f is differentiable for all x0 ∈ X .
The mapping
(19)
is called the differential map of f . $\operatorname{D}f(x0)$ is called the derivative of f in x0 .
The function f is smooth if Df is continuous, i.e. smooth is another word for continuously differentiable. We abbreviate this by writing f ∈ C 1 (X ).
Recall: H, F are Hilbert spaces and X ⊂ H is an open subset with x0 ∈ X . Further, let f be a mapping from H to F . Then the derivative of f in x0 is denoted by Df (x0 ).
It is a linear mapping Df (x0 ) : H → F . The differential map is deﬁned by
Df : X → L(H, F )
x0 → Df (x0 ).
2.1 Directional Derivatives
As before, we have two Hilbert spaces H, F with an open subset X ⊂ H and a mapping f : X → F . Furthermore, we have a direction $v \in H$. We then deﬁne the function
(21)
[ $\phi_v$ is a function of $t$ ]
where ε > 0 such that x0 + tv ⊂ X for all t ∈ (−ε, ε). If f is differentiable in x0 , it is
readily shown in Theorem 2.9 that ϕv is differentiable as well. Note, that the other way
round does not hold in general, i.e. differentiability of ϕv does not imply differentiability

[ _to('140519080010') ]
\todo{Is the $\cdot$ correct, as far as I understand $D f(x_0)$ may be in general a linear map such as $\operatorname{tr}$?}

Example 2.8. Given the Hilbert spaces H, F = R2 and the function f : H → F deﬁned
(23)
f (x, y) =
Then the directional derivatives in (0, 0) in the directions v1 =
and Dv2 f (0) =
Dv1 f (0) =
and v2 =

[ _to('140516085502') ]

This means the directional derivative in direction v is the same as applying the differentiable map to v.  
Proof. Due to the differentiability of f we have
(25)
Dividing by t yields
f (x0 + tv) − f (x0 )
o( tv )
= Df (x0 ) · v +
(26)
Therefore, computing the limits yields
= Df (x0 ) · v
since limt→0
(27)
= 0. This is equal to the directional derivative Dv f (x0 ).
As said before, the existence of directional derivatives, even if they exist for all directions, does not imply the differentiability of f at x0 . As an example, consider the
for (x, y) = (0, 0)
(28)
0 for (x, y) = (0, 0)
Here, all directional derivatives in (0, 0) exist, but f s not differentiable in (0, 0). It is
even worse: f is not even continuous in (0, 0).
However, if we know (by some other arguments), that f is differentiable in x0 , we
may use the directional derivatives to compute the derivative Df (x0 ). To that end, we
employ an argument from Linear algebra.
Theorem 2.10. If A : H → F is a linear map, then A is uniquely determined by the
images A(vi ) of a basis {v1 , . . . vn } of H.
It is thus suﬃcient to know what A is doing with an arbitrary basis of the base space
H in order to know what it does with any other element v ∈ H. The proof is very easy
and of course, the linearity of A plays the crucial role here. Let v = i λi vi the linear
combination of v with respect to the basis above. Then
A(v) = A
λi A(vi ).
(29)
In the case where H = Rn , F = Rm and the vi ’s are the standard basis vectors, this
leads us to the well known matrix representation of linear mappings. Namely, the i-th
column of A is simply the image of the i-th standard basis vector.
Example 2.11 (Continuation of the last Example).
Df (0)
Therefore, we get the matrix representation of Df (0) as
Df (0) =
(30)
_t('140516085523') 
Deﬁnition 2.12 (Partial derivatives). 
Let the Hilbert space be given by H = Rn . The directional derivatives with respect to the standard basis vectors ei = [0, . . . , 1, . . . , 0] are called partial derivatives. They are denoted by
f (x) := Dei f (x).  (31)
2.2 Riesz representation theorem
Deﬁnition 2.13. The dual space of H is the set of all (continuous) linear maps from
H to K denoted by L(H, K).
Let αy : H → K, x → x, y , then αy ∈ L(H, K).
Theorem 2.14 (Riesz). Let H be a ﬁnite dimensional Hilbert space. To each linear
mapping f ∈ L(H, K) there exists a unique vector y ∈ H such that f = αy , i.e. f (x) =
x, y for all x ∈ H.
Sketch of proof. Consider the map T : H → L(H, K), y → ·, y . Then we have to show
that T is linear, T is injective, and dim H = dim L(H, K).

[ _to('140519073908') ]
\todo{Is it correct that the gradient is one line of the Jacobi matrix?}

[_to('140516085651')]

Dependence of the Gradient on Different Scalar Products in Rn Let A ∈ Rn×n be
a positive deﬁnite matrix, then x, y A := x Ay = x, Ay is a scalar product in Rn
(all scalar products in Rn can be written in this way). For a smooth function f , the
directional derivative is independent of the scalar product on H, but according to Riesz
we have
Df (x0 ) · v = v, g(x0 )
= v, gA (x0 )
= v, AgA (x0 ) .
(33)
This equation holds true for all v ∈ Rn . Therefore, we have
A−1 g(x0 ) = gA (x0 ).
(34)
g(x0 ) = AgA (x0 )
Thus, different scalar products result in different gradients.
Steepest Descent Given a function f : H → R. In order to determine the direction of
steepest descent in x0 ∈ H we ﬁnd the minimum of all directional derivatives, i.e.
f (x0 + tv) = min Df (x0 ) · v = min
f (x0 ), v .
(35)
Here, the ﬁrst equation holds for all differentiable functions, while the second equation
holds according to Riesz’ representation theorem. To guarantee uniqueness, we restrict
the norm of v to 1. Thus, the minimization problem is now given by min v =1 f (x0 ), v .
By the Cauchy-Schwarz-Inequality we get the solution
f (x0 )
(36)
Therefore, the direction of steepest descent is given by the negative gradient.
Example 2.18. Let A, Y be (real) matrices. We are looking for a real upper triangular
matrix X that solves the equation AX = Y . This problem is approximated by the
f (X) = AX − Y 2 .
(37)
The Hilbert space of real upper triangular matrices is deﬁned as
(38)
with the inner product X1 , X2 = tr(X1 X2 ). In order to determine the gradient we
ﬁrst compute the directional derivative
DV f (X) =
f (X + tV )
tr (A(X + tV ) − Y ) (A(X + tV ) − Y )
= tr V 2(A AX − A Y )
= V, 2(A AX − A Y ) .
(39)
aij →
aij ,
Here, the trace properties tr(A) = tr(A ) and tr(AB) = tr(BA) were used in order
to obtain a representation of the directional derivative from which we can derive the
gradient according to Deﬁnition 2.15. Since 2(A AX − A Y ) is not an element of the
Hilbert space on which the function is deﬁned (i.e. it is not an upper triangular matrix),
we have to project it onto H. This is necessary for the gradient to be uniquely deﬁned.
In the case at hand, the projection of an arbitrary matrix on the set of upper triangular
matrices is achieved by simply setting all entries below the diagonal to zero. We denote
this operation with the operator upp which is deﬁned as
otherwise
(40)
This projection is well deﬁned due to the fact that the entries of 2(A AX − A Y ) that
are below the diagonal do not inﬂuence the scalar product with the upper triangular
matrix V (i.e. V, Z = V, upp(Z) for an arbitrary matrix Z). In conclusion, the
gradient of f is given by
f (X) = 2 upp(A AX − A Y ).
(41)
Note 2.19. Generally, for a Hilbert space H with the open subspace U ⊂ H the scalar
product of the elements v ∈ U and x ∈ H is deﬁned as
v, x = v, ΠU (x) ,
(42)
where ΠU is the orthogonal projection onto U . Another example is the projection of an
arbitrary matrix onto the the Hilbert space of symmetric matrices given by sym(B) :=
2 (B + B ).
2.4 The Chain and the Product Rule
Recall : The chain rule for functions f, g : R → R is given by g(f (x)) = g (f (x))f (x),
while the product rule is given by (g · f ) (x) = g (x) · f (x) + g(x) · f (x).
Let H, E be Hilbert spaces, and let X ⊂ H be an open subset. Furthermore, we have
the smooth function f, g : X → E. Then f + αg : X → E with α ∈ R is also smooth
with the differential map D(f + αg) = Df + αDg.
Proof. Due to the differentiability of f and g we have
f (x) = f (x0 ) + Df (x0 )(x − x0 ) + r(x) x − x0 ,
g(x) = g(x0 ) + Dg(x0 )(x − x0 ) + q(x) x − x0 .
(43)
Here, r(·) and q(·) are continuous functions with r(x0 ) = q(x0 ) = 0. The addition of
these two equations yields
(f +αg)(x) = (f +αg)(x0 )+(Df (x0 ) + αDg(x0 )) (x−x0 )+(r(x) + q(x)) x−x0 . (44)
Since s(x) := r(x) + αq(x) is continuous with s(x0 ) = 0, the desired result follows.
The Chain Rule Given the Hilbert spaces H, E, F and the two open subsets X ⊂ H,
Y ⊂ E. Furthermore, we have the smooth functions f : X → E with f (X ) ⊂ Y and
g : Y → F . Therefore, the composition g ◦ f : X → F is also smooth (Note: (g ◦ f )(x) =
g(f (x))) and its differential map is given by
D(g ◦ f )(x) = Dg(f (x)) ◦ Df (x).
(45)
Proof. We have Dg(f (x0 )) ◦ Df (x0 ) ∈ L(X , F ) (which is the composition of the two
linear maps Dg(f (x)) ∈ L(X , Y) and Df (x) ∈ L(Y, F )). Since both function f and g
are smooth, by the deﬁnition of differentability we have
f (x) = f (x0 ) + Df (x0 )(x − x0 ) + r(x0 ) x − x0 ,
g(y) = g(y0 ) + Dg(y0 )(y − y0 ) + q(x0 ) y − y0 ,
(46)
with the continuous functions r : X → E and q : Y → F which have the property r(x0 ) =
0 and q(y0 ) = 0. Furthermore, we deﬁne p : X → F by p(x0 ) = 0 and
p(x) := Dg(f (x0 ))r(x) + q(f (x)) Df (x0 )
+ r(x) ,
(47)
With this deﬁnition p is continuous in x0 and by setting y = f (x) (46) yield
(g ◦ f )(x) = (g ◦ f )(x) + (Dg(f (x0 )) ◦ Df (x0 ))(x − x0 ) + p(x) x − x0 .
(48)
This concludes the proof.

[ _to('140519080633') ] 
\todo{Is this equivalent to the Lie Derivative?}

The Product Rule The product rule is a direct result of the chain rule. Given the
Hilbert space H and the open subset X ⊂ H and two smooth function f, g : X → R.
Then, (f · g) : X → R is also smooth with the derivative D(f · g) = g · Df + f · Dg.
Proof. We ﬁrst deﬁne the two smooth functions
(a, b) → a · b.
(50)
x → (f (x), g(x)),
Obviously, the equation f · g = m ◦ F holds. The derivatives of m and F are given by
Dm(a, b) = [b a],
DF (x) =
Df (x)
Dg(x)
respectively. Applying the chain rule to m ◦ F yields
D(f · g)(x) = D(m ◦ F )(x) = Dm(F (x)) · Df (x)
= [g(x) f (x)] ·
= g(x) · Df (x) + f (x) · Dg(x).
(51)
Example 2.21 (Directional Derivative of the Matrix Exponential). Consider the mapping
(52)
In order to ﬁnd the directional derivative D(inv(X)) · V, we use the fact that
inv(X) · X = In .
(53)
Taking the derivative on both sides of this equation yields
0 = D(inv(X) · X) · V
= (D inv(X) · V) · X + inv(X) · V.
(54)
Solving this equation provides the wanted directional derivative
(D inv(X) · V) = −X−1 VX−1 .
(55)
Alternative: (Using the Neumann series representation) For a matrix T ∈ Rn×n with
the property that all its eigenvalues have an absolute value smaller than one, the equation
Tk = (In − T)−1
(56)
holds. This is the so-called the Neumann series which is a generalization of the geometric
series ( ∞ q k = 1−q for |q| < 1). Next, note that (X + tV)−1 can be rewritten as
(X + tV)−1 = X (In + tX−1 · V)
= In − (−tX−1 · V)
(57)
Therefore, for all t such that the absolute values of the eigenvalues of −tX−1 V are
smaller than one we can write (X + tV)−1 as the a Neumann series of the form
(X + tV)
(−tX−1 V)k X−1 .
(58)
Deriving this series for t and setting t to zero yields
D inv(X) · V =
(X + tV)−1
(59)
ktk−1 (−X−1 V)k X−1
It should come as no surprise that this is the same result as the one obtained above.
2.5 The Mean Value Theorem
The mean value theorem for real valued function says that given a continuously differentiable function f : R → R and a section [a, b], a, b ∈ R, there exists a ξ ∈ [a, b] such that
f (ξ) = f (b)−f (a) , i.e. there exists a point in the interval [a, b] where the slope of the funcb−a
tion is the same as the average slope in the interval. This can be expressed equivalently
by |b−a|·f (ξ) = |f (b)−f (a)|. In order to transfer this theorem to general Hilbert spaces,
we use a weaker version of this theorem, namely |f (b) − f (a)| ≤ supξ∈[a,b] f (ξ)|b − a|.
Theorem 2.22. Let f : H ⊃ X → E be a smooth function. Then for all X, Y ∈ X such
that (X + t(Y − X)) ⊂ X for all t ∈ [0, 1] the inequality
f (X) − f (Y)
holds. Here, ·
Df (X + t(Y − X))
(60)
is the norm in E, ·
is the norm in H, and ·
is the induced
Note 2.23. Simply put, this means that given a smooth function and to points X, Y,
the distance between the corresponding function values can not get arbitarily large.
3 Local Extrema
Let f : X → R be a smooth function. The point x0 ∈ X is a local extreme point if there
exists an open neighborhood U(x0 ) ⊂ X such that
• f (x) ≤ f (x0 ) for all x ∈ U(x0 ) (local maximum),
• f (x) ≥ f (x0 ) for all x ∈ U(x0 ) (local minimum).
In this case Df (x0 ) · v = 0 for all v ∈ H, or equivalently Df (x0 ) ≡ 0.
Deﬁnition 3.1 (Critical Point). A point x0 that satisﬁes Df (x0 ) ≡ 0 is called a critical
point. The corresponding value f (x0 ) is called critical value.
It is important to note that a point where the derivative vanishes is not necessarily
a local extremum but as for real valued functions can also be a saddle point. In this
lecture we will focus on function minimization. However, it should be noted that every
maximization problem can be equivalently stated as a minimization problem.
Example 3.2. Let the set of all real n×n matrices be denoted by
Sym(n) := {X ∈ Rn×n | X = X}.
(61)
The goal is to determine the critical points of the smooth function
f : Sym(n) → R,
(62)
In order to formally determine the critical points of this function, we ﬁrst calculate
the directional derivative
Df (X) · V =
tr (X + tV)
(63)
For X to be a critical point Df (X) · V has to be equal to zero for all V ∈ Sym(n).
This is obviously fulﬁlled for V = 0. In order to ﬁnd other critical points, recall that
tr(A B) is a scalar product on Rn×n and that the orthogonal decomposition of Rn×n
with respect to this scalar product is given by Sym(n) ⊕ Skew(n). Here,
Skew(n) := {X ∈ Rn×n | X = −X}
(64)
denotes the skew-symmetric n×n matrices. As a result, Df (X) · V is equal to zero if and
only if the matrix
X is skew-symmetric. This means that all critical points
are given by the set
(65)
4 Second derivatives of real valued functions
Let f : H ⊃ X → R be a smooth function. Then we already know that the derivative
Df : H ⊃ X → L(H, R) is a linear mapping from H to R. The vector space F :=
L(H, R) is also a Hilbert space and g := Df ∈ C(X , F) is a differentiable function with
Dg : L(H, F). By reversing the substitution we obtain D(Df ) : H ⊃ X → L(H, L(H, R)).
This means that D(Df )(x) ∈ L(H, L(H, R)) (x ∈ X ). Therefore, this operator can be
applied to an element v ∈ H, i.e. D(Df )(x) · v ∈ L(H, R) (this operation is linear in v).
Finally, this operator can be applied to another element w ∈ H: (D(Df )(x) · v) · w ∈ R
(this operation is linear in w). In the sake of simplicity we introduce the notation
D2 f (x)[v, w] := (D(Df )(x) · v) · w.
(66)
Theorem 4.1. The second derivative D2 f (x) is linear in both components and the
equation
D2 f (x)[v, w] = D2 f (x)[w, v]
(67)
holds, i.e. D2 f (x) is a symmetric bilinear form on H.
Example 4.2. H = Rn and let ei , ej be the i-th and j-th standard basis vector. Recall
that Df (x) · ei = ∂xi f (x)Then
D2 f (x)[ei , ej ] = (D(Df )(x)ei )ej = D
f (x) ej
f (x) =
f (x).
(68)
Recall the directional derivative is deﬁned as Df (x) · v = dt t=0 f (x + tv). In order to
2 f (x)[v, w] we ﬁrst consider the special case where the second derivative is
determine D
applied to the same vector twice, i.e.
D2 f (x)[v, v] = D(Df (x) · v) · v =
f (x + tv) = 2
(69)
f (x + tv).
(Df (x + tv)) · v
Now, consider D2 f (x)[v + w, v + w]. By using the linearity in both arguments we obtain
D2 f (x)[v, w] =
D2 f (x)[v + w, v + w] − D2 f (x)[v, v] − D2 f (x)[w, w] ,
(70)
which in conjunction with (69) provides the result.
Theorem 4.3 (Taylor Theorem). Let f : H ⊃ X → R be a smooth function and let
v ∈ H fulﬁll the condition that the line x + tv ∈ H for all t ∈ [0, 1], then the function
can be expressed as
f (x + v) = f (x) + Df (x) · v + 2 D2 f (x) · [v, v] + o( v 2 ).
(71)
4.1 Suﬃcient conditions for local maxima and minima
Deﬁnition 4.4. A point x0 is called
1. positive deﬁnite if D2 f (x0 )[v, v] > 0 for all v ∈ H,
2. negative deﬁnite if D2 f (x0 )[v, v] < 0 for all v ∈ H,
3. indeﬁnite if there exist v1 , v2 ∈ H such that
D2 f (x0 )[v1 , v1 ] > 0 and D2 f (x0 )[v2 , v2 ] < 0.
Theorem 4.5. Let x0 ∈ X be a critical points. Then,
1. if D2 f (x0 ) is positive deﬁnite, x0 is a local minimum,
2. if D2 f (x0 ) is negative deﬁnite, x0 is a local maximum,
3. if D2 f (x0 ) is indeﬁnite, x0 is not an extreme point.
Example 4.6. f (x) = x4 , D2 f (0) = 0
Proof of (3). If D2 f (x0 ) is indeﬁnite, then there exist v1 , v2 sucht that α := D2 f (x0 )[v1 , v1 ] >
0 and β := D2 f (x0 )[v2 , v2 ] < 0. Consider the Taylor expansion
f (x0 + tv1 ) = f (x0 ) + Df (x0 ) · (tv1 ) + 2 D2 f (x0 )[tv1 , tv1 ] + o(t2 v1 )
= f (x0 ) +
> f (x0 ) +
2 D f (x0 )[v1 , v1 ]
4 α + o(t v1 )
+ o(t2 v1 )
(72)
(note that t2 α + o(t2 ) ⇒ α + o(t2 ) →(t→0)
with a similar argument we get
f (x0 + tv2 ) < f (x0 ) +
α is positive and therefore
+ o(t2 v2 ).
(73)
5 Line search methods in Hilbert spaces
In general, optimization methods often work in the same way. At each iterate a direction
is determined in which to go in the next step. Then a line search method has to be applied
in order to determine at which point of this direction the next iterate lies.
The goal of this section is to ﬁnd an optimization procedure to minimize f : H → R.
The basic idea is to initialize the optimization algorithm with x0 ∈ H and iteratively
deﬁne xk+1 := xk + tk pk where tk ∈ R+ is a step size and pk ∈ H is a descent direction. A descent direction is an element of the Hilbert space that fulﬁlls the condition
pk , f (xk ) = Df (xk )pk < 0. A geometrical interpretation of this property is that a
descent direction points in the opposite direction of the gradient respective to the inner
product. (Recall that the gradient is the direction of steepest ascent.) An important
example for a choice of a descent direction is pk := − f (xk ) leading to the so-called
gradient (or steepest) descent methods.
For now, we will focus on the process of choosing a step size rather than the choice
of descent direction. The optimal step size would be the solution to the minimization
problem
min f (xk + tpk ).
(74)
However, if the cost function f is too complex it becomes computationally infeasible
to determine the solution to this problem. Therefore, line search methods are always a
trade-off between accuracy, i.e. a substantial decrease of the cost function, and speed,
i.e. the computational burden.
A trivial line search approach is to evaluate f (xk + tpk ) for t and choose an acceptable
value. Given a direction pk one possible requirement to the step size would be the
decrease of the function value, i.e. f (xk + tk pk ) < f (xk ). However, this condition alone
is not enough, as it is possible that the step size decreases over time so fast that the
optimization method does not converge to a minimum of the initial problem. The issue
is that the angle between the gradient f (xk ) + t f (xk ), pk and the linear function
f (xk + tpk ) becomes too large.
We can reduce the angle slightly by introducing the factor c ∈ (0, 1) to obtain f (xk ) +
ct f (xk ), pk . Then all t that suﬃce the inequality f (xk +tpk ) < f (xk )+ct f (xk ), pk
are acceptable. This is the so-called Armijo condition which can be used to design the
backtracking algorithm 1 that determines an acceptable step length.
Algorithm 1: Backtracking Algorithm
input : t > 0 large enough, c ∈ (0, 1), and α ∈ (0, 1)
while f (xk + tpk ) > f (xk ) + ct f (xk ), pk do
Set t ← αt
A problem with the Armijo condition is that it allows arbitrarilly small step sizes.
For t small enough the Armijo condition is always fulﬁlled as long as pk is a descent
direction. To avoid this problem, we introduce another condition on the curvature that
avoids this problem, namely Df (xk + tk pk ) ≥ c2 Df (xk )pk (Note that Df (xk )pk < 0 and
Df (xk + tk pk ) < 0). In order to avoid contradictions between the two conditions we
require 0 < c < c2 < 1. In combination with the Armijo condition we get
f (xk + tpk ) < f (xk ) + ct
f (xk ), pk
Df (xk + tk pk ) ≥ c2 Df (xk )pk
(75)
with 0 < c < c2 < 1. This are the so-called Wolfe conditions. Furthermore, the strong
Wolfe conditions are given by
(76)
|Df (xk + tk pk )| ≤ c2 |Df (xk )pk |
(the absolute value of the slope is not too large positive)
Theorem 5.1. Given the smooth cost function f : H → R, the current iterate xk ∈ H
and a descent direction pk ∈ H. We deﬁne
f (xk ) pk
(77)
Consider a line-search algorithm where tk satisﬁes the Wolfe conditions. Assume further
that f is bounded from below on an open supset X ⊂ H (i.e. f (x) > C for all x ∈ X )
and that the sublevel set L := {x : f (x) ≤ f (x0 )} is contained in X . Then
f (xk )
(78)
Note 5.2. The theorem states that the inﬁnite sum over all k of cos2 Θk f (xk ) 2 is
bounded. Therefore, either cos2 Θk or
f (xk ) 2 have to converge to zero. If we have
limk→∞ cos2 Θk = 0, the search direction is orthogonal to the gradient, a situation that
should be avoided as it does not provide a suﬃcient decrease in the cost function (for
the choice pk = − f (xk ) we have cos Θk = 1 for all k). If we demand | cos2 Θk | > δ > 0
for all k, i.e. | cos2 Θk | is bounded from below, the theorem implies that the norm of the
gradient converges to zero, i.e. limk→∞ f (xk ) = 0.
Unfortunatelly, this result does not guarantee convergence of the algorithm, as it does
not enforce the convergence of xk to a single point, i.e. limk→∞ xk = x , since there
could be a whole set of points that fulﬁll f (x) = 0. Nonetheless, if the local minimum
x in the sublevel set L is unique and | cos Θk | > δ > 0, then the algorithm converges
pointwise to that minimum, i.e. limk→∞ xk = x .
6 Conjugate Gradient Methods
The initial idea for the conjugate gradient (CG) method was to ﬁnd a solution to linear
equations of the form Hx = d with the invertible matrix H ∈ Rn×n by minimizing the
error term
(79)
with a descent method. This approach can be generalized to all smooth function based
on the fact that functions can be quadratically approximated. In general, this approximation is accurate at a local minimum.
6.1 Linear CG Method
The problem in Equation (79) (with d = −b) can be expressed by the function
(80)
where H is a symmetric, positive deﬁnite matrix.
Deﬁnition 6.1. The vectors u1 , . . . , un ∈ Rn are called H-conjugate if ui , Huj = 0
for all i = j. In other words, u1 , . . . , un are orthogonal with respect to the inner product
·, · H and form a basis of Rn .
Let h0 , . . . , hn−1 be an H-conjugate bases and let x0 be given. Then any vector x ∈ Rn
can be uniquely represented as
(81)
with appropriate scalars λj ∈ R, j = 0, . . . , n − 1. With this representation we can
rewrite the function f as
f (x) = f (x0 ) +
(82)
As a consequence, if the H-conjugate basis h0 , . . . , hn−1 were given, the original problem
would decompose into n independent one-dimensional problems. Namely, in ﬁnding the
optimal λj . Since each problem is quadratic, the optimal values for λj can easily be
determined. They are given by
(83)
The optimal solution to the initial problem is therefore given by
(84)
It is also possible to obtain to solution x via the iteration xi+1 = xi + λi hi .
Proposition 6.2. The recursion xi+1 = xi + λi hi is equivalent to the recursion
λi = arg min f (xi + λi hi ).
(85)
Proof. Due to the deﬁnition of the recursion we have
(86)
For i = 0, . . . , n, we will denote the gradient as gi := f (xi ) = Hxi + d. Since λj is
computed as the exact solution for the minimization along the direction hj , we have
f (xi + λi hi + thi ) = 0
(87)
Since we can rewrite the gradient as
(88)
and the vectors hj are H-conjugate, Equation (87) yields
Consequently, both recursion generate the same sequence of iterates.
(89)
Algorithm 2: Linear Conjugate Gradient
input : H ∈ Rn×n symmetric, positive deﬁnite; d ∈ Rn ; initial point x0 ∈ Rn
1 Set h0 = −g0 = −(Hx0 + d), i = 0;
2 Compute the step size
λi = arg min f (xi + λi hi ) = −
(90)
(91)
with the CG update parameter
Set i ← i + 1 and go to 2.
(92)
Up until now, we assumed that the H-conjugate basis was given. However, in general,
this is not the case. Therefore, the CG algorithm has to construct this basis “on the
run”. This is done as follows
Theorem 6.3. The optimization problem
(93)
is solved in at most n steps.
Alternative formulas for the CG update parameter. Recall that the gradient at the
(i+1)-st iterate is deﬁned as gi+1 = gi + λi Hhi . Therefore, the equation
(94)
holds, since gi+1 , hi = 0. This is the so called Hestenes-Stiefel (HS) update. Furthermore, we can use the deﬁnition of hi = −gi + γi−1 hi−1 and the fact that gi , hi−1 = 0
to obtain
(95)
which is the so called Polak-Ribi`re (PR) update.
Lemma 6.4. In the case of a quadratic cost function the gradients at two subsequent
iterates are orthogonal to each other, i.e.
(96)
Proof. As we already discussed, the equation
(97)
holds. Hence, we have
(98)
Furthermore, the equations
(99)
(100)
hold. As a consequence, we obtain
by inserting (98)-(100) which concludes the proof.
(101)
Due to Lemma 6.4, the CG update parameter can be expressed as
(102)
This is the Fletcher-Reeves (FR) update. In the case of quadratic cost functions, all
three mentioned formulations are equivalent.
6.2 Nonlinear Conjugate Gradient Method
At the beginning of this section, we mentioned that the CG method, although originally
developed for quadratic cost functions, can be easily extended to general cost functions.
The respective algorithm is given in the following.
Algorithm 3: Nonlinear Conjugate Gradient
input : Smooth cost function f : X → R; initial point x0 ∈ X
1 Set h0 = −g0 = − f (x0 ), i = 0;
2 Compute the step size αi according to a line search procedure;
3 Set
f (xi ),
(103)
where γi is one of the following update parameters
(104)
(105)
(106)
Note 6.5. While in the quadratic case the different update formulas are equivalent, this
is no longer the case for nonlinear functions. Here, the preferred formula is a matter of
heuristics. A popular choice is
γi = max 0, γi R .
(107)
This update formula incorporates an automatic reset with a steepest descent direction.
This is a tribute to the fact that, in contrast to the linear case, the iteration does not
stop after n steps.
7 Optimization on the sphere
The n-sphere is deﬁned as
(108)
In this section we will consider the problem of minimizing a function that is restricted
to the sphere, i.e.
minimize f : S n → R
(109)
with the smooth function f .
7.1 Line search on the sphere
In Hilbert spaces, descent methods work in the following way. A search direction dk
is determined according to a certain method, e.g. the negative gradient in case of the
steepest descent method. Then the function is evaluated along this direction and a the
step size tk that fulﬁlles certain conditions is determined via a line search procedure.
This provides the next iterate
(110)
However, the concept of straight lines does not directly transfer to the sphere, as the
connecting line between two points does not lie in S n . On manifolds the concept of
straight lines is expressed via geodesics. In the case of the sphere they are called great
circles.
Deﬁnition 7.1. For h = 0, x h = 0 the curve γ(x, h, ·) : R → S n
γ(x, h, t) = x cos(t h 2 ) + h
sin(t h 2 )
(111)
is a great circle on the sphere with γ(x, h, 0) = x. It fulﬁlls the conditions γ 2 = 1 and
γ(0) = h. In general, geodesics are the connections between points with the shortest
length.
Note 7.2. The property γ(0) = h justiﬁes the wording: The great circle γ(x, h, t) origi˙
nates from x in direction h.
7.2 Search directions on the sphere
In the deﬁnition of great circles we used the vector h with x h = 0 to determine in which
direction the geodesic moves starting from x. A vector h that fulﬁlls this condition as
above is tangent to the sphere. In the following, we give a more general deﬁnition.
Deﬁnition 7.3. The tangent space is deﬁned as
(112)
Tx S n := {γ(0) : γ is a curve on S n through x}.
(compare to velocities). The tangent space exhibits a vector space structure.
Lemma 7.4. The tangent space to the n-sphere at point x is given by
Proof. ⊂: Remember that for any curve γ on S n we have γ(t)
both sides yields
γ(t)
= 2γ(t) γ(t)
(113)
= 1 for all t. Deriving
= 2x γ(0).
(114)
Therefore, γ(0) is orthogonal to x for all curves γ with γ(0) = x.
⊃: Consider the great circle γ(x, h, t) as deﬁned above. Then on the one hand we
have
{γ(0) : γ is a great circle} = {h ∈ Rn+1 : x h = 0},
(115)
while on the other hand, we have
(116)
{γ(0) : γ is an arbitrary curve} ⊃ {γ(0) : γ is a great circle}.
7.3 The Riemannian Gradient on the Sphere
Given the smooth function f : S n → R. Let γ(t) be a curve in S n through x ∈ S n with
γ(0) = x. We already know that the derivative of γ evaluated at zero is an element of
the tangent space in x, i.e. γ(0) ∈ Tx S n . Examining the derivative of the function
f ◦ γ(t) : (−ε, ε) → R
(117)
which is given as
we observe that
curve γ(t).
dt t=0 f (γ(t))
f (γ(t)) = Df (γ(0)) · γ(0),
(118)
is only dependent on γ(0) and γ(0)) and not on the whole
Deﬁnition 7.5. Let γ be a curve in S n through x ∈ S n with γ(0) = x and h := γ(0) ∈
Tx S n . Then Df (x) · h = dt t=0 f (γ(t)) is the directional derivative of f in direction h.
Df (x) : Tx S n → R
(119)
is a linear map. To see this we examine the extension of f to Rn . Given the function
f : Rn → R with f |S n = f . Then by deﬁnition of the derivative Df (x) is linear in Rn
n ⊂ Rn . In the case that it is not possible to extend
and thus so is its restriction to Tx S
f to the surrounding space, it requires more work to show this property and we will not
cover these cases here.
Similar to Hilbert spaces, we require a scalar product in order to deﬁne the gradient.
It is important to note that while the manifold itself does not exhibit a vector space
structure, the tangent space does. Therefore, we deﬁne the inner product on the tangent
space, i.e.
(120)
Commonly, a restriction of the standard inner product to the tangent space is chosen,
i.e. h1 , h2 x = h1 h2 for all x ∈ S n , h1 , h2 ∈ Tx S n .
Since Df (x) is a linear map, we can employ the Riesz representation theorem. Thus,
there exists a unique g ∈ Tx S n such that
= Df (x) · h.
(121)
This element g is called the Riemannian gradient of f at x.
Theorem 7.6. The Riemannian gradient of f : S n → R in x ∈ S n is equivalent to the
projection of the Euclidean gradient of the extension f : Rn → R, f |S n = f onto Tx S n .
n → T S n , z → (I − xx )z.
The projection onto the tangent space at x is given by Πx : R
7.4 Outline for optimization methods on the sphere
Optimization methods on the sphere work similar to optimization methods in Hilbert
space. The algorithm is initialized with a starting point x0 ∈ Tx0 S n and the iteration
counter is set to k = 0. Then, an appropriate search direction pk ∈ Txk S n is chosen
according to some rule, e.g. the negative gradient in the case of steepest descent. Next,
a step size tk is selected and according to this the iterate is updated along a geodesic
via xk+1 = γ(xk , pk , tk ). This process is repeated until convergence.
The step length is determined analogously to the way it is done in Hilbert spaces.
Here, α fulﬁlls the Armijo condition if it satisﬁes
f (γ(xk , pk , α)) ≤ f (xk ) + c1 α
f (γ(xk , pk , t)).
(122)
It should be noted that due to the chain rule, we have dt t=0 f (γ(xk , pk , t)) = Df (x) · pk .
A simply technique is using a backtracking strategy to obtain a step length that fulﬁlls
the Armijo condition.
7.5 Parallel Transport
The previous section gives a general outline of optimization methods on the sphere. If
we go more into detail and recall the conjugate gradient method, we see that to ﬁnd the
next search direction this method requires the computation of the difference between two
gradients and the inner products of vectors deﬁned for different iterates. While in Hilbert
spaces this does not pose any problems, it is not possible perform these operations on
manifolds, as for example the inner product of two elements in different tangent spaces is
not deﬁned. Therefore, we require a method to identify elements in Tx S n with elements
in Ty S n (x, y ∈ S n ).
The naive idea is to choose a great circle that connects the points x and y and then
“move” the tangent space without “rotation” along this path, i.e. maintaining the angle
of the tangent space vectors to the speed vector of the great circle.
Deﬁnition 7.7. Given the point x ∈ S n , the tangent vectors h, η ∈ Tx S n and the great
circle γ(x, h, t). The parallel transport of η from Tx S n to Tγ(x,h,t) S n along γ is given by
(x h
2 sin(t
h 2 ) + h(1 − cos(t h 2 ))) .
(123)
τ (η; γ(x, h, t)) = η −
8 Analysis Operator Learning
The operator can be learned by minimizing a cost function
Ω = arg min g(Ωs) + constraints
(124)
One of these constraints needs to prevent learning identical or trivially linear dependent operator atoms ω i , i.e.
(125)
Since we also require all of the operator atoms to have unit Euclidean length |ω i | = 1,
for the scalar product of two operator atoms follows
if the two atoms are trivially linear dependent.
(126)
In order to force all operator items not to encode the same information, we therefore
use a logarithmic barrier function that penalizes the scalar product of any two different
atoms to approach ±1.
− log(1 − (ω i ω j )2 )
f (Ω) =
(127)
Here, ω i denotes the transposed of the i-th row of the operator, i.e. ω i := Ωi,: and
Since we want to solve this optimization problem using a gradient method (more
speciﬁcally the geometric conjugate gradient method on the sphere as introduced in the
previous lecture), we ﬁrst need to ﬁnd the Euclidean gradient of this function term.
To employ the gradient computation methods introduced in this lecture, we need to
reformulate the constraint function above to be a true function of Ω rather than its
individual atoms ω i . This can be achieved by using the unit vectors ei ∈ Rk which have
all zero elements and a one at index i, i.e.
log(1 − (ei ΩΩ ej )2 )
f (Ω) = −
(128)
Note that we sum over all indices i and j and thus also include the elements on the
diagonal of ΩΩ which account for the scalar products ω i ω i which are always equal
to one. To exclude those obviously unwanted elements, we end up with the constraint
log(1 − (ei (Ik − ΩΩ ) ej )2 )
(129)
where Ik is the k-dimensional identity matrix.
One way of getting the gradient is to compute the directional derivative for the entire
function at once using dt t=0 f (Ω + tH). Alternatively, we can ﬁrst decompose our
constraint function into a number of concatenated sub-functions by substitution. We
can then compute the directional derivatives for each of them individually and reassemble
our sub-results in the end to get to the gradient. This method can be useful for tackling
complicated terms.
Since
log(1 − (ei (Ik − (Ω + tH)(Ω + tH) ) ej )2 )
− log(1 − (ei (Ik − (Ω + tH)(Ω + tH) ) ej )2 ),
we are going to omit the sum over all indices i, j within the next steps and insert it again
when we reassemble the gradient in the end.
We start by decomposing our constraint function
f (Ω) = a(b(c(Ω)))
c(Ω) = ei (Ik − ΩΩ ) ej
b(β) = β 2 ,
(130)
β = ei (Ik − ΩΩ ) ej
a(α) = − log(1 − α),
Now we compute the directional derivatives individually for each sub-function denoting
the direction with latin capital letters
D a(α)A =
− log(1 − (α + tA))
a(α), A ∈ R
b(β), B ∈ R
c(Ω), C ∈ Rk×n
D b(β)B =
dt t=0 (β
D c(Ω)C =
dt t=0 ei
(Ik − (Ω + tC)(Ω + tC) ) ej
dt t=0 tr (ei
(Ik − (Ω + tC)(Ω + tC) ) ej )
= − tr (C (ej ei + ei ej ) Ω)
Applying the chain rule on our sub-functions and re-inserting the sum operator, we
put together the directional derivative for our constraint function in the direction of C
Df (Ω) C = k
i,j=1 D a(b(c(Ω))) D b(c(Ω)) D c(Ω) C
i,j=1 D a(α) D b(β) D c(Ω) C
2β tr (C (ej ei + ei ej ) Ω)
i,j=1 tr (C (ej ei + ei ej ) Ω)
Since both α and β are independent of the direction C, we get the gradient as
i,j=1 (ej ei
+ ei ej ) Ω
2ei (Ik − ΩΩ )ej
1 − (ei (Ik − ΩΩ )ej )2
8.1 Gradient Checking
fH (X) = lim
When debugging the implementation of a gradient method, it is important to be sure
that the computed gradient is correct. Therefore, it is useful to numerically check if the
gradient of the constraint or objective function was computed correctly. This can be
done using the deﬁnition of the directional derivative
f (X + tH) − f (X)
(131)
So in order to check the correctness of the gradient for a function f (X), one can check
f (X + t ei Hej ) − f (X − t ei Hej )
(132)
The parameter t ∈ R should be chosen very small but several orders of magnitude larger
than the machine precision for the chosen data type/platform. For instance something
between 10−4 and 10−10 may be appropriate. The direction matrix H should be set to
a matrix with a one for all elements. If this gradient check passes for all coordinates i, j
and for different X, the gradient is most likely correct.
fH (X)(ij) ≈
9 An outlook to optimization on manifolds
In this chapter we will brieﬂy consider optimization on manifolds. The problem is given
Minimize the function F : M → R
(133)
where the set M is a manifold.
Deﬁnition 9.1 (Manifold). A topological space X is called locally Euclidean if there
is a non-negative number n such that every point in X has a neighborhood which is
homeomorphic to Rn . A manifold is a locally Euclidean second countable Hausdorff
space.
This requires the deﬁnitions of topological space, homeomorphic mapping, second
countable, and Hausdorff space. (Which will not be given in this lecture.)
Theorem 9.2 (Regular value theorem, part I). Let H, F be Hilbert spaces and let X ⊂ H
be an open subset. Furthermore, let f : X → F be a smooth mapping. Then any point
c ∈ F for which Df (x) has full rank for all x ∈ f −1 (c) is called a regular value. The
set f −1 (c) ⊂ X is a (dim(H) − dim(F ))-dimensional manifold. To be precise, it is a
sub-manifold of H.
Example 9.3. Given the smooth function
f : Rn×n → Sym(n)
(134)
The group of orthogonal matrices is deﬁned as O(n) := {X X = In } = f −1 (In ). In
order to show that O(n) is a manifold, it remains to show that In is a regular value of
f . The directional derivative is given as
Df (X) · V = X V + V X.
(135)
If X ∈ O(n), this linear map is surjective (i.e. has full rank). In order to see this, let S be
arbitrary in Sym(n). Then Df (X) maps the matrix V = 1 X S to S. Therefore, Df (X)
is a surjective linear map. In conclusion, O(n) is a manifold of dimension n2 − n(n+1) =
n(n−1)
A differentiable manifold M is locally homeomorphic to Rn , i.e. there exists a neighborhood U for all points x ∈ M and a map called chart ϕ : U → Rn such that ϕ(U) ⊂ Rn ,
ϕ(x) = 0. Let α : (−ε, ε) → M , α(0) = x be a curve on M such that the composition
ϕ ◦ α : (−ε, ε) → Rn is smooth. We call (ϕ ◦ α) (0) the speed vector of α in x with respect
to (U, ϕ). If (V, ψ) is another chart around x, then by the chain rule we have
(ψ ◦ α) (0) = (ψ ◦ ϕ−1 ◦ ϕ ◦ α) (0) = D(ψ ◦ ϕ−1 )
ϕ(α(0))
· (ϕ ◦ α) (0)
(136)
In the following, we give a deﬁnition for tangent vectors. It should be noted that this
is just one of several possible ways to deﬁne a tangent vector.
Deﬁnition 9.4. Two curves α, β through x are equivalent if they have the same speed
vector with respect to one chart (and thus to all charts). This equivalence relation is
denoted by
α ∼x β ⇔ (ϕ ◦ α) (0) = (ϕ ◦ β) (0).
(137)
Deﬁnition 9.5. A tangent vector of M at x is deﬁned as an equivalence class of curves,
i.e.
(138)
The tangent space at x is the set of all tangent vectors and is denoted by Tx M .
The tangent space Tx M exhibits a vector space structure. To see this, consider the
τx ([α]x ) := (ϕ ◦ α) (0).
(139)
This is a bijective mapping.
• Injectivity: trivial;
• Surjectivity: Let v ∈ Rn be arbitrary. Deﬁne the curve α(t) := ϕ−1 (ϕ(x) + tv).
The derivative of the composition of the chart with this curve yields (ϕ◦α) (0) = v.
We obtain the vector space structure via
ξ + η := (τx )−1 (τx (ξ) + τx (η)),
λ · ξ := (τx )−1 (λτx (ξ)),
(140)
with ξ, η ∈ Tx M and λ ∈ R. It remains to show that this is independent of the chosen
chart.
Theorem 9.6 (Regular value theorem, part II). The tangent space at x ∈ f −1 (c) is
given by
Tx M := ker(Df (x)).
(141)
Example 9.7. Coming back to our example of orthogonal matrices. First, note that
the dimension of the tangent space is the same as the dimension of the manifold (as
Df (X) has full rank and n2 = n(n − 1)/2 + n(n + 1)/2), i.e. dim ker(Df (X)) = n(n−1) .
To determine the tangent space we have to ﬁnd the kernel of Df (X), i.e. V with
(142)
This equation holds for V = XH, H = −H, so {XH : H = −H} ⊂ ker(Df (X)).
Since we have dim{XH : H = −H} = n(n−1) = dim(ker Df (X)), the equality holds.
9.1 Derivatives on manifolds
Given the smooth function F : M → R. The tangential map in x ∈ M is deﬁned as
Tx F : Tx M → TF (x) R = R
(143)
[α]x → Tx F ([α]x ) := [F ◦ α]F (x) .
Once again, α is a curve through x ∈ M and a tangent vector is deﬁned as the equivalence
class [α]x . In our case, we have
DF (x) : Tx M → R,
DF (x) · ξ =
(144)
(F ◦ α)(t),
where [α]x = ξ. This means that derivatives operate on the tangent space and not on the
manifold. This is intuitively clear as the derivative is a linear mapping which requires
an underlying vector space structure that is not provided by the manifold.
Deﬁnition 9.8. Analogously, to the deﬁnition on Hilbert spaces, any point x ∈ M such
that DF (x) ≡ 0 is called a critical point.
A manifold with an inner product ·, ·
is called a Riemannian manifold.
at Tx M such that ·, ·
varies smoothly in x
Deﬁnition 9.9. The Riemannian gradient is the unique vector G(x) ∈ Tx M such that
DF (x) · η = G(x), η
for all η ∈ Tx M .
(145)
10 A Geometric Revisit to the Trace Quotient Problem
The optimization problem
tr(X AX)
tr(X BX)
(146)
> k is the so-called trace quotient problem. One of its applications
is in the Foley-Sammon Discriminant Analysis which was mentioned in the introduction
of the lecture. We want to take a closer look at this problem. While the set St(k, n) :=
{X ∈ Rn×k : X X = Ik } itself is a manifold (the so-called Stiefel manifold), we will
investigate the optimization problem equivalently on another manifold. The Grassmann
manifold (or Grassmannian). It is deﬁned as the set of orthogonal rank k projectors, i.e.
Gr(k, n) := {P ∈ Rn×n : P2 = P, P = P, rk(P) = k}.
(147)
It can be shown that any P ∈ Gr(k, n) can be written as P = UU with U ∈ St(k, n)
and since the trace function is invariant to the rotation in its argument, we can rewrite
Equation (146) as
tr(PA)
(148)
P∈Gr(k,n) tr(PB)
10.1 Geometry of the Grassmannian Manifold
In equivalence to the deﬁnition above, we can say that the Grassmannian is the set of
all k-dimensional subspaces in Rn . The tangent space in a point P is given as
TP Gr(k, n) := {[P, Ω] : Ω ∈ Skew(n)}
(149)
with the matrix commutator [A, B] := AB − BA. The canonical inner product for two
tangent vectors ξ1 , ξ2 ∈ TP Gr(k, n) is given by
ξ1 , ξ2 = tr(ξ1 ξ2 ).
(150)
The projection onto the tangent space is given by the mapping
ΠP (Z) = [P, [P, Z]].
(151)
The geodesic originating from a point P ∈ Gr(k, n) in direction ξ ∈ TP Gr(k, n) is deﬁned as the mapping
γP,ξ : R → Gr(k, n)
t → et[ξ,P] Pe−t[ξ,P] .
(152)
