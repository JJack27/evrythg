Appendix
A. Probability Theory
Probability Space
A Probability Space,
(Ω, F, P),
represents a mathematical model consisting of an
 Observation Space Ω:
the nonempty set of potential outputs/observations of an experiment,
 Sigma Algebra F:
a set of subsets of potential outputs/observations of an experiment,
 Probability Measure P:
a function which maps each element of A ∈ F to the interval [0, 1].
(A.1)
Deﬁnition. A Sigma Algebra F is a set of subsets (events) of Ω with
(A.2)
(A.3)
(A.4)
Deﬁnition. A Probability Measure P maps A ∈ F to P (A), the Probability of the Event A,
Nonnegativity :
Normation :
P (A) ≥ 0,
P (Ω) = 1,
(A.5)
(A.6)
P (Ai ) ,
wenn Ai ∩ Aj = ∅, ∀i = j.
(A.7)
Consequently, the following general properties of probability measures are obtained:
P (Ac ) = 1 − P (A) ,
P (∅) = 0,
P (A \ B) = P (A ∩ B c ) = P (A) − P (A ∩ B) ,
P (A ∪ B) = P (A) + P (B) − P (A ∩ B) ,
A ⊂ B ⇒ P (A) ≤ P (B) ,
(A.8)
(A.9)
(A.10)
(A.11)
(A.12)
P (Ai ) .
(A.13)
Conditional Probability and Stochastical Independence
Conditional Probability
The probability of an event changes when we are informed about the realization of another event within
the same probability space.
Deﬁnition. Given P (A) > 0 for A ∈ F, the Conditional Probability of B conditioned by A is
deﬁned as
P (B | A) = PA (B) =
P (A ∩ B)
P (A)
(A.14)
Bayes Theorem
Given a partition of Ω in disjoint sets Bi , i ∈ I, such that
(A.15)
(A.16)
then for any A ∈ F with P (A) > 0:
Theorem of total probability :
P (A | Bi ) P (Bi ) ,
(A.17)
P (A | Bj ) P (Bj )
i∈I P (A | Bi ) P (Bi )
(A.18)
P (A) =
Bayes Theorem :
P (Bj | A) =
Special Case. Given A, B ⊂ Ω and Ω = B ∪ B c , the Bayes Theorem is given by
P (B | A) =
P (A | B) P (B)
P (A | B) P (B)
P (A)
P (A | B) P (B) + P (A | B c ) (1 − P (B))
(A.19)
Illustration
Fig. A.1: Partition of Ω in B1 , . . . , B5
P (A) =
P (B2 | A) =
P (A | Bi ) P (Bi ) + 0 · P (B5 ) ,
P (A | B2 ) P (B2 )
P (A | Bi ) P (Bi )
(A.20)
(A.21)
Stochastic Independence
A und B are stochastically independent if the probability measure for B is not changed by any knowledge
about the realization of A, and vice verca, i. e. given P (A) > 0,
P (B | A) = P (B) .
(A.22)
Otherwise, with
P (B | A) =
P (A ∩ B)
P (A)
(A.23)
we obtain the following
Deﬁnition. Given (Ω, F, P) is a probability space, then A, B ∈ F are Stochastically Independent
with respect to P, if
P (A ∩ B) = P (A) P (B) .
(A.24)
Random Variables
Deﬁnition. Given a probability space (Ω, F, P) and a measure space (Ω , F ), we call the mapping X :
Ω → Ω a Random Variable, if and only if for each A ∈ F there exists an element A ∈ F, such that
A = {ω ∈ Ω | X (ω) ∈ A } ∈ F or — in shorter notation — A = {X ∈ A } ∈ F.
X (ω)
Fig. A.2: Real-valued random variable: x : Ω → R
Distribution of Real-Valued Random Variables
Deﬁnition. Given a real-valued random variable X on (Ω, F, P), the function FX : R → [0, 1], with
FX (x) = P ({X ≤ x}) ,
deﬁnes the Cumulative Distribution Function (CDF) of X .
Properties of a CDF FX : R → [0, 1]:
 FX (x) is monotonically increasing.
 limh→0 FX (x + h) = FX (x),
 limx→−∞ FX (x) = 0,
limx→∞ FX (x) = 1.
(A.25)
Deﬁnition. We call a random variable X Continuous if its CDF FX can be determined by means of
FX (x) =
fX (ξ) dξ,
(A.26)
with fX : R → [0, ∞[, which is the Probability Density Function (PDF) of X .
If FX is continous and can be diﬀerentiated almost everywhere, then the random variable X is continuous
fX (x) =
dFX (x)
(A.27)
Examples
a = 0, b = 5
a = −5, b=5
a = −3, b = −1
fX (x; a, b)
Fig. A.3: Uniform PDF: fX (x) =
1 ,a≤x≤b
b−a
Examples (Cont’d)

[ _to('140518093427') ]

Examples (Cont’d)
fX (x; λ)
Fig. A.5: Exponential PDF: fX (x) = λ e−λx ,
Multidimensional Distributions
Deﬁnition. Given a n-dimensional real-valued random variable (random vector) X = [X1 , . . . , Xn ]T on
the probability (Ω, F, P), then the Joint CDF FX1 ,...,Xn : Rn → [0, 1] is denoted as
FX (x) = P ({X ≤ x}) = P ({X1 ≤ x1 , . . . , Xn ≤ xn }) .
(A.28)
Properties of a continuous n-dimensional real-valued random variable:
FX1 ,...,Xn (x1 , . . . , xn ) =
fX1 ,...,Xn (x1 , . . . , xn ) =
fX1 ,...,Xn (ξ1 , . . . , ξn ) dξn . . . dξ1
∂ n FX (x1 , . . . , xn )
(A.29)
(A.30)
Marginal Distribution
Deﬁnition. Given a random variable [X , Y ], the Marginal Distribution of X can be obtained by
FX (x) = lim FX ,Y (x, y) = FX ,Y (x, ∞).
(A.31)
FX ,Y (x, y)
FX ,Y (x, ∞) = FX (x)
(x, y)
Conditional Distribution
Deﬁnition. Given two jointly distributed random variables X and Y , then the Conditional CDF of X
conditioned on {Y = y} is equal to FX |Y : R → [0, 1], with
FX |Y (x|y) = P ({X ≤ x} | {Y = y}) ,
if Y only takes discrete values.
(A.32)
If Y is a continuous random variable, the probability of {Y = y} is zero, and thus the Conditional
CDF must be deﬁned alternatively as
FX |Y (x|y) =
assuming the existence of the PDFs and fY (y) > 0.
fX ,Y (ξ, y)
fY (y)
(A.33)
Important Relations
Marginalization
Theorem of total probability
Normalization
fX ,Y (ξ, y) dξ = 1.
(A.35)
Mean and Covariance
Mean Value of Random Variables
Deﬁnition. The Mean Value (expectation value) of a continuous real-valued random variable X
xfX (x) dx.
(A.36)
Properties of the mean value:
for all α, β ∈ R.
(A.37)
(A.38)
Deﬁnition. The Variance of random variable X is
σX = Var [X ] = E (X − E [X ])2 = E X 2 − E [X ]2 ,
(A.39)
Deﬁnition. The Covariance and Correlation of two random variables X and Y is
cX ,Y = Cov [X , Y ] = E [(X − E [X ]) (Y − E [Y ])] = E [XY ] − E [X ] E [Y ] ,
(A.40)
(A.41)
Properties of the covariance, if X , Y , U and V are random variables and α, β, γ, δ ∈ R:
Cov [αX + β, γ Y + δ] = αγ Cov [X , Y ] ,
Cov [X + U , Y + V ] = Cov [X , Y ] + Cov [X , V ] + Cov [U , Y ] + Cov [U , V ] .
Special Case. Var [αX + β] = α2 Var [X ].
(A.42)
(A.43)
Correlation of Random Variables
Deﬁnition. Random variables X and Y are Uncorrelated if
(A.44)
otherwise we call X and Y Correlated.
Deﬁnition. By normalization we obtain the Correlation Coefficient ρX ,Y ∈ [−1, 1]
(A.45)
Stochastic interrelations of random variables:
Independent : FXY = FX FY
Uncorrelated : µXY = µX µY
Orthogonal : µXY = 0.
(A.46)
(A.47)
(A.48)
Example
The joint CDF of n real-valued Gaussian random variables
X = [X1, . . . , Xn]T is given by
fX1 ,...,Xn (x1 , . . . , xn ) = √ n √
exp − (x − µ)T C −1 (x − µ) .
2π det C
The respective Mean Vector and Covariance Matrix is given by
C = E (X − µ)(X − µ)T
respectively. A common shorthand notation is
X ∼ N (µ, C).
(A.49)
(A.50)
Covariance matrices of uncorrelated random variables X1 , . . . , Xn are diagonal with variances σi = Var [Xi ], i ∈ {1, . . . , n} as main diagonal elements.
Given n uncorrelated Gaussian random variables the joint PDF is
fX1 ,...,Xn (x1 , . . . , xn ) =
exp −
(x −µ )2
(xi − µi )σi (xi − µi )
fXi (xi ).

[ _to('140517120303') ]

Correlation between the random variables X1 , . . . , Xn leads to a non-diagonal covariance matrix C. Since
C is in general Positive Semidefinite (PSD), X ∼ N (µ, C) can be transformed to Y ∼ N (0, I), by
the linear transformation
Y = C − (X − µ) .
(A.52)
(a) Joint PDF fX1 ,X2 (x1 , x2 )
(b) Contour lines of fX1 ,X2 (x1 , x2 )
Fig. A.6: Joint PDF of the Uncorrelated bivariate standard normal distribution.
(a) Joint PDF fX1 ,X2 (x1 , x2 )
(b) Contour lines of fX1 ,X2 (x1 , x2 )
Fig. A.7: Joint PDF of the Correlated bivariate standard normal distribution.
B. Linear Algebra
Vector Spaces
Deﬁnition. A set S is a Vector Space, if for all x, y ∈ S
(B.1)
(B.2)
In the following we consider the Real Vector Space of dimension N , i.e. S ≡ RN with α ∈ R.
Deﬁnition. Column Vectors and Row Vectors are deﬁned by
(B.3)
Deﬁnition. Vectors xk , k = 1, . . . , K are Linear Independent, if
(B.4)
Deﬁnition. The maximal number of linear independent vectors determines the Dimension of the vector
space.
Deﬁnition. K ≤ N linear independent basis vectors constitute (Span) a Vector Subspace U ⊂ S
U ≡ span [x1 , x2 , . . . , xK ] ≡
Fig. B.1: Subspace U spanned by diﬀerent pairs of basis vectors
(B.5)
Deﬁnition. A vector space S is a Normed Vector Space, if any x ∈ S can be mapped to a scalar
(Norm) x ∈ R0,+ , with the following properties:
(B.6)
Normed vector spaces are also Metric Vector Spaces, with the Metric
d(x, y) = x − y .
(B.7)
A valid p-norm of S is
e.g.:
(B.8)
Deﬁnition. A vector space S is a Inner Product Space, if any pair of vectors x, y ∈ S can be
mapped to a scalar (Inner Product) x, y ∈ R,1 with the following properties:
(B.9)
(B.10)
(B.11)
Inner product spaces are also normed vector spaces, with the norm
(B.12)
A valid inner product of the real vector space S is
(B.13)
1 This only holds for real vector spaces. For complex vector spaces, the inner product is a complex number x, y ∈ C, which requires the
additional constraint x, y = y, x ∗ .
Deﬁnition. Vectors x, y ∈ S are Orthogonal to each other, if
(B.14)
Deﬁnition. A set of Orthonormal vectors uk , which are mutually orthogonal with uk = 1 for all
k = 1, . . . , K and which span a vector subspace U, is referred to as the Orthonormal System (ONS)
of U.
Properties of the ONS: ∀x ∈ U
Fourier Series :
Pythagoras :
(B.15)
(B.16)
Example
A set of non-orthogonal vectors can be orthogonalized by the Gram-Schmidt Orthogonalization.
Given x1 , x2 , x3 , the ONS is computed by
(B.17)
(B.18)
(B.19)
The respective QR-Decomposition Q · R of the
matrix X = [x1 , x2 , x3 ] is obtained as:
(B.20)
(B.21)
(B.22)
Matrix Operator
Deﬁnition. A Matrix describes the Linear Mapping of vectors, A : S → S , x → Ax, with the
following properties:
A(x + y) = Ax + Ay, ∀x, y ∈ S
A(αx) = αAx, ∀α ∈ R.
(B.23)
(B.24)
Transposition: The Transpose of a matrix AT is composed by changing columns and rows of A, i.e.
_v('140514132659')
where S ≡ RN and S ≡ RM .
Deﬁnition. The Norm Of A Matrix is inherited by the norm of the respective vectors, e.g. the
maximum norm • ∞ of A can be obtained by
= max
= max
(B.26)
with λi denoting the eigenvalues of the matrix AT A.
A further commonly used norm for matrices is the Frobenius Norm • F :
|am,n =
aT an
with am,n = [A]m,n .
 [a1 , a2 , . . . , aN ] =
(B.27)

Deﬁnition. A matrix $A \in  \mathbb{R}^{M \times N}$  S → S is always associated with Four Fundamental Subspaces, which are
[Image Space, Null Space, 2nd Image Space, 2nd Null Space] 
_v('140514134718') (B.28) (B.29) (B.30) (B.31)

Properties of the fundamental subspaces:
null [A] ⊕ span AT ≡ S
span [A] ⊕ null AT ≡ S .
(B.32)
(B.33)
A powerful tool for constructing ONSs for the fundamental subspaces of a matrix is the Singular Value
Decomposition.
Singular Value Decomposition
Any matrix A ∈ RM ×N can be decomposed into2
(B.34)
σ1 ≥ σ2 ≥ · · · ≥ σR ≥ 0, span [A] = span [u1 , . . . , uR ] , null [A]⊥ = span [v 1 , . . . , v R ] ,
(B.35)
where R ≤ min{M, N } denotes the rank of the matrix A, and the vectors ui and v i constitute an ONS
of span [A] and null [A]⊥ , respectively.

The respective matrix notation for the different dimensions is
_v('140514132955') (B.36)

with U = [u1 , . . . , uR ], Σ = diag [σ1 , . . . , σR ], V = [v 1 , . . . , v R ], and the orthogonal complement vectors
U ⊥ = [uR+1 , . . . , uM ] (V ⊥ = [v R+1 , . . . , uN ]) which appear only if M > N (M < N ).
2 The SVD consists also of any A ∈ CM ×N .

Projection Matrices
Deﬁnition. The Orthogonal Projection of any x ∈ S onto a subspace U spanned by the ONS {u1 , u2 , . . . , uK } by the Projection Matrix P U .
Fig. B.2: Orthogonal Projection of x ∈ S onto U ⊂ S
_v('140501165820') (B.37)
Special projections:
 The orthogonal projection of any x ∈ S onto a subspace U spanned by a Non-Orthonormal basis
{y 1 , y 2 , . . . , y K } can be obtained by
(B.38)
(B.39)
with Y = [y 1 , y 2 , . . . , y K ], the matrix of linear independent vectors y i ∈ U.
 The orthogonal projection matrix of any x ∈ S onto the Orthogonal Complement Space of U
is easily obtained by
(B.40)
Properties of projection matrices:
(B.41)
(B.42)
(B.43)
(B.44)
The non-zero Eigenvalues of projection matrices are obviously always constrained to 1 and the respective
Eigenvectors are elements of U.
Special Matrices
We consider a special class of matrices which can be decomposed in a Weighted Series of Mutually
Orthogonal Projection Matrices, i.e.
(B.45)
with span [P k ] ⊥ span [P ], for all k = .
This special matrices are commonly referred to as
Normal Matrices :
Symmetric (Hermitian) Matrices :
Positive Semidefinite Matrices :
(B.46)
(B.47)
(B.48)
Properties of special matrices:
Given the projection matrices P k = uk uH are constituted by mutually orthonormal basis vectors uk ,
k = 1, . . . , K, then the vectors uk are the Eigenvectors of matrix A, with the respective Eigenvalues
Given the Singular Value Decomposition (SVD) of an arbitrary matrix A ∈ CM ×N , i.e. A =
(B.49)
Obviously, the Outer Product matrices uk uH and v k v H are mutually orthogonal projection matrices,
and P Vk = v k v H ,
(B.50)
projecting onto Uk ≡ span [uk ] ⊂ CM and Vk ≡ span [v k ] ⊂ CN , respectively, i.e.
and A A =
Thus the left- and righthand side singular vectors
 uk and v k of A are Eigenvectors of the Gramian Matrices AAH and AH A,
 with corresponding Eigenvalues λk = σk ,
which are the squared Singular Values of the matrix A.
(B.51)
References
 A. Papoulis and S. U. Pillai. Probability, Random Variables and Stochastic Processes, 4th edition, Mc
Graw Hill, 2002.
 H. Stark and J. W. Woods. Probability, Random Processes, and Estimation Theory for Engineers, 2nd
edition, Prentice Hall, 1994.
 W. Utschick. Stochastische Signale, Aktuelles Manuskript zur gleichnamigen Vorlesung an der Technischen Universit¨t M¨nchen, 2012.
 G. Strang. Linear Algebra and Its Applications, Harcourt Brace Jovanovich, Publishers, 1988.
 L. N. Trefethen and D. Bau III. Numerical Linear Algebra, Society for Industrial and Applied Mathematic,
 W. Utschick. Mathematische Methoden der Signalverarbeitung, Aktuelles Manuskript zur gleichnamigen
Vorlesung an der Technischen Universit¨t M¨nchen, 2012.
