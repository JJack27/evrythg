Lecture Notes
Statistical Signal Processing
Univ.-Prof. Dr.-Ing. Wolfgang Utschick
Technische Universitaet Muenchen
Summer 2014
  © 2014 Univ.-Prof. Dr.-Ing. Wolfgang Utschick
A circulation of this document to other parties without a written consent of the author is forbidden.
Email: utschick@tum.de
Layout by LTEX2ε
Part I
Parameter Estimation
1. Statistic Modeling
Statistic Estimation treats the problem of inferring underlying characteristics of unknown random
variables on the basis of observations of outputs of those random variables.
The basic problem of statistical estimation is to infer the Probability Measure P, which the realizations
of the respective random variables X : Ω → X are subject to.
The most critical part of any parameter estimation problem is the choice of a proper Statistic Model
(Ω, F, Pθ ), with the metric space (X, B) and
Observation Space : X,
Sigma Algebra : F,
Probability Measure : Pθ , θ ∈ Θ.
(1.1)
(1.2)
(1.3)
The stochastic model is a set of Probability Spaces and the task of statistic estimation is to select
the most appropriate candidate.
Standard Model
Deﬁnition. We call the introduced statistical model Standard Model and the inference problem Parameter Estimation, if
(1.4)
and the random variable X is either Discrete or Continuous.
Commonly used terminology:
 Random variables Xi : Ω → X are Statistics, and
 its realizations xi of Xi are Observations, Samples, Measurements, etc.
Deﬁnition. A special statistic is the random variable T : X → Θ, which maps one or multiple samples
to θ ∈ Θ or another parameter depending thereof. The random variable or statistic T : X → Θ is called
Estimator.
Introductory Example
Given observations X1 , . . . , XN of a uniquely distributed random variable
with [0, θ] ⊂ R such that FX (ξ) =
(1.5)
and fX (ξ) = 1 , if 0 ≤ ξ ≤ θ.
The unknown parameter θ, which describes the random variable X is Deterministic and Unknown.
max xi θ
Fig. 1.1: Estimating the upper bound of an intervall.
How to estimate the upper bound? Any guesses?
How to estimate the upper bound?
1. Attempt: Given E [X ] = θ/2, we conclude for the statistics Xi
(1.6)
Average
2. Attempt: Since for large N the maximum observed value will be close to the upper bound, we
conclude
T2 = max {Xi } :
How realiable are these attempts?
(1.7)
Estimator T1 : According to the Law of Large Numbers (Chebyshev Inequality):
Var [X ]
(1.8)
Estimator T2 : Again with respect to the Law of Large Numbers:
Pθ (|T2 − θ| ≥ ) = Pθ (X1 ≤ θ − , . . . , XN ≤ θ − )
Pθ (Xi ≤ θ − )
(1.9)

2. Quality Criteria
Consistency and Unbiasedness
Deﬁnition. Consistent Estimators are characterized by
_v('140503091548') (2.1)
Obviously, T1 and T2 are consistent.
Deﬁnition. Unbiased Estimators are characterized by
_v('140503091610') (2.2)

T1 and T2 are random variables which depend on the randomly drawn observations X1 , . . . , XN . Unbiasedness means that averaging over the entire sample set gets the true parameter:

_t('140515215135')
T1 is Unbiased, since
E [T1 (X1 , . . . , XN )] = E
(2.3)
 T2 is Asymptotically Unbiased, since
E [T2 (X1 , . . . , XN )] =
ξ · fT2 (ξ) dξ =
(2.4)
where
fT2 (ξ) = FT2 (ξ) =
(2.5)
 Consequently, we introduce an unbiased version of estimator T2 by
(2.6)
Fig. 2.1: T1 , T2 , and T2 for 30 uniformly chosen (i.i.d.) samples of size N = 1 . . . 50 with θ = 1.
Variance
Deﬁnition. A further quality measure for an estimator is its Variance
Var [T (X1 , . . . , XN )] = E (T − E [T ])2 .
(2.7)
For the estimators T1 , T2 and T2 of the introductory example we obtain
Var [T1 ] =
Var [T2 ] =
(2.8)
(N + 1)2 (N + 2)
(2.9)
Var [T2 ] =
Var [T2 ]
N (N + 2)
(2.10)
(2.11)
Table 2.1: Variances of the estimators T1 , T2 and T2 for N = 10 observations.
Estimator
Variance σT
Mean Square Error
Deﬁnition. An extension of the Variance is the Mean Square Error (MSE)
ε[T ] = E (T − θ)2 .
(2.12)
For the estimators T1 and T2 of the introductory example the MSE is already known, since these estimators
are unbiased and thus E [T1 ] = E [T2 ] = θ and ε[T ] = Var [T ].
The MSE of the 2nd estimator can be obtained by
ε[T2 ] = E (T2 − θ)2
= E (T2 )2 − 2 E [T2 ] θ + θ2
(N + 2)(N + 1)
(2.13)
Table 2.2: MSEs of the estimators T1 , T2 and T2 for N = 10 observations.
Estimator
Mean
Variance
Fig. 2.2: Estimated (solid) and analytical (dashed-dotted) Mean, Variance, and MSE for T1 (black),
T2 (blue), and T2 (yellow) for 30 uniformly chosen (i.i.d.) samples of size N = 1 . . . 50 with θ = 1.
Bias/Variance Trade–Oﬀ
The MSE of an estimator T ,
ε[T ] = E (T (X1 , . . . , XN ) − θ)2 ,
(2.14)
can be decomposed in its Bias and its Variance, i.e.1
ε[T ] = E (T − E [T ])2 + E [T ] − θ
Bias[T ]
Var[T ]
= Var [T ] + (Bias[T ])2 .
Bias and Variance of an estimator cannot be minimized independently.
1 The
(2.15)
decomposition can easily be shown by taking the expectation of the expansion of (T − E [T ] + E [T ] − θ)2 .
(2.16)
Introductory Example (cont’d)
Another estimator Tα can be constructed by
(2.17)
Bias[Tα ] = E [Tα ] − θ = α E [T2 ] − θ =
The estimator Tα is obviously unbiased if α =
Bias[Tα ]2
(2.18)
3. Maximum Likelihood Estimation
Maximum Likelihood Principle
Given the statistical model {X, F, Pθ ; θ ∈ Θ}, the Maximum-Likelihood (ML) principle suggest to
select a candidate probability measure Pθ , such the observed outcomes of the experiment become most
probable.
A maximum likelihood estimator TML picks the θ ∈ Θ which maximizes the Likelihood Function, i.e.
TML : x1 , . . . , xN → argmax {L(x1 , . . . , xN ; θ)}.
(3.1)
Deﬁnition. The Likelihood Function depends on the statistical model {X, F, Pθ ; θ ∈ Θ}, i.e.
Discrete R.V. : L(x1 , . . . , xN ; θ) = Pθ (x1 , . . . , xN ),
Continuous R.V. : L(x1 , . . . , xN ; θ) = fX1 ,...,XN (x1 , . . . , xN ; θ).
(3.2)
(3.3)
Assuming that all observations are drawn from Identically Independently Distributed (i.i.d.)
random variables X1 , . . . , XN ,
Pθ (x1 , . . . , xN ) =
Pθ (xi ),
(3.4)
fXi (xi ; θ),
(3.5)
fX1 ,...,XN (x1 , . . . , xN ; θ) =
Log-Likelihood Function
Due to the monotonicity of the log-function and assuming i.i.d. random variables, we obtain the following
expressions:
TML : x1 , . . . , xN → argmax {log(L(x1 , . . . , xN ; θ))}
(3.6)
= argmax log(
L(xi ; θ))
(3.7)
log(L(xi ; θ)) .
(3.8)
= argmax
Note. The computation of the maximum likelihood estimate obviously involves the solution of an optimization problem.
Channel Estimation Example
We consider the estimation of the attenuation coeﬃcient h of a Single-Input Single-Output transmission
channel with Additive White Gaussian Noise (AWGN) N ∼ N (0, σ 2 ) at the receiver.
Fig. 3.1: Channel model.
The model of the receiver signal is
Yi ∼ N (hsi , σ 2 ),
(3.9)
where si denotes the i-th training signal.
Assuming N observations y1 , . . . , yN according to the training signals s1 , . . . , sN , the likelihood function is
given by L(y1 , . . . , yN ; θ) = N fYi (yi ; θ), with θ = h and
exp − 2 (yi − θsi )2 .
fYi (yi ) = √
(3.10)
The maximum-likelihood estimation is derived by
hML = argmax
= argmax
= argmin
exp − 2 · (yi − θsi )2
· (yi − θsi )2
· (yi − θsi )2
= argmin ||y − θs||2 .
= argmin
(3.11)
where s = [s1 , . . . , sN ]T . The respective estimator hML is given by the Pseudo Inverse of the vector s
of training signals, i.e.
(3.12)
The ML Estimator is obviously identical with the Least Squares Estimator. This changes considerably
when the statistics Yi are correlated or when N is non-Gaussian distributed.
Introductory Example (cont’d)
The likelihood function for a single observation (N = 1) reads
L(x; θ) =
otherwise.
L(x1 , . . . , xN ; θ) =
L(x1 , . . . , xN ; θ)
otherwise.
The ML Estimator is given by
θ = argmax {L(x1 , . . . , xN ; θ)}
= max {x1 , . . . , xN },
which corresponds to the 2nd intuitive attempt.
max {xi }
Bernoulli Experiment
We now consider a Bernoulli Experiment with success probability θ, e.g. the transmission of a data
packet over a link with probabilities
P (no erasure) = θ,
P (erasure) = 1 − θ.
(3.13)
(3.14)
We now study the maximum-likelihood framework for estimating the unknown parameter θ based on N
independent observations:
The statistical model {X, F, Pθ ; θ ∈ Θ} is
and the respective random variable X , which counts the number of successful Bernoulli trials within N attempts.
X is Binomially Distributed according to
[factorial formula for the binoial coefficients ]
[  \binom nk = \frac{n!}{k!\,(n-k)!} ]
Given the likelihood function
L(x; θ) =
θ (1 − θ)N −x ,
the log-likelihood function reads
log L(x; θ) = log
+ x log θ + (N − x) log (1 − θ).
The ML-Estimator can be obtained by
θ = argmax {log L(x; θ)},
i.e.
log L(x; θ)
+ (N − x)
(−1) = 0
Quality of
Since X is binomially distributed
Var [X ] = N θ(1 − θ),
(3.15)
(3.16)
we obtain
Var [TML ] = Var
θ(1 − θ)
= 2 Var [X ] = 2 N θ(1 − θ) =
(3.17)
(3.18)
Obviously, the ML Estimator of the success probability θ is unbiased, and the MSE is equal to the variance of the estimator,
Bias [TML ] = 0,
ε [TML ] = Var [TML ] =
Can we improve the estimator?
(3.19)
θ(1 − θ)
(3.20)
Alternative Solution:
⇒ Bias [T ] =
(3.21)
ε [T ] = Var [T ] + Bias [T ]
= Var
Var [X + 1]
(N + 2)2
Var [X ]
(N + 2)
N θ(1 − θ) + (1 − 2θ)2
(N + 2)2
(3.22)
Fig. 3.2: Comparison of the achievable MSEs for N = 10 observations.
Note. Biased estimators can provide better estimates than unbiased estimators.
Best Unbiased Estimator
ML Estimators are not necessarily the best estimators. A wide class of estimators is deﬁned by minimizing
the MSE under an unbiasedness constraint.
Deﬁnition. Given the statistical model {X, F, Pθ ; θ ∈ Θ}, we call an estimator Best Unbiased
Estimator if E [T (X1 , . . . , XN )] = θ and
Var [T (X1 , . . . , XN )] ≤ Var [S (X1 , . . . , XN )] ,
(3.23)
for any alternative unbiased estimator S .
' Best unbiased estimators are also referred to as Uniformly Minimum Variance Unbiased (UMVU) Estimators. '
The estimator T2 =
of the introductory example is a UMVU Estimator.

4. Fisher’s Information Inequality
An universal lower bound for the variance of an estimator can be introduced, if the following [ regularity ] conditions are
fullfilled:
_v('140510091627') [ (4.1) - (4.3) ]
Deﬁnition. Consequently, we deﬁne the Score Function as
$\operatorname{g}(x; \theta) = \ldots$
_v('140509215904') (4.4)
_cf('140515214623')
Cramer-Rao Lower Bound
Given the score function g(x; θ) of an estimation problem, and the Fisher Information term $I_F(\theta) = \operatorname{Var}[g(X , \theta)]$, the variance of any possible estimator can be lower bounded by
(4.5)
_v('140509220300') (4.6)
if T is unbiased.

Appling the Cauchy-Schwarz Inequality, | a, b |2 ≤ a, a · b, b , yields:
2nd Version of Fisher Information
An alternative expression for the Fisher Information term IF (θ) can be derived,
IF (θ) = Var [g(X , θ)]
= E g(X , θ)2 − E [g(X , θ)]2
= E g(X , θ)
log L(X ; θ) ,
(4.10)
(4.11)
which can be interpreted as the negative Mean Curvature of the Log-Likelihood Function.
The last step from (4.10) to (4.11) is not obvious.
log L(x; θ) =
L(x; θ)
L(x; θ)
− L(x; θ)
L(x; θ)
L(x; θ)2
L(x; θ)
L(x; θ)
L(x; θ)
⇒ E g(X , θ)
L(x; θ)
L(X ; θ)
L(X ; θ)
log L(x; θ)
L(x; θ)
L(x; θ) ∂θ
L(x; θ)
L(x; θ)2
− g(x, θ)2 .
log L(X ; θ)
L(x; θ)dx − E
log L(X ; θ) .
L(x;θ)dx=
(4.12)
(4.13)
Interpretation
The 2nd version of the Fisher Information IF (θ) as the negative Mean Curvature of the Log´
Likelihood Function allows an interpretation of the Cramer-Rao Lower Bound.
IF (θ) = − E
log L(X ; θ) .
curvature
E [log L(x; θ)]
E [log L(x; θ)]
ﬂat curvature
strong curvature
Properties of the Fisher Information:
 The log-likelihood function log(L(x1 , . . . , xN ) depends on given observations x1 , . . . , xN .
 A weak curvature of log(L(x1 , . . . , xN ) corresponds to little information in x1 , . . . , xN .
 A strong curvature of log(L(x1 , . . . , xN ) indicates more information in x1 , . . . , xN .
 The Fisher information IF (θ) refers to the negative mean curvature at the true parameter θ.
 IF (θ) is based on the expectation of the negative mean curvature of log(L(X1 , . . . , XN ) with respect to
the observation statistics X1 , . . . , XN .
 IF (θ) is a function of θ.
 IF (θ) is monotonically increasing with the number of independent observation statistics, i.e. given the
Fisher Information IF (θ) for a single observation statistic (N = 1), then
(N )
IF (θ) = N · IF (θ).
(4.14)
Exponential Models
Deﬁnition. A Exponential Model is a statistical model {X, F, Pθ ; θ ∈ Θ} with random variable X
_v('140509222738')
h(x) exp(a(θ)t(x))dx for normalization.
Then the respective Fisher Information can be directly obtained by
_v('140509222832')
For the special case that fX (x; θ) can be arranged such that E [t(X )] = θ, the unbiased estimator
T : x → t(x).
(4.17)
can directly be obtained. Otherwise, t(x) at least provides a Sufficient Statistic for estimating θ.
Given an Exponential Model it can be shown that the Unbiased Estimator T : x → t(x) is a UMVU Estimator, and thus achieves the Cramer-Rao Lower Bound.
Mean Estimation Example
We consider the estimation of the unknown mean value θ = µ of a random variable X ∼ N (µ, σ 2 ) based
on a single observation.
The respective PDF can be arranged as follows:
exp − 2 (x − θ)2
+ log(2πσ 2 )
exp − 2 x2 ,
fX (x; θ) = L(x; θ) = √
= exp
a(θ)t(x)
b(θ)
(4.18)
h(x)
with t(x) = x and a(θ) = θ/σ 2 .
Thus the single observation UMVU Estimator T (1) of θ = µ can be obtained as
T (1) : θ = x,
Bias[T (1) (X )] = 0,
which minimizes both the variance and the MSE criterion to
∂a(θ)
Var T (1) =
(4.19)
(4.20)
Asymptotically Eﬃcient Estimators
Deﬁnition. An estimator T (X1 , . . . , XN ) is Asymptotically Efficient if1
N (T(X1 , . . . , XN ) − θ) ∼ N (0, IF (θ) )
Given the following requirements are fulﬁlled,
 Θ is an open set,
 fX (x; θ) and
fX (x; θ)dx is twice diﬀerentiable with respect to θ,
 IF (θ) < ∞,
then a Maximum-Likelihood Estimator is Asymptotically Efficient.
1 Convergence
(4.21)
