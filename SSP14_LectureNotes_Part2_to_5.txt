Part II
Examples
5. ML Principle for Direction of Arrival Estimation
We consider the estimation of the Direction Of Arrival (DoA) θ of an impinging planar wavefront
by means of an antenna array with M antennas.
x0 (t)
x1 (t)
x2 (t)
Fig. 5.1: Spatial sampling of a planar wavefront.
The received signal vector at the antenna array at time instant t ∈ R is equal to
x(t) = ξas(t) + η(t)
(5.1)
with the signal at the mth antenna element as
xm (t) = ξ ej 2π sin(θ)md/λ s(t) + ηm (t),
(5.2)
The d and λ denote the distance between two adjacent antenna elements and the wavelength of the
assumed Narrowband Signal. The received signal is assumed to be corrupted by the Gaussian noise
vector η(t) ∼ N (0, C η ).
The ξ ∈ R represents the attenuation which the transmitted signal s(t) ∈ R experiences over the transmission path.
Without loss of generality we thus assume a Uniform Linear Array (ULA) with M antenna elements,
i.e.
a =  .  , with α = ej 2π sin(θ)d/λ .
(5.3)
In order to ﬁnd the ML estimate of the DoA parameter θ, we consider the respective Likelihood Function for observations of the received vector x(t) at t1 , . . . , tN ,1 implicitly assuming a stationary scenario
over the respective time intervall,
L(x1 , . . . , xN ; θ) = M N
exp −
det C η
i=1
(xi − ξasi )H C −1 (xi − ξasi ) .
(5.4)
For simplicity reasons we restrict the further analysis to the case of a single observation N = 1 and Addi2
tive White Gaussian Noise η(t) ∼ N (0, ση I), i.e.
[ Die Determinante einer Diagonalmatrix ist das Produkt der Eintraege auf der Hauptdiagonalen ]
(5.5)
simplicity of notation we use xi instead of x(ti ).
After some simple reformulation steps the ML optimization problem is equal to
min (x − ξas)H (x − ξas)
(5.6)
Since the cost function can be expanded in
(x − ξas)H (x − ξas) = xH x − 2 Re ξxH as + ξ 2 aH as2 ,
(5.7)
and 
$\alpha^H \alpha = M$ [independent of \theta ]
, it can be further reduced to
max Re xH a(θ) .
The ML Estimator θ = argmax Re xH a(θ)
(5.8)
is obviously a Nonlinear Estimator.
Note. In order to estimate the attenuation ξ, we need further information about the Training Signal
(Pilot Signal) s(t).
The Likelihood function of the given estimation problem obviously belongs to the familiy of Exponential
Distributions. We consider θ as parameter to be estimated. After some reformulation steps L(x; θ) can
be expressed as
(x − ξas)H (x − ξas)
L(x; θ) =
exp −
(πση )M
h(x) exp cH (θ)t(x)
exp(b(θ))
(5.9)
with
exp − 2
(πση )M
ξas
c(θ) = 2
ση (ξas)∗
h(x) =
t(x) =
(5.10)
(5.11)
(5.12)
\todo{Why's that?}
Note, that E[t(x)] = \sigma_{\eta} c(θ).
(5.13)
Consequently, the Fisher Information of the ML estimation problem can be obtained as2
IF (θ) =
∂c(x)
∂ E [t(x)]
a
a∗
(5.14)
a
a∗
(5.15)
Since θ is real,
IF (θ) = 2
a
a∗
ξ 2 s2 ∂a
a
a∗
(5.16)
(5.17)
Finally, with ∂αm /∂θ = j m(2πd/λ) cos θαm−1 and |αm−1 | = 1,
IF (θ) = 2 2
M (M − 1)(2M − 1)
(cos θ)2 M 3 .
= ξas.
(5.18)
References
 S. M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory, volume I, Prentice Hall
Signal Processing Series, 1993.
Part III
Estimation of Random Variables
6. Bayes Estimation
In contrast to the Maximum-Likelihood principle an A Priori information about the unknown parameter
θ ∈ Θ is now taken into account. We assume the a priori information of the unknown parameter to be
given by means of the
PDF :fθ (θ; σ)
Conditional PDF :fX |θ (x|θ),
(6.1)
(6.2)
where σ parameterizes the statistical model of the (now!) random variable θ. Instead of uniformly minimizing
the MSE criterion we consider the Mean MSE with respect to the parameter θ,
E (T (X ) − θ)2 = E E (T (X ) − θ)2 | θ
(T(x) − θ)2 fX |θ (x|θ)dx fθ (θ; σ)dθ.
E (T(X )−θ)2 |θ
(6.3)

[ _to('140528171726') ]
Conditional Mean Estimator
Theorem. The Conditional Mean Estimator (Bayes Estimator) TCM is MSE optimal,
(6.4)
i.e. TCM minimizes the Mean MSE cost criterion
(6.5)
Note. We distinguish
E [(T (X ) − θ)2 |θ], which corresponds to the MSE subject to the condition of the (unknown) deterministic outcome of the random variable θ,
and E [(T (X ) − θ)2 ] = E [E [(T(X ) − θ)2 | θ]], which assumes θ to be a random variable, such that the expectation has to be taken over both random variables.

Proof [ Optimality of the conditional mean estimator ]

Bernoulli Experiment (Cont’d)
In Section 3.4, we found the Maximum Likelihood Estimate of the success probability of a Bernoulli Experiment as
Then the Conditional Mean Estimator equals (cf. alternative solution in Section 3.4)
θCM = pX (x)−1
= pX (x)−1
= pX (x)−1
1 m-times
θpX |θ (x | θ) dθ
(6.12)
θ (1 − θ)N −x dθ
(6.13)
(x + 1)!
pX (x)−1 x + 1
x (N − x + 1)(N − x + 2) · · · (N + 2)
applying the partial integration technique (m, n ∈ N) we obtain
[ integration over a binomial distribution ]
Mean Estimation Example (Cont’d)
We again consider the estimation of the unknown mean value θ of a random variable
(6.16)
(6.17)
now based on N i.i.d. observations
drawn with respect to the Conditional PDF2
(6.18)
A priori knowledge about the unknown parameter θ is given by the PDF of the unknown parameter θ ∼
(6.19)
2 The diﬀerence between statistics drawn from a Conditional Distribution and statistics drawn from a Unconditional Distribution is
essential for the understanding of the following results.
Note. Consider the diﬀerence between
Variance Of θ :
Conditional Variance Of X :
Variance Of X :
(6.20)
(6.21)
(6.22)
with
(6.23)
Interpretation. Since X is the sum of independent random variables with θ ∼ N (m, σθ ) and η ∼ N (0, σX |θ ), the variance of the sum is equal to the sum of variances.  
(6.24)
For the computation of the Conditional Mean Estimator we need two steps:
a) Conditional PDF fθ|X1 ,...,XN (θ|x1 , . . . , xN ) (6.27)
b) Conditional Mean E [θ|x1 , . . . , xN ] (6.30)
which is Linear In Observations x1 , . . . , xN (!) and after some reformulation steps:
(6.30) can be read out from Eq. (6.27) after some reformulation steps.
(6.31)
Discussion.
 Given a large number N of observations or a small conditional variance σX |θ of observations or a large
variance σθ of the a priori distribution of the unknown parameter, it is recommended to rely on the
Maximum-Likelihood Estimator, since
lim θCM = 2lim θCM = 2
lim θCM = θML ,
(6.32)
xi .
(6.33)
i=1
 However, given a small variance σθ of the a priori distribution of the unknown parameter or a large
conditional variance σX |θ of observations, we better rely on the Mean Value of parameter θ, since
lim θCM =
lim θCM = m.
(6.34)
 The respective Minimum Mean Square Error (MMSE) is given by4
E E (TCM − θ)2 | θ
= E E (E [θ|x] − θ)2 | x
(6.35)
(6.36)
(6.37)
= E [Var [θ|x]]
(6.38)
(6.39)
 Any requisites of the CM Estimator can be found from the mean vector and covariance matrix of the
joint distribution of (X , θ)T , i.e.
Var
(6.39) can be read out from Eq. (6.27) after some reformulation steps.
(6.40)
(6.41)
Multivariate Variables
Given two random vectors X ∼ N (µX , C X ), θ ∼ N (µθ , C θ ), and the covariance matrix C z from the
joint distribution of Z = (X , θ)T ,
(6.42)
the Multivariate Conditional Mean Estimator is obtained as
x → E [θ|x] = µθ + C θ,X C −1 (x − µX ).
[ $\mu_{\theta}$ is the prior mean ]
(6.43)
The respective MMSE is equal to the Trace of the conditional covariance matrix C θ|X , this is
E (TCM − θ)2 = tr C θ|X = tr C θ − C θ,X C −1 C X ,θ .
(6.44)

Note. Given Jointly Gaussian Random Variables X and Y , the Conditional Mean Estimator E [Y |X ] is a Linear Function in X . This does not hold for arbitrarily jointly distributed random variables!

Mean Estimation Example (Cont’d)
From a multivariate r.v. perspective we obtain
X =  ...  ∼ N (µX , C X ) and θ ∼ N (µθ , σθ2),
(6.45)
and thus
(6.46)
where 1 = [1, . . . , 1]T and I is the unity matrix. With C −1 we obtain the CE Estimator
TCM = m + σθ 1T (σθ 11T + σX |θ I)−1 (x − m1) =
i=1 xi
(6.47)
Note.
Computation of C −1 can be obtained in closed form solution by appling the Matrix Inversion Lemma:
C −1 = (σθ 11T + σX |θ I)−1
(6.48)
(6.49)
where
Orthogonality Principle

The Orthogonality Principle is an inherent property of the Conditional Mean Estimator.  It describes the inherent Stochastic Orthogonality between the CM estimation error and any observations statistics or functions thereof, i.e.
_v('140514133928') (6.50)
_v('140514133929') (6.51)

where $\operatorname{h} : \ldots \mathbb{R}^N \to → \mathbb{R} \ldots$
Proof.
E [(TCM (X ) − θ)h(X )] = E [(TCM (X )h(X )] − E [θh(X )] = E [E [θ|X ] h(X )] − E [θh(X )]
= E [E [θh(X )|X ]] − E [θh(X )] = E [θh(X )] − E [θh(X )] = 0.
(6.52)
(6.53)

Mean Estimation Example (Cont’d)
The parameter θ to be estimated and the corresponding observation statistic X are Jointly Gaussian
Distributed according to
(6.54)
As an Sufficient Statistic5 for estimating the unknown realization of the random variable θ we use6
Xi .
(6.55)
i=1
Applying the Orthogonality Principle we obtain
E [(TCM − θ)S ] = E [TCM S ] − E [θS ] = 0.
(6.56)
Knowing that the MSE optimal estimator is linear in S , we substitute TCM by a linear model
TCM (X ) = aS + b.
(6.57)
For h(x1 , . . . , xN ) = S , the Orthogonality Principle is now equal to
E [(TCM − θ)S ] = E [(aS + b − θ)S ] = 0.
5 The
concept of Sufficient Statistic is later introduced in this course.
that the realizations of the statistics X1 , . . . , XN are conditioned on the unknown realization of θ.
6 Note,
(6.58)
The Orthogonality Principle leaves one Degree of Freedom which is statisﬁed by a second
orthogonality propostion with h(x1 , . . . , xN ) = 1,
E [(aS + b − θ)1] = E [aS + b] − E [θ] = 0.
(6.59)
Then we obtain
a=
and b = µθ − aµS ,
(6.60)
and the Conditional Mean Estimator is given by7
7 Since
(s − µS ) = m + 2
(s − m) =
i=1 xi
(6.61)
the realizations of the statistics X1 , . . . , XN are conditioned on the unknown realization of θ, we obtain cθ,S = E [θS ] − E [θ] E [S ] =
E [E [θS |θ]] − E [θ] E [E [S |θ]] = E θ2 − (E [θ])2 = σθ and σS = E S 2 − (E [S ])2 = E E S 2 |θ
(E [θ])2 = E
− (E [E [S |θ]])2 = E Var [S |θ] + E [S |θ]2 −
Part IV
Linear Estimation
7. Linear Estimation
In this chapter we directly focus on linear models, i.e. given an observation x corresponding to the respective random variable, we consider the estimation of the realization of the random variable y by
(7.1)
In contrast to the Maximum-Likelihood Estimators and the Conditional Mean Estimators, where the inference of the estimator is based on statistical parameters of a given Probability Distribution Function, here the estimation of y, by inference of t and m, is based on $N$ pairs of jointly drawn observations (xi , yi ), where
(7.2)
y =  .  and X =  .  .
Least Squares Estimation
In Least Squares (LS) Estimation the Linear Estimator – the inference of t and m – is based
on the minimization of the sum of squared errors between the observations and outcomes of the Linear
Model1 y = xT t.
The related Optimization Problem is equal to
The solution of the LS Problem is given by
_v('140514131838') (7.3)
tLS = (X T X)−1 X T y,
_v('140514132215') (7.4)
implicitly assuming that N is larger than the dimension of the observation vectors $x_i$ and the observation
vectors form a linear independent basis of RN .
1 The
aﬃne case y = xT t + m can similarly be treated by y = x ,T t , with t =
and x =
Alternative Solution
By means of the Singular Value Decomposition2 of the Observation Matrix X, we obtain
with X = U , U ⊥
(7.5)
After some reformulations,3 the cost function is equal to
(7.6)
Since the lower part of the vector does not depend on t, the inequality y − Xt
the minimum value is achieved if
Note. Der residual error U ⊥,T y holds and
corresponds to the orthogonal projection of y onto span U ⊥ .

real-valued matrices the singular vectors can be chosen real-valued, too.

$Q$ is unitary, $|| Q x ||_2^2 = || x ||_2^2$ 
[ the multiplication with a unitary matrix does not change the norm, thus $[[U^T], [U^{\perp, T}]]$ may be dropped ]

Mean Estimation Example (Cont’d)
In order to estimate the mean value θ of the normal distribution θ ∼ N (m, σθ ) based on observations
xi ∼ N (θ, σX |θ ), i = 1, . . . , N by means of Least Squares Estimation, we introduce the linear
model4
xi = θ,
(7.7)
and thus the LS optimization problem
min
i=1
(xi − θ)2
Comparing this linear model with the general LS model, we consequently obtain
y =  .  and x =  .  .
(7.8)
(7.9)
The resulting LS Estimator is determined by
ˆ = (1T 1)−1 1T y = 1 y
i=1 1 · xi
xi .
N i=1
4 The
xi , yi , and t are replaced by constant input 1, xi , and θ. The constant input is due to estimating the weight t = θ, cf. (7.7).
(7.10)
(7.11)
Orthogonal Projection
Given the LS solution tLS in Eq. (7.4), we obtain the vector of approximated observations as y LS =
X(X T X)−1 X T y. Consequently, the Approximation Error can be expressed as as y − y 2 =
y − X(X T X)−1 X T y = y(I − X(X T X)−1 X T )y and the matrix
P = X(X T X)−1 X T
(7.12)
is the Projection Matrix onto the subspace, which is spanned by the column vectors of X. The matrix
P maps any vector y to its Best Approximation y LS , which is element of the subspace spanned by
the column vectors of X.5
This inspires an alternative derivation of the LS estimator based on the Orthogonality Principle.
To this end, we take into account that y ∈ span [X] and obtain
y − XtLS ⊥ span [X]
(7.13)
and thus
X T (y − XtLS ) = 0.
(7.14)
tLS = (X T X)−1 X T y.
(7.15)
Consequently,
5 Note
that the column vectors of X are not the observation vectors xi .
P : RN → span [X]
span [X]⊥
span [X]
Fig. 7.1: Orthogonal Projection of y ∈ RN onto span [X] ⊂ RN .
Given N is larger than the dimension of the observation vectors xi and the observation vectors form a
linear independent basis of RN :
(7.16)
(7.17)
where the matrix
is referred to as the Moore-Penrose Pseudoinverse.
The comparison of Eq. (7.16) with the Orthogonality Principle results in the projection matrix
(7.18)
and, taking into account the Singular Value Decomposition X = U ΣV T ,6
(7.19)
and thus
RN = span [X], the matrix V of singular vectors is unitary.
(7.20)
LS Estimation (scalar)
Given a sample set {(x1 , y1 ), . . . , (xN , yN )},
elements yi ∈ R,
the LS estimator is denoted by
i.e. the vector of observations xi ∈ R and the vector of
and y =  .  ,
(7.21)
Minimizing the Euklidean Norm of $y − \hat{y}$ with y = xt by applying the Orthogonality Principle yields
\todo{Why does the orthogonal projection minimize the norm of the error?}
y − xtLS ⊥ span [x]
and thus
xT (y − xtLS ) = 0.
If follows for the optimal weight:
(7.22)
(7.23)
◦ = yi
• = yi
Fig. 7.2: Principle of scalar linear estimation: y = xt
LS Estimation (scalar and aﬃne)
Assuming the linear estimator as
the vector of observations xi becomes a matrix
(7.24)
of observation vectors xT = [xi 1], i.e.
i
Since y = Xt, or in other words y ∈ span [X], the Orthogonality Principle yields:
y − XtLS ⊥ span [X]
and
X T (y − XtLS ) = 0.
(7.25)
Given N ≥ 2 and at least 2 observation vectors form a linear independent basis of R2 ,
(7.26)
◦ = yi
• = yi
Fig. 7.3: Principle of scalar aﬃne estimation: y = xt1 + t2
LS Estimation (multi-dimensional)
We now consider the most general case with {(x1 , y 1 ), . . . , (xN , y N )}, i.e. the matrix of observation vectors
xT and y T ,
i
i
(7.27)
X =  .  and Y =  .  .
The optimal linear estimator with respect to the LS criterion
and y T = xT T LS ,
(7.28)
is found by applying the Orthogonality Principle columnwise:
columnwise :
Y − XT LS ⊥ span [X]
and
X T (Y − XT LS ) = 0.
(7.29)
If N is larger than the dimension of observation vectors xi and the observation vectors form a linear
independent basis of RN ,
(7.30)
Linear Minimum Mean Square Estimator
The Linear Minimum Mean Square Estimator (LMMSE) is the estimator which minimizes the
MSE based on a Linear Model for the estimator,
(7.31)
i.e. the LMMSE Estimator is minimizer of the optimization problem7 mint,m E
Given the joint PDF of the random variables
z = (x , y )T, with mean values and covariances
and C z =
(7.32)
the LMMSE Estimator is given by
y = µy + cy ,x C −1 (x − µx ) = µy + cy ,x C −1 x − cy ,x C −1 µx .
contrast to the standard notation, we denote random vectors by
x in order to avoid any confusions with matrix notation X.
(7.33)
The achievable MMSE is equal to
(7.34)
In the case of Zero-Mean random variables,
(7.35)
the LMMSE Estimator and its minimum MSE is
LMMSE Estimator: y = cy ,x C −1 x,
Minimum MSE:
(7.36)
(7.37)
Part V
Examples
8. Estimation of a Matrix Channel
The estimation of a Multiple-Input Multiple Output (MIMO) channel is considered. In particular
we assume a MIMO channel with K antenna elements at the transmitter and M antenna elements at the
receiver, which means KM transmission channels to be estimated.
Fig. 8.1: Estimation of a MIMO channel.
Three Linear Estimators,
are introduced and compared:
 The Minimum Mean Square Error Estimator (MMSE),
 the Maximum Likelihood Estimator (ML),
 and the ”Matched Filter” Estimator (MF).
(8.1)
Model for Training Channel
The addressed model allows to cover a variety of scenarios, including Time-Variant and Dispersive channels. For the sake of simplicity, we assume the simplest case of an Time-Invariant NonDispersive MIMO channel. The task is to ﬁnd good estimates of the channel coeﬃcients
(8.2)
where hm,k denotes the channel coeﬃcient from the kth transmitter to the mth receiver.
To this end, we assume N vectors of Training Signals sn ∈ CK , n = 1, . . . , N . The estimation of the
channel coeﬃcients hm,k is based on the received signal vectors
(8.3)
where H, y n and η n denotes the matrix of channel coeﬃcients, the received signal vector and the noise
corruption at the receiver for the nth training vector, respectively.
The model for the training channel is thus given by
(8.4)
i.e. Y = H S + N .
By stacking the column vectors of the matrices, y = vec(Y ), h = vec(H) and n = vec(N ), we obtain1
= (S ⊗ I M )h + n,
The so called Kronecker Produkt S ⊗ I M means
1 Here
(8.5)
we use the following identity for matrix equations: AXB = C ⇔ (B T ⊗ A) vec(X) = vec(C).
(8.6)
(8.7)
Example. K=M=N=2.
Fig. 8.2: Model for the estimation of a MIMO channel.
Further Assumptions.
We further assume that the stacked vector of channel coeﬃcients
distortions n ∈ CN M are Gaussian Distributed as2
h ∼ N (0, C h )
and
and the stacked vector of
n ∼ N (0, C n ) .
(8.8)
In the following we assume no correlations between noise vectors at diﬀerent time instances, thus leading
to a covariance C n = I N ⊗ C η ∈ CN M .
Additionally, assuming no correlations between distortions of adjacent antenna elements leads to
The covariance matrix of the channel vector in general also shows additional structural properties, which
are not taken into account in the following.
The channel vectors h and the noise distortions n are assumed to be Stochastically Independent,
and thus Uncorrelated ( Cov h, nH = 0 ).
Note. Not taking into account these structural properties does not change the channel estimates, however,
might be very useful to design more eﬃcient algorithms.
contrast to the standard notation, we again denote random vectors by
x in order to avoid any confusions with matrix notation X.
Due to the Linear Channel Model y = Sh + n, we conclude that the stacked vector of signal vector
at the receiver and the unknown channel vector z = [y T , hT ] are Jointly Gaussian Distributed.
The covariances of the Joint Gaussian PDF is equal to
(8.9)
with
C y = Var [y ] = E
yy H = E (S h + n)(S h + n)H · · · = SC h S H + C n ,
C y ,h = Cov [y , h] = E yhH = E (S h + n)hH · · · = SC h ,
(8.10)
(8.11)
(8.12)
where we intensively applied Cov h, nH = E hnH 0 − µh µT = 0.
Note. For the following estimators, we assume full knowledge of the covariance matrices
and C y ,h ∈ CN M ×KM .
In practice the required covariance matrices must be estimated as well!
(8.13)
Minimum Mean Square Error Estimator
Since we assume Jointly Gaussian Distributed random variables, the Conditional Mean
Estimator is identical with the MMSE Estimator T MMSE , the minimizer of
min E
h − T (Sh + n)
(8.14)
which is given by
(8.15)
(8.16)
(8.17)
(8.18)
Maximum-Likelihood Estimator
If we consider the unknown channel vector as deterministic, the Jointly Distributed Random Variables y
and h are replaced by y ∼ N Sh, C y |h =h , with C y |h =h = C n , and the Conditional PDF
fy |h (y|h) = π N M det C n
exp −(y − Sh)H C −1 (y − Sh) .
(8.19)
Consequently, the ML Estimator T ML is the minimizer of the optimization problem
min (y − Sh)H C −1 (y − Sh) ,
(8.20)
which results into
(8.21)
(8.22)
Note. Given the ML Estimator T ML the MMSE Estimator, can be denoted as
(8.23)
(8.24)
Correlation Estimator
Assuming training signals such that S H S ∝ N I KM ×KM and further assuming AGWN, the ML Estimator
is equal to the so called Correlation Estimator (C)
(8.25)
(8.26)
Note. The Correlation Estimator is the simplest estimator which is often applied beyond cases
where it is identical with the ML Estimator.
Matched Filter Estimator
The Matched Filter Estimator (MF) takes the principle of Strong Correlations between the
channel vector h and its h as a cost function. In other words, the MF Estimator T MF is the maximizer of
the optimization problem
(8.27)
max
T  Var [T n ] 
The corresponding MF estimator is equal to
(8.28)
(8.29)
Special Case
In order to simplify the further analysis we assume training signals such that
(8.30)
(8.31)
where σs and ση are the variances of single training and noise distortions signals.
Using these assumptions the proposed estimators can be denoted as follows:
(8.32)
(8.33)
(8.34)
The simpliﬁed case allows a convenient asymptotic analysis for the three estimators.
High Noise Regime :
lim T MMSE = T ML
(8.35)
lim ση T MMSE ∝ T MF .
Low Noise Regime:
(8.36)
Using the Eigen Value Decomposition of the channel covariance matrix
λi ui uH ,
i
(8.37)
i=1
the estimators are equal to
(8.38)
(8.39)
(8.40)
Interpretation. Denoting the MMSE estimate in a series of the eigenvectors ui , i = 1, . . . , KM ,
i=1
λi
λi +
ui uH hML ,
i
(8.41)
the improvement by the MMSE Estimator can be interpreted as a Weighting Of The ML Estimate
which is optimally adapted to the eigenspaces of the random channel vector.
Bias/Variance Trade–Oﬀ
For the analysis of all introduced channel estimators we consider the decomposition of the Mean Square
Error into the estimator’s Squared Bias and Variance. Since we compare estimators of diﬀerent
paradigms with respect to A Priori Information about the unkown parameter, we study the mean
average of the respective Bias/Variance Decomposition. To this end, the MSE is decomposed by the
following steps3
h − T (Sh + n)
= E E h − T (Sh + n)
(8.42)
= E E  (I KM − T S)h +T n  | h
Bias
= E  (I KM − T S)h
Squared Bias
Average Squared Bias
(8.43)
Variance
Average Variance = Variance
= tr (I KM − T S)C h (I KM − T S)H + tr T C n T H .
3 The
last step is found by using the identity E
(8.44)
(8.45)
For the simpliﬁed assumptions S H S = N σs I KM and C n = ση I N M :
Estimator
Averaged Squared Bias
ML/Correlator
λi λi − 1
i=1
i=1
λi 
Matched Filter
Variance
λi N σ s
i=1
i=1
λi
λi N σs
The minimum MSE achieved by the MMSE Estimator is given by
h − T (Sh + n)
i=1
λi
λi +
(8.46)
Numerical Experiments
We consider
 a Non-Dispersive MIMO channel h = vec[H],
 with M = K = 8 antenna elements,
 a Uniform Linear Arrays at the receiver,
 and N = 16 training signal vectors sn ∈ {−1, +1}K (Binary).
 The covariance matrix C h corresponds to 8 impinging planar wavefronts at
(8.47)
where each Azimuth Angle represents a cluster of wavefronts spread according to a Laplace
Angular Power Spectrum with spread σ = 5◦
to kth transmitter
Fig. 8.3: Array geometry.
maximum likelihood
matched ﬁlter
red.-rank ML (R = 30)
red.-rank ML (opt. rank)
Fig. 8.4: Normalized MSE of channel estimators for M = 8, K = 8, and N = 16.
Numerical Experiments (cont’d)
We now consider
 a Dispersive Single-Input Multiple-Output (SIMO) channel h = vec[H],
 with M = 8 receiver antenna elements, K = 1 transmitter antenna elements,
 a Uniform Linear Arrays at the receiver,
 with dispersion order L = 4,
 and N = 16 training signal vectors sn ∈ {−1, +1}K (Binary).
 The covariance matrix C h corresponds to 5 impinging planar wavefronts at
(8.48)
per delay, where each Azimuth Angle represents a cluster of wavefronts spread according to a
Laplace Angular Power Spectrum with spread σ = 5◦
maximum likelihood
correlator
matched ﬁlter
Fig. 8.5: Normalized MSE of channel estimators for M = 8, K = 1, L = 4 (Dispersive), and N = 16.
References
 S. M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory, volume I, Prentice Hall
Signal Processing Series, 1993.
 L. L. Scharf. Statistical Signal Processing: Detection, Estimation, and Time Series Analysis, 1st edtition,
Prentice Hall, 1991.
 F. A. Dietrich and W. Utschick. Pilot assisted channel estimation based on second order statistics. IEEE
Transactions on Signal Processing, 53(3):1178–1193, 2005.

_gt('SSP14_LectureNotes_Part6.txt')
