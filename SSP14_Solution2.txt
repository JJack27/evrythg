Statistical Signal Processing
2. Maximum Likelihood Estimation
Problem 2.1
a)
Maximum Likelihood Estimation
The statistical model is given as {X, F, Pθ ; θ ∈ Θ} with:
• the sample space/observation space X, here R2 ,
• a sigma algebra F, here the family of subsets of R2 , i.e., the set that contains all compact
rectangles with real valued corner points,
• a probability measure Pθ on (X, F) which depends on the constant but unknown parameter
The random variable x = [x1 , x2 ]T can be expressed as
deterministic
random
or alternatively, which will be helpful for sub-problem d), as
deterministic
random
The random variable x follows a Gaussian distribution as x is an affine transformation of n and n is Gaussian.
It follows that x ∼ N(µx , Cx ) with [ expected value ]
Cx = E (x − µx )(x − µx )T = E nnT = C.
Thus, the likelihood function is given as
L(x; h) =
exp
(x − Sh)T C−1 (x − Sh) ,
(det (2πC))1/2
and the log-likelihood function (x; h) = log(L(x; h)) is given as
(x; h) = − log(det (2πC)) − (x − Sh)T C−1 (x − Sh).
The maximizer hˆ ML of the likelihood function, also maximizes the log-likelihood function as log( · )
is monotonically increasing, therefore it follows that
hˆ ML = argmax {L(x; h)}
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
= argmax { (x; h)}
= argmax − log(det (2πC)) − (x − Sh)T C−1 (x − Sh)
= argmax − (x − Sh)T C−1 (x − Sh)
= argmin (x − Sh)T C−1 (x − Sh) .
This is an unconstrained convex optimization problem (OP) as a convex function is minimized
without any constraints. Hence, the condition
(x − Sh)T C−1 (x − Sh) = −2ST C−1 (x − Sh) = 0
is necessary and sufficient for global optimality and we determine hˆ ML by
⇒ hˆ ML = (ST C−1 S)−1 ST C−1 x =
Remark 1: The ML estimator hˆ ML does not depend on the noise covariance matrix C as S = sI2 .
Remark 2: Without taking into account that the OP is convex, the second derivative given as
(x − Sh)T C−1 (x − Sh) = 2ST C−1 S
is necessary in order to assure that the stationary point actually is a minimizer. Note that a function
is convex if and only if its second derivative is non-negative. As C 0, the observation that
(x − Sh)T C−1 (x − Sh) ≥ 0
with equality if and only if x − Sh = 0 can be alternatively used to determine hˆ ML directly.
b) As n(1) , . . . , n(N) are uncorrelated and Gaussian, the observations x (1) , . . . , x (N) are independent and Gaussian with x (i) ∼ N(S(i) h, C). Thus, the likelihood function is given as
L(x , . . . , x ; h) =
(1)
(N)
L(x(i) ; h)
exp
(det (2πC))N/2
(x(i) − S(i) h)T C−1 (x(i) − S(i) h)
and the log-likelihood function is given as
(x , . . . , x ; h) = − log(det (2πC)) −
(1)
(x(i) − S(i) h)T C−1 (x(i) − S(i) h).
(N)
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
The ML estimator hˆ ML is given as the solution of
This again is an unconstrained convex OP as the objective function is a sum of convex functions
and therefore convex as well. Thus, the condition [ ... ]
is necessary and sufficient for global optimality and we determine hˆ ML as
('140503084231')
Remark: Without taking into account that the OP is convex, the second derivative given as [ ... ] as the sum of positive definite matrices is again positive definite, is necessary in order to assure that the stationary point actually is a minimizer.
c) If only one observation is taken into account, the estimator is given as [ ... ] which actually is the ML estimator if only one observation is available, cf. sub-problem a). 
The
ˆ (1) ) is unbiased as
ˆ (1) ) is not consistent as
The estimator h(x
x(1)
n(1)
(1)
(1)
(hs
s(1)
s(1)
s(1)
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
d) As n(1) , . . . , n(N) are correlated and jointly Gaussian, the observations x (1) , . . . , x (N) are dependent and jointly Gaussian. Thus, we formulate the joint PDF of all N observations. To this end,
the noise realizations are stacked into the vector n = [n1(1) , n2(1) , . . . , n1(N) , n2(N) ]T and the covariance
matrix of n is given by
For the observations x (1) , . . . , x (N) we obtain
x = [x1(1) , x2(1) , . . . , x1(N) , x2(N) ]T
with S = [S(1) , S(2) , . . . , S(N) ]T ∈ R2N×2 . The covariance of x is Cx = Cn . From this we obtain the
likelihood function
L(x ; h) =
exp − (x − S h)T C−1
x (x − S h) .
(det (2πCx ))
Following the solution of sub-problem a), we obtain the ML estimator as
hˆ ML = (S T C−1
e) As n(1) , . . . , n(N) are uncorrelated and Gaussian, the observations x (1) , . . . , x (N) are independent
and Gaussian with x (i) ∼ N(S(i) h, C). It is convenient to parametrize the likelihood function by
means of the inverse of the covariance matrix K = C−1 which is equivalent to a parametrization in
C. Therefore, the likelihood function is given as
L(x , . . . , x ; K) =
(1)
(N)
(det 2πK
exp −
(x(i) − S(i) h)T K(x(i) − S(i) h) .
and the log-likelihood function is given as
(x(i) − S(i) h)T K(x(i) − S(i) h)
(x , . . . , x ; K) = − log det 2πK −1 −
(i)
(i) T 
(i)
(i)
= log det
(x
h)(x
(1)
(N)
log det
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
with the, by assumption and for N ≥ 2 actually almost surely, positive definite matrix
(x(i) − S(i) h)(x(i) − S(i) h)T .

_t('140503090533')
Note that the [ trace ] tr {·} operator is linear and [ invariant against cyclic rotations ]

The objective function is the sum of − log det(K), which is a convex function in K 0, and tr KC , which is a linear function of K, and therefore convex. The constraint set is the convex cone of ˆ ML is convex. In order to solve this OP, positive definite matrices. Thus, the OP which determines K we search for a stationary point of the objective function
and observe that only one stationary point
exists, which actually is a stationary point of the unconstrained OP where the constraint K 0
has been dropped. As K ∗ is in the interior of the constraint set K 0 and the constrained OP is
ˆ ML = K ∗ = C −1 and that the corresponding ML estimator Cˆ ML for
convex, we can conclude that K
the covariance matrix C is given as
(x(i) − S(i) h)(x(i) − S(i) h)T .
f) The mean of each sample x (i) given as E x (i) = S(i) h is assumed to be known. For the true
covariance matrix C we have
C = E x (i) x (i),T − E x (i) E x (i)
= E x (i) x (i),T − S(i) hhT S(i),T ,
or equivalently
E x (i) x (i),T = C + E x (i) E x (i)
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
The ML estimator for the covariance matrix is unbiased as
Remark: When the mean S(i) h is unknown and needs to be estimated as well, the likelihood
function is given as L(x(1) , . . . , x(N) ; h, K). For hˆ ML , this leads to exactly the same result as derived
in sub-problem b). The ML estimator Cˆ ML for C in the case of an unknown mean is given as
(x(i) − S(i) hˆ ML )(x(i) − S(i) hˆ ML )T .
In this case, Cˆ ML is a biased estimator and an unbiased estimator for C is given by
(x(i) − S(i) hˆ ML )(x(i) − S(i) hˆ ML )T .
However, note that the ML estimator Cˆ ML is asymptotically unbiased.
c Associate Institute for Signal Processing
Technische Universit¨at M¨unchen
Statistical Signal Processing
B. Multivariate Derivatives
In Problem 2.1 we use the following multivariate derivative
∂(x − Sh)T C−1 (x − Sh)
= −2ST C−1 (x − Sh).
Here is one possible and very detailed derivation.
The gradient of a function f : RN → R is
For f = aT z =
an zn , we have

The differential [ the jacobian ] of a [ vector valued ] function f : RN → R M is

_v('140503201539')
A product rule: the differential of [ an (standard) inner product ] 

For f = zT Az we can identify u = Az and v = z and therefore
Now it is straight forward to see that
Exercise: Try to compute above result by finding applying an appropriate chain rule.
