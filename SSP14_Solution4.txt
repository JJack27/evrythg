4. Bayesian Estimation
Problem 4.1
Variance of a Gaussian Random Variable
a) In case some prior knowledge on the random parameter θ is available, this knowledge can be
used for designing an improved estimator. In case of the Bayes estimator, the prior knowledge is
given by a prior probability density function fθ (θ) of the parameter θ. An uniformly minimumvariance unbiased estimator (UMVUE) is an estimator T that is unbiased and has the lowest possible
variance for unbiased estimation. This means, that we have
Var [T ] ≤ Var [S] ,
where S is any other unbiased estimator, for all θ ∈ Θ. In case unbiased estimators with minimum
variance can not be found for all θ ∈ Θ, an UMVUE is not available. In this case, a reasonable
approach is to search for an estimator that minimizes the mean squared error (MSE) over the
value range of the parameter which is possible if a probability density is assigned to θ. The
conditional mean estimator minimizes the MSE, where expectation is taken with respect to fxθ (x, θ).
However, we assume that θ is an realization of the random variable θ drawn from fθ (θ)
which is not necessarily the true PDF. The conditional mean estimator is in general only MSE
optimal with respect to the assumed prior PDF fθ (θ) and not an UMVUE. If the assumed prior PDF
differs significantly from the true PDF of θ, this may have an impact on the performance of the
estimator.

Bayes’ rule for the posterior PDF 
![()](140518091016.png)

where we denote the posterior PDF by $\pi^{(N)}(\theta)$ for notational convenience.
c) As fx1 ,...,xN |θ (x1 , . . . , xN ) is a Gaussian PDF, fθ (θ) is the PDF of a Gamma distribution, and the
denominator is for normalization, we have that
1 α−1
πθ|x (θ) = γ √
exp 
(xi − µ)2  βα
θ exp (−βθ)
2 i=1
Γ(α)
( 2π)N
∝ θ 2 exp 
(xi − µ)2  θα−1 exp (−βθ)
2 i=1
∝ exp − β +
(xi − µ)  θ θ(α+ 2 )−1 ,
2 i=1
4. Bayesian Estimation
where γ is the normalization factor. We identify the posterior PDF as the PDF of a Gamma
distribution with the parameters
(xi − µ)2 ,
2 i=1
i.e., we have that θ|x1 , . . . xN ∼ Γ(α , β ).
d) The conditional mean estimator for the precision θ is given as the mean of the posterior PDF.
Thus, we have that
θˆCM = E[θ|x1 , . . . , xN ] =
which leads to the conditional mean estimator for the variance σ2 = θ−1 given as
ˆ 2CM
i=1 (xi
α + N2
− µ)2
The ML estimator for the variance assuming a known mean µ is, cf. problem 2.1f), given as
ˆ 2ML
(xi − µ)2 ,
which results in the conditional mean estimator
ˆ 2CM
2β + N σ
ˆ 2ML
2α + N
ˆ 2ML
The conditional mean estimator converges to the ML estimator σ
ˆ 2ML for a large number of observations N, i.e.,
ˆ 2ML
N→∞ 2α + 1
lim σ
ˆ 2CM = lim
ˆ 2ML .
The conditional mean estimator converges to the mean of the prior PDF β/α for a small number of
observations N, i.e,
2β + N σ
ˆ 2ML β
N→0
2α + N
lim σ
ˆ 2CM = lim
N→0
4. Bayesian Estimation
Problem 4.2
Mean of a Gaussian Random Variable
a) Following Bayes’ rule, the posterior PDF can be determined as
π(N)
(θ) := fθ|x1 ,...,xN (θ|x1 , . . . , xN ) =
fx1 ,...,xN |θ (x1 , . . . , xN ) fθ (θ)
fx1 ,...,xN ,ξ (x1 , . . . , xN , ξ)dξ
where π(N)
(θ) is introduced for notational convenience.
b) As fx1 ,...,xN |θ (x1 , . . . , xN ) is Gaussian, fθ (θ) is Gaussian, γ is a normalization factor, we have
 exp − 1 (θ − mθ )T C−1
(xi − θ)T C−1
π(N)
(θ) = γ exp −
x|θ (xi − θ)
θ (θ − mθ )
2 i=1
 1 N
T −1
∝ exp −
(xi − θ)T C−1
x|θ i
2 i=1
∝ exp −
T −1
T −1
(xTi C−1
x|θ xi − xi Cx|θ θ − θ Cx|θ xi
T −1
T −1
T −1
T −1
+ θT C−1
−1
−1
∝ exp − θ (NCx|θ + Cθ )θ − 2θ C−1
xi +
For a Gaussian PDF fx (x) with x ∼ N(µx , Cx ), we have
C−1
fx (x) ∝ exp − (x − mx )T C−1
x (x − mx )
T −1
∝ exp − xT C−1
x x − 2x Cx mx
Comparing the expression found for fx (x) and π(N)
(θ), we can observe that the posterior PDF π(N)
(θ)
is Gaussian with
−1
C(N),−1
≡ NC−1
C(N),−1
m(N)
≡ C−1
−1 ˆ
−1
xi + C−1
Thus, the conditional mean of the posterior PDF π(N)
(θ) is given as
−1
m(N)
= C(N)
C−1
with the covariance matrix of the posterior PDF given as
−1
C(N)
= NC−1
−1
4. Bayesian Estimation
c) Reformulating C(N)
using the provided hint gives
C(N)
 1
−1
−1
−1 
−1
−1
which are two different but equivalent expressions for C(N)
. They can be used to determine
−1
m(N)
= C(N)
C−1
−1
Cx|θ C−1
−1
−1
−1
Cθ C−1
The conditional mean estimator is given as the mean of the posterior PDF, i.e.,
θˆ CM = E(N)
[θ|x1 , . . . , xN ]
−1
−1
d) Compare lecture notes Section 6.3 Mean Estimation Example (Contd). For scalar random
variables, the result of the previous sub-problem simplifies to
−1
σ + σ2θ
θˆML + σ2x|θ
σ2x|θ
Nσ2θ
θˆML + 2
σx|θ + Nσ2θ
σx|θ + Nσ2θ
σ + σ2θ
θˆCM = σ2θ
−1
The conditional mean estimator is a convex combination of the ML estimator θˆML and the mean
of the prior PDF mθ . Therefore, θˆCM is in between mθ and θˆML . In case σ2θ 0 and an increasing
number of observations we belief more in the ML estimate θˆML and less in the prior mθ . In case
σ2θ = 0, the prior is correct and the estimate is mθ independent of the number of iterations N.
Contrary, in case σ2θ
σ2x|θ , the prior contains so much uncertainty that the ML estimate θˆML is
used. Given either a large number of observations N, a small conditional variance σ2x|θ , or a large
variance σ2θ of the a prior distribution of θ, it is recommended to rely on the ML estimator, since
lim θˆCM = lim θˆCM = lim θˆCM = θˆML .
σ2x|θ →0
σ2θ →∞
4. Bayesian Estimation
However, given either a small number of observations N, a large conditional variance σ2x|θ of
observations, or a small variance σ2θ of the a prior distribution of θ, we better rely on the mean of
the prior distribution, since
lim θˆCM = lim θˆCM = lim θˆCM = mθ .
N→0
σ2x|θ →∞
σ2θ →0
4. Bayesian Estimation
Problem 4.3
Gaussian Transmission Model
a) In order to formulate the conditional posterior PDF fθ|x (θ|x), let
H M×L I M×M
I L×L 0L×M
As θ and n are independent and Gaussian, z is Gaussian as well. The posterior density fθ|x (θ|x) is
again Gaussian, see provided hint, with expected value
Eθ|x [θ|x] = E θ + Cθx C−1
x (x − E [x])
and covariance matrix
Cθ|x = Cθ − Cθx C−1
From the transmission model, we have that
E [x] = E Hθ + n = Hµθ ,
Cθx = E (θ − µθ )(x − E [x])T = E (θ − µθ )(H(θ − µθ ) + n)T
= E (θ − µθ )(θ − µθ )T HT = Cθ HT = CTxθ ,
Cx = E (x − Hµθ )(x − Hµθ )T
= E (H(θ − µθ ) + n)(H(θ − µθ ) + n)T
= H E (θ − µθ )(θ − µθ )T HT + Cn = HCθ HT + Cn .
From this we obtain the conditional mean
Eθ|x [θ|x] = µθ + Cθ HT (HCθ HT + Cn )−1 (x − Hµθ )
and the covariance matrix
Cθ|x = Cθ − Cθ HT (HCθ HT + Cn )−1 HCθ
of the Gaussian posterior PDF.
The conditional mean estimator is given as the mean of the posterior PDF. Thus, we have
θˆ CM = Eθ|x [θ|x] = µθ + Cθ HT (HCθ HT + Cn )−1 (x − Hµθ ).
c) We consider x = y + n with the random mean y = Hθ, where θ ∼ N(µθ , Cθ ). The prior PDF
of the random mean y is y ∼ N(µy , Cy ) with µy = Hµθ and Cy = HCθ HT . y and x are jointly
Gaussian and for the posterior PDF of y given x we need
E [x] = E [y + n] = µy ,
Cyx = E (y − µy )(x − E [x])T = E (y − µy )((y − µy ) + n)T
4. Bayesian Estimation
= E (y − µy )(y − µy )T = Cy = CTxy ,
The mean of the posterior PDF is given as
E[y |x] = E [y ] + Cyx C−1
x (x − E [x])
−1
(x − Hµθ ).
This leads to the conditional mean estimator for the mean, cf. sub-problem 4.2b),
−1
−1
(x − Hµθ )
(x − Hµθ )
