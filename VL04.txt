Exact Solutions to MDPs
Institute for Data Processing
Technische Universität München
Part I
Recap
The RL Framework
Agent
state
reward
action
Environment
Agent and environment interact at discrete time steps,
State at t: st  S
Action at t: at  A(st )  A
Successor state at t + 1: st+1  S
Reward at state st+1 : |rt+1 | < ∞
Modelling RL as a stochastic process
RL as a stochastic process
Sequence of interactions {sk , ak , rk , sk +1 , ak +1 . . .} can be
modeled as a stochastic process
s1 → a1 → s2 → a2 → . . . → s∞
Deﬁnition
A state st is Markov if and only if
Markov Decision Processes
A Markov decision process is a tuple (S, A, P, R, γ)
S is a ﬁnite set of states
A is a ﬁnite set of actions
P is a state transition probability matrix, {pss = P(s |s, a)}
R is a reward function, rss := r(s, a) = E[r|s, a]  R
γ  [0, 1] is a discount factor
What is value function
Problem
Given a sequence of rewards after step t as
[ note different (better) indexing of states cf. 140430084615 ]
how to evaluate the states, in associated with the goal?
Main idea: to use the reward signal to formalize the idea of
a goal, i.e. evaluating states;
Solution: to deﬁne a value function V , which is a mapping
from states to real numbers, i.e.
The return
Hypothesis (the reward hypothesis)
That all of what we mean by goals and purposes can be well
thought of as the maximization of the cumulative sum of a
received reward signal.
Deﬁnition
The discounted return is deﬁned as
where γ  [0, 1]
Value Functions
Value function: the value of a state s for a given policy π
The optimal value of a state s
V ∗ (s) := max Eπ∗ 
γt rt+1 |s0 = s, a0 = a 
aA(s)
The Bellman equations
Proposition
For any policy π, the value function V π  RK satisﬁes
π(s, a)
The optimal value function V ∗  RK satisﬁes
V ∗ (s) = max
aA(s)
Two types of RL problems
Goals of RL
Prediction: given a policy π, to evaluate the future, i.e. the
value function V π
Control: to ﬁnd an optimal policy π∗ , in order to optimize
the future
Part II
Bellman operator
Bellman operator for policy π
Recall: the Bellman equation
π(s, a)
The Bellman operator T π : RK → RK , component-wise,
\todo{Is this a typo i.e. should it read $V(s) \mapsto \ldots$? 
Properties of the Bellman operator
Lemma
For any policy π, the Bellman operator T π is a contraction in
the max-norm, i.e. for any V1 , V2  RK ,
Theorem
For any policy π, the value function V π  RK is a ﬁxed point of
the Bellman operator T π .
Properties of the Bellman operator
Given two estimates of value function V1 , V2  RK , for all s  S,
we have
\todo{where does the 1st inequality come from?}
Hence,
Optimal Bellman operator
Recall: the optimal Bellman equation
V ∗ (s) = max
aA(s)
The optimal Bellman operator T ∗ : RK → RK ,
component-wise,
V (s ) → max
aA(s)
Properties of the optimal Bellman operator
Lemma
The optimal Bellman operator T ∗ is a contraction in the
max-norm, i.e. for any V1 , V2  RK ,
Theorem
The value function V ∗  RK is a ﬁxed point of the optimal
Bellman operator T ∗ .
Properties of the optimal Bellman operator
Given two estimates of value function V1 , V2  RK , for all s  S,
we have
T ∗ V1 (s) − T ∗ V2 (s) = max
aA(s)
− max
aA(s)
≤ γ max
aA(s)
Hence,
Part III
Prediction
Task of prediction
Policy evaluation
Given a policy π, to evaluate the value function V π (s).
State transition matrix
Let K be the cardinality of the state space, i.e., K = |S|.

Then, state transition matrix P deﬁnes transition probabilities from all states s to all successor states s ,
where each row of P sums to one.
[$P_{i, j}$ gives the probaltiy to go from  $s_j$ to $s_i$ ]
Known as [right stochastic matrix ] Markov matrix.

Bellman equation in matrix form
The Bellman expectation equations can be expressed as
where V  RK with one entry per state, i.e.
Solution: solve the linear equation for V  RK
Prediction method 1: closed form
The solution is
with IK − γP being invertible (why?)
\todo{Why}
Computational complexity is O(n3 ) for n states
[ matrix inversion via. Gauss-Jordan ]
Direct solution only possible for small MRPs
Closed form solution of prediction
Lemma
Every eigenvalue λi of a Markov matrix P  RK ×K satisﬁes
|λi | ≤ 1 for all i = 1, . . . , K .
Hence eigenvalues of (IK − γP π ) satisfy
λi (IK − γP π ) ≥ 1 − γ.
[ assume that the inequality is meant componentwise ]
For γ < 1, we have (IK − γP π ) is invertible.
\todo{Why}
Closed form solution
Eigenvalue of Markov matrices
Proof.
Let λ  C be an eigenvalue of M and x  Cm b a corresponding
eigenvector. Then Mx = λx. Let k be the index of the largest
entry of x, i.e. |xk | ≥ |xi | for all i = 1, . . . , n. Then
[ as $\sum\limits_j=1 A_{k, j} = 1$ ]
Hence |λ| ≤ 1.
Prediction method 2: convex optimization
The solution is given as
argmin (IK − γP) V − R
Convex optimization methods, such as gradient descent
method, conjugate gradient method, etc.
Prediction method 3: iterative policy evaluations
Fact
For any policy π, the Bellman operator T π is a contraction
in the max-norm, i.e. for any V1 , V2  RK ,
For any policy π, the value function V π  RK is a ﬁxed
point of the Bellman operator T π .
Iterative policy evaluation
Abstract of IPE
Apply the Bellman operator iteratively
a sweep
Pseudocode (Policy Evaluation)
For all s  S, iterate
aA(s)
π(s, a) rss + γ
until converge
Convergence properties of IPE
Calculation
Lemma
For a given policy π and an arbitrary V0  RK , the IPE
converges to the true value function V π .
Part IV
Control
Task of control
Policy search
To ﬁnd an optimal policy π∗ , in order to optimize the future
Motivation
Agent
state
reward
action
Environment
Idea
Given a policy π, we can solve the prediction task. Can we use
value functions to structure the search for good policies?
Policy improvement
Given a deterministic policy function π, instead of taking
a = π(s), we can improve the policy by acting greedily
πk +1 (s) = argmax
aA(s)
This is an improvement, π ≥ π, because for any state s,
Control method 1: policy iteration
Idea
We can combine policy evaluation and improvement to obtain a
sequence of monotonically improving policies and value
functions
→: policy evaluation
→: policy improvement (“greediﬁcation”)
Policy iteration (Pseudocode)
Pseudocode (PI)
Choose any initial policy π0 , iterate
Policy evaluation: iterate until converge
aA(s)
πk (s, a)
Policy improvement: compute
πk +1 (s) = argmax
aA(s)
until converge
Convergence of PI
Theorem
Policy iteration generates a sequence of policies with
increasing performance (V πk +1 ≥ V πk ) and it terminates in a
ﬁnite number of steps with the optimal policy π∗ .
The sequence of (V πk )k is non-decreasing and we have a ﬁnite
state and action space and therefore a ﬁnite number of possible
policies.
Policy iteration comments
In each step of policy iteration
Policy evaluation involves solving a set of linear equations
Policy improvement: straightforward
Each step of policy iteration is guaranteed to strictly
improve the policy at some state, when improvement is
possible
Control solution 2: value iteration
Fact
The optimal Bellman operator T ∗ is a contraction in the
max-norm, i.e. for any V1 , V2  RK ,
The optimal value function V ∗  RK is a ﬁxed point of the
optimal Bellman operator T ∗ .

Value iteration (Pseudocode)
Pseudocode (VI)
For all s  S, iterate
Vk +1 (s) = max rss + γ
aA(s) 
until converge
\todo{Shouldn't the reward $r_{s, s'}^a$ be weighted with the transition probability?}

Convergence properties of VI
Calculation
Lemma
For an arbitrary V0  RK , the VI
converges to the optimal value function V ∗ .
Asynchronous DP
All the DP methods described so far require exhaustive
sweeps of the entire state set, known as synchronous DP
Asynchronous DP does not use sweeps. Instead it works
Pick a state at random and apply the appropriate backup
Repeat until converge
Example: asynchronous PI, asynchronous VI
Still need lots of computation, but does not get locked into
hopelessly long sweeps
Generalized policy iteration
Generalized Policy Iteration
Generalized Policy Iteration (GPI): !
any interaction of policy evaluation and policy improvement, !
independent of their granularity.!
A geometric metaphor for!
convergence of GPI: !
R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction!
R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction
Take-home messages
Policy evaluation (PE): closed form solution, iterative policy
evaluation,
Policy improvement (PI): form a greedy policy, if only
locally
Policy iteration: alternate between PE and PI
Value iteration: intreating the optimal Bellman operator
Asynchronous DP: a way to avoid exhaustive sweeps
