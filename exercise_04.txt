TECHNISCHE UNIVERSITAT MUNCHEN
Fakult¨t f¨r Elektrotechnik und Informationstechnik
Lehrstuhl f¨r Datenverarbeitung
Prof. Dr.-Ing. K. Diepold

Data Analysis for Computer Engineering
Introduction to Reinforcement Learning

## Exercise MC & TD

due to 03.06.2014
1. Monte Carlo & Temporal Difference Learning (5 pts.)
Suppose you observe the following 12 episodes generated by an unknown Markov reward process, where A and B are states and the numbers are rewards:
A,0,B,1
A,0,B,0,A,2
A,0,B,0,A,0,B,1
B,0,A,2
B,0,A,0,B,1
B,0,A,0,B,0,A,2
(a) (2 Pt.) Give the values for states A and B that would be obtained by the batch ﬁrst-visit Monte-Carlo method using this data set (assuming no discounting). You may express your answer using fractions. Explain how you arrived at your answer.

[
###(a)
![()](140603183708.png)
    Make a table of returns $G_i(.)$ following the $1$st visit to the states $\{A, B\}$ and obtain the estimate of $V(.)$ as 
    $$\tilde{V}(.) = \frac{\sum\limits_i^I G_i(.)}{I}$$
    ]

(b) (2 pts) If you were to form a maximum-likelihood model of a Markov reward process on the basis of these episodes (and these episodes alone), what would it be (sketch its state-transition diagram)? Explain how you arrived at your answer.

[
### (b)
    Find unique tupels $(s, r, s')$  and count their occurences to arrive at a maximum likelihood estimate of the state transition probabilities in the Markov Reward Process
![()](140603183007.png)
    The estimates are
    $$P_{(s, r, s')}(A, 0, B) = \frac{7}{12} \; P_{(s, r, s')}(A, 2, T) = \frac{5}{12}$$
    where $T$ denotes the terminal state
    $$P_{(s, r, s')}(B, 0, A) = \frac{1}{2}  \; P_{(s, r, s')}(B, 1, T) = \frac{1}{2}$$
    ]

(c) (1 pt.) Give the values for states A and B that would be obtained by the batch TD method.
Explain how you arrived at your answer. You may express your answer using fractions.

[
!### (c)
Assume that the value functions  $V(s)$ obtained above are correct[()](140603182500.png)
 ]
