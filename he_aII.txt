Herbert Amann
Joachim Escher
Analysis II
Translated from the German
by Silvio Levy and Matthew Cargo
Birkhäuser
Basel · Boston · Berlin
Authors:
Herbert Amann
Institut für Mathematik
Universität Zürich
Winterthurerstr. 190
8057 Zürich
Switzerland
e-mail: herbert.amann@math.uzh.ch
Institut für Angewandte Mathematik
Universität Hannover
Welfengarten 1
30167 Hannover
Germany
e-mail: escher@ifam.uni-hannover.de
Originally published in German under the same title by Birkhäuser Verlag, Switzerland
© 1999 by Birkhäuser Verlag
2000 Mathematics Subject Classification: 26-01, 26A42, 26Bxx, 30-01
Library of Congress Control Number: 2008926303
Bibliographic information published by Die Deutsche Bibliothek
Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliografie; detailed
bibliographic data is available in the Internet at <http://dnb.ddb.de>.
ISBN 3-7643-7472-3 Birkhäuser Verlag, Basel – Boston – Berlin
This work is subject to copyright. All rights are reserved, whether the whole or part of the
material is concerned, specifically the rights of translation, reprinting, re-use of illustrations,
recitation, broadcasting, reproduction on microfilms or in other ways, and storage in data
banks. For any kind of use permission of the copyright owner must be obtained.
© 2008 Birkhäuser Verlag AG
P.O. Box 133, CH-4010 Basel, Switzerland
Part of Springer Science+Business Media
Printed on acid-free paper produced of chlorine-free pulp. TCF
Printed in Germany
ISBN 978-3-7643-7472-3
987654321
e-ISBN 978-3-7643-7478-5
www.birkhauser.ch
Foreword
As with the first, the second volume contains substantially more material than can
be covered in a one-semester course. Such courses may omit many beautiful and
well-grounded applications which connect broadly to many areas of mathematics.
We of course hope that students will pursue this material independently; teachers
may find it useful for undergraduate seminars.
For an overview of the material presented, consult the table of contents and
the chapter introductions. As before, we stress that doing the numerous exercises
is indispensable for understanding the subject matter, and they also round out
and amplify the main text.
In writing this volume, we are indebted to the help of many. We especially
thank our friends and colleages Pavol Quittner and Gieri Simonett. They have
not only meticulously reviewed the entire manuscript and assisted in weeding out
errors but also, through their valuable suggestions for improvement, contributed
essentially to the final version. We also extend great thanks to our staff for their
careful perusal of the entire manuscript and for tracking errata and inaccuracies.
Our most heartfelt thank extends again to our “typesetting perfectionist”,
without whose tireless effort this book would not look nearly so nice.1 We also
thank Andreas for helping resolve hardware and software problems.
Finally, we extend thanks to Thomas Hintermann and to Birkh¨user for the
good working relationship and their understanding of our desired deadlines.
Z¨ rich and Kassel, March 1999
1 The
H. Amann and J. Escher
text was set in L TEX, and the figures were created with CorelDRAW! and Maple.
Foreword
Foreword to the second edition
In this version, we have corrected errors, resolved imprecisions, and simplified
several proofs. These areas for improvement were brought to our attention by
readers. To them and to our colleagues H. Crauel, A. Ilchmann and G. Prokert,
we extend heartfelt thanks.
Z¨ rich and Hannover, December 2003
Foreword to the English translation
It is a pleasure to express our gratitude to Silvio Levy and Matt Cargo for their
careful and insightful translation of the original German text into English. Their
effective and pleasant cooperation during the process of translation is highly appreciated.
Z¨ rich and Hannover, March 2008
Contents
Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Chapter VI
Integral calculus in one variable
Continuous extensions . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Cauchy–Riemann Integral . . . . . . . . . . . . . . . . . . . . . . .
Properties of integrals . . . . . . . . . . . . . . . . . . . . . . . . . . .
Integration of sequences of functions . . .
The oriented integral . . . . . . . . . . . .
Positivity and monotony of integrals . . .
Componentwise integration . . . . . . . . .
The first fundamental theorem of calculus
The indefinite integral . . . . . . . . . . .
The mean value theorem for integrals . . .
The integral of staircase functions . . . . . . . . . . . . . . . . . . . . .
The integral of jump continuous functions . . . . . . . . . . . . . . . .
Riemann sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The extension of uniformly continuous functions . . . . . . . . . . . . .
Bounded linear operators . . . . . . . . . . . . . . . . . . . . . . . . . .
The continuous extension of bounded linear operators . . . . . . . . . .
Jump continuous functions . . . . . . . . . . . . . . . . . . . . . . . . .
Staircase and jump continuous functions . . . . . . . . . . . . . . . . .
A characterization of jump continuous functions . . . . . . . . . . . . .
The Banach space of jump continuous functions . . . . . . . . . . . . .
The technique of integration . . . . . . . . . . . . . . . . . . . . . . . .
Variable substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Integration by parts . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The integrals of rational functions . . . . . . . . . . . . . . . . . . . . .
viii
Contents
Sums and integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Bernoulli numbers . . . . . . .
Recursion formulas . . . . . . . . .
The Bernoulli polynomials . . . . .
The Euler–Maclaurin sum formula
Power sums . . . . . . . . . . . . .
Asymptotic equivalence . . . . . . .
The Riemann ζ function . . . . . .
The trapezoid rule . . . . . . . . .
Fourier series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The L2 scalar product . . . . . . . . . . . . .
Approximating in the quadratic mean . . . . .
Orthonormal systems . . . . . . . . . . . . . .
Integrating periodic functions . . . . . . . . .
Fourier coeﬃcients . . . . . . . . . . . . . . .
Classical Fourier series . . . . . . . . . . . . .
Bessel’s inequality . . . . . . . . . . . . . . . .
Complete orthonormal systems . . . . . . . .
Piecewise continuously differentiable functions
Uniform convergence . . . . . . . . . . . . . .
Improper integrals . . . . . . .
Admissible functions . . . . . .
Improper integrals . . . . . . . .
The integral comparison test for
Absolutely convergent integrals
The majorant criterion . . . . .
The gamma function . . . . . . . . . . . . . . . .
Euler’s integral representation . . . . . . . . . . .
The gamma function on C\(−N) . . . . . . . . .
Gauss’s representation formula . . . . . . . . . . .
The reﬂection formula . . . . . . . . . . . . . . .
The logarithmic convexity of the gamma function
Stirling’s formula . . . . . . . . . . . . . . . . . .
The Euler beta integral . . . . . . . . . . . . . . .
series
Chapter VII Multivariable differential calculus
Continuous linear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
The completeness of L(E, F ) . . . . . . .
Finite-dimensional Banach spaces . . . .
Matrix representations . . . . . . . . . .
The exponential map . . . . . . . . . . .
Linear differential equations . . . . . . .
Gronwall’s lemma . . . . . . . . . . . . .
The variation of constants formula . . .
Determinants and eigenvalues . . . . . .
Fundamental matrices . . . . . . . . . .
Second order linear differential equations
Multilinear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
Continuous multilinear maps . . . .
The canonical isomorphism . . . . .
Symmetric multilinear maps . . . .
The derivative of multilinear maps
Multivariable differentiation rules . . . . . . . . . . . . . . . . . . . . . 166
Linearity . . . . . . . . . . . . . . . . . . . . . . . . . .
The chain rule . . . . . . . . . . . . . . . . . . . . . . .
The product rule . . . . . . . . . . . . . . . . . . . . .
The mean value theorem . . . . . . . . . . . . . . . . .
The differentiability of limits of sequences of functions
Necessary condition for local extrema . . . . . . . . . .
Differentiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
The definition . . . . . . . . . . .
The derivative . . . . . . . . . . .
Directional derivatives . . . . . .
Partial derivatives . . . . . . . . .
The Jacobi matrix . . . . . . . . .
A differentiability criterion . . . .
The Riesz representation theorem
The gradient . . . . . . . . . . . .
Complex differentiability . . . . .
Higher derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
Definitions . . . . .
Higher order partial
The chain rule . . .
Taylor’s formula . .
derivatives
Functions of m variables . . . . . . . . . . . . . . . . . . . . . . . . . . 186
Suﬃcient criterion for local extrema . . . . . . . . . . . . . . . . . . . . 188
. . . . . . . . . . . 195
Nemytskii operators and the calculus of variations
Nemytskii operators . . . . . . . . . . . . . . . . . . .
The continuity of Nemytskii operators . . . . . . . .
The differentiability of Nemytskii operators . . . . .
The differentiability of parameter-dependent integrals
Variational problems . . . . . . . . . . . . . . . . . .
The Euler–Lagrange equation . . . . . . . . . . . . .
Classical mechanics . . . . . . . . . . . . . . . . . . .
Inverse maps
Implicit functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
Differentiable maps on product spaces
The implicit function theorem . . . . .
Regular values . . . . . . . . . . . . . .
Ordinary differential equations . . . . .
Separation of variables . . . . . . . . .
Lipschitz continuity and uniqueness . .
The Picard–Lindel¨f theorem . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
The derivative of the inverse of linear maps . . .
The inverse function theorem . . . . . . . . . . .
Diffeomorphisms . . . . . . . . . . . . . . . . . . .
The solvability of nonlinear systems of equations
Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Submanifolds of Rn . . . . . . . .
Graphs . . . . . . . . . . . . . . .
The regular value theorem . . . .
The immersion theorem . . . . .
Embeddings . . . . . . . . . . . .
Local charts and parametrizations
Change of charts . . . . . . . . .
10 Tangents and normals
. . . . . . . . . . . . . . . . . . . . . . . . . . . 260
The tangential in R . . . . . . . . . . .
The tangential space . . . . . . . . . . .
Characterization of the tangential space
Differentiable maps . . . . . . . . . . . .
The differential and the gradient . . . . .
Normals . . . . . . . . . . . . . . . . . .
Constrained extrema . . . . . . . . . . .
Applications of Lagrange multipliers . .
Chapter VIII Line integrals
Curves and their lengths . . . . . . . . . . . . . . . . . . . . . . . . . . 281
The total variation .
Rectifiable paths . .
Differentiable curves
Rectifiable curves . .
Curves in Rn
Line integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
The definition . . . . . . . . . . . . . . . .
Elementary properties . . . . . . . . . . .
The fundamental theorem of line integrals
Simply connected sets . . . . . . . . . . . .
The homotopy invariance of line integrals .
Pfaff forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
Vector fields and Pfaff forms . .
The canonical basis . . . . . . .
Exact forms and gradient fields
The Poincar´ lemma . . . . . .
Dual operators . . . . . . . . . .
Transformation rules . . . . . .
Modules . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
Unit tangent vectors . . . . . . . . . . . .
Parametrization by arc length . . . . . . .
Oriented bases . . . . . . . . . . . . . . . .
The Frenet n-frame . . . . . . . . . . . . .
Curvature of plane curves . . . . . . . . .
Identifying lines and circles . . . . . . . . .
Instantaneous circles along curves . . . . .
The vector product . . . . . . . . . . . . .
The curvature and torsion of space curves
Holomorphic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
Complex line integrals . . . .
Holomorphism . . . . . . . . .
The Cauchy integral theorem
The orientation of circles . . .
The Cauchy integral formula .
Analytic functions . . . . . . .
Liouville’s theorem . . . . . .
The Fresnel integral . . . . . .
The maximum principle . . .
xii
Harmonic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
Goursat’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
The Weierstrass convergence theorem . . . . . . . . . . . . . . . . . . . 356
Meromorphic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
The Laurent expansion . . . . . . . . . .
Removable singularities . . . . . . . . . .
Isolated singularities . . . . . . . . . . .
Simple poles . . . . . . . . . . . . . . . .
The winding number . . . . . . . . . . .
The continuity of the winding number .
The generalized Cauchy integral theorem
The residue theorem . . . . . . . . . . .
Fourier integrals . . . . . . . . . . . . . .
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
Chapter VI
Integration was invented for finding the area of shapes. This, of course, is an
ancient problem, and the basic strategy for solving it is equally old: divide the
shape into rectangles and add up their areas.
A mathematically satisfactory realization of this clear, intuitive idea is amazingly subtle. We note in particular that is a vast number of ways a given shape
can be approximated by a union of rectangles. It is not at all self-evident they all
lead to the same result. For this reason, we will not develop the rigorous theory
of measures until Volume III.
In this chapter, we will consider only the simpler case of determining the area
between the graph of a suﬃciently regular function of one variable and its axis.
By laying the groundwork for approximating a function by a juxtaposed series of
rectangles, we will see that this boils down to approaching the function by a series
of staircase functions, that is, functions that are piecewise constant. We will show
that this idea for approximations is extremely ﬂexible and is independent of its
original geometric motivation, and we will arrive at a concept of integration that
applies to a large class of vector-valued functions of a real variable.
To determine precisely the class of functions to which we can assign an integral, we must examine which functions can be approximated by staircase functions.
By studying the convergence under the supremum norm, that is, by asking if a
given function can be approximated uniformly on the entire interval by staircase
functions, we are led to the class of jump continuous functions. Section 1 is devoted
to studying this class.
There, we will see that an integral is a linear map on the vector space of
staircase functions. There is then the problem of extending integration to the
space of jump continuous functions; the extension should preserve the elementary
properties of this map, particularly linearity. This exercise turns out to be a special
case of the general problem of uniquely extending continuous maps. Because the
extension problem is of great importance and enters many areas of mathematics, we
VI Integral calculus in one variable
will discuss it at length in Section 2. From the fundamental extension theorem for
uniformly continuous maps, we will derive the theorem of continuous extensions
of continuous linear maps. This will give us an opportunity to introduce the
important concepts of bounded linear operators and the operator norm, which
play a fundamental role in modern analysis.
After this groundwork, we will introduce in Section 3 the integral of jump
continuous functions. This, the Cauchy–Riemann integral, extends the elementary integral of staircase functions. In the sections following, we will derive its
fundamental properties. Of great importance (and you can tell by the name) is
the fundamental theorem of calculus, which, to oversimplify, says that integration
reverses differentiation. Through this theorem, we will be able to explicitly calculate a great many integrals and develop a ﬂexible technique for integration. This
will happen in Section 5.
In the remaining sections — except for the eighth — we will explore applications of the so-far developed differential and integral calculus. Since these are not
essential for the overall structure of analysis, they can be skipped or merely sampled on first reading. However, they do contain many of the beautiful results of
classical mathematics, which are needed not only for one’s general mathematical
literacy but also for numerous applications, both inside and outside of mathematics.
Section 6 will explore the connection between integrals and sums. We derive
the Euler–Maclaurin sum formula and point out some of its consequences. Special
mention goes to the proof of the formulas of de Moivre and Sterling, which describe
the asymptotic behavior of the factorial function, and also to the derivation of
several fundamental properties of the famous Riemann ζ function. The latter is
important in connection to the asymptotic behavior of the distribution of prime
numbers, which, of course, we can go into only very brieﬂy.
In Section 7, we will revive the problem — mentioned at the end of Chapter V — of representing periodic functions by trigonometric series. With help from
the integral calculus, we can specify a complete solution of this problem for a
large class of functions. We place the corresponding theory of Fourier series in
the general framework of the theory of orthogonality and inner product spaces.
Thereby we achieve not only clarity and simplicity but also lay the foundation for
a number of concrete applications, many of which you can expect see elsewhere.
Naturally, we will also calculate some classical Fourier series explicitly, leading
to some surprising results. Among these is the formula of Euler, which gives an
explicit expression for the ζ function at even arguments; another is an interesting
expression for the sine as an infinite product.
Up to this point, we have will have concentrated on the integration of jump
continuous functions on compact intervals. In Section 8, we will further extend
the domain of integration to cover functions that are defined (and integrated)
on infinite intervals or are not bounded. We content ourselves here with simple
but important results which will be needed for other applications in this volume
VI Integral calculus in one variable
because, in Volume III, we will develop an even broader and more ﬂexible type of
integral, the Lebesgue integral.
Section 9 is devoted to the theory of the gamma function. This is one of
the most important nonelementary functions, and it comes up in many areas of
mathematics. Thus we have tried to collect all the essential results, and we hope
you will find them of value later. This section will show in a particularly nice way
the strength of the methods developed so far.
1 Jump continuous functions
In many concrete situations, particularly in the integral calculus, the constraint of
continuity turns out to be too restrictive. Discontinuous functions emerge naturally in many applications, although the discontinuity is generally not very pathological. In this section, we will learn about a simple class of maps which contains
the continuous functions and is especially useful in the integral calculus in one
independent variable. However, we will see later that the space of jump continuous functions is still too restrictive for a ﬂexible theory of integration, and, in the
context of multidimensional integration, we will have to extend the theory into an
even broader class containing the continuous functions.
In the following, suppose
• E := (E, · ) is a Banach space;
I := [α, β] is a compact perfect interval.
Staircase and jump continuous functions
We call Z := (α0 , . . . , αn ) a partition of I, if n ∈ N× and
α = α0 < α1 < · · · < αn = β .
If {α0 , . . . , αn } is a subset of the partition Z := (β0 , . . . , βk ), Z is called a refinement
of Z, and we write Z ≤ Z.
The function f : I → E is called a staircase function on I if I has a partition
Z := (α0 , . . . , αn ) such that f is constant on every (open) interval (αj−1 , αj ). Then
we say Z is a partition for f , or we say f is a staircase function on the partition Z.
A staircase function
If f : I → E is such that the limits f (α + 0), f (β − 0), and
f (x ± 0) :=
lim f (y)
y→x±0
exist for all x ∈ ˚ we call f jump continuous.1 A jump continuous function is
piecewise continuous if it has only finitely many discontinuities (“jumps”). Finally,
1 Note
that, in general, f (x + 0) and f (x − 0) may differ from f (x).
VI.1 Jump continuous functions
we denote by
T (I, E),
S(I, E),
SC(I, E)
the sets of all functions f : I → E that are staircase, jump continuous, and piecewise continuous, respectively.2
A piecewise continuous function
Not a jump continuous function
1.1 Remarks (a) Given partitions Z := (α0 , . . . , αn ) and Z := (β0 , . . . , βm ) of I,
the union {α0 , . . . , αn } ∪ {β0 , . . . , βm } will naturally define another partition Z ∨ Z
of I. Obviously, Z ≤ Z ∨ Z and Z ≤ Z ∨ Z. In fact, ≤ is an ordering on the set of
partitions of I, and Z ∨ Z is the largest from {Z, Z}.
(b) If f is a staircase function on a partition Z, every refinement of Z is also a
partition for f .
(c) If f : I → E is jump continuous, neither f (x + 0) nor f (x − 0) need equal f (x)
for x ∈ I.
(d) S(I, E) is a vector subspace of B(I, E).
Proof The linearity of one-sided limits implies immediately that S(I, E) is a vector
space. If f ∈ S(I, E)\B(I, E), we find a sequence (xn ) in I with
f (xn ) ≥ n
(1.1)
Because I is compact, there is a subsequence (xnk ) of (xn ) and x ∈ I such that xnk → x
as k → ∞. By choosing a suitable subsequence of (xnk ), we find a sequence (yn ),
that converges monotonically to x.3 If f is jump continuous, there is a v ∈ E with
lim f (yn ) = v and thus lim f (yn ) = v (compare with Example III.1.3(j)). Because
every convergent sequence is bounded, we have contradicted (1.1). Therefore S(I, E) ⊂
B(I, E).
(e) We have sequences of vector subspaces
T (I, E) ⊂ SC(I, E) ⊂ S(I, E)
and C(I, E) ⊂ SC(I, E) .
(f ) Every monotone function f : I → R is jump continuous.
2 We usually abbreviate T (I) := T (I, K) etc, if the context makes clear which of the fields R
or C we are dealing with.
3 Compare with Exercise II.6.3.
This follows from Proposition III.5.3.
(g) If f belongs to T (I, E), S(I, E), or SC(I, E), and J is a compact perfect
subinterval of I, then f |J belongs to T (J, E), S(J, E), or SC(J, E).
(h) If f belongs to T (I, E), S(I, E), or SC(I, E), then f
S(I, R), SC(I, R).
belongs to T (I, R),
A characterization of jump continuous functions
1.2 Theorem A function f : I → E is jump continuous if and only if there is a
sequence of staircase functions that converges uniformly to it.
Proof “= Suppose f ∈ S(I, E) and n ∈ N× . Then for every x ∈ I, there are
numbers α(x) and β(x) such that α(x) < x < β(x) and
f (s) − f (t) < 1/n for s, t ∈ α(x), x ∩ I
or s, t ∈ x, β(x) ∩ I .
Because α(x), β(x) ; x ∈ I is an open cover of the compact interval I, we can
find elements x0 < x1 < · · · < xm in I such that I ⊂ j=0 α(xj ), β(xj ) . Letting
η0 := α, ηj+1 := xj for j = 0, . . . , m, and ηm+2 := β, we let Z0 = (η0 , . . . , ηm+2 )
be a partition of I. Now we select a refinement Z1 = (ξ0 , . . . , ξk ) of Z0 with
f (s) − f (t) < 1/n for s, t ∈ (ξj−1 , ξj ) and j = 1, . . . , k ,
and set
fn (x) :=
f (x) ,
f (ξj−1 + ξj )/2 ,
x ∈ {ξ0 , . . . , ξk } ,
x ∈ (ξj−1 , ξj ) , j = 1, . . . , k .
Then fn is a staircase function, and by construction
f (x) − fn (x) < 1/n for x ∈ I .
Therefore f − fn
< 1/n.
“⇐ Suppose there is a sequence (fn ) in T (I, E) that converges uniformly to
f . The sequence also converges to f in B(I, E). Let ε > 0. Then there is an n ∈ N
such that f (x) − fn (x) < ε/2 for all x ∈ I. In addition, for every x ∈ (α, β]
there is an α ∈ [α, x) such that fn (s) = fn (t) for s, t ∈ (α , x). Consequently,
f (s) − f (t) ≤ f (s) − fn (s) + fn (s) − fn (t) + fn (t) − f (t) < ε
(1.2)
for s, t ∈ (α , x).
Suppose now (sj ) is a sequence in I that converges from the left to x. Then
there is an N ∈ N such that sj ∈ (α , x) for j ≥ N , and (1.2) implies
f (sj ) − f (sk ) < ε
Therefore f (sj ) j∈N is a Cauchy sequence in the Banach space E, and there is
an e ∈ E with limj f (sj ) = e. If (tk ) is another sequence in I that converges from
the left to x, then we can repeat the argument to show there is an e ∈ E such
that limk f (tk ) = e . Also, there is an M ≥ N such that tk ∈ (α , x) for k ≥ M .
Consequently, (1.2) gives
f (sj ) − f (tk ) < ε
After taking the limits j → ∞ and k → ∞, we find e − e ≤ ε. Now e and e
agree, because ε > 0 was arbitrary. Therefore we have proved that limy→x−0 f (y)
exists. By swapping left and right, we show that for x ∈ [α, β) the right-sided
limits limy→x+0 f (y) exist as well. Consequently f is jump continuous.
1.3 Remark If the function f ∈ S(I, R) is nonnegative, the first part of the above
proof shows there is a sequence of nonnegative staircase functions that converges
uniformly to f .
The Banach space of jump continuous functions
1.4 Theorem The set of jump continuous functions S(I, E) is a closed vector
subspace of B(I, E) and is itself a Banach space; T (I, E) is dense in S(I, E).
From Remark 1.1(d) and (e), we have the inclusions
T (I, E) ⊂ S(I, E) ⊂ B(I, E) .
According to Theorem 1.2, we have
T (I, E) = S(I, E) ,
(1.3)
when the closure is formed in B(I, E). Therefore S(I, E) is closed in B(I, E) by
Proposition III.2.12. The last part of the theorem follows from (1.3).
1.5 Corollary
(i) Every (piecewise) continuous function is the uniform limit of a sequence of
staircase functions.
(ii) The uniform limit of a sequence of jump continuous functions is jump continuous.
(iii) Every monotone function is the uniform limit of a sequence of staircase functions.
Proof The statement (i) follows directly from Theorem 1.2; (ii) follows from
Theorem 1.4, and statement (iii) follows from Remark 1.1(f).
Exercises
1 Verify that, with respect to pointwise multiplication, S(I, K) is a Banach algebra with
unity.
Define f : [−1, 1] → R by
f (x) :=
n+1 n
x=0.
Prove or disprove that
(a) f ∈ T [−1, 1], R ;
(b) f ∈ S [−1, 1], R .
Prove or disprove that SC(I, E) is a closed vector subspace of S(I, E).
Show these statements are equivalent for f : I → E:
(i) f ∈ S(I, E);
(ii) ∃ (fn ) in T (I, E) such that
< ∞ and f =
5 Prove that every jump continuous function has at most a countable number of discontinuities.
Denote by f : [0, 1] → R the Dirichlet function on [0, 1]. Does f belong to S [0, 1], R ?
Define f : [0, 1] → R by
1/n ,
x ∈ Q with x in lowest terms m/n ,
otherwise .
Prove or disprove that f ∈ S [0, 1], R .
Is f jump continuous?
sin(1/x) ,
x ∈ (0, 1] ,
Suppose Ej , j = 0, . . . , n are normed vector spaces and
f = (f0 , . . . , fn ) : I → E := E0 × · · · × En .
f ∈ S(I, E) ⇐ fj ∈ S(I, Ej ) ,
j = 0, . . . , n .
10 Suppose E and F are normed vector spaces and that f ∈ S(I, E) and ϕ : E → F
are uniformly continuous. Show that ϕ ◦ f ∈ S(I, F ).
Suppose f, g ∈ S(I, R) and im(g) ⊂ I. Prove or disprove that f ◦ g ∈ S(I, R).
2 Continuous extensions
In this section, we study the problem of continuously extending a uniformly continuous map onto an appropriate superset of its domain. We confine ourselves here
to when the domain is dense in that superset. In this situation, the continuous
extension is uniquely determined and can be approximated to arbitrary precision
by the original function. In the process, we will learn an approximation technique
that recurs throughout analysis.
The early parts of this section are of fundamental importance for the overall
continuous mathematics and have numerous applications, and so it is important
to understand the results well.
The extension of uniformly continuous functions
2.1 Theorem (extension theorem) Suppose Y and Z are metric spaces, and Z is
complete. Also suppose X is a dense subset of Y , and f : X → Z is uniformly
continuous.1 Then f has a uniquely determined extension f : Y → Z given by
f (y) = x→y f (x)
and f is also uniformly continuous.
Proof (i) We first verify uniqueness. Assume g, h ∈ C(Y, Z) are extensions of f .
Because X is dense in Y , there is for every y ∈ Y a sequence (xn ) in X such that
xn → y in Y . The continuity of g and h implies
g(y) = lim g(xn ) = lim f (xn ) = lim h(xn ) = h(y) .
Consequently, g = h.
(ii) If f is uniformly continuous, there is, for every ε > 0, a δ = δ(ε) > 0 such
that
(2.1)
d f (x), f (x ) < ε for x, x ∈ X and d(x, x ) < δ .
Suppose y ∈ Y and (xn ) is a sequence in X such that xn → y in Y . Then there is
an N ∈ N such that
(2.2)
d(xj , y) < δ/2 for j ≥ N ,
and it follows that
d(xj , xk ) ≤ d(xj , y) + d(y, xk ) < δ
From (2.1), we have
d f (xj ), f (xk ) < ε
1 As
usual, we endow X with the metric induced by Y .
VI.2 Continuous extensions
Therefore f (xj ) is a Cauchy sequence in Z. Because Z is complete, we can find
a z ∈ Z such that f (xj ) → z. If (xk ) is another sequence in X such that xk → y,
we reason as before that there exists a z ∈ Z such that f (xk ) → z . Moreover, we
can find M ≥ N such that d(xk , y) < δ/2 for k ≥ M . This, together with (2.2),
implies
d(xj , xk ) ≤ d(xj , y) + d(y, xk ) < δ for j, k ≥ M ,
and because of (2.1), we have
(2.3)
Taking the limits j → ∞ and k → ∞ in (2.3) results in d(z, z ) ≤ ε. This being
true every positive ε, we have z = z . These considerations show that the map
y → lim f (x)
is well defined, that is, it is independent of the chosen sequence.
For x ∈ X, we set xj := x for j ∈ N and find f (x) = limj f (xj ) = f (x).
Therefore f is an extension of f .
(iii) It remains to show that f is uniformly continuous. Let ε > 0, and choose
δ > 0 satisfying (2.1). Also choose y, z ∈ Y such that d(y, z) < δ/3. Then there
are series (yn ) and (zn ) in X such that yn → y and zn → z. Therefore, there is
an N ∈ N such that d(yn , y) < δ/3 and d(zn , z) < δ/3 for n ≥ N . In particular,
we get
d(yN , zN ) ≤ d(yN , y) + d(y, z) + d(z, zN ) < δ
and also
d(yn , yN ) ≤ d(yn , y) + d(y, yN ) < δ ,
d(zn , zN ) ≤ d(zn , z) + d(z, zN ) < δ
for n ≥ N . From the definition of f , Example III.1.3(l), and (2.1), we have
d f (y), f (z) ≤ d f (y), f (yN ) + d f (yN ), f (zN ) + d f (zN ), f (z)
= lim d f (yn ), f (yN ) + d f (yN ), f (zN ) + lim d f (zN ), f (zn )
< 3ε .
Therefore f is uniformly continuous.
2.2 Application Suppose X is a bounded subset of Kn . Then the restriction2
T : C(X) → BUC(X) ,
(2.4)
2 BUC(X) denotes the Banach space of bounded and uniformly continuous functions on X,
as established by the supremum norm. Compare to Exercise V.2.1.
is an isometric isomorphism.
Proof (i) Suppose u ∈ C(X). Then, because X is compact by the Heine–Borel theorem,
Corollary III.3.7 and Theorem III.3.13 imply that u — and therefore also T u = u | X — is
bounded and uniformly continuous. Therefore T is well defined. Obviously T is linear.
(ii) Suppose v ∈ BUC(X). Because X is dense in X, there is from Theorem 2.1 a
uniquely determined u ∈ C(X) such that u | X = v. Therefore T : C(X) → BUC(X) is
a vector space isomorphism.
(iii) For u ∈ C(X), we have
= sup |T u(x)| = sup |u(x)| ≤ sup |u(x)| = u
On the other hand, there is from Corollary III.3.8 a y ∈ X such that u
choose a sequence (xn ) in X such that xn → y and find
= |u(y)| = | lim u(xn )| = | lim T u(xn )| ≤ sup |T u(x)| = T u
= |u(y)|. We
This shows that T is an isometry.
Convention If X is a bounded open subset of Kn , we always identify BUC(X)
with C(X) through the isomorphism (2.4).
Bounded linear operators
Theorem 2.1 becomes particularly important for linear maps. We therefore compile
first a few properties of linear operators.
Suppose E and F are normed vector spaces, and A : E → F is linear. We
call A bounded3 if there is an α ≥ 0 such that
Ax ≤ α x
for x ∈ E .
(2.5)
We define
L(E, F ) :=
A ∈ Hom(E, F ) ; A is bounded
For every A ∈ L(E, F ), there is an α ≥ 0 for which (2.5) holds. Therefore
A := inf{ α ≥ 0 ;
is well defined. We call A
L(E,F )
Ax ≤ α x , x ∈ E }
:= A the operator norm of A.
3 For historical reasons, we accept here a certain inconsistency in the nomenclature: if F is
not the null vector space, there is (except for the zero operator) no bounded linear operator that
is a bounded map in the terms of Section II.3 (compare Exercise II.3.15). Here, a bounded linear
operator maps bounded sets to bounded sets (compare Conclusion 2.4(c)).
2.3 Proposition For A ∈ L(E, F ) we have 4
A = sup
= sup Ax = sup Ax = sup Ax .
x∈BE
x =1
x ≤1
The result follows from
A = inf{ α ≥ 0 ; Ax ≤ α x , x ∈ E }
≤ α, x ∈ E \{0}
= inf α ≥ 0 ;
; x ∈ E \{0}
Ay ≤ sup Ax .
y =1
For every x ∈ E such that 0 < x ≤ 1, we have the estimate
Ax ≤
Ax = A
Therefore we find
Ax ≤ sup
Ay .
Thus we have shown the theorem’s first three equalities.
For the last, let a := supx∈BE Ax and y ∈ B := BE . Then 0 < λ < 1 means
λy is in B. Thus the estimate Ay = A(λy) /λ ≤ a/λ holds for 0 < λ < 1, as
Ay ≤ a shows. Therefore
Ax ≤ sup Ax ≤ sup
Ax .
2.4 Conclusions (a) If A ∈ L(E, F ), then
Ax ≤ A
(b) Every A ∈ L(E, F ) is Lipschitz continuous and therefore also uniformly continuous.
Proof If x, y ∈ E, then Ax − Ay = A(x − y) ≤ A
continuous with Lipschitz constant A .
4 Here
x − y . Thus A is Lipschitz
and in similar settings, we will implicitly assume E = {0}.
(c) Let A ∈ Hom(E, F ). Then A belongs to L(E, F ) if and only if A maps bounded
sets to bounded sets.
Proof “= Suppose A ∈ L(E, F ) and B is bounded in E. Then there is a β > 0 such
that x ≤ β for all x ∈ B. It follows that
x ≤ A β
Therefore the image of B by A is bounded in F .
“⇐ Since the closed unit ball BE is closed in E, there is by assumption an α > 0
such that Ax ≤ α for x ∈ BE . Because y/ y ∈ BE for all y ∈ E \{0}, it follows that
Ay ≤ α y for all y ∈ E.
(d) L(E, F ) is a vector subspace of Hom(E, F ).
Let A, B ∈ L(E, F ) and λ ∈ K. For every x ∈ E, we have the estimates
(A + B)x = Ax + Bx ≤ Ax + Bx ≤
A + B
(2.6)
λAx = |λ| Ax ≤ |λ| A
(2.7)
Therefore A + B and λA also belong to L(E, F ).
(e) The map
A→ A
L(E, F ) → R+ ,
is a norm on L(E, F ).
Proof Because of (d), it suﬃces to check directly from the definition of a norm. First,
because A = 0, it follows from Proposition 2.3 that A = 0. Next choose x ∈ E such
that x ≤ 1. Then it follows from (2.6) and (2.7) that
(A + B)x ≤ A + B
λAx = |λ| Ax
and taking the supremum of x verifies the remaining parts of the definition.
(f ) Suppose G is a normed vector space and B ∈ L(E, F ) and also A ∈ L(F, G).
Then we have
AB ∈ L(E, G) and
AB ≤ A B .
This follows from
ABx ≤ A
Bx ≤ A
for x ∈ E ,
and the definition of the operator norm.
(g) L(E) := L(E, E) is a normed algebra5 with unity, that is, L(E) is an algebra
and 1E = 1 and also
AB ≤ A
for A, B ∈ L(E) .
The claim follows easily from Example I.12.11(c) and (f).
5 Compare
with the definition of a Banach algebra in Section V.4.
Convention In the following, we will always endow L(E, F ) with the operator
norm. Therefore
L(E, F ) := L(E, F ), ·
is a normed vector space with · := ·
L(E,F ) .
The next theorem shows that a linear map is bounded if and only if it is continuous.
2.5 Theorem Let A ∈ Hom(E, F ). These statements are equivalent:
(i) A is continuous.
(ii) A is continuous at 0.
(iii) A ∈ L(E, F ).
Proof “(i)=
⇒(ii)” is obvious, and “(iii)=
⇒(i)” was shown in Conclusions 2.4(b).
“(ii)=
⇒(iii)” By assumption, there is a δ > 0 such that Ay = Ay −A0 < 1
for all y ∈ B(0, δ). From this, it follows that
Ax =
sup A(δx) =
sup Ay ≤ ,
δ x ≤1
and therefore A is closed.
The continuous extension of bounded linear operators
2.6 Theorem Suppose E is a normed vector space, X is a dense vector subspace
in E, and F is a Banach space. Then, for every A ∈ L(X, F ) there is a uniquely
determined extension A ∈ L(E, F ) defined through
Ae = lim Ax for e ∈ E ,
x→e
and A
(2.8)
L(X,F ) .
Proof (i) According to Conclusions 2.4(b), A is uniformly continuous. Defining
f := A, Y := E, and Z := F , it follows from Theorem 2.1 that there exists a
uniquely determined extension of A ∈ C(E, F ) of A, which is given through (2.8).
(ii) We now show that A is linear. For that, suppose e, e ∈ E, λ ∈ K, and
(xn ) and (xn ) are sequences in X such that xn → e and xn → e in E. From the
linearity of A and the linearity of limits, we have
A(e + λe ) = lim A(xn + λxn ) = lim Axn + λ lim Axn = Ae + λAe .
Therefore A : E → F is linear. That A is continuous follows from Theorem 2.5,
because A belongs to L(E, F ).
(iii) Finally we prove that A and A have the same operator norm. From the
continuity of the norm (see Example III.1.3(j)) and from Conclusions 2.4(a), it
follows that
Ae = lim Axn = lim Axn ≤ lim A
xn = A
lim xn = A
for every e ∈ E and every sequence (xn ) in X such that xn → e in E. Consequently,
A ≤ A . Because A extends A, Proposition 2.3 implies
Ay ≥ sup
y <1
and we find A
Ax = sup
x <1
Ax = A ,
In the following exercises, E, Ej , F , and Fj are normed vector spaces.
Suppose A ∈ Hom(E, F ) is surjective. Show that
A−1 ∈ L(F, E) ⇐ ∃ α > 0 : α x ≤ Ax ,
If A also belongs to L(E, F ), show that A−1 ≥ A
−1
x∈E .
2 Suppose E and F are finite-dimensional and A ∈ L(E, F ) is bijective with a continuous inverse6 A−1 ∈ L(F, E). Show that if B ∈ L(E, F ) satisfies
A−B
< A−1
L(F,E)
then it is invertible.
3 A ∈ End(Kn ) has in the standard basis the matrix elements [ajk ]. For Ej := (Kn , |·|j )
with j = 1, 2, ∞, verify for A ∈ L(Ej ) that
(i) A L(E1 ) = maxk j |ajk |,
(ii)
(iii)
j,k |ajk |
L(E∞ ) = maxj
k |ajk |.
L(E2 )
Show that δ : B(Rn ) → R, f → f (0) belongs to L B(Rn ), R , and find δ .
Suppose Aj ∈ Hom(Ej , Fj ) for j = 0, 1 and
A0 ⊗ A1 : E0 × E1 → F0 × F1 ,
(e0 , e1 ) → (A0 e0 , A1 e1 ) .
Verify that
A0 ⊗ A1 ∈ L(E0 × E1 , F0 × F1 ) ⇐ Aj ∈ L(Ej , Fj ) ,
j = 0, 1 .
6 Suppose (An ) is a sequence in L(E, F ) converging to A, and suppose (xn ) is a sequence
in E whose limit is x. Prove that (An xn ) in F converges to Ax.
Show that ker(A) of A ∈ L(E, F ) is a closed vector subspace of E.
E is a finite-dimensional normed vector space, then Hom(E, F ) = L(E, F ) (see Theorem VII.1.6). Consequently, we need not assume that A, A−1 , and B are continuous.
6 If
VI.3 The Cauchy–Riemann Integral
3 The Cauchy–Riemann Integral
Determining the area of plane geometrical shapes is one of the oldest and most
prominent projects in mathematics. To compute the areas under graphs of real
functions, it is necessary to simplify and formalize the problem. It will help to gain
an intuition for integrals. To that end, we will first explain, as simply as possible,
how to integrate staircase functions; we will then extend the idea to jump continuous functions. These constructions are based essentially on the results in the
first two sections of this chapter. These
present the idea that the area of a graph
is made up of the sum of areas of rectangles that are themselves determined by
choosing the best approximation of the
graph to a staircase function. By the
“unlimited refinement” of the width of
the rectangles, we anticipate the sum of
their areas will converge to the area of
the figure.
In the following we denote by
• E := (E, |·|), a Banach space;
I := [α, β], a compact perfect interval.
The integral of staircase functions
Suppose f : I → E is a staircase function and Z := (α0 , . . . , αn ) is a partition of I
for f . Define ej through
f (x) = ej
for x ∈ (αj−1 , αj ) and j = 1, . . . , n ,
that is, ej is the value of f on the interval (αj−1 , αj ). We call
ej (αj − αj−1 )
the integral of f with respect to the partition Z. Obviously (Z) f is an element
of E. We note that the integral does not depend on the values of f at its jump
discontinuities. In the case that E = R, we can interpret |ej | (αj − αj−1 ) as the
area of a rectangle with sides of length |ej | and (αj − αj−1 ). Thus (Z) f represents
a weighted sum of rectangular areas, where the area |ej | (αj − αj−1 ) is weighted
by the sign of ej . In other words, those rectangles that rise above the x-axis are
weighted by 1, whereas those below are given −1.
The following lemma shows that
of partition Z.
f depends only on f and not on the choice
3.1 Lemma If f ∈ T (I, E), and Z and Z are partitions for f , then
(Z )
Proof (i) We first treat the case that Z := (α0 , . . . , αk , γ, αk+1 , . . . , αn ) has
exactly one more partition point than Z := (α0 , . . . , αn ). Then we compute
ej (αj − αj−1 ) + ek+1 (αk+1 − αk ) +
j=k+2
ej (αj − αj−1 ) + ek+1 (αk+1 − γ) + ek+1 (γ − αk ) +
(ii) If Z is an arbitrary refinement of Z, the claim follows by repeatedly
applying the argument in (i).
(iii) Finally, if Z and Z are arbitrary partitions of f , then Z ∨ Z , according
to Remark 1.1(a) and (b), is a refinement of both Z and Z and is therefore also a
partition for f . Then it follows from (ii) that
(Z∨Z )
Using Lemma 3.1, we can define for f ∈ T (I, E) the integral of f over I by
using an arbitrary partition Z for f . Obviously, the integral induces a map from
T (I, E) to E, namely,
: T (I, E) → E ,
which we naturally also call the integral. We explain next the first simple properties of this map.
3.2 Lemma If f ∈ T (I, E), then |f | ∈ T (I, R) and
Also
∞ (β
belongs to L T (I, E), E , and
Proof The first statement is clear. To prove the inequality, suppose f ∈ T (I, E)
and (α0 , . . . , αn ) is a partition for f . Then
ej (αj − αj−1 ) ≤
|ej | (αj − αj−1 ) =
≤ max |ej |
1≤j≤n
(αj − αj−1 ) ≤ sup |f (x)| (β − α) = (β − α) f
x∈I
The linearity of α follows from Remark 1.1.(e) and the definition of α . Conβ
≤ β − α. For the
sequently, α belongs to L T (I, E), E , and we have
constant function 1 ∈ T (I, R) with value 1, we have
claim follows.
1 = β − α, and the last
The integral of jump continuous functions
From Theorem 1.4, we know that the space of jump continuous functions S(I, E)
is a Banach space when it is endowed with the supremum norm and that T (I, E) is
a dense vector subspace of S(I, E) (see Remark 1.1(e)). If follows from Lemma 3.2
that the integral α is a continuous, linear map from T (I, E) to E. We can apply
the extension Theorem 2.6 to get a unique continuous linear extension of α into
the Banach space S(I, E). We denote this extension using the same notation, so
∈ L S(I, E), E .
From Theorem 2.6, it follows that
f = lim
fn in E
for f ∈ S(I, E) ,
where (fn ) is an arbitrary sequence of staircase functions that converges uniformly
to f . The element α f of E is called the (Cauchy–Riemann) integral of f or the
integral of f over I or the integral of f from α to β. We call f the integrand.
3.3 Remarks Suppose f ∈ S(I, E).
(a) According to Theorem 2.6 α f is well defined, that is, this element of E is
independent of the approximating sequence of staircase functions. In the special
case E = R, we interpret α fn as the weighted (or “oriented”) area of the graph
of fn in the interval I.
Because the graph of fn approximates f , we can view α fn as an approximation
to the “oriented” area of the graph of f in the interval I, and we can therefore
identify α f with this oriented area.
(b) There are a number of other notations for
f , namely,
(c) We have
f (x) dx ,
f ∈ E, with |
f dx| ≤ (β − α) f
This follows from Lemma 3.2 and Theorem 2.6.
Riemann sums
If Z := (α0 , . . . , αn ) is a partition of I, we call
:= max (αj − αj−1 )
the mesh of Z, and we call any element ξj ∈ [αj−1 , αj ] a between point. With this
terminology, we now prove an approximation result for α f .
3.4 Theorem
Suppose f ∈ S(I, E). There there is for every ε > 0 a δ > 0 such
f (ξj )(αj − αj−1 ) < ε
for every partition Z := (α0 , . . . , αn ) of I of mesh
the between point ξj .
< δ and for every value of
Proof (i) We treat first the staircase functions. Suppose then that f ∈ T (I, E)
and ε > 0. Also let Z := (α0 , . . . , αn ) be a partition of f , and let ej = f |(αj−1 , αj )
for 1 ≤ j ≤ n. We set δ := ε 4n f ∞ and choose a partition Z := (α0 , . . . , αn ) of
I such that Z < δ. We also choose between points ξj ∈ [αj−1 , αj ] for 1 ≤ j ≤ n.
For (β0 , . . . , βm ) := Z ∨ Z we have
f (ξj )(αj − αj−1 ) =
ei (αi − αi−1 ) −
f (ξj )(αj − αj−1 )
ek (βk − βk−1 ) −
ek (βk − βk−1 )
(3.1)
(ek − ek )(βk − βk−1 ) ,
and we specify ek and ek as
ξ ∈ (βk−1 , βk ) ,
if [βk−1 , βk ] ⊂ [αj−1 , αj ] .
ek := f (ξ) ,
ek := f (ξj ) ,
Obviously, ek = ek can only hold on the partition points {α0 , . . . , αn }. Therefore
the last sum in (3.1) has at most 2n terms that do not vanish. For each of these,
we have
|(ek − ek )(βk − βk−1 )| ≤ 2 f ∞ Z < 2 f ∞ δ .
From (3.1) and the value of δ, we therefore get
f (ξj )(αj − αj−1 ) < 2n · 2 f
(ii) Suppose now that f ∈ S(I, E) and ε > 0. Then, according to Theorem 1.4, there is a g ∈ T (I, E) such that
(3.2)
3(β − α)
From Remark 3.3(c), we have
(f − g) ≤ f − g
(β − α) < ε/3 .
(3.3)
From (i), there is a δ > 0 such that for every partition Z := (α0 , . . . , αn ) of I with
Z < δ and every value ξj ∈ [αj−1 , αj ], we have
g(ξj )(αj − αj−1 ) < ε/3 .
(3.4)
The estimate
g(ξj ) − f (ξj ) (αj − αj−1 ) ≤ f − g
(β − α) < ε/3
(3.5)
is obvious. Altogether, from (3.3)–(3.5), we get
(f − g) dx +
g(ξj )(αj − αj−1 )
g(ξj ) − f (ξj ) (αj − αj−1 ) < ε ,
and the claim is proved.
3.5 Remarks (a) A function f : I → E is said to Riemann integrable if there is
an e ∈ E with the property that for every ε > 0 there is a δ > 0, such that
e−
for every partition Z := (α0 , . . . , αn ) of I with Z < δ and for every set of between
points ξj ∈ [αj−1 , αj ].
If f is Riemann integrable, the value e is called the Riemann integral of f
over I, and we will again denote it using the notation α f dx. Consequently, Theorem 3.4 may be stated as, Every jump continuous function is Riemann integrable,
and the Cauchy–Riemann integral coincides with the Riemann integral.
(b) One can show S(I, E) is a proper subset of the Riemann integrable functions.1
Consequently, the Riemann integral is a generalization of the Cauchy–Riemann
integral.
We content ourselves here with the Cauchy–Riemann integral and will not
seek to augment the set of integrable functions, as there is no particular need for
it now. However, in Volume III, we will introduce an even more general integral,
the Lebesgue integral, which satisfies all the needs of rigorous analysis.
(c) Suppose f : I → E, and Z := (α0 , . . . , αn ) is a partition of I with between
points ξj ∈ [αj−1 , αj ]. We call
f (ξj )(αj − αj−1 ) ∈ E
1 The follows for example from that a bounded function is Riemann integrable if and only if
the set of its discontinuities vanishes in the Lebesgue measure (see Theorem X.5.6 in Volume III).
the Riemann sum. If f is Riemann integrable, then
f (ξj )(αj − αj−1 ) ,
f dx = lim
Z →0
expresses its integral symbolically.
Define [·] to be the ﬂoor function. For n ∈ N× , compute the integrals
[nx2 ]
(iv)
sign x dx .
Compute
Suppose F is a Banach space and A ∈ L(E, F ). Then show for f ∈ S(I, E) that
f for the function f of Exercise 1.2 and also
f for the f of Exercise 1.7.
Af := x → A(f (x)) ∈ S(I, F )
Af .
4 For f ∈ S(I, E), there is, according to Exercise 1.4, a sequence (fn ) of jump continuous
functions such that n fn ∞ < ∞ and f = n fn . Show that I f = n I fn .
Prove that for every f ∈ S(I, R), f + and f − also belong to S(I, R), and
6 Show that if f : I → E is Riemann integrable, then f ∈ B(I, E), that is, every
Riemann integrable function is bounded.
Suppose f ∈ B(I, R) and Z = (α0 , . . . , αn ) is a partition of I. Define
S(f, I, Z) :=
sup f (ξ) ; ξ ∈ [αj−1 , αj ] (αj − αj−1 )
inf f (ξ) ; ξ ∈ [αj−1 , αj ] (αj − αj−1 )
and call them the upper and lower sum of f over I with respect to the partition Z.
Prove that
(i) if Z is a refinement of Z, then
S(f, I, Z ) ≤ S(f, I, Z) ,
S(f, I, Z) ≤ S(f, I, Z ) ;
(ii) if Z and Z are partitions of I, then
S(f, I, Z) ≤ S(f, I, Z ) .
8 Suppose f ∈ B(I, R) and Z := (β0 , . . . , βm ) is a refinement of Z := (α0 , . . . , αn ).
Show that
S(f, I, Z) − S(f, I, Z ) ≤ 2(m − n) f ∞ ΔZ ,
S(f, I, Z ) − S(f, I, Z) ≤ 2(m − n) f
Let f ∈ B(I, R). From Exercise 7(ii), we know the following exist in R:
f := inf S(f, I, Z) ; Z is a partition of I
f := sup S(f, I, Z) ; Z is a partition of I
−I
We call I f the over Riemann(–Darboux) integral of f over I; likewise we call
under Riemann integral.
f the
(ii) for every ε > 0 there is a δ > 0 such that for every partition Z of I with ΔZ < δ,
we have the inequalities
0 ≤ S(f, I, Z) −
0≤
f − S(f, I, Z) < ε .
(Hint: There is a partition Z of I such that S(f, I, Z ) <
partition of I. From Exercise 8, it follows that
S(f, I, Z) ≤ S(f, I, Z ∨ Z ) + 2m f
f + ε/2. Let Z be an arbitrary
In addition, Exercise 7(i) gives S(f, I, Z ∨ Z ) ≤ S(f, I, Z ).)
Show these statements are equivalent for f ∈ B(I, R):
(i) f is Riemann integrable.
(iii) For every ε > 0 there is a partition Z of I such that S(f, I, Z) − S(f, I, Z) < ε.
(Hint: “(ii)⇐
⇒(iii)” follows from Exercise 9.
“(i)=
⇒(iii)” Suppose ε > 0 and e := I f . Then there is a δ > 0 such that
f (ξj )(αj − αj−1 ) < e +
for every partition Z = (α0 , . . . , αn ) of I with ΔZ < δ. Consequently, S(f, I, Z) ≤ e + ε/2
and S(f, I, Z) ≥ e − ε/2.)
VI.4 Properties of integrals
4 Properties of integrals
In this section, we delve into the most important properties of integrals and, in
particular, prove the fundamental theorem of calculus. This theorem says that
every continuous function on an interval has a primitive function (called the antiderivative) which gives another form of its integral. This antiderivative is known
in many cases and, when so, allows for the easy, explicit evaluation of the integral.
As in the preceding sections, we suppose
• E = (E, |·|) is a Banach space; I = [α, β] is a compact perfect interval.
Integration of sequences of functions
The definition of integrals leads easily to the following convergence theorem, whose
theoretical and practical meaning will become clear.
4.1 Proposition (of the integration of sequences and sums of functions) Suppose
(fn ) is a sequence of jump continuous functions.
(i) If (fn ) converges uniformly to f , then f is jump continuous and
(ii) If
fn converges uniformly, then
lim fn =
fn is jump continuous and
Proof From Theorem 1.4 we know that the space S(I, E) is complete when endowed with the supremum norm. Both parts of the theorem then follow from the
facts that α is a linear map from S(I, E) to E and that the uniform convergence
agrees with the convergence in S(I, E).
4.2 Remark The statement of Proposition 4.1 is false when the sequence (fn ) is
only pointwise convergent.
We set I = [0, 1], E := R, and
x=0,
⎪ 0,
x ∈ (0, 1/n) ,
⎩ 0,
x ∈ [1/n, 1] ,
for n ∈ N× . Then fn is in T (I, R), and (fn ) converges
pointwise, but not uniformly, to 0. Because I fn = 1
for n ∈ N× and I 0 = 0, the claim follows.
For the first application of Proposition 4.1, we prove that the important
statement of Lemma 3.2 — about interchanging the norm with the integral — is
also correct for jump continuous functions.
4.3 Proposition For f ∈ S(I, E) we have |f | ∈ S(I, R) and
|f | dx ≤ (β − α) f
Proof According to Theorem 1.4, there is a sequence (fn ) in T (I, E) that converges uniformly to f . Further, it follows from the inverse triangle inequality that
|fn (x)|E − |f (x)|E ≤ |fn (x) − f (x)|E ≤ fn − f
for x ∈ I .
Therefore |fn | converges uniformly to |f |. Because every |fn | belongs to T (I, R),
it follows again from Theorem 1.4 that |f | ∈ S(I, R).
From Lemma 3.2, we get
|fn | dx ≤ (β − α) fn
(4.1)
Because (fn ) converges uniformly to f and (|fn |) converges likewise to |f |, we can,
with the help of Proposition 4.1, pass the limit n → ∞ in (4.1). The desired
inequality follows.
The oriented integral
If f ∈ S(I, E) and γ, δ ∈ I, we define1
f (x) dx :=
(4.2)
and call γ f “the integral of f from γ to δ”. We call γ the lower limit and δ the
upper limit of the integral of f , even when γ > δ. According to Remark 1.1(g),
the integral of f from γ to δ is well defined through (4.2), and
1 Here
and hence, we write simply
(4.3)
f | J, if J is a compact perfect subinterval of I.
4.4 Proposition (of the additivity of integrals) For f ∈ S(I, E) and a, b, c ∈ I we
have
Proof It suﬃces to check this for a ≤ b ≤ c. If (fn ) is a sequence of staircase
functions that converge uniformly to f and J is a compact perfect subinterval of
I, then
fn |J ∈ T (J, E)
and fn |J
uniformly
(4.4)
The definition of the integral of staircase functions gives at once that
(4.5)
Using (4.4) and Proposition 4.1, we pass the limit n → ∞ and find
Positivity and monotony of integrals
Until now, we have considered the integral of jump continuous functions taking
values in arbitrary Banach spaces. For the following theorem and its corollary, we
will restrict to the real-valued case, where the ordering of the reals implies some
additional properties of the integral.
4.5 Theorem For f ∈ S(I, R) such that f (x) ≥ 0 for all x ∈ I, we have
f ≥ 0.
Proof According to Remark 1.3, there is a sequence (fn ) of nonnegative staircase
functions that converges uniformly to f . Obviously then, fn ≥ 0, and it follows
that f = limn fn ≥ 0.
4.6 Corollary
have α f ≤
Suppose f, g ∈ S(I, R) satisfy f (x) ≤ g(x) for all x ∈ I. Then we
g, that is, integration is monotone on real-valued functions.
This follows from the linearity of integrals and from Proposition 4.5.
4.7 Remarks (a) Suppose V is a vector space. We call any linear map from V
to its field is a linear form or linear functional.
Therefore in the scalar case, that is, E = K, the integral α is a continuous
linear form on S(I).
(b) Suppose V is a real vector space and P is a nonempty set with
(x ∈ P, λ ∈ R+ ) = λx ∈ P ,
x, −x ∈ P = x = 0.
In other words, P satisfies P + P ⊂ P , R+ P ⊂ P and P ∩ (−P ) = {0}. We then
call P a (in fact convex) cone in V (with point at 0). This designation is justified
because P is convex and, for every x, it contains the “half ray” R+ x. In addition,
P contains “straight lines” Rx only when x = 0.
We next define ≤ through
(4.6)
and thus get an ordering on V , which is linear on V (or compatible with the vector
space structure of V ), that is, for x, y, z ∈ V ,
(x ≤ y, λ ∈ R+ ) = λx ≤ λy .
Further, we call (V, ≤) an ordered vector space, and ≤ is the ordering induced
by P . Obviously, we have
P = {x ∈ V ; x ≥ 0} .
(4.7)
Therefore we call P the positive cone of (V, ≤).
If, conversely, a linear ordering ≤ is imposed on V , then (4.7) defines convex
cone in V , and this cone induces the given ordering. Hence there is a bijection
between the set of convex cones on V and the set of linear orderings on V . Thus
we can write either (V, ≤) or (V, P ) whenever P is a positive cone in V .
(c) When (V, P ) is an ordered vector space and is a linear form on V , we call
positive if (x) ≥ 0 for all x ∈ P .2 A linear form on V is then positive if and only
if it is increasing3.
Suppose : V → R is linear. Because (x−y) = (x)− (y), the claim is obvious.
(d) Suppose E is a normed vector space (or a Banach space) and ≤ is a linear
ordering on E. Then we say E := (E, ≤) is an ordered normed vector space (or
an ordered Banach space) if its positive cone is closed.
2 Note
3 We
that the null form is positive under this definition.
may also say increasing forms are monotone linear forms.
Suppose E is an ordered normed vector space and (xj ) and (yj ) are sequences
in E with xj → x and yj → y. Then xj ≤ yj for almost all j ∈ N, and it follows
that x ≤ y.
Proof If P is a positive cone in E, then yj − xj ∈ P for almost all j ∈ N. Then it follows
from Proposition II.2.2 and Remark II.3.1(b) that
y − x = lim yj − lim xj = lim(yj − xj ) ∈ P ,
and hence P is closed.
(e) Suppose X is a nonempty set. The pointwise ordering of Example I.4.4(c),
namely,
f ≤ g :⇐ f (x) ≤ g(x) for x ∈ X
and f, g ∈ RX makes RX an ordered vector space. We call this ordering the natural
ordering on RX . In turn, it induces on every subset M of RX the natural ordering
on M (see Example I.4.4(a)). Unless stated otherwise, we shall henceforth always
provide RX and any of its vector subspaces the natural ordering. In particular,
B(X, R) is an ordered Banach space with the positive cone
B + (X) := B(X, R+ ) := B(X, R) ∩ (R+ )X .
Therefore every closed vector subspace F(X, R) of B(X, R) is an ordered Banach
space whose positive cone is given through
F+ (X) := F(X, R) ∩ B + (X) = F(X, R) ∩ (R+ )X .
It is obvious that B + (X) is closed in B(X, R).
(f ) From (e) it follows that S(I, R) is an ordered Banach space with positive cone
S + (I), and Proposition 4.5 says the integral α is a continuous positive (and
therefore monotone) linear form on S(I, R).
The staircase function f : [0, 2] → R defined by
x=1,
otherwise
obviously satisfies f = 0. Hence there is a nonnegative function that does
not identically vanish but whose integral does vanish. The next theorem gives a
criterion for the strict positivity of integrals of nonnegative functions.
4.8 Proposition
f (a) > 0, then
Suppose f ∈ S + (I) and a ∈ I. If f is continuous at a with
f >0.
Proof (i) Suppose a belongs to I. Then continuity of f at a and f (a) > 0 implies
there exists a δ > 0 such that [a − δ, a + δ] ⊂ I and
|f (x) − f (a)| ≤
f (a) for x ∈ [a − δ, a + δ] .
Therefore we have
f (x) ≥
From f ≥ 0 and Proposition 4.5, we get
Proposition 4.4 and Corollary 4.6 imply
a−δ
a+δ
f ≥ 0 and
f (a)
f ≥ 0. Now
1 = δf (a) > 0 .
(ii) When a ∈ ∂I, the same conclusion follows an analogous argument.
Componentwise integration
4.9 Proposition The map f = (f 1 , . . . , f n ) : I → Kn is jump continuous if and
only if it is so for each component function f j . Also,
f 1, . . . ,
Proof From Theorem 1.2, f is jump continuous if and only if there is a sequence
(fk ) of staircase function that uniformly converge to it. It is easy to see that the
last holds if and only if for every j ∈ {1, . . . , n}, the sequence (fk )k∈N converges
uniformly to f . Because f belongs to S(I, K) for every j, Theorem 1.2 implies
the first claim. The second part is obvious for staircase functions and follows here
because Proposition 4.1 also holds when taking limits for f ∈ S(I, Kn ).
4.10 Corollary Suppose g, h ∈ RI and f := g + i h. Then f is jump continuous if
and only if both g and h are. In this case,
For f ∈ S(I, E) we use the integral to define the map
F: I→E ,
f (ξ) dξ ,
whose properties we now investigate.
(4.8)
4.11 Theorem For f ∈ S(I, E), we have
|F (x) − F (y)| ≤ f
|x − y| for x, y ∈ I .
Therefore the integral is a Lipschitz continuous function on the entire interval.
From Proposition 4.4, we have
F (x) − F (y) =
f (ξ) dξ −
f (ξ) dξ =
f (ξ) dξ
for x, y ∈ I .
The claim now follows immediately from Proposition 4.3.
Our next theorem says F is differentiable where f is continuous.
4.12 Theorem (of the differentiability in the entire interval) Suppose f ∈ S(I, E)
is continuous at all a ∈ I. Then F is differentiable at a and F (a) = f (a).
For h ∈ R× such that a + h ∈ I, we have
F (a + h) − F (a)
We note hf (a) =
f (ξ) dξ .
f (a) dξ, so that
F (a + h) − F (a) − f (a)h
f (ξ) − f (a) dξ .
Because f is continuous at a, there is for every ε > 0 a δ > 0 such that
|f (ξ) − f (a)|E < ε for ξ ∈ I ∩ (a − δ, a + δ) .
Using Proposition 4.3, we have the estimate
f (ξ) − f (a)
for h such that 0 < |h| < δ and a + h ∈ I, Therefore F is differentiable at a and
F (a) = f (a).
From the last result, we get a simpler, but also important conclusion.
4.13 The second fundamental theorem of calculus Every f ∈ C(I, E) has an
antiderivative, and if F is such an antiderivative,
F (x) = F (α) +
Proof We set G(x) := α f (ξ) dξ for x ∈ I. According to Theorem 4.12, G is
differentiable with G = f . Therefore G is an antiderivative of f .
Suppose F is an antiderivative of f . From Remark V.3.7(a) we know that
there is a c ∈ E such that F = G + c. Because G(α) = 0, we have c = F (α).
4.14 Corollary For every antiderivative F of f ∈ C(I, E), we have
f (x) dx = F (β) − F (α) =: F |β .
We say F |β is “F evaluated between α and β”.
The indefinite integral
If the antiderivative of f is known, Corollary 4.14 reduces the problem of calcuβ
lating the integral α f to the trivial difference F (β) − F (α). The corollary also
shows that, in some sense, integration is the inverse of differentiation.
Although the fundamental theorems of calculus guarantee an antiderivative
for every f ∈ C(I, E), it is not possible in the general case to find it explicitly.
To calculate an integral according to Corollary 4.14, we must provide, by some
method, its antiderivative. Obviously, we can get a basic supply of antiderivatives
by differentiating known differentiable functions. Using the results of Chapter IV
and V, we will build a list of important antiderivatives, which we will give following
Examples 4.15. In the next sections, we will see how using some simple rules and
transformations yields an even larger class of antiderivatives. It should also be
pointed out that there are extensive tables listing thousands of antiderivatives
(see for example [Ape96], [GR81] or [BBM86]).
Instead of painstakingly working out integrals on paper, it is much simpler to
use software packages such as Maple or Mathematica, which can do “symbolic”
integration. These programs “know” many antiderivatives and can solve many
integrals by manipulating their integrands using a set of elementary rules.
For f ∈ C(I, E) we call
in the interval of f ’s antiderivative, the indefinite integral of f . This is a symbolic
notation, in that it omits the interval I (although the context should make it
clear), and it suggests that the antiderivative is only determined up to an additive
constant c ∈ E. In other words: f dx+c is the equivalence class of antiderivatives
of f , where two antiderivatives are equivalent if they only differ by a constant. In
the following, when we only want to talk about this equivalence class, we will drop
the constant and write simply f dx.
4.15 Examples (a) In the following list of basic indefinite integrals, a and c are
complex numbers and x is a real variable in I. Here, I must lie in the domain of
definition of f but is otherwise arbitrary.
a = −1 ,
a>0,
eax ,
a∈C
a=1,
tan x
−1/ sin x
1 1 − x2
a / log a
sin x
1/(1 + x )
arcsin x
arctan x
The validity of this list follows from Examples IV.1.13 and the use of IV.2.10.
(b) Suppose a =
ak X k ∈ K[[X]] with radius of convergence ρ > 0. Then we
ak xk dx =
1/ cos2 x
eax /a
/(a + 1)
xk+1
This follows from Remark V.3.7(b).
(c) Suppose f ∈ C 1 (I, R) such that f (x) = 0 for all x ∈ I. Then we have
Suppose f (x) > 0 for all x ∈ I. From the chain rule it follows that4
(log |f |) = (log f ) = f /f .
If f (x) < 0 for x ∈ I, we have in analogous fashion that
(log |f |) = log(−f )
= (−f ) /(−f ) = f /f .
Therefore log |f | is an antiderivative of f /f .
The mean value theorem for integrals
Here, we determine a mean value theorem for the integrals of real-valued continuous functions.
4 We
call f /f the logarithmic derivative of f .
4.16 Proposition Suppose f, ϕ ∈ C(I, R) and ϕ ≥ 0. Then there is an ξ ∈ I such
f (x)ϕ(x) dx = f (ξ)
ϕ(x) dx .
(4.9)
Proof If ϕ(x) = 0 for all x ∈ I, then (4.9) obviously holds for every ξ ∈ I.
Thus we can assume ϕ(x) > 0 for some x ∈ I. Then Proposition 4.8 implies the
inequality α ϕ(x) dx > 0.
Letting m := minI f and M := maxI f , we have mϕ ≤ f ϕ ≤ M ϕ because
ϕ ≥ 0. Then the linearity and monotony of integrals implies the inequalities
The choice of m and M and the intermediate value theorem (Theorem III.5.1)
immediately proves the theorem.
4.17 Corollary For f ∈ C(I, R) there is a ξ ∈ I such that
f dx = f (ξ)(β − α).
Set ϕ := 1 in Proposition 4.16.
We note that — in contrast to the mean value theorem of derivatives (Theorem IV.2.4) — the point ξ need not lie in the interior of the interval.
We illustrate Corollary 4.17 with the following figures:
The point ξ is selected so that the function’s oriented
area in the interval I agrees with the oriented contents
f (ξ)(β − α) of the rectangle with sides |f (ξ)| and (β − α).
Prove the statement of Remark 4.7(b).
For f ∈ S(I), show
3 The two piecewise continuous functions f1 , f2 : I → E differ only at their discontinuβ
ities. Show that α f1 = α f2 .
For f ∈ S(I, K) and p ∈ [1, ∞) suppose
|f (x)|p dx
and let p := p/(p − 1) denote p’s dual exponent (with 1/0 = ∞).
Prove the following:
(i) For f, g ∈ S(I, K), the H¨lder inequality holds, that is,
(ii) For f, g ∈ S(I, K), the Minkowski inequality holds, that is,
(iii) C(I, K), ·
is a normed vector space.
(iv) C(I, K), ·
is an inner product space.
(v) For f ∈ C(I, K) and 1 ≤ p ≤ q ≤ ∞, we have
≤ (β − α)1/p−1/q f
(Hint: (i) and (ii) Consult the proof of Applications IV.2.16(b) and (c). (v) Use (i).)
5 Show that if q ≥ p, the norm ·
not equivalent when p = q
on C(I, K) is stronger5 than · p . These norms are
6 Show that C(I, K), · p is not complete for 1 ≤ p < ∞. Therefore C(I, K), ·
is not a Banach space for 1 ≤ p < ∞, and C(I, K), · 2 is not a Hilbert space.
Suppose f ∈ C 1 (I, R) has f (α) = f (β) = 0. Show that
f 2 + (f )2 dx .
(4.10)
How must (4.10) be modified when f ∈ C 1 (I, R) but only f (α) = 0?
(Hint: Suppose x0 ∈ I has f 2 (x0 ) = f 2 . Show that f 2 (x0 ) = α 0 f f dx −
Then apply Exercise I.10.10.)
Suppose f ∈ C 1 (I, K) has f (α) = 0. Show that
|f |2 dx .
5 If ·
1 and · 2 are norms on a vector space E, we say · 1 is stronger than · 2 if there is
a constant K ≥ 1 such that x 2 ≤ K x 1 for all x ∈ E. We say weaker in the opposite case.
(Hint: If F (x) :=
|f (ξ)| dξ then |f (x)| ≤ F (x) and
The function f ∈ C 2 (I, R) satisfies f ≤ f
(Hint: Let x0 ∈ I such that f (x0 ) = f
2F F dx = F 2 (β).)
and f (α) = f (β) = 0. Show that
0 ≤ max f (x) ≤ √
Then show f (x0 ) ≤ 0, so that f = 0.
If f (x0 ) > 0, there exists a ξ ∈ (x0 , β) such that f (x) > 0 for x ∈ [x0 , ξ) and
f (ξ) = 0 (see Theorem IV.2.3). Then consider x0 (f f − f f ) dx and apply Exercise 7.)
10 Prove the second mean value theorem of integrals: Suppose f ∈ C(I, R) and that
g ∈ C 1 (I, R) is monotone.6 Then there is a ξ ∈ I such that
f (x)g(x) dx = g(α)
f (x) dx + g(β)
(Hint: Letting F (x) :=
f (x) dx .
f (s) ds, show that
f (x)g(x) dx = F g
F (x)g (x) dx .
Proposition 4.16 can be used for the integral on the right side.)
Suppose a > 0 and f ∈ C [−a, a], E . Prove that
(i) if f is odd, then
(ii) if f is even, then
f (x) dx = 0;
−a
f (x) dx = 2 0
f (x) dx.
Define F : [0, 1] → R through
F (x) :=
x2 sin(1/x2 ) ,
(i) F is differentiable,
(ii) F ∈ S(I, R).
That is, there are functions that are not jump continuous but nevertheless have an
antiderivative. (Hint for (ii): Remark 1.1(d).)
Suppose f ∈ C [a, b], R and g, h ∈ C 1 [α, β], [a, b] . Further suppose
h(x)
g(x)
Show that F ∈ C 1 [α, β], R . How about F ?
6 One can show that the second mean value theorem for integrals remains true when g is only
assumed to be monotone and therefore generally not regular.
Suppose n ∈ N× and a1 , . . . , an ∈ R. Prove that there is an α ∈ (0, π) such that
ak cos(kα) = 0 .
(Hint: Use Corollary 4.17 with f (x) :=
ak cos(kx) and I := [0, π].)
(n + 1)2
(2n − 1)2
(i) lim n
np−1
(ii) lim
for p, q ∈ N and q < p.
(Hint: Consider the appropriate Riemann sum for f (x) := 1/(1 + x)2 on [0, 1] or, for (ii),
f (x) := 1/x on [q, p].)
Suppose f ∈ C(I × I, E) and
g: I →E ,
f (x, y) dy .
Show that g ∈ C(I, E).
5 The technique of integration
Through the fundamental theorem of calculus, rules for taking derivatives become corresponding rules for integration. In this section, we will carry out this
conversion for the chain rule and the product rule, and so attain the important
substitution rule and the method of integration by parts.
In this section we denote by
• I = [α, β] a compact perfect interval.
Variable substitution
5.1 Theorem (substitution rule) Suppose E is a Banach space, f ∈ C(I, E), and
suppose ϕ ∈ C 1 [a, b], R satisfies −∞ < a < b < ∞ and ϕ [a, b] ⊂ I. Then
ϕ(b)
f ϕ(x) ϕ (x) dx =
f (y) dy .
ϕ(a)
Proof The fundamental theorem of calculus says that f has an antiderivative
F ∈ C 1 (I, E). From the chain rule (Theorem IV.1.7), there then exists a function
F ◦ ϕ ∈ C 1 [a, b], E such that
(F ◦ ϕ) (x) = F ϕ(x) ϕ (x) = f ϕ(x) ϕ (x)
for x ∈ [a, b] .
Then Corollary 4.14 gives
f ϕ(x) ϕ (x) dx = F ◦ ϕ
= F ϕ(b) − F ϕ(a)
At this point, we should explain the meaning of the symbol “dx” which,
although present in the notation and obviously marks the integration variable,
has otherwise not been clarified. In Section VIII.3, we will give a more formal
definition, which we will then justify and make precise. Here we will be content
with a heuristic observation.
5.2 Remarks (a) Suppose ϕ : I → R is differentiable. We call1 dϕ := ϕ dx the
differential of ϕ. If we choose ϕ to be the identity function idI , then dϕ = 1 dx.
1 The notation dϕ = ϕ dx is here only a formal expression in which the function is understood
to be differentiable. In particular, ϕ dx is not a product of independent objects.
VI.5 The technique of integration
It is obvious that the symbol 1 dx can also be written as dx. Therefore dx is the
differential of the identity map x → x.
(b) In practice, the substitution rule is written in the compact form
(c) Suppose ϕ : I → R is
differentiable and x0 ∈ I.
In the following heuristic approach, we will explore ϕ in
an “infinitesimal” neighborhood of x0 . We then regard
dx as an increment between
x0 and x, hence as a real variable in itself.
Suppose tx0 is tangent to ϕ at the point x0 , ϕ(x0 ) :
tx 0 : R → R ,
x → ϕ(x0 ) + ϕ (x0 )(x − x0 ) .
In addition, define Δϕ(x0 ) := ϕ(x0 + dx) − ϕ(x0 ) as the increment in ϕ, and
define d ϕ(x0 ) := ϕ (x0 ) dx as the increment along the tangent tx0 . From the
differentiability of ϕ, we have
Δϕ(x0 ) = d ϕ(x0 ) + o(dx)
as dx → 0 .
For small increments dx, Δϕ(x0 ) can be approximated through the “linear approximation” dϕ(x0 ) = ϕ (x0 ) dx.
5.3 Examples
(a) For a ∈ R× and b ∈ R, we have
cos(ax + b) dx =
sin(aβ + b) − sin(aα + b) .
Proof The variable substitution y(x) := ax + b for x ∈ R gives dy = a dx. Therefore, it
follows from Example 4.15(a) that
aβ+b
aα+b
(b) For n ∈ N× , we have
xn−1 sin(xn ) dx =
(1 − cos 1) .
Here we set y(x) := xn . Then dy = nxn−1 dx and thus
sin y dy = −
Integration by parts
The second fundamental integration technique, namely, integration by parts, follows from the product rule.
5.4 Proposition For u, v ∈ C 1 (I, K), we have
(5.1)
Proof The claim follows directly from the product rule (uv) = u v + v u and
Corollary 4.14.
In differentials, (5.1) assumes the practical form
5.5 Examples
(5.2)
x sin x dx = sin β − sin α − β cos β + α cos α.
We set u(x) := x and v := sin. Then u = 1 and v = − cos. Therefore
x sin x dx = −x cos x
cos x dx = (sin x − x cos x)
(b) (the area of a circle) A circle of radius R has area πR2 .
Proof We may place the origin of a rectangular coordinate system at the center. We
can express points with radius at most R through the closed set
(x, y) ∈ R2 ; |(x, y)| ≤ R
(x, y) ∈ R2 ; x2 + y 2 ≤ R2
We can also express KR as the union of the (upper) half disc
(x, y) ∈ R2 ; x2 + y 2 ≤ R2 , y ≥ 0
with the corresponding lower half −HR . They intersect at
HR ∩ (−HR ) = [−R, R] × {0} =
(x, 0) ∈ R2 ; −R ≤ x ≤ R
We can write the upper boundary
of HR as the graph of the function
R 2 − x2 ,
and the area of HR , according to our
earlier interpretation, as
R2 − x2 dx .
Here it is irrelevant whether the “lower boundary” [−R, R] × {0} is included in HR ,
because the area of a rectangle with width 0 is itself 0 by definition. By symmetry2 , the
area AR of the disc KR is simply double the area of the half disc HR . Therefore
AR = 2
To evaluate this integral, it is convenient to adopt polar coordinates. Therefore we set
x(α) := R cos α for α ∈ [0, π]. Then we have dx = −R sin α dα, and the substitution rule
gives
AR = −2R
= 2R2
R2 − R2 cos2 α sin α dα
1 − cos2 α sin α dα = 2R2
sin2 α dα .
We integrate 0 sin2 α dα by parts. Putting u := sin and v := sin, we have u = cos and
v = − cos. Therefore
sin2 α dα = − sin α cos α
cos2 α dα
and we find
(1 − sin2 α) dα = π −
sin2 α dα ,
sin2 α dα = π/2. Altogether we have AR = R2 π.
(c) For n ∈ N suppose In :=
sinn x dx. We then get the recursion formula3
nIn = (n − 1)In−2 − cos x sinn−1 x
for n ≥ 2 ,
(5.3)
with I0 = X and I1 = − cos.
Obviously, I0 = X + c and I1 = − cos +c. If n ≥ 2, then
In =
sinn−2 x (1 − cos2 x) dx = In−2 −
sinn−2 x cos x cos x dx .
2 In this example we consider the absolute area and not the oriented area, and we forgo a
precise definition of area, using instead heuristically evident arguments. In Volume III, we will
delve into this concept and related issues and justify these heuristic arguments.
3 Note the agreement with Example 4.15.
By setting u(x) := cos x and v (x) := sinn−2 x cos x, we easily find that u = − sin and
v = sinn−1 /(n − 1). Integration by parts then finishes the proof.
(d) (the Wallis product) This says4
4k 2
= lim
4k 2 − 1 n→∞
4k 2 − 1
2 2 4 4 6 6
1 3 3 5 5 7
2k − 1 2k + 1
We get the claimed product by evaluating (5.3) on the interval [0, π/2]. We set
π/2
sinn x dx
An :=
From (5.3), it follows that
A0 = π/2 ,
A1 = 1 ,
An =
n−1
An−2
for n ≥ 2 .
A simple induction argument gives
A2n =
(2n − 1)(2n − 3) · · · · · 3 · 1 π
2n(2n − 2) · · · · · 4 · 2
A2n+1 =
(2n + 1)(2n − 1) · · · · · 5 · 3
From this follows the relations
2n · 2n(2n − 2)(2n − 2) · · · · · 4 · 4 · 2 · 2
A2n+1
(2n + 1)(2n − 1) (2n − 1)(2n − 3) · · · · · [5 · 3][3 · 1] π
(2k)2
(2k)2 − 1
A2n+2
2n + 1
n→∞ 2n + 2
(5.4)
(5.5)
Because sin2 x ≤ sin x ≤ 1 for x ∈ [0, π/2], we have A2n+2 ≤ A2n+1 ≤ A2n . Therefore
≤1
and the claim is a consequence of (5.4) and (5.5).
4 Suppose (a ) is a sequence in K and p :=
k=0 ak for n ∈ N. If the sequence (pn ) converges,
we call its limit the infinite product of ak and write
ak := lim
ak .
The integrals of rational functions
By the elementary functions we mean all the maps generated by taking polynomials, exponential, sines and cosines and applying to them the four fundamental
operations of addition, subtraction, multiplication, and division, together with
functional composition and inversion. Altogether, these operations generate a
vast set of functions.
5.6 Remarks (a) The class of elementary functions is “closed under differentiation”, that is, the derivative of an elementary function is also elementary.
Proof This follows from Theorems IV.1.6–8 as well as from Examples IV.1.13, Application IV.2.10 and Exercise IV.2.5.
(b) Antiderivatives of elementary functions are unfortunately not generally elementary. In other words, the class of elementary functions is not closed under
integration.
For every a ∈ (0, 1), the function
f : [0, a] → R ,
x→1
1 − x4
is continuous. Therefore
1 − y4
for 0 ≤ x < 1 ,
is a well-defined antiderivative of f . Because f (x) > 0 for x ∈ (0, 1), F is strictly
increasing. Thus according to Theorem III.5.7, F has a well-defined inverse function G.
It is known5 that there is a subset M of C that is countable and has no limit points,
such that G has a uniquely determined analytic continuation G on C \ M . It is also
known that G is doubly periodic, that is, there are two R-linearly independent periods
ω1 , ω2 ∈ C such that
G(z + ω1 ) = G(z + ω2 ) = G(z)
Because elementary functions are at most “simply periodic”, G cannot be elementary.
Therefore F is also not elementary. Hence the elementary function f has no elementary
antiderivative.
In the following we will show that rational functions have elementary antiderivatives. We begin with simple examples, which can then be extended to the
general case.
5 Proving this and the following statement about the periodicity of G widely surpasses the
scope of this text (see for example Chapter V of [FB95]).
5.7 Examples
(a) For a ∈ C\I, we have
X −a
log |X − a| + c ,
log(X − a) + c ,
a∈R,
a ∈ C\R .
This follows from Examples 4.15(c) and IV.1.13(e).
(b) Suppose a ∈ C\I and k ∈ N with k ≥ 2. Then
(X − a)
(k − 1)(X − a)k−1
(c) Suppose a, b ∈ R with D := a2 − b < 0. Then
X +a
arctan √
+ 2aX + b
First,
q(X) := X 2 + 2aX + b = (X + a)2 − D = |D| 1 +
Then, by defining y := |D|−1/2 (x + a), the substitution rule gives
y(β)
y(α)
arctan
1 + y2
(d) For a, b ∈ R with D := a2 − b < 0, we have
= log(X 2 + 2aX + b) − √
In the notation of the proof of (c), we find
1 2X + 2a
and the claim quickly follows from Example 4.15(c) and (c).
(e) For a, b ∈ R with D := a2 − b = 0 and −a ∈ I, we have
X 2 + 2aX + b
Because q = (X + a)2 , the claim follows from (b).
(f ) Suppose a, b ∈ R and D := a2 − b > 0. For −a ±
D ∈ I, we have
X +a− D
X +a+ D
Proof The quadratic polynomial q has real zeros z1 := −a +
Therefore q = (X − z1 )(X − z2 ). This suggests the ansatz
X − z1
X − z2
D and z2 := −a −
(5.6)
By multiplying this equation by q, we find
1 = (a1 + a2 )X − (a1 z2 + a2 z1 ) .
Therefore, from the identity theorem for polynomials (see Remark I.8.19(c)) and by
comparing coeﬃcients, we have a1 = −a2 = 1/2 D. The claim then follows from (a).
In the last examples, we integrated the rational function 1/(X 2 + 2aX + b)
using the decomposition (5.6), thereby reducing it to two simpler rational functions
1/(X − z1 ) and 1/(X − z2 ). Such a “partial fraction decomposition” is possible
for every rational function, as we will now show.
A polynomial is said to be normalized if its highest coeﬃcient is 1. By the
fundamental theorem of algebra, every normalized polynomial q of positive degree
has a factor decomposition
(X − zj )mj ,
(5.7)
in which z1 , . . . , zn are the zeros of q in C and m1 , . . . , mn are the corresponding
multiplicities. Therefore
mj = deg(q)
(see Example III.3.9(b) and Remark I.8.19(b)).
Suppose now p, q ∈ C[X] with q = 0. Then according to Proposition I.8.15,
it follows by polynomial division that there are uniquely determined s, t ∈ C[X]
such that
= s + , where deg(t) < deg(q) .
(5.8)
Thus, in view of Example 4.15(b), we can restrict our proof to the elementary
integrability of the rational function r := p/q for deg(p) < deg(q), and we can also
assume that q is normalized. The basis of the proof is then the following theorem
about the partial fraction expansion.
5.8 Proposition Suppose p, q ∈ C[X] with deg(p) < deg(q), and q is normalized.
Then, with the notation of (5.7), we have
j=1 k=1
(X − zj )k
(5.9)
with uniquely determined ajk ∈ C.
We make the ansatz
(X − z1 )
(5.10)
where a ∈ C, p1 ∈ C[X], and
q1 :=
By multiplying (5.10) by q, we get
(X − zj )mj + (X − z1 )p1 ,
(5.11)
from which we read off6
(z1 − zj )mj .
a = p(z1 )
(5.12)
Therefore a1m1 := a is uniquely determined. From (5.11) follows
mj ∨ deg(p) ≤ deg(q) − 1 = deg(q1 ) .
deg(p1 ) <
Next we can apply the above argument to p1 /q1 and so get the claim after finitely
many steps.
5.9 Corollary Every rational function has an elementary integral.
This follows immediately from (5.8), (5.9), and Examples 5.7(a) and (b).
5.10 Remarks (a) If all the zeros of the denominator are simple, the partial
fraction expansion (5.9) becomes
6 As
p(zj )
q (zj ) X − zj
usual, the “empty product” for n = 1 is given the value 1.
(5.13)
In the general case, one makes an ansatz of the form (5.9) with undetermined
coeﬃcients ajk and then determines the coeﬃcients after multiplying through by q
(see the proof of Example 5.7(f)).
The statement (5.13) follows easily from (5.12).
(b) Suppose s ∈ R[X]. Every zero z ∈ C of s has a complex conjugate zero z with
the same multiplicity.
From Proposition I.11.3, s(z) = s(z).
(c) Suppose p, q ∈ R[X] and z ∈ C\R is a zero of q with multiplicity m. Further
suppose ak and bk for 1 ≤ k ≤ m are the coeﬃcients of (X − z)−k and (X − z)−k ,
respectively, in the expansion (5.9). Then bk = ak .
Proof From (b) we know that z is also a zero of q with multiplicity m. Therefore bk is
uniquely determined. For x ∈ C\{z1 , . . . , zn }, it follows from (5.9) that
p(x)
= (x) =
(x − zj )k
q(x)
(x − z j )k
Now the claim follows from the uniqueness result of Proposition 5.8.
Suppose now that r := p/q for p, q ∈ R[X] with deg(p) < deg(q) is a real
rational function. If z ∈ C\R is a zero of q with multiplicity m, then, by invoking
the partial fraction expansion (5.9), we get, according to Remark 5.10(c), the
summand
(a + a)X − (za + za)
Re(a)X − Re(za)
=2 2
(X − z)(X − z)
X − 2 Re(z)X + |z|2
and we have D := (Re z)2 − |z|2 < 0. Then Examples 5.7(c) and (d) show that
= A log x − 2 Re(z)x + |z|
x − Re z
+ B arctan √
(5.14)
for x√ I, where the coeﬃcients A and B follow uniquely from Re a, Re z, Re(za)
and −D.
For 2 ≤ k ≤ m, it follows from Remark 5.10(c) and Example 5.7(b) that
(x − z)k
k − 1 (x − z)k−1
(x − z)k−1
−2 Re b(x − z)k−1
(k − 1) x2 − 2 Re(z)x + |z|2
k−1
(5.15)
for x ∈ I. Therefore from Proposition 5.8 as well as (5.14) and (5.15), we get the
antiderivative in real form.
The exercises give concrete examples of integrating by using partial fraction
expansions.
Calculate the indefinite integrals
a2 x2 + bx + c
x4 + 1
− 2x + 3
x2 dx
x5 dx
x2 + x + 1
(ax2 + bx + c)2
(vi)
By clever substitution, transform the following integrals
ex − 1
ex + 1
x 1+ 3x
into integrals of rational functions. What are the corresponding antiderivatives?
Suppose f ∈ C 1 (I, R). Show7 limλ→∞
Suppose f ∈ C [0, 1], R . Verify that
xf (sin x) dx =
f (sin x) dx,
f (cos2 x) cos x dx.
f (sin 2x) cos x dx =
(Hints: (i) Substitute t = π − x.
(ii) Observe π/4 f (sin 2x) cos x dx =
tion sin 2x = cos2 t.)
π/4
f (sin 2x) sin x dx, and then use the substitu-
The Legendre polynomials Pn are defined by8
∂ n (X 2 − 1)n
2n n!
Pn (X) :=
Prove
f (x) sin(λx) dx = 0.
By using the substitution y = tan x, prove that
(Hint: 1 + tan x =
7 See
8 See
log(1 + x)
dx = log 2 .
1 + x2
2 sin(x + π/4)
Remark 7.17(b).
Exercise IV.1.12.
4 2 − 8x3 − 4 2 x4 − 8x5
1 − x8
(Hints: Substitute x = y 2 and
(y − 1)(y 8 − 16) = (y 2 − 2)(y 2 − 2y + 2)(y 5 + y 4 + 2y 3 − 4).)
Suppose k, m ∈ N× and c ∈ (0, 1). Show that
xk−1
1 − xm
xnm = 1/(1 − xm ).)
9 In Exercise 8, put c = 1 2, m = 8, and k = 1, 4, 5, 6. Construct using Exercise 7
the following form of π:
(Hint:
16n 8n + 1
8n + 4
8n + 5
8n + 6
Supply recursion formulas for the integrals
(i) In =
xn ex dx ,
(ii) In =
x(log x)n dx ,
(iii) In =
(iv) In =
(x2 + 1)n
6 Sums and integrals
Suppose m, n ∈ Z with m < n and f : [m, n] → E is continuous. Then
f (k)
k=m+1
can be interpreted as a Riemann sum that approximates
(6.1)
In the two previous sections, we have learned effective methods for calculating
integrals, all of which followed from the fundamental theorem of calculus. In fact,
in many cases, it is simpler to calculate the integral (6.1) than its corresponding
sum. This brings up an interesting idea: “turn the tables” by using the known
integral to approximate the sum. The technique for approximating sums will prove
quite useful.
Naturally, the effectiveness of this idea will depend on how well the error
(6.2)
can be controlled. In this context, the Bernoulli numbers and polynomials are of
interest, and we turn to them first.
The Bernoulli numbers
We begin with the proof that the function1
h(z) :=
(ez − 1)/z ,
z=0,
is analytic and has a disc around 0 on which h does not vanish.2
6.1 Lemma Suppose h ∈ C ω (C, C) such that h(z) = 0 for z ∈ 2πB.
Proof 3 (i) By Proposition II.9.4, the power series g(X) =
1/(k + 1)! X k has
an infinite convergence radius, by Proposition V.3.5, it is analytic. We will now
verify the equality of g and h. Obviously g(0) = h(0), and therefore z ∈ C× . Then
g(z) = h(z) follows from z · g(z) = ez − 1.
1 For
the following, note also Exercises V.3.2–4.
existence of an η > 0 such that h(z) = 0 for z ∈ B(0, η) follows, of course, from h(0) = 1
and the continuity of h.
3 See also Exercise V.3.3.
2 The
VI.6 Sums and integrals
(ii) For z ∈ C\2πiZ, we have
z ez + 1
ez − 1 2
2 ez − 1
(6.3)
Therefore, our theorem follows because h(0) = 1.
From the last proof, it follows in particular that
f : 2πB → C , z → z
e −1
is well defined.4 Furthermore, the function f is analytic in a neighborhood of 0,
as the next theorem shows.
6.2 Proposition There is a ρ ∈ (0, 2π) such that f ∈ C ω (ρB, C).
The recursive algorithm of Exercise II.9.9 secures the existence of a power
bk X k with positive radius of convergence ρ0 and the property
(k + 1)!
bk X k = 1 ∈ C[[X]] .
With ρ := ρ0 ∧ 2π, we set
Then
ez − 1
f (z) =
bk z k = 1 for z ∈ ρB .
Consequently, Lemma 6.1 and the choice of ρ give
f (z) = f (z) = z
and now the analyticity of f follows from Proposition V.3.5.
In Remark VIII.5.13(c), we will show f is analytic without using recursive
division of power series (Exercise II.9.9).
The Bernoulli numbers Bk are defined for k ∈ N through
(6.4)
with properly chosen ρ > 0. From Proposition 6.2 and because of the identity theorem for power series (Corollary II.9.9), the Bk are uniquely determined through
(6.4). The map f with f (z) = z/(ez − 1) is called the generating function of Bk .
4 This
means that we can interpret z/(ez − 1) as equaling 1 at z = 0.
Recursion formulas
From (6.4) we can use the Cauchy product of power series to easily derive the
recursion formula for the Bernoulli numbers.
6.3 Proposition The Bernoulli numbers Bk satisfy
n=0,
(ii) B2k+1 = 0 for k ∈ N× .
By Proposition II.9.7, we have for z ∈ ρB
z = (e − 1)f (z) = z
n=0 k=0
k! (n + 1 − k)!
The identity theorem for power series then implies
The theorem then follows from multiplying this identity by (n + 1)! .
(ii) On the one hand, we find
f (z) − f (−z) = z
e−z − 1 + ez − 1
ez − 1 e − 1
1 − ez − e−z + 1
On the other, we have the power series expansion
f (z) − f (−z) =
(−z)k = 2
B2k+1 2k+1
(2k + 1)!
Therefore, from the identity theorem for power series, we get B2k+1 = 0 for
6.4 Corollary For the Bernoulli numbers, we have
B0 = 1 ,
B1 = −1/2 ,
B2 = 1/6 ,
B4 = −1/30 ,
B6 = 1/42 ,
Using the Bernoulli numbers, we can find a series expansion for the cotangent,
which will prove useful in the next section.
6.5 Application For suﬃciently small5 z ∈ C, we have
(−1)n
z cot z = 1 +
B2n z 2n .
(2n)!
Using (6.3), (6.4), and Proposition 6.3(ii), we get from B1 = −1/2 that
z 2n
2 n=0
By replacing z with 2iz, the theorem follows.
The Bernoulli polynomials
For every x ∈ C the function
zexz
is analytic. In analogy to the Bernoulli numbers, we will define the Bernoulli
polynomials Bk (X) through
Bk (x) k
for z ∈ ρB and x ∈ C .
(6.5)
By the identity theorem for power series, the functions Bk (X) on C are uniquely
determined through (6.5), and the next theorem shows that they are indeed polynomials.
6.6 Proposition For n ∈ N, we have
(i) Bn (X) =
k=0 k
(ii) Bn (0) = Bn ,
(iii) Bn+1 (X) = (n + 1)Bn (X),
(iv) Bn (X + 1) − Bn (X) = nX n−1 ,
(v) Bn (1 − X) = (−1)n Bn (X).
Proof The first statement comes, as in the proof of Proposition 6.3, from using
the Cauchy product of power series and comparing coeﬃcients. On one hand, we
5 From
the proof of Application 7.23(a), it will follow that this holds for |z| < 1.
Fx (z) = exz f (z) =
(n − k)! k!
and, alternately, Fx (z) =
Bn (x)z n /n! .
The statement (ii) follows immediately from (i). Likewise from (i) we get (iii)
because
Bk X n−k (n + 1 − k)
Bn+1 (X) =
= (n + 1)
Bk X n−k = (n + 1)Bn (X) .
Finally, (iv) and (v) follow from Fx+1 (z) − Fx (z) = zexz and F1−x (z) = Fx (−z)
by comparing coeﬃcients.
6.7 Corollary The first four Bernoulli polynomials read
B0 (X) = 1 ,
B1 (X) = X − 1/2 ,
B2 (X) = X − X + 1/6 ,
B3 (X) = X 3 − 3X 2 /2 + X/2 .
The Bernoulli polynomials will aid in evaluating the error in (6.2). But first, let
us prove a helpful result.
6.8 Lemma Suppose m ∈ N× and f ∈ C 2m+1 [0, 1]. Then we have
f (x) dx =
f (0) + f (1) −
B2k (2k−1)
(2k)!
(2m + 1)!
B2m+1 (x)f (2m+1) (x) dx .
Proof We apply the functional equation Bn+1 (X) = (n + 1)Bn(X) and continue
to integrate by parts. So define u := f and v := 1. Then u = f and v =
B1 (X) = X − 1/2. Consequently,
f (x) dx = B1 (x)f (x)
B1 (x)f (x) dx
(6.6)
= f (0) + f (1) −
B1 (x)f (x) dx .
Next we set u := f and v := B1 (X). Because u = f and v = B2 (X)/2, we find
B1 (x)f (x) dx =
B2 (x)f (x)
B2 (x)f (x) dx .
From this, it follows with u := f and v := B2 (X)/2 (and therefore with u = f
and v = B3 (X)/3! ) the equation
B2 (x)f (x) − B3 (x)f (x)
B3 (x)f (x) dx .
A simple induction now gives
2m+1
(−1)j
Bj (x)f (j−1) (x)
From Proposition 6.6, we have Bn (0) = Bn , Bn (1) = (−1)n Bn and B2n+1 = 0
for n ∈ N× , and the theorem follows.
In the following, we denote by Bn the 1-periodic continuation of the function
Bn (X)|[0, 1) on R, that is,
Bn (x) := Bn x − [x]
Obviously the Bn are jump continuous on R (that is, on every compact interval).
For n ≥ 2, the Bn are actually continuous (see Exercise 4).
6.9 Theorem (Euler–Maclaurin sum formula) Suppose a, b ∈ Z with a < b and
f belongs to C 2m+1 [a, b] for some m ∈ N× . Then
f (k) =
f (x) dx +
f (a) + f (b) +
Proof We decompose the integral a f dx into a sum of terms
which we apply Lemma 6.8. Therefore
b−a−1
a+k+1
fk (y) dy
with fk (y) := f (a + k + y) for y ∈ [0, 1]. Then
fk (0) + fk (1) =
f (a + k) + f (a + k + 1)
f (k) −
f (a) + f (b)
(2m+1)
B2m+1 (y)fk
B2m+1 (a + k + y)f (2m+1) (a + k + y) dy
(y) dy =
The theorem then follows from Lemma 6.8.
Power sums
In our first application of Euler–Maclaurin sum formula, we choose f to be a
monomial X m . The formula then easily evaluates the power sums n k m , which
we have already determined for m = 1, 2 and 3 in Remark I.12.14(c).
For m ∈ N with m ≥ 2, we have
6.10 Example
B2j m−2j+1
nm+1
2j − 1 2j
2j<m+1
In particular,
k2 =
n(n + 1)(2n + 1)
+ nB2 =
k3 =
n2 (n + 1)2
+ n2 B2 =
For f := X m , we have
and, for 2j − 1 < m,
B2j (2j−1)
(2j − 1)! nm−2j+1 =
nm−2j+1 .
(n) =
(2j)!
(2j)! 2j − 1
2j 2j − 1
Because f (m) (x)
= 0, the formulas follow from Theorem 6.9 with a = 0 and b = n.
Asymptotic equivalence
The series (ak ) and (bk ) in C× are said to be asymptotically equivalent if
lim(ak /bk ) = 1 .
In this case, we write ak ∼ bk (k → ∞) or (ak ) ∼ (bk ).
(a) It is not diﬃcult to check that ∼ is an equivalence relation
6.11 Remarks
on (C× )N .
(b) That (ak ) and (bk ) are asymptotically equivalent does not imply that (ak )
or (bk ) converges, nor that (ak − bk ) is a null sequence.
Consider ak := k2 + k and bk := k2 .
(c) If (ak ) ∼ (bk ) and bk → c, it follows that ak → c.
This follows immediately from ak = (ak /bk ) · bk .
(d) Suppose (ak ) and (bk ) are series in C× and (ak − bk ) is bounded. If |bk | → ∞
it then follows that (ak ) ∼ (bk ).
We have |(ak /bk ) − 1| = |(ak − bk )/bk | → 0 as k → ∞.
With the help of the Euler–Maclaurin sum formula, we can construct important examples of asymptotically equivalent series. For that, we require the
following result.
6.12 Lemma
Suppose z ∈ C is such that Re z > 1. Then the limit
exists in C and
Bk (x)
dx := lim
Re z − 1
Suppose 1 ≤ m ≤ n. Then we have
x− Re z dx
− Re z−1
Re z − 1 mRe z−1
This estimate shows that 1 Bk (x)/xz dx n∈N× is a Cauchy series in C and that
the stated limit exists. In the above estimate, we set m = 1 and take the limit
n → ∞, thus proving the lemma’s second part.
6.13 Examples (a) (the formula of de Moivre and Stirling) For n → ∞, we have
n! ∼ 2πn nn e−n .
Proof (i) We set a := 1, b := n, m := 1 and f (x) := log x. Then, from the Euler–
Maclaurin sum formula, we have
We note log x = x(log x − 1)
B2 1
B3 (x)
and B2 = 1/6, which give
log n + n = log n! n−n−1/2 en
−1 +
12 n
Now Lemma 6.12 implies that the sequence log(n! n−n−1/2 en ) n∈N converges.
Thus there is an A > 0 such that (n! n−n−1/2 en ) → A as n → ∞. Hence
n! ∼ Ann+1/2 e−n (n → ∞) .
(6.7)
(ii) To determine the value of A, apply the Wallis product representation of π/2:
(2k − 1)(2k + 1)
(see Example 5.5(d)). Therefore we have also
k=1 (2k)
24n (n! )4
= π/2 .
(2n)! (2n + 1)!
(6.8)
On the other hand, from (6.7), we have
A2 n2n+1 e−2n
(n! )2
= 2−2n−1 A 2n ,
2n+1/2 e−2n
A(2n)
whence the asymptotic equivalence
2n (2n)!
(6.9)
follows. We note finally that 2n (2n)! ∼ (2n)! (2n + 1)! and A > 0, and it then follows
from (6.8), (6.9), and Remark 6.11(c) that A = 2π. Inserting this into (6.7), we are
done.
(b) (Euler’s constant) The limit
C := lim
exists in R and is called Euler’s constant or the Euler–Mascheroni constant.6 In
addition,
∼ log n (n → ∞) .
Proof For a := 1, b := n, m := 0, and f (x) := 1/x it follows from the Euler–Maclaurin
sum formula that
B1 (x)
From Lemma 6.12, C is in R. The desired asymptotic equivalence follows from Remark 6.11(d).
The Riemann ζ function
We now apply the Euler–Maclaurin sum formula to examples which important in
the theory of the distribution of prime numbers.
Suppose s ∈ C satisfies Re s > 1, and define f (x) := 1/xs for x ∈ [1, ∞).
f (k) (x) = (−1)k s(s + 1) · · · · · (s + k − 1)x−s−k = (−1)k
s+k−1
for k ∈ N× . From the Euler–Maclaurin sum formula (with a = 1 and b = n) it
follows for m ∈ N that
dx 1
B2j s + 2j − 2 −s−2j+1
1+ s −
2j − 1
s + 2m
2m + 1
−(s+2m+1)
B2m+1 (x) x
Because |n−s | = n− Re s → 0 as n → ∞ and Re s > 0 we get
as well as
6 The
1 − s−1
s−1
|n−s−2j+1 | → 0
for j = 1, . . . , m ,
initial decimal digits of C are 0.577 215 664 901 . . .
(6.10)
as n → ∞. We note in addition Lemma 6.12, so that, in the limit n → ∞, (6.10)
becomes
B2j s + 2j − 2
2 s − 1 j=1 2j
(6.11)
From this, it follows in particular that the series 1/ns converges for every s ∈ C
with Re s > 1.7
The formula (6.11) permits another conclusion. For that, we set
Fm (s) :=
B2m+1 (x) x−(s+2m+1) dx
and Hm := { z ∈ C ; Re z > −2m}. According to Lemma 6.12, Fm : Hm → C is
well defined, and it is not hard to see that Fm is also analytic (see Exercise 4).
We now consider the map
Fm (s) for m ∈ N ,
and get the following property:
6.14 Lemma
For n > m, Gn is an extension of Gm .
With H := { z ∈ C ; Re z > 1 } it follows from (6.11) that
Gm (s) = Gn (s) =
2 s−1
Both Fk and Gk are analytic on Hk for every k ∈ N. The claim therefore follows
from the identity theorem for analytic functions (Theorem V.3.13).
6.15 Theorem The function
{ z ∈ C ; Re z > 1 } → C ,
has a uniquely analytic continuation ζ on C\{1}, called the Riemann ζ function.
For m ∈ N and s ∈ C with Re s > −2m and s = 1, we have
ζ(s) =
7 Note
Fm (s) .
This follows immediately from (6.11) and Lemma 6.14.
also Remark 6.16(a).
6.16 Remarks
Re s > 1.
1/ns converges absolutely for every s ∈ C with
(a) The series
For s ∈ C with Re s > 1, we have
|ζ(s)| =
= ζ(Re s) ,
nRe s
and the claim follows from the majorant criterion.
(b) (product representation of the ζ function) We denote the sequence of prime
numbers by (pk ) with p1 < p2 < p3 < · · ·. Then
1 − p−s
for Re s > 1 .
Because |1/ps | = 1 pRe s < 1 we have (the geometric series)
It therefore follows for every m ∈ N that
k=1 j=0 k
contains all numbers of the form 1/ns ,
where, after “multiplying out”, the series
whose prime factor decomposition n = q1 1 · · · · · q has no other prime numbers from
(1/n ) indeed contains all numbers n ∈ N with n ≤ pm . The
p1 , . . . , pm . Therefore
absolute convergence of the series n (1/ns ) then implies
ζ(s) −
From Exercise I.5.8(b) (a theorem of Euclid), it follows that pm → ∞ for m → ∞.
Re s
Therefore, from (a), the remaining series
) is a null sequence.
n>pm (1/n
(c) The Riemann ζ function has no zeros in { z ∈ C ; Re z > 1 }.
Proof Suppose s ∈ C satisfies Re s > 1 and ζ(s) = 0. Because Re s > 1, we then have
that limk (1 − p−s ) = 1. Hence there is an m0 ∈ N that log(1 − p−s ) is well defined for
k ≥ m0 , and we find
k=m0
The convergence of
imply that the series
log(1 − p−s )
for N ≥ m0 .
(6.12)
−s −1
, the continuity of the logarithm, and
k=m0 (1 − pk )
log(1 − p−s ) converges. In particular, the remaining
k≥m0
terms
m≥m0
form a null sequence. Then it follows from (6.12) that
(1 − p−s )−1 = 1 .
k=m1 +1 (1 − pk )
Consequently there exists an m1 ≥ m0 such that
is not zero, it follows that
k=1 (1 − pk )
0 = ζ(s) =
k=m1 +1
= 0. Now because
which is impossible.
(d) The series
1/pk diverges.
From the considerations in the proof of (b), we have the estimate
1−
where the product has all prime numbers
≤ m. Now we consider the sum a Riemann
sum and find that function x → 1/x is decreasing:
Because the logarithm is monotone, it follows that
log 1 −
for m ≥ 2 .
(6.13)
When |z| < 1, we can use the series expansion of the logarithm (Theorem V.3.9)
log(1 − z)−1 = − log(1 − z) =
to get
p≤m k=1
(6.14)
where
p≤m k=2
p(p − 1)
j(j − 1)
It now follows from (6.13) and (6.14) that
for m ≥ 2 ,
which, since the sum is over all prime numbers ≤ m, proves the claim.
(6.15)
(e) The preceding statements contain information about the density of prime numbers. Because
1/pk diverges (according to (6.15), at least as fast as log log m),
the sequence (pk ) cannot go too quickly to ∞. In studies of the prime number
distribution, π(x) for x ∈ R× denotes the number of prime numbers ≤ x. The
famous prime number theorem says
π(n) ∼ n/ log n (n → ∞) .
To get more information, one can study the asymptotic behavior of the relative error
π(n) − n/ log n
π(n)
r(n) :=
as n → ∞. It is possible to show that
r(n) = O
(n → ∞) .
It however only conjectured that for every ε > 0 there is a distinctly better error
estimate:
r(n) = O 1/2−ε (n → ∞) .
This conjecture is equivalent to the celebrated Riemann hypothesis:
The ζ function has no zeros s such that Re s > 1/2 .
We know from (c) that ζ has no zeros in { z ∈ C ; Re z > 1 }. It is also known that
the set of zeros of ζ with real part ≤ 0 correspond to −2N× . Calling these zeros
trivial, one can conclude from the properties of the ζ function that the Riemann
hypothesis is equivalent to the following statement:
All nontrivial zeros of the Riemann ζ function
lie on the line Re z = 1/2 .
It is known that ζ has infinitely many zeros on this line and, for a certain large
value of K, there are no nontrivial zeros s with | Im s| < K for which Re s = 1/2.
Proving the Riemann hypothesis stands today as one of the greatest efforts in
mathematics.
For a proof of the prime number theorem, for the stated asymptotic error
estimates, and for further investigations, consult the literature of analytic number
theory, for example [Br¨95], [Pra78], [Sch69].
The trapezoid rule
In most practical applications, a given integral must be approximated numerically,
because no antiderivative is known. From the definition of integrals α f dx, every
jump continuous function f can be integrated approximately as a Riemann sum
over rectangles whose (say) top-left corners meet at the graph of f . Such an
approximation tends to leave gaps between the rectangles and the function, and
this slows its convergence as the mesh decreases. If the function is suﬃciently
smooth, one can anticipate that (in the real-valued case) the area under the graph
of f can be better estimated by trapezoids.
Here h f (x + h) + f (x) 2 is the (oriented) area of the trapezoid T .
The next theorem shows that if f is smooth, the “trapezoid rule” for small
steps h gives a good approximation to f . As usual, we take −∞ < α < β < ∞
and E to be a Banach space.
6.17 Theorem Suppose f ∈ C 2 [α, β], E , n ∈ N× , and h := (β − α)/n. Then
f (α) +
|R(f, h)| ≤
f (α + kh) + f (β) h + R(f, h) ,
β−α 2
Proof From the additivity of integrals and the substitution rule, it follows after
defining x(t) := α + kh + th and gk (t) := f (α + kh + th) for t ∈ [0, 1] that
α+(k+1)h
f (x) dx = h
gk (t) dt .
Formula (6.6) implies8
gk (t) dt =
gk (0) + gk (1) −
B1 (t)gk (t) dt .
8 Note that the formula (6.6) is entirely elementary and the theory of the Bernoulli polynomials
is not needed for this derivation.
We set u := gk and v := B1 (X), and thus u = gk and v = −X(1 − X)/2, as well
as v(0) = v(1) = 0. Integrating by parts, we therefore get
B1 (t)gk (t) dt =
v(t)gk (t) dt .
Defining
R(f, h) := h
v(t)gk (t) dt ,
we arrive at the representation
f (α + kh) + f (β) + R(f, h) .
To estimate the remainder R(f, h), we note first that
v(t)gk (t) dt ≤ gk
t(1 − t)
12 k
The chain rule implies
= max |gk (t)| = h2
t∈[0,1]
α+kh≤x≤α+(k+1)h
|f (x)| ≤ h2 f
Consequently,
|R(f, h)| ≤ h
v(t)gk (t) dt ≤
which shows the dependence on h.
The trapezoid rule constitutes the simplest “quadrature formula” for numerically approximating integrals. For further exploration and more eﬃcient methods,
see lectures and books about numerical methods (see also Exercise 8).
Prove the statements (iv) and (v) in Proposition 6.6.
Compute B8 , B10 , and B12 , as well as B4 (X), B5 (X), B6 (X) and B7 (X).
Show that for n ∈ N×
(i) B2n+1 (X) has in [0, 1] only the zeros 0, 1/2, and 1;
(ii) B2n (X) has in [0, 1] only two zeros x2m and x2m , with x2m + x2m = 1.
Denote by Bn the 1-periodic continuation of Bn (X) | [0, 1) onto R. Then show these:
(i) Bn ∈ C n−2 (R) for n ≥ 2.
Bn (x) dx = 0 for k ∈ Z and n ∈ N.
(iii) For every n ∈ N map
Fn : { z ∈ C ; Re z > −2n} → C ,
B2n+1 (x)x−(s+2n+1) dx
is analytic.
Prove Remark 6.11(a).
Show for the ζ function that limn→∞ ζ(n) = 1.
Suppose h > 0 and f ∈ C 4 [−h, h], R . Show
f (x) dx −
f (−h) + 4f (0) + f (h)
h5 (4)
(Hints: Integrate by parts and use the mean value theorem for integrals.)
8 Suppose f ∈ C 4 [α, β], R and let αj := α + jh for 0 ≤ j ≤ 2n with h := (β − α)/2n
and n ∈ N× . Further let9
f (α) + f (β) + 2
f (α2j ) + 4
f (α2j−1 ) .
This is Simpson’s rule, another rule for approximating integrals. Show it satisfies the
error estimate
≤ (β − α)
f (x) dx − S f, [α, β], h
f (4)
(Hint: Exercise 7.)
Calculate the error in
= log 2
when using the trapezoid rule for n = 2, 3, 4 and Simpson’s rule for n = 1, 2.
Show that Simpson’s rule with a single inner grid point gives the correct value of
p for any polynomial p ∈ R[X] of degree at most three.
9 As
usual, we assign the “empty sum”
· · · the value 0.
VI.7 Fourier series
7 Fourier series
At the end of Section V.4, we asked about the connection between trigonometric
series and continuous periodic functions. With the help of the available integration
theory, we can now study this connection more precisely. In doing so, we will get a
first look at the vast theory of Fourier series, wherein we must concentrate on only
the most important theorems and techniques. For simplicity, we treat only the
case of piecewise continuous 2π-periodic functions, because studying more general
classes of functions will call for the Lebesgue integration theory, which we will not
see until Volume III.
The theory of Fourier series is closely related to the theory of inner product
spaces, in particular, with the L2 space. For these reasons we first consider the
L2 structure of spaces of piecewise continuous functions as well as orthonormal
bases of inner product spaces. The results will be of fundamental importance for
determining the how Fourier series converge.
The L2 scalar product
Suppose I := [α, β] is a compact perfect interval. From Exercise 4.3 we know
that α f dx for f ∈ SC(I) is independent of the value of f on its discontinuities.
Therefore we can fix this value arbitrarily for the purposes of studying integrals.
On these grounds, we will consider only “normalized” functions, as follows. We
call f : I → C normalized when
f (x) = f (x + 0) + f (x − 0)
2 for x ∈ I ,
(7.1)
f (α) = f (β) = f (α + 0) + f (β − 0)
(7.2)
Denote by SC(I) the set of normalized, piecewise continuous functions f : I → C.1
The meaning of the assignment (7.2) will be clarified in the context of periodic
functions.
1 We
naturally write SC[α, β] instead of SC [α, β] , and C[α, β] instead of C [α, β] etc.
7.1 Proposition SC(I) is a vector subspace of S(I), and
(f |g)2 :=
for f, g ∈ SC(I) ,
defines a scalar product (·|·)2 on SC(I).
Proof The first claim is clear. The linearity of integrals implies that (·|·)2 is
a sesquilinear form on SC(I). Corollary 4.10 implies (f |g)2 = (g |f )2 for f, g ∈
SC(I). From the positivity of integrals, we have
(f |f )2 =
|f |2 dx ≥ 0
for f ∈ SC(I) .
If f ∈ SC(I) is not the zero function, then there is, by the normalizability, a point
a at which f , and therefore also |f |2 , is continuous and for which |f (a)|2 > 0. It
then follows from Proposition 4.8 that (f |f ) > 0. Thus (·|·)2 is an inner product
on SC(I).
The norm
|f |2 dx
induced by (·|·)2 is called the L2 norm, and we call (·|·)2 itself the L2 scalar
product on SC(I). In the following, we always equip SC(I) with this scalar
product and understand
SC(I) := SC(I), (·|·)2
unless explicitly stated otherwise. Thus SC(I) is an inner product space.
7.2 Remarks (a) The L2 norm on SC(I) is weaker2 than the supremum norm.
Precisely, we have
(b) The inequality f ∞ < ε holds for f ∈ SC(I), and so the entire graph of f
lies in an “ε-strip” in the interval I. On the other hand, even when f 2 < ε,
f itself can assume very large values, that is, the inequality only requires that
the (oriented) area in I of the graph of |f |2 is smaller than ε2 . We say that the
function f is smaller than ε in its quadratic mean. Convergence in the L2 norm
is also called convergence in the quadratic mean.
2 See
Exercise 4.5.
(c) Although the sequence (fn ) may converge in SC(I) — and therefore also in its
quadratic mean — to zero, the values fn (x) for x ∈ I need not converge to zero.
Proof For n ∈ N× suppose j, k ∈ N are the unique numbers with n = 2k + j and j < 2k .
Then we define fn ∈ SC[0, 1] by f0 := 1 and
x ∈ j2−k , (j + 1)2−k ,
Because fn 2 = 2−k , (fn ) converges in SC[0, 1] to 0; however, because the “bump” in
f continues to sweep the interval from left to right each time it narrows, fn (x) does
not converge for any x ∈ [0, 1].
Approximating in the quadratic mean
It is obvious that C(I) is not dense in SC(I) in the supremum norm. However in
L2 norm, the situation is different, as the following theorem about approximating
in the quadratic mean shows. Here we set
C0 (I) :=
u ∈ C(I) ; u(α) = u(β) = 0
C0 (I) is obviously a closed vector subspace of C(I) (in the maximum norm), and
it is therefore a Banach space.
7.3 Proposition C0 (I) is dense in SC(I).
Proof Suppose f ∈ SC(I) and ε > 0. From Theorem 1.2, there is a g ∈ T (I)
such that f − g ∞ < ε/2 β − α. Therefore3 f − g 2 < ε/2. It then suﬃces to
find an h ∈ C0 (I) such that g − h 2 < ε/2.
Suppose therefore that f ∈ T (I) and ε > 0. Then there is a partition
(α0 , . . . , αn ) for f and functions gj such that
x ∈ (αj , αj+1 ) ,
x ∈ I \(αj , αj+1 ) ,
gj (x) =
for 0 ≤ j ≤ n − 1 ,
and f (x) = j=0 gj (x) for x = αj . On the other hand, by the triangle inequality,
it suﬃces to find h0 , . . . , hn−1 ∈ C0 (I) such that gj − hj 2 < ε/n.
Thus we can assume that f is a nontrivial staircase function “with only one
step”. In other words, there are numbers α, β ∈ I and y ∈ C× with α < β which
can be chosen so that
f (x) =
x ∈ I α, β ,
Let ε > 0. Then we choose δ > 0 such that δ < β − α
g(x) :=
2 ∧ ε2 /|y|2 and define
x ∈ I \ α, β ,
Then g belongs to C0 (I), and we have
|f − g|2 dx ≤ δ |y|2 < ε2 .
7.4 Corollary
(i) C(I), (·|·)2 is not a Hilbert space.
(ii) The maximum norm on C(I) is strictly stronger than the L2 norm.
Proof (i) Suppose f ∈ SC(I) \ C(I). Then, by Proposition 7.3, there is a sequence (fj ) in C(I) that converges in the L2 norm to f . Therefore, according
to Proposition II.6.1, (fj ) is a Cauchy sequence in SC(I) and therefore also in
3 Of
course, u
is defined for every u ∈ S(I).
E := C(I), · 2 . Were E a Hilbert space, and therefore complete, then there
would be a g ∈ E with fj → g in E. Therefore (fj ) would converge in SC(I) to
both f and g, and we would have f = g, which is impossible. Therefore E is not
complete.
(ii) This follows from (i) and the Remarks 7.2(a) and II.6.7(a).
Orthonormal systems
We recall now some ideas from the theory of inner product spaces (see Exercise II.3.10). Let E := E, (·|·) be an inner product space. Then u, v ∈ E are
orthogonal if (u |v) = 0, and we also write u ⊥ v. A subset M of E is said to be
an orthogonal system if every pair of distinct elements of M are orthogonal. If in
addition u = 1 for u ∈ M , we say M is an orthonormal system (ONS).
7.5 Examples
(a) For k ∈ Z, let
ek (t) := √ ei kt
Then { ek ; k ∈ Z } is an ONS in the inner product space SC[0, 2π].
Because the exponential function is 2πi -periodic, we find
i (j−k)t
(ej | ek )2 =
ej ek dt =
−i
2π 0
2π(j − k)
as desired.
(b) Suppose
c0 (t) := √
ck (t) := √ cos(kt)
sk (t) := √ sin(kt)
for t ∈ R and k ∈ N× . Then { c0 , ck , sk ; k ∈ N× } is an orthonormal system in
the inner product space SC [0, 2π], R .
Euler’s formula (III.6.1) implies ek = (ck + i sk )
2 for k ∈ N× . Then
2(ej | ek )2 = (cj | ck )2 + (sj | sk )2 + i (sj | ck )2 − (cj | sk )2
(7.3)
for j, k ∈ N . Because (ej | ek )2 is real, we find
(sj | ck )2 = (cj | sk )2
Integration by parts yields
(sj | ck )2 =
sin(jt) cos(kt) dt = − (cj | sk )2
(7.4)
for j, k ∈ N× . Therefore we get with (7.4)
(1 + j/k)(sj | ck )2 = 0
We can then state
(sj | ck )2 = 0
because this relation is trivially true for the remaining cases j = 0 or k = 0.
Using the Kronecker symbol, we get from (7.3) and (a) that
(cj | ck )2 + (sj | sk )2 = 2δjk
(7.5)
(cj | ck )2
(sj | sk )2 =
Therefore
(7.6)
(1 + j/k)(cj | ck )2 = 2δjk
From this and (7.6), it follows that
(cj | ck )2 = (sj | sk )2 = 0
Finally, (c0 | cj )2 = (c0 | sj )2 = 0 is obvious for j ∈ N× and c0
= 1.
Integrating periodic functions
We now gather elementary but useful properties of periodic functions.
7.6 Remarks Suppose f : R → C is p-periodic for some p > 0.
(a) If f |[0, p] is jump continuous, then f is jump continuous on every compact
subinterval I of R. Similarly, if f |[0, p] belongs to SC[0, p], f |I belongs to SC(I).
(b) If f |[0, p] is jump continuous, we have
From the additivity of integrals, we have
Using the substitution y(x) := x − p in the second integral on the right hand side, we
get, using the p-periodicity of f , that
f (y + p) dy =
From Remark V.4.12(b), we may confine our study of periodic functions to
the case p = 2π. So we now take I := [0, 2π] and define the L2 scalar product on
this interval.
Fourier coeﬃcients
Consider
ak cos(kt) + bk sin(kt) ,
(7.7)
a trigonometric polynomial. Using
ck := (ak − i bk )/2 ,
c0 := a0 /2 ,
c−k := (ak + i bk )/2
(7.8)
for 1 ≤ k ≤ n, we can write Tn in the form
ck ei kt
Tn (t) =
(7.9)
(see (V.4.5) and (V.4.6)).
7.7 Lemma The coeﬃcients ck can be found as
Tn (t)e−i kt dt = √ (Tn |ek )2
Proof We form the inner product in SC(I) between Tn ∈ C(I) and ej . Then it
follows from (7.9) and Example 7.5(a) that
(Tn |ej )2 =
ck (ek |ej )2 =
2π cj
Suppose now (ak ) and (bk ) are series in C, and ck is defined for k ∈ Z through
(7.8). Then we can write the trigonometric series
ak cos(k·) + bk sin(k·) =
k≥1
in the form
ck ei k· =
a0 √
ck e k .
(ak ck + bk sk )
(7.10)
Convention In the rest of this section, we understand
k∈Z gk to be the
gk n∈N , rather than the sequence of the
sequence of partial sums
sum of the two single series k≥0 gk and k>1 g−k .
The next theorem shows when the coeﬃcients of the trigonometric series (7.1) can
be extracted from the function it converges to.
7.8 Proposition If the trigonometric series (7.10) converges uniformly on R, it
converges to a continuous 2π-periodic function f , and
f (t)e−i kt dt = √ (f |ek )2
Proof Because the partial sum Tn is continuous and 2π-periodic, the first claim
follows at once from Theorem V.2.1. Therefore define
cn ei nt := lim Tn (t)
f (t) :=
The series (Tn ek )n∈N converges uniformly for every k ∈ Z and therefore converges
in C(I) to f ek . Thus we get from Proposition 4.1 and Lemma 7.7
(f |ek )2 =
f ek dt = lim
Tn ek dt =
2π ck
Classical Fourier series
We denote by SC2π the vector subspace of B(R) that contains all 2π-periodic
functions f : R → C with f |I ∈ SC(I) and that is endowed with scalar product
for f, g ∈ SC2π .
(7.11)
is well defined for f ∈ SC2π and k ∈ Z and is called the k-th (classical) Fourier
coeﬃcient of f . The trigonometric series
(f |ek )ek
fk ei k· =
(7.12)
is the (classical) Fourier series of f . Its n-th partial sum
fk ei k·
is its n-th Fourier polynomial.
7.9 Remarks (a) SC2π is an inner product space, and C2π , the space of the
continuous 2π-periodic functions4 f : R → C, is dense in SC2π .
Proof That a periodic function is determined by its values on its periodic interval follows
from the proof of Theorems 7.1 and 7.3. Here we note that the 2π-periodic extension of
g ∈ C0 (I) belongs to C2π .
(b) The Fourier series Sf can be expressed in the form
ak := ak (f ) :=
(ak ck + bk sk ) ,
f (t) cos(kt) dt = √ (f |ck )
f (t) sin(kt) dt = √ (f |sk )
bk := bk (f ) :=
for k ∈ N . Also
a0 := a0 (f ) :=
This follows from (7.8) and Euler’s formula
(f |c0 ) .
f (t) dt =
2 ek = ck + i sk .
(c) If f is even, then Sf is a purely a cosine series:
ak cos(k·) =
with
ak = ak (f ) =
f (t) cos(kt) dt
ak c k
If f is odd, then Sf is purely a sine series:
bk sin(k·) =
4 See
Section V.4.
bk = bk (f ) =
Proof The cosine is even and the sine is odd.
is odd. If f is odd, f ck is odd and f sk is even.
Exercise 4.11,
1 π
f (t) cos(kt) dt =
ak =
f (t) sin(kt) dt
(7.13)
Then if f is even, f ck is even and f sk
Consequently, from Remark 7.6(b) and
f (t) sin(kt) dt = 0
if f is even. If f is odd, then ak = 0 and (7.13) gives the bk .
7.10 Examples
(a) Define f ∈ SC2π by f (t) = sign(t) for t ∈ (−π, π).
k≥0
sin (2k + 1)·
2k + 1
The figure graphs a sequence of Fourier polynomials in [−π, π]:
(On the basis of these graphs, we might think that Sf converges pointwise to f .)
Because f is odd, and because
f (t) sin(kt) dt =
cos(kt)
sin(kt) dt = −
the claim follows from Remark 7.9(c).
(b) Suppose f ∈ C2π such f (t) = |t| for |t| ≤ π. In other words,
f (t) = 2π zigzag(t/2π) for t ∈ R ,
(see Exercise III.1.1).
k ∈ 2N + 1 ,
k ∈ 2N ,
cos (2k + 1)·
(2k + 1)2
This series converges normally on R (that is, it is norm convergent).
Because f is even, and because integration by parts integration gives
t cos(kt) dt = −
sin(kt) dt =
(−1)k − 1 ,
πk2
for k ≥ 1, the first claim follows from Remark 7.9(c). Now, from Example II.7.1(b),
c2k+1 /(2k + 1)2 . The second claim then
(2k + 1)−2 is a convergent majorant for
follows from the Weierstrass majorant criterion (Theorem V.1.6).
(c) Suppose z ∈ C\Z, and fz ∈ C2π is defined by fz (t) := cos(zt) for |t| ≤ π. Then
sin(πz) 1
n≥1
cos(n·) .
This series converges normally on R.
Proof Since f is even, Sfz is a cosine series. Using the first addition theorem from
Proposition III.6.3(i), we find
cos (z + n)t + cos (z − n)t
π 0
sin(πz)
= (−1)n
an =
cos(zt) cos(nt) dt =
for n ∈ N. Therefore Sfz has the stated form.
For n > 2 |z|, we have |an | < |sin(πz)| |z| |z 2 − n2 | < 2 |sin(πz)| |z|/n2 . The normal
convergence again follows from the Weierstrass majorant criterion.
Bessel’s inequality
Let E, (·|·) be an inner product space, and let { ϕk ; k ∈ N } be an ONS in5 E.
In a generalization of the classical Fourier series, we call the series in E
(u |ϕk )ϕk
5 Note
that this implies that E is infinite-dimensional (see Exercise II.3.10(a)).
the Fourier series for u ∈ E with respect to the ONS { ϕk ; k ∈ N }, and (u |ϕk )
is the k-th Fourier coeﬃcient of u with respect to { ϕk ; k ∈ N }.
For n ∈ N, let En := span{ϕ0 , . . . , ϕn }, and define
Pn : E → En ,
(u |ϕk )ϕk .
The next theorem shows that
Pn u for each u ∈ E yields the
unique element of En that is
closest in distance to u, and
that u − Pn u is perpendicular
to En .6
7.11 Proposition For u ∈ E and n ∈ N, we have
(i) u − Pn u ∈ En ;
u − Pn u = minv∈En u − v = dist(u, En ) and
for v ∈ En and v = Pn u ;
(u |ϕk ) .
(i) For 0 ≤ j ≤ n, we find
(u − Pn u |ϕj ) = (u |ϕj ) −
(u |ϕk )(ϕk |ϕj ) = (u |ϕj ) − (u |ϕj ) = 0 ,
because (ϕk |ϕj ) = δkj .
(ii) Suppose v ∈ En . Because Pn u − v ∈ En , it follows then from (i) (see
(II.3.7) that
= (u − Pn u) + (Pn u − v)
= (u − Pn u)
+ (Pn u − v)
Therefore u − v > u − Pn u for every v ∈ En with v = Pn u.
(iii) Because
− 2 Re(u |Pn u) + Pn u
the claim follows from (ϕj |ϕk ) = δjk .
6 See
Exercise II.3.12.
7.12 Corollary (Bessel’s inequality)
Fourier coeﬃcients converges, and
For every u ∈ E, the series of squares of
(u |ϕk )
From Proposition 7.11(iii), we get
Thus the claim is implied by Theorem II.7.7.
7.13 Remarks (a) According to (7.11), the relation (f |ek )2 = 2π fk holds
for f ∈ SC2π between the k-th Fourier coeﬃcient of f with respect to the ONS
{ ek ; k ∈ Z } and the classical k-th Fourier coeﬃcient fk . This difference in
normalization is historical. (Of course, it is irrelevant that, in the classical case,
the ONS is indexed with k ∈ Z rather than k ∈ N.)
(b) For n ∈ N, we have
Pn ∈ L(E) ,
im(Pn ) = En .
(7.14)
A linear vector space map A satisfying A2 = A is called a projection. Therefore Pn
is a continuous linear projection from E onto En , and because u−Pn u is orthogonal
to En for every u ∈ E, Pn is an orthogonal projection. Proposition 7.11 then
says every u has a uniquely determined closest element from En ; that element is
obtained by orthogonally projecting u onto En .
We leave the simple proof of (7.14) to you.
Complete orthonormal systems
The ONS { ϕk ; k ∈ N } in E is said to be complete or an orthonormal basis
(ONB) of E if, for every u ∈ E, Bessel’s inequality becomes the equality
for u ∈ E .
(7.15)
This statement is called a completeness relation or Parseval’s theorem.
The following theorem clarifies the meaning of a complete orthonormal system.
7.14 Theorem The ONS { ϕk ; k ∈ N } in E is complete if and only if for every
u ∈ E the Fourier series (uk |ϕk )ϕk converges to u, that is, if
According to Proposition 7.11(iii), Pn u → u, and therefore
u = lim Pn u =
if and only if Parseval’s theorem holds.
After these general considerations, which, besides giving the subject a geometrical interpretation, are also of theoretical and practical interest, we can return
to the classical theory of Fourier series.
7.15 Theorem The functions { ek ; k ∈ Z } form an ONB on SC2π .
Proof Suppose f ∈ SC2π and ε > 0. Then Remark 7.9(a) supplies a g ∈ C2π
such that f − g 2 < ε/2. Using the Weierstrass approximation theorem (Corollary V.4.17), we find an n := n(ε) and a trigonometric polynomial Tn such that
g − Tn ∞ < ε 2 2π .
From Remark 7.2(a) and Proposition 7.11(ii) we then have
< ε/2 ,
and therefore
where, in the first step of the second row, we have used the minimum property,
that is, Proposition 7.11(ii). Finally, Proposition 7.11(iii) gives
0≤ f
(f |ek )2
for m ≥ n. This implies the completeness relation holds for every f ∈ SC2π .
7.16 Corollary For every f ∈ SC2π , the Fourier series Sf converges in the
quadratic mean to f , and Parseval’s theorem reads
|f |2 dt =
7.17 Remarks
(a) The real ONS { c0 , ck , sk ; k ∈ N× } is an ONB in the
space SC2π . Defining Fourier coeﬃcients by ak := ak (f ) and bk := bk (f ), we
1 2π 2
|f | dt = 0 +
(a2 + b2 )
for real-valued f ∈ SC2π .
Example 7.5(b), Remark 7.9(b), and Euler’s formula
2 (f | ek ) = (f | ck ) − i (f | sk ) ,
imply
2 (f | ek )
= (f | ck )2 + (f | sk )2 = π(a2 + b2 )
having additionally used that a−k = ak and b−k = −bk .
(b) (Riemann’s lemma) For f ∈ SC[0, 2π], we have
f (t) sin(kt) dt → 0 and
f (t) cos(kt) dt → 0 (k → ∞) .
This follows immediately from the convergence of the series in (a).
(c) Suppose
2 (Z)
is the set of all series x := (xk )k∈Z ∈ CZ such that
|xk |2 < ∞ .
|x|2 :=
Then 2 (Z) is a vector subspace of CZ and a Hilbert space with the scalar product
(x, y) → (x|y)2 := k=−∞ xk y k . Parseval’s theorem then implies that the map
SC2π →
2π fk
is a linear isometry. This isometry is however not surjective and therefore not
an isometric isomorphism, as we shall see in Volume III in the context of the
Lebesgue integration theory. Hence there is an orthogonal series k∈Z ck ek such
that k=−∞ |ck |2 < ∞, which nevertheless does not converge to any function in
f ∈ SC2π . From this it also follows that SC2π — and therefore SC[0, 2π] — is not
complete and is therefore not a Hilbert space. In Volume III, we will remedy this
defect by learning how to extend SC[0, 2π] to a complete Hilbert space, called
L2 (0, 2π).
We leave the proof of some of these statements as Exercise 10.
Parseval’s theorem can be used to calculate the limit of various series, as we
demonstrate in the following examples.7
also Application 7.23(a).
7.18 Examples
= 1 + 2 + 2 + 2 + ···
This follows from Example 7.10(a) and Remark 7.17(a).
(b) The series
1 (2k + 1)4 has the value π 4 /96.
This is a consequence of Example 7.10(b) and Remark 7.17(a).
We now turn to the question of the uniform convergence of Fourier series. To get
a simple suﬃcient criterion, we must require more regularity of the functions we
consider.
Suppose J := [α, β] is a compact perfect interval. We say f ∈ SC(J) is
piecewise continuously differentiable (or has piecewise continuous derivatives) if
there is a partition (α0 , α1 , . . . , αn ) of J such that fj := f |(αj , αj+1 ) for 0 ≤ j ≤
n − 1 has a uniformly continuous derivative.
7.19 Lemma The function f ∈ SC(J) is piecewise continuously differentiable if
and only if there is a partition (α0 , . . . , αn ) of J with these properties:
(i) f |(αj , αj+1 ) ∈ C 1 (αj , αj+1 ) for 0 ≤ j ≤ n − 1.
(ii) For 0 ≤ j ≤ n − 1 and 1 ≤ k ≤ n, the limits f (αj + 0) and f (αk − 0) exist.
Proof “= Because f is piecewise continuously differentiable, (i) follows by de⇒”
finition, and (ii) is a consequence of Theorem 2.1.
“⇐ According to Proposition III.2.24, if the partition satifies properties (i)
and (ii), then fj ∈ C(αj , αj+1 ) has a continuous extension on [αj , αj+1 ]. Therefore
Theorem III.3.13 implies fj is uniformly continuous.
If f ∈ SC(J) is piecewise continuously differentiable, Lemma 7.19 guarantees
the existence of a partition (α0 , . . . , αn ) of J and a unique normalized piecewise
continuous function f , which we call the normalized derivative, such that
f |(αj−1 , αj ) = f |(αj−1 , αj )
for 0 ≤ j ≤ n − 1 .
Finally, we call f ∈ SC2π piecewise continuously differentiable if f |[0, 2π] has
these properties.
7.20 Remarks (a) If f ∈ SC2π is piecewise continuously differentiable, then f
belongs to SC2π .
Proof This is a direct consequence of the definition of the normalization on the interval
boundaries.
(b) (derivative rule) If f ∈ C2π is piecewise continuously differentiable, then
fk = i k fk
where fk := (f )k .
Proof Suppose 0 < α1 < · · · < αn−1 < 2π are all the discontinuities of f in (0, 2π).
Then it follows from the additivity of integrals and from integration by parts (with
α0 := 0 and αn := 2π) that
2π fk =
f (t)e−i kt dt =
f (t)e−i kt
αj+1
= ik
f (t)e−i kt dt
+ ik
f (t)e−i kt dt = 2πi kfk
for k ∈ Z, where the first sum in the third equality vanishes by the continuity of f and
its 2π-periodicity.
Uniform convergence
We can now prove the following simple criterion for the uniform and absolute
convergence of Fourier series.
7.21 Theorem Suppose f : R → C is 2π-periodic, continuous, and piecewise
continuously differentiable. The Fourier series Sf in R converges normally to f .
8 That
The Remarks 7.20(a) and (b), the Cauchy–Schwarz inequality for series8
is, the H¨lder inequality for p = 2.
from Exercise IV.2.18, and the completeness relation implies
f ≤ 2
2 1/2
Therefore, because fk ei k· ∞ = fk , Sf has the convergent majorant k∈Z fk .
The Weierstrass majorant criterion of Theorem V.1.6 then implies the normal
convergence of the Fourier series of f . We denote the value of Sf by g. Then g is
continuous and 2π-periodic, and we have Sn f −g ∞ → 0 for n → ∞. Because the
maximum norm is stronger than the L2 norm, we have also lim Sn f − g 2 = 0,
that is, Sf converges in SC2π to g. According to Corollary 7.16, Sf converges
in SC2π to f . Then it follows from the uniqueness of its limit that Sf converges
normally to f .
7.22 Examples
(a) For |t| ≤ π, we have
cos (2k + 1)t
and the series converges normally on R.
Proof It follows from the previous theorem and Example 7.10(b) that the (normalized)
derivative f of the function f ∈ C2π with f (t) = |t| for |t| ≤ π is given by map of
Example 7.10(a).
(b) (partial fraction decomposition of the cotangent)
For z ∈ C\Z, we have
π cot(πz) =
z n=1 z + n z − n
(7.16)
From Theorem 7.21 and Example 7.10(c) we get
cos(zt) =
cos(nt)
z n=1
for |t| ≤ π. Finally, put t = π.
The partial fraction decomposition of the cotangent has the following two interesting and beautiful consequences which, like the partial fraction decomposition
itself, go back to Euler.
7.23 Applications
(a) (the Euler formula for ζ(2k)) For k ∈ N× , we have
ζ(2k) =
(−1)k+1 (2π)2k
B2k .
2(2k)!
In particular, we find
ζ(2) = π 2 /6 ,
ζ(4) = π 4 /90 ,
ζ(6) = π 6 /945 .
From (7.16), we get
πz cot(πz) = 1 + 2z 2
z 2 − n2
(7.17)
If |z| ≤ r < 1, then |n2 − z 2 | ≥ n2 − r 2 > 0 for n ∈ N× . This implies that the series in
(7.17) converges normally on rB. The geometric series gives
=− 2
− n2
for z ∈ B and n ∈ N× .
Thus we get from (7.17) that
πz cot(πz) = 1 − 2z 2
=1−2
k=1 n=1
z 2k
(7.18)
where, in the last step, we have exchanged the order of summation, which we can do
because the double series theorem allows it (as you may verify).
Now we get the stated relation from Application 6.5 using the identity theorem for
power series.9
(b) (the product representation of the sine) For z ∈ C, we have
sin(πz) = πz
Putting z = 1/2 here gives the Wallis product (see Example 5.5(d)).
We set
and f (0) := 0. Then it follows from the power series expansion of the sine that
f (z) :=
(−1)k
π 2k z 2k
(7.19)
The convergence radius of this power series is ∞. Then, according to Proposition V.3.5,
f is analytic on C.
As for the next, we set
g(z) :=
9 This
verifies the power series expansion in Application 6.5 for |z| < 1.
We want to show that g is also analytic on C. So we fix N ∈ N× and z ∈ N B and then
consider
1− 2 .
(7.20)
log 1 − 2 = log
n=2N
From the power series expansion of the logarithm (see Theorem V.3.9), we get
|z|2
4 N2
|z|2k
≤ 2
kn2k
n 1 − (z/n)2
3 n2
for z ∈ N B and n ≥ 2N . Therefore, by the Weierstrass majorant criterion, the series
n≥2N
converges absolutely and uniformly on N B.
From (7.20) and the continuity of the exponential function, we can conclude that
= lim exp
log 1 − 2
(7.21)
= exp
and that the convergence is uniform on N B. Consequently, the product
2N−1
(7.22)
also converges uniformly on N B. Observing this holds for every N ∈ N× , we see g is well
defined on C, and, using
gm (z) :=
for z ∈ C and m ∈ N× ,
we find that gm → g locally uniformly on C. Because the “partial product” gm is analytic
on C, it then follows from the Weierstrass convergence theorem for analytic functions10
(Theorem VIII.5.27) that g is also analytic on C.
Because of the identity theorem for analytic functions, the proof will be complete
if we can show that f (x) = g(x) for x ∈ J := (−1/π, 1/π).
From (7.19) and Corollary II.7.9 to the Leibniz criterion for alternating series, we
|f (x) − 1| ≤ π 2 x2 /6 < 1
Therefore, by Theorem V.3.9, log f is analytic on J, and we find
(log f ) (x) = π cot(πx) − 1/x
10 Naturally
for x ∈ J \{0} .
the proof of Theorem VIII.5.27 is independent of the product representation of
the sine, so that we may anticipate this step.
From (7.17) we can read off
(log f ) (x) =
x2 − n2
for x ∈ J \{0} ,
(7.23)
where the right hand series converges normally for every r ∈ (0, 1) in [−r, r]. In particular,
it follows that (7.23) is valid for all x ∈ J.
From formulas (7.21) and (7.22), we learn the relation
log g(x) =
Now it follows from Corollary V.2.9 (the differentiability of the limit of a series of functions) that log g is differentiable on J and that
(log g) (x) =
Consequently, (log f ) = (log g) on J. Finally, because log f (0) = 0 = log g(0) and from
the uniqueness theorem for antiderivatives (Remark V.3.7(a)), we get log f = log g on
the entire J.
We have studied criteria governing whether Fourier series converge uniformly
or in the quadratic mean. It is also natural to inquire about their pointwise convergence. On that subject, there are a multitude of results and simple, suﬃcient criteria coming from classical analysis. Some of the simplest of these criteria are presented in common textbooks (see for example [BF87], [Bla91], [K¨n92], [Wal92]).
For example, it is easy to verify that the Fourier series of Example 7.10(a) converges to the given “square wave” function.
We will not elaborate here, having confined our attention to the essential
results of classical Fourier series in one variable. Of perhaps greater significance
is the L2 theory of Fourier series, which, as we have attempted to show above,
is very general. It finds uses in many problems in mathematics and physics —
for example, in the theory of differential equations in one and more variables —
and plays an extremely important role in modern analysis and its applications.
You will likely meet the Hilbert space theory of orthogonality in further studies of
analysis and its applications, and we also will give an occasional further look at
this theory.
Suppose f ∈ SC2π has
f = 0. Show that there is a ξ ∈ [0, 2π] such that f (ξ) = 0.
Verify that for f ∈ C2π defined by f (t) := | sin(t)| on t ∈ [0, 2π], we have
cos(k·)
4k2 − 1
ek = 1 + 2 cos +2 cos(2·) + · · · + 2 cos(n·)
is called the Dirichlet kernel of degree n. Show that
Dn (t) =
sin (n + 1/2)t
sin(t/2)
Define f ∈ SC2π by
Dn (t) dt = 1 for n ∈ N .
0 < t < 2π ,
t=0.
sin(k·)
Dn (s) ds = (t−π)+2 sin t+· · ·+sin(nt)/n for t ∈ (0, 2π). Exercise 3
(Hints: In (t) :=
for t ∈ (0, 2π).)
and integration by parts result in |In (t)| ≤ 2 (n + 1/2) sin(t/2)
5 Show that the 1-periodic extension of Bn from Bn (X) | [0, 1) to R has the following
Fourier expansion:
B2n = 2
B2n+1 = 2
(−1)n−1 (2n)!
(2π)2n
(−1)n−1 (2n + 1)!
(2π)2n+1
cos(2πk·)
(7.24)
(7.25)
sin(2πk·)
k2n+1
(Hints: It suﬃces to prove (7.24) and (7.25) on [0, 1]. For the restrictions U2n and U2n+1
of the series in (7.24) and (7.25), respectively, to [0, 1], show that Um+1 = (m + 1)Um for
m ∈ N× . From Exercise 4, it follows that U1 (t) = B1 (t). Thus Proposition 6.6(iii) shows
that for m ≥ 2, there is a constant cm such that Um (t) = Bm (t)+cm for t ∈ [0, 1]. Finally
verify 0 Um (t) dt = 0 (Proposition 4.1(ii)) and 0 Bm (t) dt = 0 (Exercise 6.4(iii)). Then
it follows that cm = 0 for m ≥ 2.)
Verify the asymptotic equivalences
|B2n | ∼ 2
∼ 4 πn
(Hints: Exercise 5 shows
B2n = B2n (0) = (−1)n−1 2
Further note Exercise 6.6, Application 7.23(a) and Exercise 6.3(a).)
7 Prove Wirtinger’s inequality: For any −∞ < a < b < ∞ and f ∈ C 1 [a, b] such that
f (a) = f (b) = 0, we have
|f |2 ≤ (b − a)2 /π 2
|f |2 .
The constant (b − a)2 /π 2 cannot be made smaller.
(Hint: Using the substitution x → π(x − a)/(b − a), it suﬃces to consider a = 0 and
b = π. For g : [−π, π] → K such that g(x) := f (x) for x ∈ [0, π] and g(x) := −f (−x) for
x ∈ [−π, 0], show that g ∈ C 1 [−π, π] and that g is odd. Thus it follows from Parseval’s
theorem and Remark 7.20(b) that
|g |2 =
i k gk
|g|2
because g(0) = 0. The claim then follows from the construction of g.)
For f, g ∈ SC2π ,
f (x − y)g(y) dy
is called the convolution of f with g. Show that
(i) f ∗ g = g ∗ f ;
(ii) if f, g ∈ C2π , then f ∗ g ∈ C2π ;
(iii) en ∗ em = δnm en for m, n ∈ Z.
Denote by Dn the n-th Dirichlet kernel. Show that if f ∈ SC2π , then Sn f = Dn ∗ f .
Verify the statements of Remark 7.17(c):
(ii) SC2π
is a Hilbert space;
→ 2 (Z), f →
is a linear isometry.
Prove the generalized Parseval’s theorem
(Hint: Note that
|z + w|2 − |z − w|2 + i |z + i w|2 − i |z − i w|2
and apply Corollary 7.16.)
8 Improper integrals
Until now, we have only been able to integrate jump continuous functions on compact intervals. However, we have seen already in Lemma 6.12 that it is meaningful
to extend the concept of integration to the case of continuous functions defined on
noncompact intervals. Considering the area of a set F that lies between the graph
of a continuous function f : R+ → R+ and the positive half axis, we will show it
is finite if f goes to zero suﬃciently quickly as x → ∞.
To calculate F , we first calculate the area
of F in the interval [0, n], that is, we truncate
the area at x = n, and we then let n go to
∞. The following development will refine this
idea and extend the scope of integration to
functions defined on an arbitrary interval.1
In this section, we assume
• J is an interval with inf J = a and sup J = b;
E := (E, |·|) is a Banach space.
Admissible functions
The map f : J → E is said to be admissible if its restriction to every compact
interval J is jump continuous.
8.1 Examples
(a) Every f ∈ C(J, E) is admissible.
(b) If a and b are finite, then every f ∈ S [a, b], E is admissible.
(c) If f : J → E is admissible, then so is |f | : J → R.
Improper integrals
An admissible function f : J → E is said to be (improperly) integrable, if there
is c ∈ (a, b) for which the limits
α→a+0
β→b−0
exist in E.
1 We confine our presentation in this section to a relatively elementary generalization of integration, saving the complete integration theory for Volume III.
VI.8 Improper integrals
8.2 Lemma If f : J → E improperly integrable, then the limits
exist for every c ∈ (a, b). In addition,
f + lim
for every choice of c, c ∈ (a, b).
By assumption, there is a c ∈ (a, b) such that
ea,c := lim
and ec,b := lim
exist in E. Suppose now c ∈ (a, b). Because of Theorem 4.4,
limα→a+0 α
for every α ∈ (a, b). Thus, the limit ea,c :=
given by ea,c = ea,c +
f exists in E and is
f . It follows likewise that
ec ,b := lim
exists in E with ec ,b = ec,b +
f . Therefore we find
ea,c + ec ,b = ea,c +
f + ec,b +
f = ea,c + ec,b .
Suppose f : J → E is improperly integrable and c ∈ (a, b). We then call
f (x) dx := lim
the (improper) integral of f over J. We may also use the notations
Instead of saying “f is improperly integrable” we may also say, “The integral
a f dx exists or converges.” Lemma 8.2 shows that the improper integral is well
defined, that is, it is independence of the choice of c ∈ (a, b).
8.3 Proposition For −∞ < a < b < ∞ and f ∈ S [a, b], E , the improper integral
coincides with the Cauchy–Riemann integral.
Proof This follows from Remark 8.1(b), the continuity of integrals as functions
of their limits, and Proposition 4.4.
8.4 Examples
(a) Suppose a > 0 and s ∈ C. Then
exists ⇐ Re s > 1 ,
a1−s
Proof According to Remark 8.1(a), the function (a, ∞) → C x → 1/xs is admissible
for every s ∈ C.
(i) We first consider the case s = 1. Then
x1−s
1−s
− s−1
1 − s β s−1
From |β 1−s | = β 1−Re s , it follows that
→ 0 (β → ∞) ⇐ Re s > 1
β s−1
and that limβ→∞ 1/β s−1 does not exist for Re s < 1. Finally, the limit does not exist if
Re s = 1 because s = 1 + i τ for τ ∈ R× implies β s−1 = β i τ = ei τ log β and because the
exponential function is 2πi -periodic.
(ii) If s = 1 then
= lim (log β − log a) = ∞ .
Therefore the function x → 1/x is not integrable on (a, ∞).
(b) Suppose b > 0 and s ∈ C. Then
exists ⇐ Re s < 1 ,
x−s dx = b1−s /(1 − s) for Re s < 1.
The proof is just like the proof of (a).
(c) The integral
xs dx does not exist for any s ∈ C.
(d) The integral
does not exist.2
(e) We have
1 − x2
This follows because
lim arctan
α→−1+0
β→1−0
arcsin
where we have used Example 4.15(a).
The integral comparison test for series
In light of Section 6, we anticipate a connection between infinite series and improper integrals.3 The following theorem produces such a relationship in an important case.
8.5 Proposition Suppose f : [1, ∞) → R+ is admissible and decreasing. Then
f (n) < ∞ ⇐
f (x) dx exists .
(i) The assumption implies
f (n) ≤ f (x) ≤ f (n − 1)
for x ∈ [n − 1, n] and n ∈ N with
n ≥ 2. Therefore we have
f (n) ≤
f (x) dx ≤ f (n − 1) ,
and we find after summing over n that
N −1
f (x) dx ≤
f (n) for N ≥ 2 .
(8.1)
that −∞ x dx does not exist, even though limγ→∞ −γ x dx = 0! It important when
verifying the convergence of an integral to check that the upper and lower limits can be passed
independently.
3 See also the proof of Remark 6.16(d).
(ii) The series
f (n) converges. Then the bound
follows from (8.1) and f ≥ 0. Therefore the function β →
and bounded. Then 1 f dx by Proposition III.5.3.
(iii) The integral
f (x) dx is increasing
f dx converges. Then it follows from (8.1) that
f (x) dx
for N ≥ 2 .
Therefore Theorem II.7.7 implies
f (n) exists also.
8.6 Example For s ∈ R, the series
n≥2
n(log n)s
converges if and only if s > 1.
Proof If s ≤ 0, then
case s > 0. Because
1/n is a divergent minorant. It therefore suﬃces to consider the
(log β)1−s − (log 2)1−s (1 − s) ,
log(log β) − log(log 2) ,
x(log x)s
s=1,
dx x(log x)s exists if and only if s > 1. Then Proposition 8.5 finishes the proof.
Suppose f : J → E is admissible. Then we say f is absolutely integrable (over J)
if the integral a |f (x)| dx exists in R. In this case, we also say a f is absolutely
convergent.
The next theorem shows that every absolutely integrable function is (improperly) integrable.
8.7 Proposition
If f is absolutely integrable, then the integral
Proof Suppose c ∈ (a, b). Because
a δ > 0 such that
f dx exists in
|f | dx exists in R, there is for every ε > 0
for α1 , α2 ∈ (a, a + δ) .
Proposition 4.3 then implies
|f | < ε for α1 , α2 ∈ (a, a + δ) .
(8.2)
Suppose now (αj ) is a sequence in (a, c) such that lim αj = a. Due to (8.2),
α f dx is then a Cauchy sequence in E. Hence there is an e ∈ E such that
f dx → e as j → ∞. If (αj ) is another sequence in (a, c) such then lim αj = a,
then, analogously, there is an e ∈ E such that limj
f dx = e in E.
Now we choose N ∈ N with αj , αj ∈ (a, a + δ) for j ≥ N . From (8.2), we
then get α f − α f < ε for j ≥ N , after which taking the limit j → ∞ gives
the inequality |e − e | ≤ ε. Because this holds for every ε > 0, it follows that
e = e , and we have proved that limα→a+0 α f dx exists in E. Analogously, one
shows that limβ→b−0 c f dx exists in E. Therefore the improper integral a f dx
exists in E.
The majorant criterion
Similarly to the situation with series, the existence of an integrable majorant of f
secures the absolute convergence of a f dx.
8.8 Theorem Suppose f : J → E and g : J → R+ is admissible with
|f (x)| ≤ g(x)
for x ∈ (a, b) .
If g is integrable, then f is absolutely integrable.
Suppose c ∈ (a, b) and α1 , α2 ∈ (a, c). Then using Corollary 4.6, we have
If a g dx exists in R, there is for every ε > 0 a δ > 0 such that
for α1 , α2 ∈ (a, a + δ), and we get
This statement makes possible a corresponding reiteration of the proof Theob
rem 8.7, from which the absolute convergence of a f dx follows.
8.9 Examples Suppose f : (a, b) → E is admissible.
(a) If f is real-valued with f ≥ 0, then f is absolutely integrable if and only if f
is integrable.
(b) (i) Suppose a > 0 and b = ∞. If there are numbers ε > 0, M > 0, and c > a
|f (x)| ≤ 1+ε for x ≥ c ,
then
f is absolutely convergent.
(ii) Suppose a = 0 and b > 0. If there are numbers ε > 0, M > 0 and
c ∈ (0, b) such that
|f (x)| ≤ 1−ε for x ∈ (0, c) ,
This follows from Theorem 8.8 and Examples 8.4(a) and (b).
sin(x) (1 + x2 ) dx converges absolutely.
Proof Obviously, x → 1/(1 + x2 ) is a majorant of x → sin(x)/(1 + x2 ). In addition,
Example 8.4(e) implies the integral 0 dx/(1 + x2 ) exists.
8.10 Remarks (a) Suppose fn , f ∈ S [a, b], E and (fn ) converges uniformly to f .
Then, we have proved in Theorem 4.1 that a fn n∈N converges in E to a f . The
analogous statement does not hold for improper integrals.
We consider the sequence of functions
1 −x/n
for x ∈ R+ and n ∈ N× .
Then every fn belongs to C(R+ , R). In addition, the sequence (fn ) converges uniformly
to 0, since fn ∞ = 1/n.
On the other hand, we have
fn (x) dx =
Altogether, it follows that limn
dx = lim −e−x/n
α→0+
fn dx = 1, but
limn fn dx = 0.
(b) When a sequence of functions is also the integrand in an improper integral, one
can inquire whether it is valid to exchange the order of the corresponding limits.
We refrain from this now and instead develop the results in Volume III as part
of the more general Lebesgue integration theory, to which the necessary analysis
is better suited than the (simpler) Cauchy–Riemann integration theory. In the
framework of Lebesgue theory, we will find a very general and ﬂexible criterion for
when taking limits can be exchanged with integration.
Prove the convergence of these improper integrals:
sin(1/x)
In which cases do the integrals converge absolutely?
Which of these improper integrals have a value?
(Hint for (iii): Consider
xn e−x dx .
log sin x dx = −π log 2. (Hint: sin 2x = 2 sin x cos x.)
4 Suppose −∞ < a < b ≤ ∞ and f : [a, b) → E is admissible. Show then that a f
exists if and only if for every ε > 0 there is a c ∈ [a, b) such that α f < ε for α, β ∈ [c, b).
5 Show that the integral 0
t cos(t2 ) dt converges.4
Suppose −∞ < a < b ≤ ∞, and f : [a, b) → R is admissible.
(i) If f ≥ 0, show
f (x) dx if and only if K := supc∈[a,b)
(ii) Suppose f ∈ C [a, b), R and g ∈ C [a, b), R . Show that if K < ∞ and g(x) tends
monotonically to 0 as x → b − 0, then a f g converges.
7 Suppose that f ∈ C 1 [a, ∞), R with a ∈ R, and suppose that f is increasing with
limx→∞ f (x) = ∞. Prove that a sin f (x) dx converges.
(Hint: Substitute y = f (x) and recall Theorem III.5.7.)
The function f ∈ C [0, ∞), R satisfies sup0<a<b<∞
f (ax) − f (bx)
dx = f (0) log
f < ∞. Show then that
for 0 < a < b .
(Hint: Let c > 0. The existence of ξ(c) ∈ [ac, bc] such that
f (x)
dx = f ξ(c) log
follows from Theorem 4.16.)
Suppose −∞ < a < 0 < b < ∞, and f : [a, 0) ∪ (0, b] → R is admissible. If
ε→0
exists in R, then this limit is called the Cauchy principal value of
P V a f . Compute
−π/2
f , and we write5
x(6 + x − x2 )
4 This exercise shows in particular that the convergence of ∞ f does not allow one to conclude
that f (x) → 0 for x → ∞. Compare this result to Theorem II.7.2.
5 P V stands for “principal value”.
9 The gamma function
In this section, which closes for the time being the theory of integration, we will
study one of the most important nonelementary functions of mathematics, namely,
the gamma function. We will derive its essential properties and, in particular, show
that it interpolates the factorial n → n! , that is, it allows this function to take
arguments that are not whole numbers. In addition, we will prove a refinement
of the de Moivre–Stirling asymptotic form of n! and — as a by-product of the
general theory — calculate the value of an important improper integral, namely,
the Gaussian error integral.
Euler’s integral representation
We introduce the gamma function for z ∈ C with Re z > 0 by considering a
parameter-dependent improper integral.
9.1 Lemma For z ∈ C such that Re z > 0, the integral
tz−1 e−t dt
converges absolutely.
Proof First, we remark that the function t → tz−1 e−t on (0, ∞) is admissible.
(i) We consider the integral on its lower limit. For t ∈ (0, 1], we have
|tz−1 e−t | = tRe z−1 e−t ≤ tRe z−1 .
From Example 8.9(b), it then follows that
1 z−1 −t
e dt
is absolutely convergent.
(ii) For m ∈ N , we have the estimate
= et
for t ≥ 0 .
After multiplying by tRe z−1 , we find
|tz−1 e−t | = tRe z−1 e−t ≤
tm−Re z+1
We choose m ∈ N× such that m > Re z and get from Example 8.9(b) the absolute
convergence of 1 tz−1 e−t dt.
For z ∈ C with Re z > 0, the integral
Γ(z) :=
(9.1)
is called the second Euler integral or Euler’s gamma integral, and (9.1) defines the
gamma function for [Re z > 0] := { z ∈ C ; Re z > 0 }.
VI.9 The gamma function
9.2 Theorem The gamma function satisfies the functional equation
Γ(z + 1) = zΓ(z) for Re z > 0 .
Γ(n + 1) = n! for n ∈ N .
Integrating by parts with u(t) := tz and v (t) := e−t gives
tz e−t dt = −tz e−t
for 0 < α < β < ∞ .
From Proposition III.6.5(iii) and the continuity of the power function on [0, ∞),
we get tz e−t → 0 as t → 0 and as t → ∞. Consequently, we have
Γ(z + 1) = lim
tz e−t dt = z
tz−1 e−t dt = zΓ(z)
and therefore the stated functional equation.
Finally, completing the induction, we have
Γ(n + 1) = nΓ(n) = n(n − 1) · · · · · 1 · Γ(1) = n! Γ(1) for n ∈ N .
Because Γ(1) =
= −e−t
= 1, we are done.1
The gamma function on C\(−N)
For Re z > 0 and n ∈ N× , it follows from Theorem 9.2 that
Γ(z + n) = (z + n − 1)(z + n − 2) · · · · · (z + 1)zΓ(z) ,
Γ(z) =
Γ(z + n)
z(z + 1) · · · · · (z + n − 1)
(9.2)
The right side of (9.2) is defined for every z ∈ C \ (−N) with Re z > −n. This
formula suggests an extension of the function z → Γ(z) onto C\(−N).
9.3 Theorem (and Definition)
Γn (z) :=
For z ∈ C\(−N)
is independent of n ∈ N× if n > − Re z. Therefore we can define through
Γ(z) := Γn (z) for z ∈ C\(−N) and n > − Re z
1 Naturally
is an abbreviation for limb→∞ f
100
an extension to C\(−N) of the function given in (9.1). This extension is called the
gamma function. It satisfies the functional equation
Γ(z + 1) = zΓ(z) for z ∈ C\(−N) .
Proof Due to (9.2), Γn agrees on [Re z > 0] with the function defined in (9.1).
Also, for m, n ∈ N such that n > m > − Re z, we have
Γ(z + n) = (z + n − 1) · · · · · (z + m)Γ(z + m) .
(z + n − 1) · · · · · (z + m)Γ(z + m)
z(z + 1) · · · · · (z + m − 1)(z + m) · · · · · (z + n − 1)
Γ(z + m)
= Γm (z) .
z(z + 1) · · · · · (z + m − 1)
Γn (z) =
Thus the functions Γn and Γm agree where they are both defined. This shows
that the gamma function is well defined and agrees with the Euler gamma integral
when [Re z > 0].
For z ∈ C\(−N) and n ∈ N× such Re z > −n, we have
Γ(z + 1) =
z(z + n)Γ(z + n)
Γ(z + n + 1)
= zΓ(z)
(z + 1) · · · · · (z + n)
z(z + 1) · · · · · (z + n)
since (z + n)Γ(z + n) = Γ(z + n + 1).
Gauss’s representation formula
The gamma function has another representation, with important applications, due
to Gauss.
9.4 Theorem For z ∈ C\(−N), we have
n→∞ z(z + 1) · · · · · (z + n)
Γ(z) = lim
(i) We consider first the case Re z > 0. Because
and, according to Theorem III.6.23,
e−t = lim 1 −
for t ≥ 0 ,
(9.3)
(9.4)
we can guess the formula
tz−1 1 −
for Re z > 0 .
We prove it below.
(ii) Suppose Re z > 0. We integrate (9.4) by parts using u(t) := (1 − t/n)n
and v (t) := tz−1 :
tz 1 −
Another integration by parts gives
z n(z + 1)
tz+1 1 −
n−2
and, completing the induction, we find
tz+n−1 dt
n(z + 1) n(z + 2)
n(z + n − 1) 0
n(z + n − 1) z + n
The claim then follows in this case from (9.4).
(iii) We set
γn (z) :=
for z ∈ C\(−N) .
(9.5)
γn (z) =
z(z + 1) · · · · · (z + k − 1)
· 1+
γn (z + k)
for every k ∈ N× . We now choose k > − Re z, and so, from (ii) and Theorem 9.3,
Γ(z + k)
= Γ(z) .
lim γn (z) =
102
Then it only remains to show (9.4).
(iv) To prove (9.4) for constant z, we can set f (t) := tz−1 e−t and
tz−1 (1 − t/n)n ,
fn (t) :=
0<t≤n,
Then it follows from (9.3) that
lim fn (t) = f (t) for t > 0 .
(9.6)
Because the sequence (1 − t/n)n
(see (v)), we also have
converges monotonically to e−t for t > 0
|fn (t)| ≤ g(t) for t > 0 and n ∈ N ,
(9.7)
where we define the integrable function t → g(t) := tRe z−1 e−t on (0, ∞). Now
it follows from (9.6), (9.7), and the convergence theorem of Lebesgue (proved in
Volume III) that
tz−1 (1 − t/n)n dt = lim
fn (t) dt =
f (t) dt = Γ(z) ,
and therefore (9.4).
To avoid looking ahead to the theorem of Lebesgue, we will directly verify
(9.4). But first, we must prove that the convergence in (9.3) is monotonic and
locally uniform.
(v) From the Taylor expansion of the logarithm (Application IV.3.9(d) or
Theorem V.3.9), we have
log(1 − s)
= −1 −
for s ∈ (0, 1) ,
↑ −1 (s → 0+) .
We now set in (9.8) an s := t/n such that t > 0, and therefore
n log 1 −
(9.8)
↑ −t (n → ∞) .
The monotonicity and the continuity of the exponential function then shows that
the sequence [1 − t/n]n n∈N converges to e−t for every t ≥ 0.
The uniform convergence of this sequence on compact subintervals of R+
follows from the uniform continuity of the map s → log(1−s) s on such intervals
(see Exercise III.3.11). Namely, suppose T > 0 and ε > 0. Then there is a
δ > 0 such that
(9.9)
+ 1 < ε for 0 < s < δ .
Now, we indeed have t/n < δ for. n > T /δ and t ∈ [0, T ]. Defining s := t/n, it
therefore follows from (9.9) that
+ 1 < T ε for 0 ≤ t ≤ T .
Thus the series n log(1−t/n) n∈N converges uniformly to −t on [0, T ]. Because the
exponential function is uniformly continuous on [0, T ], the series (1 − t/n)n n∈N
converges uniformly in t ∈ [0, T ] to e−t .
(vi) Finally, we prove (9.4). Suppose then ε > 0. The proof of Lemma 9.1
shows that there is an N0 ∈ N such that
tRe z−1 e−t dt < ε/3 .
Therefore, from (v), we have
tRe z−1 1 −
tRe z−1 e−t dt ≤
tRe z−1 e−t dt <
for n ≥ N0 . Finally, there is, for the same reasons as in (v), an N ∈ N such that
tRe z−1 e−t − 1 −
For n ≥ N0 ∨ N , we now get
Γ(z) −
≤ Γ(z) −
tz−1 e−t dt +
which shows (9.4).
tRe z−1 e−t dt +
tz−1 e−t dt −
104
The reﬂection formula
As an application of the Gauss product representation, we will deduce an important relationship between the gamma function and the sine. To that end, we first
prove a product form of 1/Γ, which is due to Weierstrass.
9.5 Proposition For z ∈ C\(−N), we have
= zeCz
Γ(z)
(9.10)
where C is the Euler–Mascheroni constant. The infinite product converges absolutely and is locally uniform. In particular, the gamma function has no zeros.
Obviously
= z exp z
γn (z)
for z ∈ C\(−N). Defining ak (z) := (1 + z/k)e−z/k , we have for |z| ≤ R
|ak (z) − 1| = (1 + z/k) 1 − z/k +
(−1)j z
− 1 ≤ c/k 2 ,
(9.11)
with an appropriate constant c := c(R). Hence there is a constant K ∈ N× such
that |ak (z) − 1| ≤ 1/2 for k ≥ K and |z| ≤ R. From this, it follows that
ak (z) exp
log ak (z)
(9.12)
k=K+1
for n > K and |z| ≤ R. From the power series representation of the logarithm, we
get the existence of a constant M ≥ 1 such that | log(1 + ζ)| ≤ M |ζ| for |ζ| ≤ 1/2.
Thus we find using (9.11) (which changes everything) the estimate
| log ak (z)| ≤ M
k −2
|ak (z) − 1| ≤ cM
for |z| ≤ R. The series k>K log ak (z) then converges absolutely and uniformly for
|z| ≤ R due to the Weierstrass majorant criterion. From (9.12) and the properties
of the exponential function, this holds also for the infinite product appearing in
(9.10). The claim now follows from Theorem 9.4 and Example 6.13(b).
We can use this product representation to get the previously mentioned relationship between the sine and the gamma function.
9.6 Theorem (reﬂection formula for the Γ function2 ) For z ∈ C\Z, we have
Γ(z)Γ(1 − z) =
The representation (9.10) implies (see Proposition II.2.4(ii))
Γ(z) Γ(1 − z)
−zΓ(z)Γ(−z)
for z ∈ C\Z. The claim now follows from Application 7.23(b).
Using the reﬂection formula, we can compute the value of some important
improper integrals.
9.7 Application (Gaussian error integral3 )
The identity says
e−x dx = Γ(1/2) =
Using the substitution x =
Γ(1/2) =
because x → e
−x2
t, we find4
e−t √ = 2
e−x dx =
e−x dx ,
is even. The claim then follows from the reﬂection formula.
In Volume III, we will learn an easier method for calculating the Gaussian
error integral, which uses a multivariable integration.
Using
ϕn (z) := z log n −
log(z + k) + log(n!) for n ∈ N× ,
z ∈ C\(−N) ,
we get from (9.5) the representation γn = eϕn . Then, by taking the logarithmic
derivative γn /γn of γn , we find the simple form
ψn (z) :=
(9.13)
called because its left side is even about z = −1/2.
name “error integral” comes from the theory of probability, in which the function x →
e−x plays a fundamental role.
4 This substitution is carried out in the proper integral
, and then the limits α → 0+ and
β → ∞ are taken.
2 So
3 The
106
and, further,
ψn (z) =
(z + k)2
(9.14)
for n ∈ N× and z ∈ C\(−N).
We can now prove the following forms of the first two logarithmic derivatives
of the gamma function.
9.8 Proposition Γ ∈ C 2 C\(−N) . Then we have
Γ (z)
(z) =
(9.15)
(9.16)
for z ∈ C\(−N), where C is the Euler–Mascheroni constant.
Proof We first must show that the sequence (ψn ) and (ψn ) on C\(−N) is locally
uniformly convergent. So we consider
ψn (z) = log n −
Thus, we must show the local uniform convergence of the series
k(z + k)
(z + k)−2
(9.17)
on C\(−N).
Suppose z0 ∈ C\(−N) and 0 < r < dist(z0 , −N). Then, for z ∈ B(z0 , r), we
have the estimate
|z + k| ≥ |z0 + k| − |z − z0 | ≥ k − |z0 | − r ≥ k/2
such that k ≥ k0 := 2(|z0 | + r). Consequently, we find
≤ 2 ,
|z + k|2
for z ∈ B(z0 , r) and k ≥ k0 . Thus follows the uniformly convergence on B(z0 , r) —
and accordingly the local uniform convergence on C\(−N) — of the series (9.17)
from the Weierstrass majorant criterion (Theorem V.1.6). Therefore we have
ψ(z) := lim ψn (z) = −C −
(9.18)
From the theorem of the differentiability of the limits of series of functions (Theorem V.2.8), it further follows that ψ is continuously differentiable on C\ (−N),
ψ (z) = lim ψn (z) =
(9.19)
For z ∈ C\(−N), we can also write ϕn (z) in the form
ϕn (z) = − log z + z log n −
− log 1 +
log(1 + z/k) = z/k + O (z/k)2
(k → ∞)
and Example 6.13(b) imply that ϕn (z) converges to
ϕ(z) := − log z − Cz +
as n → ∞. Thus we get from γn = eϕn and Theorem 9.4 that
Γ(z) = eϕ(z)
(9.20)
Because ϕn = ψn and the sequence (ψn ) is locally uniformly convergent, Theorem V.2.8 then guarantees that ϕ is continuously differentiable with ϕ = ψ. Now
it follows from (9.20) that Γ is also continuously differentiable with
Γ = ϕ eϕ = ψΓ .
(9.21)
Then (9.15) follows from (9.18), and (9.16) is a consequence of (9.19). Finally, that
Γ ∈ C 2 C\(−N) follows from (9.21) and the continuous differentiability of ψ.
9.9 Remarks (a) The above theorem and Theorem VIII.5.11 imply that Γ is
analytic on C\(−N).
(b) Because (Γ /Γ) = Γ Γ − (Γ )2
Γ (x)Γ(x) > Γ (x)
Γ2 , we get from (9.16) that
≥ 0 for x ∈ R\(−N) .
Therefore sign Γ (x) = sign Γ(x) for x ∈ R\ (−N). From Gauss’s formula of
Theorem 9.4, we read off that
sign Γ(x) =
(−1)k ,
x>0,
−k < x < −k + 1 , k ∈ N .
Therefore Γ is convex on (0, ∞) and the intervals (−2k, −2k + 1) but concave on
the intervals (−2k − 1, −2k), where k ∈ N.
108
(c) A function f that is everywhere strictly positive on a perfect interval is said
to be log convex if log f is convex. If f is twice differentiable, then, according
to Corollary IV.2.13, f is log convex if and only if f f − (f )2 ≥ 0. Therefore
the gamma function is log convex on (0, ∞). One can show that Γ|(0, ∞) is the
unique function f : (0, ∞) → (0, ∞) that satisfies the conditions
(i) f is log convex,
(ii) f (x + 1) = xf (x) and for x > 0, and
(iii) f (1) = 1.
For a proof and a construction of the theory of the gamma function on the basis
of these properties, we refer to [Art31] (see also [K¨n92] and Exercise 6).
Stirling’s formula
The de Moivre–Stirling formula describes the asymptotic behavior of the factorial
n → n! as n → ∞. Example 6.13(a) says
Γ(n) = (n − 1)! ∼
2π nn− 2 e−n .
(9.22)
The following theorem refines and amplifies this statement.
9.10 Theorem (Stirling’s formula) For every x > 0, there is a θ(x) ∈ (0, 1) such
Γ(x) = 2π xx−1/2 e−x eθ(x)/12x .
For γn , we get from (9.5) that
log γn (x) = log n! + x log n −
log(x + k) for x > 0 .
To the sum, we apply the Euler–Maclaurin formula (Theorem 6.9) with a := 0,
b := n and m := 0 and find
log(x + k) =
log(x + t) dt +
log x + log(x + n) + Rn (x) ,
B1 (t)
0 x+t
Integrating by parts with u(t) := log(x + t) and v = 1 then gives
Rn (x) :=
log(x + t) dt = (x + t) log(x + t) − 1
= (x + n) log(x + n) − n − x log x ,
and thus
log γn (x) = x −
log(x + n) − Rn (x) .
Because log(x + n) = log n + log(1 + x/n), it follows that
log(x + n) = x log n + log n(n+ 2 ) + x +
log 1 +
+ log 1 +
and we get
log x − log 1 +
+ log n+1/2 −n − Rn (x) .
(9.23)
To estimate Rn (x), we consider
−Rn (x) = −
g(x + k)
g(x) := −
t − 1/2
−1=
−1 ,
1−y
where we have set y := 1/(2x + 1). For x > 0 we have 0 < y < 1. Therefore, from
log (1 + y)/(1 − y) = log(1 + y) − log(1 − y)
and the power series expansion of the logarithm
y 2k+1
110
0 < g(x) =
y 2k
y 2k =
1 y2
3 1 − y2
=: h(x)
12x(x + 1)
12x 12(x + 1)
for x > 0 .
Thus k h(x + k) represents a convergent majorant of the series
g(x + k).
R(x) := lim Rn (x) = −
g(x + k) ,
exists, and we have
for x > 0 ,
since 1/12x is the value of the series k h(x + k) and g(x) < h(x) for x > 0.
Now we pass the limit n → ∞ in (9.23) and find from Theorem 9.4 that
0 < −R(x) <
log Γ(x) = x −
log x − x + log 2π − R(x) ,
where we have reused the de Moivre–Stirling formula (9.22). The theorem now
follows with θ(x) := −12xR(x).
Stirling’s formula allows the approximate calculation of the gamma function,
and, in particular, it √
approximates large arguments of the factorial n! . If one
approximates Γ(x) as 2π xx−1/2 e−x , the relative error is smaller than e1/12x − 1.
This error diminishes rapidly as x grows large.
The Euler beta integral
The integral
tp−1 (1 − t)q−1 dt ,
(9.24)
called the first Euler integral, plays an important role in the context of the gamma
function.
9.11 Proposition The integral (9.24) converges absolutely for p, q ∈ [Re z > 0].
Because
|tp−1 (1 − t)q−1 | dt =
it suﬃces to consider the case p, q ∈ R.
tRe p−1 (1 − t)Re q−1 dt ,
(i) Suppose q ∈ R. Then there are numbers 0 < m < M such that
m ≤ (1 − t)q−1 ≤ M
for 0 ≤ t ≤ 1/2 .
Consequently, it follows from Example 8.9(b) that
tp−1 (1 − t)q−1 dt exists ⇐ p > 0 .
(ii) Suppose now p > 0. Then there are numbers 0 < m < M such that
m ≤ tp−1 ≤ M
for 1/2 ≤ t ≤ 1 .
Therefore 1/2 tp−1 (1 − t)q−1 dt exists if and only if 1/2 (1 − t)q−1 dt exists. Using
the substitution s := 1 − t, it follows5 from Example 8.9(b) that
(1 − t)q−1 dt =
sq−1 ds exists ⇐ q > 0 ,
as claimed.
Using these theorems, the Euler beta function B is well defined through
B : [Re z > 0] × [Re z > 0] → C ,
(p, q) →
tp−1 (1 − t)q−1 dt .
9.12 Remarks (a) The gamma function and the beta function are connected by
the functional equation
Γ(p)Γ(q)
= B(p, q)
Γ(p + q)
for p, q ∈ [Re z > 0] .
(9.25)
A proof of this statement for p, q ∈ (0, ∞) is outlined in Exercises 12 and 13. We
treat the general case in Volume III.
(b) For p, q ∈ [Re z > 0], we have
B(p, q) = B(q, p)
and B(p + 1, q) =
B(p, q + 1) .
Proof The first statement follows immediately from (9.25). Likewise (9.25) and Theorem 9.2 imply
pΓ(p)Γ(q)
B(p, q) ,
B(p + 1, q) =
(p + q)Γ(p + q)
and then also, by symmetry, B(p, q + 1) = q/(p + q) B(p, q).
5 As already noted for improper integrals, we make these substitutions before passing any
limits to infinity.
112
(c) Using the functional equation (9.25) and the properties of the gamma function,
the beta function can be defined for all p, q ∈ C such that p, q, (p + q) ∈ −N.
2k − 1
= 1 · 3 · · · · · (2n − 1) n
Show that Γ belongs to C ∞ C\(−N), C with
Γ(n) (z) =
tz−1 (log t)n e−t dt
for n ∈ N and Re z > 0 .
(Hint: Consider (9.4).)
Let C be the Euler–Mascheroni constant. Verify these statements:
e log t dt = −C.
e t log t dt = 1 − C.
∞ −t n−1
log t dt = (n − 1)!
2 −t
(log t) e dt = C 2 + π 2 /6.
1/k − C for n ∈ N and n ≥ 2.
(Hint: To compute the integrals in (iv), consider Application 7.23(a).)
Show that the gamma function has the representation
(− log t)z−1 dt
for z ∈ C such that Re z > 0.
Suppose f and g are log convex. Show that f g is also log convex.
Suppose the function f : (0, ∞) → (0, ∞) is differentiable6 and satisfies
(i) f is log convex;
(ii) f (x + 1) = xf (x) for x > 0;
Show then that f = Γ | (0, ∞).
(Hints: (α) Define h := log(Γ/f ). Using (iii), it suﬃces to verify h = 0.
(β) From (ii) and Theorem 9.2, it follows that h is 1-periodic.
(γ) From (i) it follows that (log f ) is increasing (see Theorem IV.2.12). This implies
0 ≤ (log f ) (x + y) − (log f ) (x) ≤ (log f ) (x + 1) − (log f ) (x) = 1/x
for 0 < y ≤ 1, where the last step follows from reusing (ii). An analogous estimate holds
also for (log Γ) . Therefore
−1/x ≤ h (x + y) − h (x) ≤ 1/x
6 One
for y ∈ (0, 1] and x ∈ (0, ∞) .
can do without the differentiability of f (see [Art31], [K¨n92]).
Then that h, h , and h (· + y) are 1-periodic implies that
−1/(x + n) ≤ h (x + y) − h (x) ≤ 1/(x + n)
for x, y ∈ (0, 1] and n ∈ N× .
For n → ∞, it now follows that h = 0.)
Prove the Legendre duplication formula
= x−1 Γ(x)
for x ∈ (0, ∞) .
(Hint: Consider h(x) := 2x Γ(x/2) Γ (x + 1)/2 and apply Exercise 6.)
For x ∈ (−1, 1), verify the power series expansion
(log Γ)(1 + x) = −Cx +
ζ(k) k
(Hint: Expand log(1 + x/n) as a power series for n ∈ N× , and look to Proposition 9.5.)
9 Show 0 log Γ(x) dx = log 2π. (Hint: Exercise 8.4 and Theorem 9.6).
Verify that for 0 < a < b, we have
(x − a)(b − x)
11 For fixed q ∈ (0, ∞), show the function (0, ∞) → (0, ∞), p → B(p, q) is differentiable
and log convex. (Hint: Show ∂p log tp−1 (1 − t)q−1 = 0 for p, q ∈ (0, ∞) and t ∈ (0, 1).)
Prove (without using (9.25)) that for p, q ∈ (0, ∞), we have
B(p + 1, q) = pB(p, q)/(p + q) .
For p, q ∈ (0, ∞), show
Γ(p) Γ(q)
= B(p, q) .
(Hints: Fix q ∈ (0, ∞) and let
f : (0, ∞) → (0, ∞) ,
p → B(p, q) Γ(p + q)/Γ(q) .
According to Exercise 11 and Proposition 9.8, f is differentiable, and Exercises 5 and 11
show that f is log convex. Further, it follows from Exercise 12 that f satisfies the
functional equation f (p + 1) = pf (p). Then f (1) = 1 and Exercise 6 implies the claim.)
Chapter VII
Multivariable differential
calculus
In Volume I, we used the differential calculus to extract deep insight about the
“fine structure” of functions. In that process, the idea of linear approximations
proved to be extremely effective. However, we have until now concentrated on
functions of one variable.
This chapter is mainly devoted to extending the differential calculus to functions of multiple variables, but we will also further explore the simple idea of
linear approximations. Indeed — in contrast to the one-dimensional case — the
issues here are intrinsically more complicated, because linear maps in the multidimensional case show a considerably richer structure than those in one dimension.
As before, we prefer to work in a coordinate-free representation. In other
words, we develop the differential calculus for maps between Banach spaces. This
representation is conceptually simple and actually makes many expressions look
much simpler. The classical formulas for the derivatives in the usual coordinate
representation follow easily from the general results using the product structure
of finite-dimensional Euclidean spaces.
Because linear maps between Banach spaces underlie the linear approximation and therefore also differentiability, the first section is devoted to studying
spaces of linear operators. Special interest falls naturally on the finite-dimensional
case, which we develop using basic rules for matrices from linear algebra.
As an application of these results, we will study the exponential function in
the algebra of endomorphisms of Banach spaces and derive from it the basic facts
about systems of ordinary differential equations and then second order differential
equations with constant coeﬃcients.
Section 2 establishes another central concept, the Fr´chet derivative. Beyond
this, we consider directional derivatives, which arise naturally from partial deriv-
116
VII Multivariable differential calculus
atives and the representation of the Fr´chet derivative by means of the Jacobi
matrix. Finally, we examine the connection between the differentiability of functions of a complex variable and the total differentiability of the corresponding real
representation. We characterize complex differentiability by the Cauchy–Riemann
equations.
In Section 3, we put together the rules for differentiation and derive the
important mean value theorem through a simple generalization of the one variable
case.
Before we can turn to higher derivatives, we must clarify the connection
between multilinear maps and linear-map-valued linear operators. The simple
results developed in Section 4 will build the foundation for a clear representation
of higher derivatives in multiple variables.
In Section 5, we explain the concept of higher order derivatives. In particular, we prove the fundamental Taylor’s formula — both for maps between Banach
spaces and for functions of finitely many variables. Generalizing the criterion in
the one-dimensional case, we will specify suﬃcient conditions for the presence of
local extrema of functions of multiple variables.
Section 6 plays a special role. Here, we shall see the first reward for developing
the differential calculus on Banach spaces. Using it, we will explain the calculus
of variations and derive the important Euler–Lagrange differential equations. In
the process, we hope you will appreciate the power of abstract approach, in which
functions are regarded as points in an infinite-dimensional Banach space. This
simple geometrical idea of “functions of functions” proves to have wide-ranging
consequences. Here, in the calculus of variations, we seek to minimize a certain
function of a function, whose form is usually determined by a physical principle,
such as “least action”. The minima, if present, occur at critical points, and this
criterion leads to the Euler–Lagrange differential equation(s), whose importance
for mathematics and physics cannot be overstated.
After this excursion into “higher analysis”, we prove in Section 7 perhaps
the most important theorem of differential calculus, namely, the inverse function
theorem. Equivalent to this result is the implicit function theorem, which we derive
in the subsequent sections. Without extra trouble, we can also prove this theorem
for maps between Banach spaces. And again, the coordinate-free representation
yields significant gains in clarity and elegance.
In Section 8, we give a glimpse of the theory of nonlinear ordinary differential equations. Using the implicit function theorem, we discuss first order scalar
differential equations. We also prove the Picard–Lindel¨f theorem, which is the
fundamental existence and uniqueness theorem for ordinary differential equations.
In the balance of this chapter, we will illustrate the importance of the implicit
function theorem in the finite-dimensional case. With its help, we will characterize submanifolds of Rn ; these are subsets of Rn that locally “look like” Rm for
m < n. Through numerous examples of curves, surfaces, and higher-dimensional
VII Multivariable differential calculus
submanifolds, we will see that this resemblance can be better visualized and more
precisely described through their tangential spaces. Here, we will concentrate on
submanifolds of Euclidean vector spaces, as they are conceptually simpler than
abstract manifolds (which are not defined as subsets of Rn ). However, we will
lay the groundwork for analysis on general manifolds, which we will then tackle
in Volume III. As a practical application of these geometric ideas, we will study
constrained minimization problems and their solution by the method of Lagrange
multipliers. After deriving the relevant rules, we will treat two nontrivial examples.
118
1 Continuous linear maps
As we just said, differentiation in the multivariable case depends on the idea of
local approximations by aﬃne functions. Of course, from Proposition I.12.8, the
nontrivial part of an aﬃne function between vectors spaces is just its linear map.
Thus we will concentrate next on linear maps between normed vector spaces.
After deducing a few of their fundamental properties, we will use them to study
the exponential map and the theory of linear differential equations with constant
coeﬃcients.
• E = (E, · ) and F = (F, · ) are normed vector spaces1 over the field K.
The completeness of L(E, F )
From Section VI.2, we know already that L(E, F ), the space of all bounded linear
maps from E to F , is a normed vector space. Now we explore the completeness of
this space.
1.1 Theorem If F is a Banach space, then so is L(E, F ).
Proof (i) Suppose (An ) is a Cauchy sequence in L(E, F ). Then for every ε > 0,
there is an N (ε) ∈ N such that An − Am < ε for m, n ≥ N (ε). In particular, for
every x ∈ E, we have
An x − Am x ≤ An − Am
for m, n ≥ N (ε) .
Therefore (An x) is a Cauchy sequence in F . Because F is complete, there is a
unique y ∈ F such that lim An x = y. We can then define a map A : E → F by
x → lim An x. From the linearity of limits it follows at once that A is linear.
(ii) Because it is a Cauchy sequence, (An ) is bounded in L(E, F ) (see Proposition II.6.3). Thus there is an α ≥ 0 such that An ≤ α for all n ∈ N. From
this, it follows that
An x ≤ An
for n ∈ N and x ∈ E .
Leaving out the middle term, the limit n → ∞ implies the estimate Ax ≤ α x
for every x ∈ E. Therefore A belongs to L(E, F ).
(iii) Finally we prove that the sequence (An ) in L(E, F ) converges to A.
From (1.1), it follows that
An x − Am x ≤ ε for n, m ≥ N (ε) and x ∈ BE .
1 When
necessary, we distinguish the norms in E and F by corresponding indices.
VII.1 Continuous linear maps
For m → ∞ this implies
An x − Ax ≤ ε
for n ≥ N (ε) and x ∈ BE ,
and, as claimed, we get the inequality
An − A = sup
for n ≥ N (ε)
by forming the supremum over BE .
1.2 Corollary
(i) L(E, K) is a Banach space.
(ii) If E is Banach space, then L(E) is a Banach algebra with unity.
(i) is clear, and (ii) follows from Remark VI.2.4(g).
Finite-dimensional Banach spaces
The normed vector spaces E and F are called (topologically) isomorphic if there is
a continuous linear isomorphism A from E to F such that A−1 is also continuous,
that is, if A belongs to L(F, E). Then A is a topological isomorphism from E
to F . We denote by
Lis(E, F )
the set of all topological isomorphisms from E to F , and we write E ∼ F if
Lis(E, F ) is not empty.2 Also, we set
Laut(E) := Lis(E, E) .
Thus Laut(E) is the set of all topological automorphisms of E.
1.3 Remarks (a) The spaces L(K, F ) and F are isometrically3 isomorphic. More
precisely, using the isometric isomorphism
L(K, F ) → F ,
A → A1 ,
we canonically identify L(K, F ) and F as L(K, F ) = F .
Proof It is clear that the map (1.2) is linear and injective. For v ∈ F , consider
Av ∈ L(K, F ) with Av x := xv. Then Av 1 = v. Therefore A → A1 is a vector space
isomorphism. Furthermore,
= |x| A1
≤ A1
for x ∈ BK and A ∈ L(K, F ) .
2 Note that in normed vector spaces ∼ always means “topologically isomorphic” and not just
that there exists a vector space isomorphism.
3 See Example III.1.3(o).
120
This implies A
L(K,F )
= A1
Therefore (1.2) is an isometry.
(b) Laut(E) is a group, the group of topological automorphisms of E, where the
multiplication is defined through the composition of the two linear maps.
This follows from Remark VI.2.4(g).
(c) If E and F are isomorphic, then E is a Banach space if and only if F is.
Proof It is easy to see that a topological isomorphism maps Cauchy sequences to Cauchy
sequence and maps convergent sequences to convergent sequences.
(d) Suppose E and F are Banach spaces, and A ∈ L(E, F ) is bijective. Then A
is a topological isomorphism, that is, A ∈ Lis(E, F ).
Proof This theorem is a standard part of functional analysis (see the Banach homomorphism theorem, also known as the open mapping theorem).
1.4 Theorem Suppose {b1 , . . . , bn } is a basis of E. Then
T : E → Kn ,
xj bj → (x1 , . . . , xn )
is a topological isomorphism, that is, every finite-dimensional normed vector space
is topologically isomorphic to a Euclidean vector space.
Obviously, T is well defined, linear, and bijective, and
T −1 x =
for x = (x1 , . . . , xn ) ∈ Kn ,
(see Remark I.12.5). From the Cauchy–Schwarz inequality of Corollary II.3.9, we
T −1x ≤
with β :=
|xj |2
. Therefore T −1 belongs to L(Kn , E).
We set |x|• := T −1 x for x ∈ Kn . It is not diﬃcult to see that |·|• is a norm
on K (see Exercise II.3.1). In Example III.3.9(a), we have seen that all norms
are equivalent on Kn . Hence there is an α > 0 such that |x| ≤ α |x|• for x ∈ Kn .
Thus we have
|T e| ≤ α |T e|• = α e
that is, T ∈ L(E, Kn ), and we are done.
for e ∈ E ,
1.5 Corollary If E is finite-dimensional, these statements hold:
(i) All norms on E are equivalent.
(ii) E is complete and therefore a Banach space.
Let A := T −1 with the T of (1.3).
(i) For j = 1, 2, let · j be norms on E. Then x → |x|(j) := Ax
on Kn . Hence, there is from Example III.3.9(a) an α ≥ 1 such that
α−1 |x|(1) ≤ |x|(2) ≤ α |x|(1)
Because e
are norms
= |A−1 e|(j) it follows from this that
α−1 e
Hence ·
and ·
≤ e
≤α e
for e ∈ E .
are equivalent norms on E.
(ii) This follows from Remark 1.3(c) and Theorems 1.4 and II.6.5.
1.6 Theorem Let E be finite-dimensional. Then Hom(E, F ) = L(E, F ). In
other words, every linear operator on a finite-dimensional normed vector space is
continuous.
Proof We define T ∈ Lis(E, Kn ) through (1.3). Then there exists a τ > 0 such
that |T e| ≤ τ e for e ∈ E.
Now suppose A ∈ Hom(E, F ). Then Ae =
Cauchy–Schwarz inequality (with xe := T e) says
Ae ≤
|xe | = α |xe |
xj Abj . Consequently, the
where we have set α :=
. Thus we get
Ae ≤ α |xe | = α |T e| ≤ ατ e
Finally, according to Theorem VI.2.5, A is continuous.
1.7 Remarks (a) The statements of Corollary 1.5 and Theorem 1.6 do not hold
for infinite-dimensional normed vector spaces.
Proof (i) We set E := BC 1 (−1, 1), R . In the normed vector space (E, ·
consider the sequence (un ) with un (t) := t2 + 1/n for t ∈ [−1, 1] and n ∈ N× .
It is easy to see that (un ) is a Cauchy sequence in (E, · ∞ ). We now assume
(E, · ∞ ) is complete. Then there is a u ∈ E such that lim un − u ∞ = 0. In particular,
(un ) converges pointwise to u.
122
Obviously, un (t) = t2 + 1/n → |t| as n → ∞ for t ∈ [−1, 1]. Therefore, it follows
from the uniqueness of limits (with pointwise convergence) that
(t → |t|) = u ∈ BC 1 (−1, 1), R ,
which is false. The contradiction shows (E, ·
is not complete.
Finally, we know from Exercise V.2.10 that BC 1 (−1, 1), R , · • with the norm
u • := u ∞ + u ∞ is a Banach space. Thus, due to Remark II.6.7(a), · and · •
cannot be equivalent.
(ii) We set
E := C 1 [0, 1], R , ·
F := C [0, 1], R , ·
and consider A : E → F , u → u . Then E and F are normed vector spaces, and A is
linear. We assume that A is bounded and therefore continuous. Because (un ) with
un (t) := (1/n) sin(n2 t)
for n ∈ N× and t ∈ [0, 1]
converges to zero in E, we have Aun → 0 in F .
From (Aun )(t) = n cos(n2 t), we have (Aun )(0) = n. But, since (Aun ) converges to
0 in F and since uniformity implies pointwise convergence, this is not possible. Thus A
does not belong to L(E, F ), that is, Hom(E, F )\L(E, F ) = ∅.
(b) Every finite-dimensional inner product space is a Hilbert space.
(c) Suppose E is a finite-dimensional Banach space. Then there is an equivalent
Hilbert norm on E, that is, a norm induced by a scalar product. In other words,
every finite-dimensional Banach space can be renormed into a Hilbert space.4
Suppose n := dim E and T ∈ Lis(E, Kn ). Then
(x | y)E := (T x | T y)
for x, y ∈ E
defines a scalar product on E. The claim then follows from Corollary 1.5(i).
Matrix representations
Let m, n ∈ N× . We denote by Km×n the set of
⎡ 1
a1 · · ·
[ak ] = ⎣ .
all (m × n) matrices
· · · am
with entries aj in K. Here the upper index is the row number, and the lower index
is the column number.5 When required for clarity, we will write [aj ] as
4 Assuming
a finite dimension is important here.
of aj , we will occasionally write ajk or ajk , where the first index is always the row
number and the second is always the column number.
5 Instead
[aj ] 1≤j≤m .
1≤k≤n
Finally, we set
|aj |2
and M := [aj ] ∈ Km×n .
(1.4)
We will assume you are familiar with the basic ideas of matrices in linear algebra
and, in particular, you know that Km×n is a K-vector space of dimension mn with
respect to the usual matrix addition and multiplication by scalars.
1.8 Proposition
(i) By (1.4), we define a norm || ·| on Km×n , called the Hilbert-Schmidt norm.
Km×n := (Km×n , || ·| )
is a Banach space.
(ii) The map
[aj ] → (a1 , . . . , a1 , a2 , . . . , a2 , . . . , am , . . . , am )
We leave the simple proof to you.6
In the following, we will always implicitly endow Km×n with Hilbert–Schmidt
Suppose E and F are finite-dimensional, and suppose
E = {e1 , . . . , en } and
F = {f1 , . . . , fm }
are (ordered) bases of E and F . According to Theorem 1.6, Hom(E, F ) = L(E, F ).
We will represent the action of A ∈ L(E, F ) on vectors in E using the bases
E and F . First, for k = 1, . . . , n, there are unique numbers a1 , . . . , am such
that Aek =
j=1 ak fj . Thus from the linearity of A, we have for every x =
k=1 x ek ∈ E that
aj xk fj =
(Ax)j fj
aj xk
(Ax)j :=
xk Aek =
also Exercise II.3.14.
for j = 1, . . . , m .
124
[A]E,F := [aj ] ∈ Km×n
and call [A]E,F the representation matrix of A in the bases E of E and F of F .
(If E = F , we simply write [A]E for [A]E,E .)
Now let [aj ] ∈ Km×n . For x =
xk ek ∈ E we set
aj xk fj .
Ax :=
Then A := (x → Ax) is a linear map from E to F whose representation matrix
is [A]E,F = [aj ].
The next theorem summarizes the most important properties of the representation matrices of linear maps between finite-dimensional vector spaces.
1.9 Theorem
(i) We set n := dim E and m := dim F , and let E and F be respective bases of
E and F . Then the matrix representation
L(E, F ) → Km×n ,
A → [A]E,F
(1.5)
is a topological isomorphism.
(ii) Suppose G is a finite-dimensional normed vector space and G is a basis of G.
Then,7
[AB]E,G = [A]F ,G [B]E,F
for A ∈ L(F, G) and B ∈ L(E, F ) .
Proof (i) The map (1.5) is clearly linear. The above considerations show that it
is bijective also (see also [Gab96, Chapter D.5.5]). In particular, dim L(E, F ) =
dim Km×n . Because Km×n ∼ Kmn , the space L(E, F ) has dimension mn. Hence
we can apply Theorem 1.6, and we find
A → [A]E,F ∈ L L(E, F ), Km×n and [A]E,F → A ∈ L Km×n , L(E, F ) .
Therefore A → [A]E,F is a topological isomorphism.
(ii) This fact is well known in linear algebra (for example, [Gab96, Chapter D.5.5]).
In analysis, we will consider all maps of metric spaces in the space of continuous linear maps between two Banach spaces E and F . If these are finitedimensional, then, according to Theorem 1.9, we can consider the maps to be
matrices. In this case, it is easy to prove from the above results that the maps are
continuous. As the next important corollary shows, it suﬃces to verify that the
entries in the representation matrix are continuous.
7 For
two composed linear maps, we usually write AB instead of A ◦ B.
1.10 Corollary Suppose X is a metric space and E and F are finite-dimensional
Banach spaces with respective dimensions n and m and ordered bases E and F .
Let A(·) : X → L(E, F ), and let aj (x) be the representation matrix A(x) E,F
of A(x) for x ∈ X. Then
A(·) ∈ C X, L(E, F )
aj (·) ∈ C(X, K), 1 ≤ j ≤ m, 1 ≤ k ≤ n .
Proof This follows immediately from Theorems 1.9, Proposition 1.8(ii), and
Proposition III.1.10.
Convention In Kn , we use the standard basis {e1 , . . . , en } defined in Example I.12.4(a) unless we say otherwise. Also, we denote by [A] the representation matrix of A ∈ L(Kn , Km ).
The exponential map
Suppose E is a Banach space and A ∈ L(E). From Corollary 1.2(ii), we know that
L(E) is a Banach algebra. Thus Ak belongs to L(E) for k ∈ N, and
Ak ≤ A
Therefore the exponential series
exponential series
αk /k! is for every α ≥ A a majorant of the
Ak /k!
(1.6)
in L(E). Hence from the majorant criterion (Theorem II.8.3), the series (1.6)
converges absolutely in L(E). We call its value
eA :=
the exponential of A, and
L(E) → L(E) ,
A → eA
is the exponential map in L(E).
If A and t ∈ K, then tA also belongs to L(E). Therefore the map
U := UA : K → L(E) ,
t → etA
(1.7)
is well defined. In the next theorem, we gather the most important properties of
the function U .
126
1.11 Theorem Suppose E is a Banach space and A ∈ L(E). Then
(i) U ∈ C ∞ K, L(E) and U = AU ;
(ii) U is a group homomorphism from the additive group (K, +) to the multiplicative group Laut(E).
(i) It is enough to show that for every r ≥ 1, we have
U ∈ C 1 rBC , L(E)
and U (t) = AU (t) for |t| < r .
The conclusion then follows by induction.
For n ∈ N and |t| < r, define fn (t) := (tA)n /n! . Then fn ∈ C 1 rBC , L(E) ,
fn converges pointwise to U |rBC . Also f˙n = Afn−1 , and thus
fn−1 (t) ≤ (r A )n /(n − 1) for t ∈ rBC and n ∈ N× .
f˙n (t) ≤ A
Therefore a scalar exponential series is a majorant series for
f˙n . Hence
converges normally to AU |rBC on rBC , and the theorem then follows from Corollary V.2.9.
(ii) For s, t ∈ K, sA and tA commute in L(E). The binomial theorem implies
(Theorem I.8.4)
(sA)k (tA)n−k .
(sA + tA)n =
Using the absolute convergence of the exponential series, we get, as in the proof
of Example II.8.12(a),
U (s + t) = e(s+t)A = esA+tA = esA etA = U (s)U (t)
1.12 Remarks
Suppose A ∈ L(E) and s, t ∈ K.
(a) The exponentials esA and etA commute: esA etA = etA esA .
etA ≤ e|t|
This follows from Remark II.8.2(c).
(c) (etA )−1 = e−tA .
(d) ∂t etA = An etA = etA An for n ∈ N.
By induction, this follows immediately from Theorem 1.11(i).
1.13 Examples
(a) If E = K, the exponential map agrees with the exponential
(b) Suppose N ∈ L(E) is nilpotent, that is, there is an m ∈ N such that N m+1 = 0.
=1+N +
eN =
where 1 denotes the unit element of L(E).
(c) Suppose E and F are Banach spaces, A ∈ L(E), and B ∈ L(F ). Also suppose
A⊗B: E×F →E×F ,
(x, y) → (Ax, By) .
eA⊗B = eA ⊗ eB .
This follows easily from (A ⊗ B)n = An ⊗ B n .
(d) For A ∈ L(Km ) and B ∈ L(Kn ), we have the relation
[eA ]
[eA⊗B ] =
[eB ]
This follows from (c).
(e) Suppose λ1 , . . . , λm ∈ K, and define diag(λ1 , . . . , λm ) ∈ L(Km ) by
.. 0 ⎦
diag(λ1 , . . . , λm ) := ⎣
=: diag[λ1 , . . . , λm ] .
ediag(λ1 ,...,λm ) = diag(eλ1 , . . . , eλm ) .
This follows by induction using (a) and (d).
(f ) For A ∈ L(R2 ) such that
[A] =
[etA ] =
sin t
− sin t
Exercise I.11.10 implies easily that
(tA)2n+1 = (−1)n t2n+1 A ,
(tA)2n = (−1)n t2n 1R2
for t ∈ R and n ∈ N .
Because an absolutely convergent series can be rearranged arbitrarily, we get
(tA)k
(tA)2n
(tA)2n+1
(2n + 1)!
which proves the claim.
1 2+
(2n)! R
t2n+1
128
Linear differential equations
For a ∈ E, the map u(·, a) : R → E, t → etA a is smooth and satisfies the
“differential equation” x = Ax, which follows at once from Theorem 1.11(i). We
will now study such differential equations more thoroughly.
Suppose A ∈ L(E) and f ∈ C(R, E). Then
x = Ax + f (t) for t ∈ R ,
(1.8)
is called a first order linear differential equation in E. If f = 0, (1.8) is homogeneous; otherwise, it is inhomogeneous. Should A be independent of the “time” t,
we say that x = Ax is a linear homogeneous differential equation with constant
coeﬃcients. By a solution to (1.8), we mean a function x ∈ C 1 (R, E) that satisfies
(1.8) pointwise, that is, x(t) = A x(t) + f (t) for t ∈ R.
Let a ∈ E. Then
with x(0) = a
(1.9)a
is an initial value problem for the differential equation (1.8), and a is an initial
value. A solution to this initial value problem is a function x ∈ C 1 (R, E) that
satisfies (1.9)a pointwise.
1.14 Remarks (a) By specifying the argument t of f in (1.8) and (1.9)a (but not
of x), we are adopting a less precise, symbolic notation, which we do because it
emphasizes that f generally depends on t. For a full description of (1.8) or (1.9)a ,
one must always specify what is meant by a solution to such a differential equation.
(b) For simplicity, we consider here only the case that f is defined on all of R. We
leave to you the obvious modifications needed when f is defined and continuous
only on a perfect subinterval.
(c) Because C 1 (R, E) ⊂ C(R, E) and the derivative operator ∂ is linear (see
Theorem IV.1.12), the map
∂ − A : C 1 (R, E) → C(R, E) ,
u → u − Au
is linear. Defining
ker(∂ − A) =
x ∈ C 1 (R, E) ; x is a solution of x = Ax
shows the solution space V of homogeneous differential equations x = Ax to be a
vector subspace of C 1 (R, E).
(d) The collective solutions to (1.8) form an aﬃne subspace u + V of C 1 (R, E),
where u is (any) solution to (1.8).
Proof For w := u + v ∈ u + V , we have (∂ − A)w = (∂ − A)u + (∂ − A)v = f . Therefore
w is a solution to (1.8). If, conversely, w is a solution to (1.8), then v := u − w belongs
to V because (∂ − A)v = (∂ − A)u − (∂ − A)w = f − f = 0.
(e) For a ∈ E and x ∈ C(R, E), let
Ax(s) + f (s) ds .
Φa (x)(t) := a +
Obviously, Φa is an aﬃne map from C(R, E) to C 1 (R, E), and x is a solution
of (1.9)a if and only if x is a fixed point of Φa in8 C(R, E), that is, when x solves
the linear integral equation
x(t) = a +
Ax(s) + f (s) ds
Gronwall’s lemma
An important aid to exploring differential equations is the following estimate,
known as Gronwall’s lemma.
1.15 Lemma (Gronwall) Suppose J is an interval, t0 ∈ J, and α, β ∈ [0, ∞).
Further, suppose y : J → [0, ∞) is continuous and satisfies
y(t) ≤ α + β
y(τ ) dτ
(1.10)
y(t) ≤ αeβ |t−t0 |
(i) We consider first the case t ≥ t0 and set
s → βeβ(t0 −s)
h : [t0 , t] → R+ ,
From Theorem VI.4.12 and (1.10), it follows that
h (s) = −βh(s) + βeβ(t0 −s) y(s) ≤ αβeβ(t0 −s) =
−αeβ(t0 −s)
for s ∈ [t0 , t] .
Integrating this inequality from to t0 to t gives
h(t) = βeβ(t0 −t)
y(τ ) dτ ≤ α − αeβ(t0 −t) ,
where we have used h(t0 ) = 0. Thus we have
y(τ ) dτ ≤ αeβ(t−t0 ) − α ,
course, we identify Φa with i ◦ Φa , where i denotes the inclusion of C 1 (R, E) in C(R, E).
130
and (1.10) finishes the case.
(ii) When t ≤ t0 , we set
h(s) := −βeβ(s−t0 )
for s ∈ [t, t0 ] ,
and proceed as in (i).
Using Gronwall’s lemma, we can easily prove the next statement about the
solution space of equation (1.8).
1.16 Proposition
(i) Suppose x ∈ C 1 (R, E) solves x = Ax. Then
x = 0 ⇐ x(t0 ) = 0 for some t0 ∈ R .
(ii) The initial value problem (1.9)a has at most one solution.
(iii) The initial value problem x = Ax, x(0) = a has the unique solution t → etA a
for every a ∈ E.
(iv) The solution space V ⊂ C 1 (R, E) of x = Ax is a vector space isomorphic
to E. An isomorphism is given by
E→V ,
a → (t → etA a) .
(v) Suppose x1 , . . . , xm solves x = Ax. Then these statements are equivalent:
(α) x1 , . . . , xm are linearly independent in C 1 (R, E).
(β) The x1 (t), . . . , xm (t) are linearly independent in E for every t ∈ R.
(γ) There is a t0 ∈ R such that x1 (t0 ), . . . , xm (t0 ) are linearly independent
in E.
(i) It suﬃces to check the implication “⇐ So let t0 ∈ R with x(t0 ) = 0.
x(τ ) dτ
x(t) = A
(see Remark 1.14(e) and Exercise VI.3.3). Then y := x satisfies the inequality
y(t) ≤ A
x(τ ) dτ ≤ A
x(τ ) dτ = A
and the claim follows from Gronwall’s lemma.
(ii) If u and v solve (1.9)a , then w = u − v also solves x = Ax with w(0) = 0.
From (i), it follows that w = 0.
(iii) This follows from Theorem 1.11 and (ii).
(iv) This is a consequence of (iii).
(v) Due to (iv) and Theorem 1.11(ii), it suﬃces to prove the implication
“(α)=
⇒(β)”. Hence suppose for t0 ∈ R that λj ∈ K satisfy m λj xj (t0 ) = 0
for j = 1, . . . , m. Because
j=1 λj xj solves x = Ax, it follows from (i) that
j=1 λj xj = 0. By assumption, x1 , . . . , xm in C (R, E) are linearly independent.
Therefore, by the linear independence of x1 (t0 ), . . . , xm (t0 ) in E, we have λ1 =
· · · = λm = 0.
The variation of constants formula
From Proposition 1.16(iii), the initial value problem for the homogeneous equation
x = Ax has a unique solution for every initial value. To solve the inhomogeneous
equation (1.8), we refer to Remark 1.14(d): If the solution space V of the homogeneous equation x = Ax is known, any solution to (1.8) in fact gives rise to all
the solutions.
To construct such a particular solution, suppose x ∈ C 1 (R, E) solves (1.8)
(1.11)
T ∈ C 1 R, L(E) such that T (t) ∈ Laut(E)
for t ∈ R. Further define
y(t) := T −1 (t)x(t) ,
where T −1 (t) := T (t)
and t ∈ R .
The freedom to choose the “transformation” T can be used to find the “simplest
possible” differential equation for y. First, the product rule9 and that x solves (1.8)
give
y(t) = (T −1 ) (t)x(t) + T −1 (t)x(t)
−1 .
= (T ) (t)x(t) + T (t)Ax(t) + T −1 (t)f (t)
= (T −1 ) (t)T (t)y(t) + T −1 (t)AT (t)y(t) + T −1 (t)f (t) .
(1.12)
Next, by differentiating the identity
T −1 (t)T (t) = idE
(using the product rule), we have
(T −1 ) (t)T (t) + T −1 (t)T (t) = 0 for t ∈ R ,
9 It
(T −1 ) (t) = −T −1 (t)T (t)T −1 (t)
is easy to verify that the proof of the product rule in Theorem IV.1.6(ii) is also valid in
the current, more general, situation. See also Example 4.8(b).
132
Together with (1.12), this becomes
y(t) = T −1 (t) AT (t) − T (t) y(t) + T −1 (t)f (t) for t ∈ R .
(1.13)
According to Theorem 1.11, t → T (t) := etA satisfies AT − T = 0 and (1.11).
With this choice of T , it follows from (1.13) that
y(t) = T −1 (t)f (t)
y(t) = y(0) +
e−τ A f (τ ) dτ
Because y(0) = x(0), we have
x(t) = etA x(0) +
e(t−τ )A f (τ ) dτ
(1.14)
Now we can easily prove an existence and uniqueness result for (1.9)a .
1.17 Theorem
u(· ; a) to
For every a ∈ E and f ∈ C(R, E), there is a unique solution
x = Ax + f (t) and x(0) = a
which is given through the variation of constants formula
u(t; a) = etA a +
(1.15)
Proof Due to Proposition 1.16(ii), we need only show that (1.15) solves (1.9)a .
This, however, follows immediately from Theorems 1.11 and VI.4.12.
1.18 Remark To clarify the name “variation of constants formula”, we consider
the simplest case, namely, the R-valued equation
x = ax + f (t) for t ∈ R .
(1.16)
If a ∈ R, the homogeneous equation x = ax has a solution v(t) := eta . Thus
the general solution of the homogeneous equation is given by cv, where c ∈ R
is a constant. That is, { cv ; c ∈ R } is the solution space of the homogeneous
equation.
To specify a particular solution to (1.16), we “vary the constant”, that is, we
make the ansatz x(t) := c(t)v(t) with a to-be-determined function c. Because x
must satisfy the equation (1.16), we have
cv + cv = acv + f ,
and thus, from cv = c(av), we have c = f /v. Then (1.14) follows by integration.
Determinants and eigenvalues
We next assemble some important results from linear algebra.
1.19 Remarks (a) The determinant det[aj ] of [aj ] ∈ Km×m is defined through
the signature formula
det[aj ] :=
sign(σ)a1 · · · · · am
σ(1)
σ(m)
(see [Gab96, Chapter A3]), where sign is the sign function from Exercise I.9.6.10
(b) Let E be a finite-dimensional Banach space. The determinant function
det : L(E) → K
can be expressed for A ∈ L(E) as det(A) := det [A]E , where E is a basis of E.
Then det is well defined, that is, independent of the particular basis E, and we
det(1E ) = 1 and det(AB) = det(A) det(B)
for A, B ∈ L(E) ,
(1.17)
det(A) = 0 ⇐ A ∈ Laut(E) .
(1.18)
Letting λ − A := λ1E − A and m := dim(E), we have
det(λ − A) =
(−1)k αm−k λm−k
(1.19)
with αk ∈ C and
αm = 1 ,
αm−1 = tr(A) ,
α0 = det(A) .
(1.20)
The zeros of the characteristic polynomial (1.19) are the eigenvalues of A, and the
set of all eigenvalues is the spectrum σ(A) of A.
The number λ ∈ K is an eigenvalue of A if and only if the eigenvalue equation
Ax = λx
(1.21)
has a nontrivial solution x ∈ E \ {0}, that is, if ker(λ − A) = {0}. For11 λ ∈
σ(A) ∩ K, the geometric multiplicity of λ is given by dim ker(λ − A) , and every
10 sign(σ)
is also called the signature of the permutation σ.
that the spectrum of A is also a subset of C if E is a real Banach space. If K = R and
λ belongs to σ(A)\R, then the equation (1.21) is not meaningful. In this case, we must allow for
the complexification of E and A, so that λ can be allowed as a solution to the eigenvalue equation
(see, for example, [Ama95, § 12]). Then the geometric multiplicity of λ ∈ σ(A) is defined in
every case.
11 Note
134
x ∈ ker(λ−A)\{0} is an eigenvector of A with eigenvalue λ. The number of times λ
appears as a zero of the characteristic polynomial is the algebraic multiplicity of λ.
The geometric multiplicity is no greater than the algebraic multiplicity. Finally, λ
is a simple eigenvalue of A if its algebraic multiplicity is 1 and, therefore, if λ is
a simple zero of the characteristic polynomial. If the geometric and the algebraic
multiplicity are equal, then λ is semisimple.
Every A ∈ L(E) has exactly m eigenvalues λ1 , . . . , λm if they are counted
according to their (algebraic) multiplicity. The coeﬃcients αk of the characteristic polynomial of A are the elementary symmetric functions of λ1 , . . . , λm . In
particular,
tr(A) = λ1 + · · · + λm and det(A) = λ1 · · · · · λm .
(1.22)
If E is a basis of E such that [A]E is an (upper) triangular matrix,
[A]E = ⎢
· · · a1
· · · a2
(1.23)
that is, aj = 0 for k < j, then the eigenvalues of A appear with their algebraic
multiplicity along the diagonal.
Proof The properties (1.17)–(1.20) of the determinants of square matrices are standard
in linear algebra, and we assume you know them.
If F is another basis of E, there is an invertible matrix T ∈ Km×m such that
[A]F = T [A]E T −1
(for example, [Koe83, Chapter 9.3.2]). Therefore det [A]F = det [A]E follows from the
matrix rules (1.17). This shows that det(A) is well defined for A ∈ L(E). Now it is
obvious that (1.17)–(1.20) hold for A ∈ L(E).
The existence of eigenvalues of A and the statement that A has exactly m eigenvalues if they are counted by their algebraic multiplicity both follow from the fundamental
theorem of algebra (see Example III.3.9(b)).
(c) Suppose p ∈ K[X] is a polynomial of degree n ≥ 1 over the field K. Then p
splits in K if there are k, ν1 , . . . , νk ∈ N× and a, λ1 , . . . , λk ∈ K such that
(X − λj )νj .
The fundamental theorem of algebra implies that every nonconstant polynomial
p ∈ C[X] splits in C (see Example III.3.9(b)). In general, a polynomial need not
split. For example, if K is an ordered field, then X 2 + 1 does not split in K.
(d) Suppose λ ∈ K and p ∈ N× . Then the J(λ, p) ∈ Kp×p defined
λ 1
··· ··· 0
J(λ, 1) := [λ] and J(λ, p) := ⎢
··· · 1
for p ≥ 2 is called the elementary Jordan matrix of size p for λ.
For μ = α + iω ∈ C such that α ∈ R and ω > 0, let
A2 (μ) :=
∈ R2×2 ,
and let 12 denote the unity element in R2×2 . Then we call the J(μ, p) ∈ R2p×2p
defined by
A2 (μ) 12
J(μ, 1) := A2 (μ) and J(μ, p) := ⎢
12 ⎦
A2 (μ)
for p ≥ 2 the extended Jordan matrix of size 2p for μ.
(e) Suppose E is a finite-dimensional Banach space over K and A ∈ L(E). Also
suppose the characteristic polynomial of A splits in K. In linear algebra, one shows
that there is then a basis E of E and p1 , . . . , pr ∈ N× such that
J(λ1 , p1 )
[A]E = ⎣
(1.24)
J(λ , p )
where {λ1 , . . . , λr } = σ(A). One calls (1.24) the Jordan normal form of A. It is
unique up to ordering of the blocks J(λj , pj ). The basis E is called the Jordan
basis of E for A. If A has only semisimple eigenvalues, then r = dim(E) and
pj = 1 for j = 1, . . . , r. Such an A is said to be diagonalizable.
(f ) The characteristic polynomial of
A = ⎣ −1
0 0 ⎦ ∈ R3×3
reads
p = X 3 − X 2 + X − 1 = (X − 1)(X 2 + 1) .
136
Therefore p does not split in R, and σ(A) = {1, i, −i }.
In this case, A cannot be put into the form (1.24). However, it can be put
into an extended Jordan normal form.
(g) Suppose E is a finite-dimensional real Banach space and the characteristic
polynomial p of A ∈ L(E) does not split in R. Then there is a nonreal eigenn
value μ of A. Because the characteristic polynomial p = k=0 ak X k has only real
coeﬃcients, it follows from
ak μk = p(μ)
ak μk =
0 = 0 = p(μ) =
that μ is also an eigenvalue of A.
Suppose now
σ := {μ1 , μ1 , . . . , μ , μ } ⊂ σ(A)
≥1,
is the set of all nonreal eigenvalues of A, where, without loss of generality, we may
assume that μj = αj + iωj with αj ∈ R and ωj > 0. If A also has real eigenvalues,
we denote them by λ1 , . . . , λk .
Then there is a basis E of E and p1 , . . . , pr , q1 , . . . , qs ∈ N× such that
J(λr , pr )
(1.25)
J(μ1 , q1 )
J(μs , qs )
Here, λj ∈ σ(A)\ σ for j = 1, . . . , r and μk ∈ σ for k = 1, . . . , s. We call (1.25)
the extended Jordan normal form of A. It is unique up to the ordering of the
constituent matrices. (see [Gab96, Chapter A5]).
Fundamental matrices
Suppose E is a vector space of dimension n over K and A ∈ L(E). Also denote
by V ⊂ C 1 (R, E) the solution space of homogeneous linear differential equations
x = Ax. By Proposition 1.16(iv), V and E are isomorphic and, in particular, have
the same dimension. We call every basis of V a fundamental system for x = Ax.
1.20 Examples
(a) Let
A :=
∈ K2×2 .
To get a fundamental system for x = Ax, we solve first the initial value problem
x1 = ax1 + bx2 ,
x1 (0) = 1 ,
x2 = cx2 ,
x2 (0) = 0 .
(1.26)
From Application IV.3.9(b), it follows that x2 = 0 and x1 (t) = eat for t ∈ R.
Therefore x1 := t → (eat , 0) is the solution of (1.26). We get a second solution
x2 linearly independent of x1 by solving
x1 (0) = 0 ,
x2 (0) = 1 .
In this case, we have x2 (t) = ect for t ∈ R, and the variation of constants formula
x1 (t) =
ea(t−τ ) becτ dτ = beat
eτ (c−a) dτ
b(ect − eat )/(c − a) ,
btect ,
a=c,
a=c.
x1 (t) = (eat , 0) ,
b(ect − eat ) (c − a), ect ,
(bteat , eat ) ,
x2 (t) =
where t ∈ R, is a fundamental system for x = Ax.
(b) Suppose ω > 0 and
x1 = −ωx2 and x2 = ωx1
it follows that xj + ω 2 xj = 0 for j = 0, 1. Exercise IV.3.2(a) shows that
x1 (t) = cos(ωt), sin(ωt) and x2 (t) = − sin(ωt), cos(ωt)
is fundamental system for x = Ax.
If A ∈ Kn×n and {x1 , . . . , xn } is a fundamental system for x = Ax, we call
the map
x1 (t) · · · x1 (t)
X : R → Kn×n , t → x1 (t), . . . , xn (t) = ⎣ .
x1 (t) · · · xn (t)
the fundamental matrix for x = Ax. If X(0) = 1n , we call X the principal
fundamental matrix.
138
1.21 Remarks
Let A ∈ Kn×n .
(a) The map
X → AX
is linear. Thus, in addition to the linear differential equation x = Ax in Kn , we can
consider the linear differential equation X = AX in Kn×n , which is then solved
by any fundamental matrix for x = Ax.
(b) Suppose X is a fundamental matrix for x = Ax and t ∈ R. Then
(i) X(t) ∈ Laut(Kn );
(ii) X(t) = etA X(0).
Proof (i) is a consequence of Proposition 1.16(v).
(ii) follows from (a), Theorem 1.11, and Proposition 1.16(iii).
(c) The map t → etA is the unique principal fundamental matrix for x = Ax.
This follows from (b).
(d) For s, t ∈ R, we have
e(t−s)A = X(t)X −1 (s) .
This also follows from (b).
(e) Using the Jordan normal form and Examples 1.13(b)–(e), we can calculate
etA . We demonstrate this for n = 2.
The characteristic polynomial of
∈ R2×2
gives det(λ − A) = λ2 − λ(a + d) + ad − bc and has zeros
λ1,2 = a + d ± D) 2 , where D := (a − d)2 + 4bc .
1. Case: K = C, D < 0 Here A has two simple, complex conjugate eigenvalues λ := λ1 and λ = λ2 . According to Remark 1.19(e), there is a basis B of C2
such that [A]B = diag[λ, λ], and Example 1.13(e) implies
et[A]B = diag[eλt , eλt ] for t ∈ R .
Denote by T ∈ Laut(C2 ) the basis change from the standard basis to B. Then,
using Theorem 1.9, we have
[A]B = [T AT −1 ] = [T ]A[T ]−1 .
Therefore, from Exercise 11, we get
et[A]B = [T ]etA [T ]−1 ,
etA = [T ]−1 diag[eλt , eλt ][T ] for t ∈ R .
2. Case: K = C or K = R, D > 0 Then A has two simple, real eigenvalues
λ1 and λ2 , and it follows as in the first case that
etA = [T ]−1 diag[eλ1 t , eλ2 t ][T ] for t ∈ R .
3. Case: K = C or K = R, D = 0 Now A has the eigenvalue λ := (a + d)/2
with algebraic multiplicity 2. Thus, according to Remark 1.19(e), there is a basis B
of K2 such that
[A]B =
0 λ
commute, it follows from Exercise 11 and Example 1.13(e) that
et[A]B = eλt exp t
= eλt 12 + t
= eλt
Thus we get
etA = eλt [T ]−1
4. Case: K = R, D < 0 A has two complex conjugate eigenvalues λ := α +
i ω and λ, where α := (a + d)/2 and ω := −D 2. From Remark 1.19(g), there is
a basis B of R2 such that
The matrices
α 0
0 α
commute. Thus from Example 1.13(f) we have
exp t
= etα exp tω
0 −1
= etα
cos(ωt) − sin(ωt)
sin(ωt)
cos(ωt)
From this we derive
etA = eαt [T ]−1
− sin(ωt)
where T is the basis change from the standard basis to B (see Example 1.13(f)).
140
Until now, we have considered only first order linear differential equations, in which
there was a linear relationship between the sought-for function and its derivative.
Many applications, however, require differential equations of higher order. Second
order occurs frequently, and we shall now concentrate on that case.12
In the following, let
• b, c ∈ R and g ∈ C(R, K).
For a second order linear differential equation with constant coeﬃcients given by
u + bu + cu = g(t) for t ∈ R ,
(1.27)
we say u ∈ C 2 (R, K) is a solution if it satisfies (1.27) pointwise. The results below
show that (1.27) is equivalent to a first order differential equation of the form (1.8)
in the (u, u) plane, called the phase plane.
1.22 Lemma
(i) If u ∈ C 2 (R, K) solves (1.27), then (u, u) ∈ C 1 (R, K2 ) solves the equation
x = Ax + f (t)
(1.28)
in K2 , where
and f := (0, g) .
(1.29)
(ii) If x ∈ C 1 (R, K2 ) solves (1.28), then u := pr1 x solves (1.27).
(i) We set x := (u, u) and get from (1.27) that
−cu − bu + g(t)
= Ax + f (t) .
(ii) Let x = (u, v) ∈ C 1 (R, K2 ) solve (1.28). Then
v = −cu − bv + g(t) .
(1.30)
The first equation says u belongs to C 2 (R, K), and together the equations imply
that u solves the differential equation (1.27).
Lemma 1.22 allows us to apply our knowledge of first order differential equations to (1.27). We will need the eigenvalues of A. From det(λ − A) = λ(λ + b) + c,
we find that the eigenvalues λ1 and λ2 of A are the zeros of the polynomial
12 For differential equations of even higher order, consult the literature of ordinary differential
equations, for example [Ama95].
X 2 + bX + c, called the characteristic polynomial of u + bu + cu = 0, and we
λ1,2 = −b ± D 2 , where D = b2 − 4c .
Now we consider the initial value problem
with u(0) = a1 ,
u(0) = a2 ,
(1.31)
where (a1 , a2 ) ∈ K2 . From the above, we can easily prove a basic theorem.
1.23 Theorem
(i) For every (a1 , a2 ) ∈ K2 , there is a unique solution u ∈ C 2 (R, K) to the initial
value problem (1.31).
(ii) The set of solutions to u + bu + cu = 0 span a two-dimensional vector sub¨
space V of C 2 (R, K) through
{eλ1 t , eλ2 t } if D > 0 or (D < 0 and K = C) ,
{eαt , teαt } if D = 0 ,
eαt cos(ωt), eαt sin(ωt)
where α := −b/2 and ω :=
if D < 0 and K = R ,
−D 2.
(iii) The set of solutions to (1.27) forms a two-dimensional aﬃne subspace v + V
of C 2 (R, K), where v is any solution of (1.27).
(iv) Suppose the equations u + bu + cu = 0 and w := u1 u2 − u1 u2 have linearly
independent solutions u1 , u2 ∈ V . Then
v(t) :=
u1 (τ )g(τ )
dτ u2 (t) −
w(τ )
u2 (τ )g(τ )
dτ u1 (t) for t ∈ R
solves (1.27).
(i) We find this immediately from Lemma 1.22 and Theorem 1.17.
(ii) and (iii) These are implied by Lemma 1.22, Proposition 1.16(iv) and
Remarks 1.14(d) and 1.21(e).
(iv) Let A and f be as in (1.29). Then by Lemma 1.22 and Remark 1.21(a),
is a fundamental matrix for x = Ax. Because
e(t−τ )A = X(t)X −1 (τ )
142
(see Remark 1.21(d)), it follows from the variation of constants formula of Theorem 1.17 that
y(t) = X(t)
X −1 (τ )f (τ ) dτ
(1.32)
solves x = Ax + f (t). Because det(X) = w, we have
X −1 =
−u1
−u2
where the function w has no zeros, due to Proposition 1.16(v). Therefore we conclude that X −1 f = (−u2 g/w, u1 g/w). The theorem then follows from Lemma 1.22
and (1.32).
1.24 Examples
(a) The characteristic polynomial X 2 − 2X + 10 of
u − 2u + 10u = 0
has zeros 1 ± 3i . Thus et cos(3t), et sin(3t)
general solution therefore reads
is a fundamental system.13 The
u(t) = et a1 cos(3t) + a2 sin(3t)
where a1 , a2 ∈ R.
Phase plane
(t, u) plane
(b) To solve the initial value problem
u − 2 u + u = et ,
u(0) = 0 ,
u(0) = 1 ,
(1.33)
we first find a fundamental system for the homogeneous equation u − 2u + u = 0.
The associated characteristic polynomial is
X 2 − 2X + 1 = (X − 1)2 .
13 As with first order linear differential equations, we call any basis of the solution space a
fundamental system.
Then, according to Theorem 1.23(iii), u1 (t) := et and u1 (t) := tet for t ∈ R form
a fundamental system. Because
w(τ ) = u1 (τ )u2 (τ ) − u1 (τ )u2 (τ ) = e2τ
the equation
v(t) =
u1 g
u2 g
dτ u1 (t) = et
is particular solution of (1.33). Thus the general solution reads
x(t) = a1 et + a2 tet +
t2 t
for t ∈ R and a1 , a2 ∈ R .
Because x(0) = a1 and x(0) = a1 + a2 , we find finally that the unique solution of
(1.33) is
u(t) = (t + t2 /2)et for t ∈ R .
(c) The differential equation of the harmonic oscillator (or the undamped oscillator) reads
u + ω0 u = 0 ,
where ω0 > 0 is the (angular) frequency. From Theorem 1.23, the general solution
has the form
u(t) = a1 cos(ω0 t) + a2 sin(ω0 t) for t ∈ R ,
where a1 , a2 ∈ R. Obviously, all the solutions are periodic with period 2π/ω0 , and
they rotate about the center of the phase plane.
(d) The differential equation of the damped oscillator is
u + 2αu + ω0 u = 0 .
Here, α > 0 is the damping constant, and ω0 > 0 is the frequency of the undamped
oscillator. The zeros of the characteristic polynomial are
λ1,2 = −α ±
α2 − ω0 .
144
(i) (Strong damping: α > ω0 ) In this case, there are two negative eigenvalues
λ1 < λ2 < 0, and the general solution reads
u(t) = a1 eλ1 t + a2 eλ2 t
Thus all solutions fade away exponentially. In the phase plane, there is a “stable
node” at the origin.
(ii) (Weak damping: α < ω0 ) In this case, the characteristic polynomial has
two complex conjugate eigenvalues λ1,2 = −α ± i ω, where ω := ω0 − α2 . Thus
the general solution, according to Theorem 1.22, is
u(t) = e−αt a1 cos(ωt) + a2 sin(ωt)
In this case, solutions also damp exponentially, and the phase plane’s origin is a
“stable vortex”.
(iii) (Critical damping: α = ω0 ) Here, λ = −α is an eigenvalue of the characteristic polynomial with algebraic multiplicity 2. Therefore the general solution
u(t) = e−αt (a1 + a2 t) for t ∈ R and a1 , a2 ∈ R .
The phase plane’s origin is now called a “stable virtual node”.
1 Suppose E and Fj are Banach spaces and Aj ∈ Hom(E, Fj ) for j = 1, . . . , m. Also
let F := m Fj . For A := x → (A1 x, . . . , Am x) ∈ Hom(E, F ), show that
A ∈ L(E, F ) ⇐ Aj ∈ L(E, Fj ), j = 1, . . . , m .
For a square matrix A := [aj ] ∈ Km×m , the trace is defined by
aj .
tr(A) :=
(i) the map tr : Km×m → K is linear;
(ii) tr(AB) = tr(BA) for A, B ∈ Km×m .
3 Two matrices A, B ∈ Km×m are similar if there is an invertible matrix S ∈ Km×m
(that is, if S belongs to Laut(Km )) such that A = SBS −1 . Show that tr(A) = tr(B) if
A and B are similar.
4 Suppose E is a finite-dimensional normed vector space. Prove that for A ∈ L(E), we
tr [A]E = tr [A]F ,
where E and F are bases of E.
From this it follows that the trace of A ∈ L(E) is well defined through
tr(A) := tr [A]E ,
where E is any basis of E.
Show also that
(i) tr ∈ L L(E), K ;
(ii) tr(AB) = tr(BA) for A, B ∈ L(E).
146
5 Suppose E, (· | ·)E and F, (· | ·)F are finite-dimensional Hilbert spaces. Verify that
to each A ∈ L(E, F ) there is a unique A∗ ∈ L(F, E), called the adjoint operator (or,
simply, adjoint) A∗ of A, such that
(Ax | y)F = (x | A∗ y)E
for x ∈ E and y ∈ F .
Show that the map
A → A∗
L(E, F ) → L(F, E) ,
is conjugate linear and satisfies
(i) (AB)∗ = B ∗ A∗ ,
(ii) (A∗ )∗ = A, and
(iii) (C −1 )∗ = (C ∗ )−1
for A, B ∈ L(E, F ) and C ∈ Lis(E, F ). For A = [aj ] ∈ Km×n , show A∗ = ak ∈ Kn×m .
This matrix is the Hermitian conjugate or Hermitian adjoint of the matrix A.
If A = A∗ , (and therefore E = F ), we say A is self adjoint or symmetric. Show that
A∗ A and AA∗ are self adjoint for A ∈ L(E, F ).
6 Let E := E, (· | ·) be a finite-dimensional Hilbert space. Then show for A ∈ L(E)
(i) tr(A∗ ) = tr(A);
(ii) tr(A) =
j=1 (Aϕj
| ϕj ), where {ϕ1 , . . . , ϕn } is an orthonormal basis of E.
Suppose E and F are finite-dimensional Hilbert spaces. Prove that
(i) L(E, F ) is a Hilbert space with the inner product
(A, B) → A : B := tr(B ∗ A) ;
(ii) if E := Kn and F := Km , then
|A|L(E,F ) ≤ A : A = ||A|
for A ∈ L(E, F ) .
8 Suppose [gjk ] ∈ Rn×n is symmetric and positive definite, that is, gjk = gkj and there
is a γ > 0 such that
gjk ξ j ξ k ≥ γ |ξ|2
j,k=1
(x | y)g :=
defines a scalar product on Rn .
9 Suppose E is a Banach space and A, B ∈ L(E) commute, that is, AB = BA. Prove
AeB = eB A and eA+B = eA eB .
Find A, B ∈ K2×2 such that
(i) AB = BA and eA+B = eA eB ;
(ii) AB = BA and eA+B = eA eB .
(Hint for (ii): e2kπi = 1 for k ∈ Z.)
Suppose E is a Banach space, A ∈ L(E) and B ∈ Laut(E). Show
eBAB
= BeA B −1 .
Suppose E is a finite-dimensional Hilbert space and A ∈ L(E). Show
(i) (eA )∗ = eA ;
(ii) if A is self adjoint, then so is eA ;
(iii) if A is anti self adjoint or skew-symmetric, that is A∗ = −A, then [eA ]∗ eA = 1.
13 Calculate eA for
1 1 1 1
⎢ 1 1 1 1 ⎥
⎣ 1 1 1 1 ⎦ ,
A ∈ L(R4 )
having the
1 0 ⎥
2 1 ⎦
representation
2 1 0
Determine the A ∈ R3×3 that gives
cos t − sin t
etA = ⎣ sin t
0 ⎦
matrices
0 ⎥
Find the general solution to
x = 3x − 2y + t ,
y = 4x − y + t2 .
Solve the initial value problem
x(0) = 0 ,
y =z+e ,
y(0) = 3/2 ,
z(0) = 1 .
17 Suppose A = [aj ] ∈ Rn×n is a Markov matrix, that is, it has aj ≥ 0 for all j = k
and n aj = 1 for j = 1, . . . , n. Also suppose
k=1 k
(x1 , . . . , xn ) ∈ Rn ;
j=1 x
Show that every solution of x = Ax with initial values in Hc remains in Hc for all time,
that is, etA Hc ⊂ Hc for t ∈ R.
18 Suppose b, c ∈ R, and z ∈ C 2 (R, C) solves u + bu + cu = 0. Show that Re z and Im z
are real solutions of u + bu + cu = 0.
Let b, c ∈ R, and suppose u solves u + bu + cu = g(t). Show that
(i) k ∈ N ∪ {∞} and g ∈ C k (R, K) implies u ∈ C k+2 (R, K);
148
(ii) if g ∈ C ω (R), then u ∈ C ω (R).
The differential equation for the damped driven oscillator is
u + 2αu + ω0 u = c sin(ωt)
where α, ω0 , ω > 0 and c ∈ R. Find its general solution.
Find the general solution on (−π/2, π/2) for
(i) u + u = 1/ cos t;
(ii) u + 4u = 2 tan t.
22 Suppose E is a finite-dimensional Banach space and A ∈ L(E). Show these statements are equivalent:
(i) Every solution u of x = Ax in E satisfies limt→∞ u(t) = 0.
(ii) Re λ < 0 for λ ∈ σ(A).
In the remaining exercises, E is a Banach space, and A ∈ L(E).
If λ ∈ K has Re λ > A , then
(λ − A)−1 =
e−t(λ−A) dt .
(Hint: Set R(λ) :=
e−t(λ−A) dt and calculate (λ − A)R(λ).)
Show the exponential map has the representation
etA = lim 1 −
(Hint: The proof of Theorem III.6.23.)
Let C be a closed, convex subset of E. Show these are equivalent:
(i) etA C ⊂ C for all t ∈ R;
(ii) (λ − A)−1 C ⊂ C for all λ ∈ K such that Re λ > A .
(Hint: Exercises 23 and 24.)
VII.2 Differentiability
2 Differentiability
In this section, we explain the central concepts of “differentiability”, “derivative”,
and “directional derivative” of a function1 f : X ⊂ E → F . Here E and F are
Banach spaces, and X is a subset of E. After doing so, we will illustrate their
power and ﬂexibility by making specific assumptions about the spaces E and F .
If E = Rn , we explain the notion of partial derivatives and investigate the
connection between differentiability and the existence of partial derivatives.
In applications, the case E = Rn and F = R is particularly important. Here
we introduce the concept of the gradient and clarify with the help of the Riesz
representation theorem the relationship between the derivative and the gradient.
In the case E = F = C, we derive the important Cauchy–Riemann equations. These connect the complex differentiability of f = u + i v : C → C and the
differentiability of (u, v) : R2 → R2 .
Finally, we show that these new ideas agree with the well-established E = K
from Section IV.1.
• E = (E, · ) and F = (F, · ) are Banach spaces over the field K;
X is an open subset of E.
The definition
A function f : X → F is differentiable at x0 ∈ X if there is an Ax0 ∈ L(E, F )
f (x) − f (x0 ) − Ax0 (x − x0 )
x→x0
x − x0
The next theorem reformulates this definition and gives the first properties of
differentiable functions.
2.1 Proposition Let f : X → F and x0 ∈ X.
(i) These statements are equivalent:
(α) f is differentiable at x0 .
(β) There exist Ax0 ∈ L(E, F ) and rx0 : X → F , where rx0 is continuous
at x0 and satisfies rx0 (x0 ) = 0, such that
f (x) = f (x0 ) + Ax0 (x − x0 ) + rx0 (x) x − x0
(γ) There exists Ax0 ∈ L(E, F ) such that
f (x) = f (x0 ) + Ax0 (x − x0 ) + o( x − x0 ) (x → x0 ) .
notation f : X ⊂ E → F is a shorthand for X ⊂ E and f : X → F .
150
(ii) If f is differentiable at x0 , then f is continuous at x0 .
(iii) Suppose f is differentiable at x0 . Then the linear operator Ax0 ∈ L(E, F )
from (2.1) is uniquely defined.
Proof (i) The proof is just like the proof of Theorem IV.1.1 and is left to you.
(ii) If f is differentiable at x0 , then its continuity at x0 follows directly
from (iβ).
(iii) Suppose B ∈ L(E, F ), and
f (x) = f (x0 ) + B(x − x0 ) + o( x − x0 ) (x → x0 ) .
Subtracting (2.3) from (2.2) and dividing the result by x − x0 , we find
lim (Ax0 − B)
Suppose y ∈ E has y = 1 and xn := x0 + y/n for n ∈ N× . Then because
lim xn = x0 and (xn − x0 )/ xn − x0 = y, we find
(Ax0 − B)y = lim(Ax0 − B)
xn − x0
Since this holds for every y ∈ ∂BE , it follows that Ax0 = B.

[ _to('140520154304') ]
The derivative
Suppose f : X → F is differentiable at x0 ∈ X. Then we denote by ∂f (x0 ) the linear operator $A_{x_0} \in \mathcal{L(E, F)}$ uniquely determined by Proposition 2.1. This is called the derivative of f at x0 and will also be written
$D_f(x_0)$ or f (x0 ) .
Therefore ∂f (x0 ) ∈ L(E, F ), and
f (x) − f (x0 ) − ∂f (x0 )(x − x0 )
If f : X → F is differentiable at every point x ∈ X, we say f is differentiable
and call the map
$\partial f: X \to \mathcal{L}(E, F}, \; x \mapsto \partial f(x)$
the derivative of f .

When L(E, F ) is a Banach space, we can meaningfully speak of the continuity
of the derivative. If ∂f is continuous, that is, ∂f ∈ C X, L(E, F ) , we call f
continuously differentiable. We set
C 1 (X, F ) := { f : X → F ; f is continuously differentiable } .
2 To clearly distinguish the derivative ∂f (x ) ∈ L(E, F ) at a point x from the derivative
∂f : X → L(E, F ), we may also speak of the derivative function ∂f : X → L(E, F ).
2.2 Remarks (a) These statements are equivalent:
(i) f : X → F is differentiable at x0 ∈ X.
(ii) There is a ∂f (x0 ) ∈ L(E, F ) such that
f (x) = f (x0 ) + ∂f (x0 )(x − x0 ) + o( x − x0 ) (x → x0 ) .
(iii) There is a ∂f (x0 ) ∈ L(E, F ) and an rx0 : X → F that is continuous at x0
and satisfies rx0 (x0 ) = 0, such that
f (x) = f (x0 ) + ∂f (x0 )(x − x0 ) + rx0 (x) x − x0
(b) From (a), it follows that f is differentiable at x0 if and only if f is approximated
at x0 by the aﬃne map
g: E →F ,
x → f (x0 ) + ∂f (x0 )(x − x0 )
such that the error goes to zero faster than x goes to x0 , that is,
f (x) − g(x)
Therefore, the map f is differentiable at x0 if and only if it is approximately linear
at x0 (see Corollary IV.1.3).
(c) The concepts of “differentiability” and “derivative” are independent of the
choice of equivalent norms in E and F .
This follows, for example, from Remark II.3.13(d).
(d) Instead of saying “differentiable” we sometimes say “totally differentiable” or
“Fr´chet differentiable”.
(e) (the case E = K) In Remark 1.3(a), we saw that the map L(K, F ) → F , A →
A1 is an isometric isomorphism, with which we canonically identify L(K, F ) and F .
This identification shows that if E = K, the new definitions of differentiability and
derivative agree with those of Section IV.1.
(f ) C 1 (X, F ) ⊂ C(X, F ), that is, every continuously differentiable function is also
2.3 Examples (a) For A ∈ L(E, F ), the function A = (x → Ax) is continuously
differentiable, and ∂A(x) = A for x ∈ E.
Because Ax = Ax0 + A(x − x0 ), the claim follows from Remark 2.2(a).
(b) Suppose y0 ∈ F . Then the constant map ky0 : E → F , x → y0 is continuously
differentiable, and ∂ky0 = 0.
This is obvious because ky0 (x) = ky0 (x0 ) for x, x0 ∈ E.
152
(c) Suppose H is a Hilbert space, and define b : H → K, x → x 2 . Then b is
continuously differentiable, and
∂b(x) = 2 Re(x|·)
For every choice of x, x0 ∈ H such that x = x0 , we have
− x0
= x − x0 + x0
= x − x0
+ 2 Re(x0 | x − x0 )
b(x) − b(x0 ) − 2 Re(x0 | x − x0 )
= x − x0 .
This implies ∂b(x0 )h = 2 Re(x0 | h) for h ∈ H.

[ _to('140520154645') ]
Directional derivatives
Suppose f : X → F , x0 ∈ X and v ∈ E \ {0}. Because X is open, there is an
ε > 0 such that x0 + tv ∈ X for |t| < ε. Therefore the function
(−ε, ε) → F ,
t → f (x0 + tv)
is well defined. When this function is differentiable at the point 0, we call its derivative the directional derivative of f at the point x0 in the direction $v$ and denote it by $D_v f(x0)$. Thus
Dv f (x0 ) = lim
t→0
f (x0 + tv) − f (x0 )

2.4 Remark The function
fv,x0 : (−ε, ε) → F ,
can be viewed as a “curve” in E × F ,
which lies “on the graph of f ”. Then
Dv f (x0 ) represents for v = 1 the slope
of the tangent to this curve at the point
x0 , f (x0 ) (see Remark IV.1.4(a)).
The next result says that f has a directional derivative at x0 in every direction
if f is differentiable there.

[ _to('140520154935') ]
[ Relation between directional and general derivative ]
2.5 Proposition Suppose f : X → F is differentiable at x0 ∈ X. Then Dv f (x0 ) exists for every v ∈ E \{0}, and Dv f (x0 ) = ∂f (x0 )v.
Proof: For v ∈ E \{0}, it follows from Remark 2.2(a) that
f (x0 + tv) = f (x0 ) + ∂f (x0 )(tv) + o( tv ) = f (x0 ) + t∂f (x0 )v + o(|t|)
as t → 0. The theorem now follows from Remark IV.3.1(c).

2.6 Remark The converse of Proposition 2.5 is false, that is, a function having a
directional derivative in every direction need not be differentiable.
We consider a function f : R2 → R defined by
x2 y
(x, y) = (0, 0) ,
2 + y2
f (x, y) :=
(x, y) = (0, 0) .
For every v = (ξ, η) ∈ R2 \ (0, 0) , we have
f (tv) =
t3 ξ 2 η
= tf (v) .
t2 (ξ 2 + η 2 )
Dv f (0) = lim f (tv)/t = f (v) .
If f were differentiable at 0, Proposition 2.5 would imply that ∂f (0)v = Dv f (0) = f (v)
for every v ∈ R2\ (0, 0) . Because v → ∂f (0)v is linear, but f is not, this is not possible.
Therefore f is not differentiable at 0.
Partial derivatives
If E = Rn , the derivatives in the direction of the coordinate axes are particularly
important, for practical and historical reasons. It is convenient to introduce a
specific notation for them. We thus write ∂k or ∂/∂xk for the derivatives in the
direction of the standard basis vectors3 ek for k = 1, . . . , n. Thus
∂k f (x0 ) :=
f (x0 + tek ) − f (x0 )
(x0 ) := Dek f (x0 ) = lim
for 1 ≤ k ≤ n ,
and ∂k f (x0 ) is called the partial derivative with respect to xk of f at x0 . The function f is said to be partially differentiable at x0 if all ∂1 f (x0 ), . . . , ∂n f (x0 ) exist,
and it is called [continuously] partially differentiable if it is partially differentiable
at every point of X [if ∂k f : X → F is continuous for 1 ≤ k ≤ n].
2.7 Remarks (a) When the k-th partial derivative exists at x0 ∈ X, we have
f (x1 , . . . , xk−1 , xk + h, xk+1 , . . . , xn ) − f (x0 )
h→0
∂k f (x0 ) = lim
for 1 ≤ k ≤ n .
Therefore f is partially differentiable at x0 in the k-th coordinate if and only if the
function t → f (x1 , . . . , xk−1 , t, xk+1 , . . . , xn ) of one real variable is differentiable
at xk .
3 Occasionally,
we also write ∂xk for ∂/∂xk .
154
(b) The partial derivative ∂k f (x0 ) can be defined if xk is a cluster point of the set
ξ ∈ R ; (x1 , . . . , xk−1 , ξ, xk+1 , . . . , xn ) ∈ X
In particular, X must not be open.
(c) If f is differentiable at x0 , then f is partially differentiable at x0 .
This is a special case of Proposition 2.5.
(d) If f is partially differentiable at x0 , it does not follow that f is differentiable
at x0 .
Remark 2.6.
(e) That f is partially differentiable at x0 does not imply it is continuous there.
We consider f : R2 → R with
(x2 + y 2 )2
Then f (h, 0) = f (0, h) = 0 for all h ∈ R. Therefore ∂1 f (0, 0) = ∂2 f (0, 0) = 0. Then f is
partially differentiable at (0, 0).
Because f (0, 0) = 0 and f (1/n, 1/n) = n2 /4 for n ∈ N× , we see that f is not
continuous at (0, 0).
(f ) To clearly distinguish “differentiability” from “partial differentiability”, we
sometimes also say f is totally differentiable [at x0 ] if f is differentiable [at x0 ].
Next, we seek concrete representations for ∂f (x0 ) when E = Rn or F = Rm .
These will allow us explicitly calculate partial derivatives.
2.8 Theorem
(i) Suppose E = Rn and f : X → F is differentiable at x0 . Then
∂k f (x0 )hk
∂f (x0 )h =
for h = (h1 , . . . , hn ) ∈ Rn .
(ii) Suppose E is a Banach space and f = (f 1 , . . . , f m ) : X → Km . Then f
is differentiable at x0 if and only if all of the coordinate functions f j for
1 ≤ j ≤ m are differentiable at x0 . Then
∂f (x0 ) = ∂f 1 (x0 ), . . . , ∂f m (x0 ) ,
that is, vectors are differentiated componentwise.
Proof (i) Because h = k hk ek for h = (h1 , . . . , hn ) ∈ Rn , it follows from the
linearity of ∂f (x0 ) and Proposition 2.5 that
hk ∂f (x0 )ek =
∂k f (x0 )hk .
(ii) For A = (A1 , . . . , Am ) ∈ Hom(E, Km ), we have
A ∈ L(E, Km ) ⇐ Aj ∈ L(E, K) for 1 ≤ j ≤ m ,
(see Exercise 1.1). Now it follows from Proposition II.3.14 and Proposition 2.1(iii)
is equivalent to
f j (x) − f j (x0 ) − ∂f j (x0 )(x − x0 )
= 0 for 1 ≤ j ≤ m ,
as the theorem requires.

[ _to('140520152918') ] 
The Jacobi matrix
Suppose X is open in Rn and f = (f 1 , . . . , f m ) : X → Rm is partially differentiable at x0 . We then call
[ _v('140520153019') ]
the Jacobi matrix of f at x0 .

2.9 Corollary If f is differentiable at x0 , every coordinate function f j is partially
differentiable at x0 and
∂1 f 1 (x0 )
∂f (x0 ) = ∂k f j (x0 ) = ⎣
∂1 f (x0 )
∂n f 1 (x0 )
· · · ∂n f m (x0 )
that is, the representation matrix (in the standard basis) of the derivative of f is
the Jacobi matrix of f .
156
Proof For k = 1, . . . , n, we have ∂f (x0 )ek = m aj ej for unique aj ∈ R. From
j=1 k
the linearity of ∂f (x0 ) and Proposition 2.8, it follows that
∂f (x0 )ek = ∂f 1 (x0 )ek , . . . , ∂f m (x0 )ek = ∂k f 1 (x0 ), . . . , ∂k f m (x0 )
∂k f j (x0 )ej .
Therefore aj = ∂k f j .
A differentiability criterion
Remark 2.6 shows that the existence of all partial derivatives of a function does not
imply its total differentiability. To guarantee differentiability, extra assumptions
are required. One such assumption is that the partial derivatives are continuous.
As the next theorem — the fundamental differentiability criterion — shows, this
means the map is actually continuously differentiable.
2.10 Theorem Suppose X is open in Rn and F is a Banach space. Then f : X → F
is continuously differentiable if and only if f has continuous partial derivatives.
Proof “= follows easily from Proposition 2.5.
“⇐ Let x ∈ X. We define a linear map A(x) : Rn → F through
h := (h1 , . . . , hn ) → A(x)h :=
∂k f (x)hk .
Theorem 1.6 says A(x) belongs to L(R , F ). Our goal is to show ∂f (x) =
A(x), that is,
f (x + h) − f (x) − A(x)h
We choose ε > 0 with B(x, ε) ⊂ X and set x0 := x and xk := x0 +
1 ≤ k ≤ n and h = (h1 , . . . , hn ) ∈ B(x, ε). Then we get
hj ej for
f (x + h) − f (x) =
f (xk ) − f (xk−1 ) ,
and the fundamental theorem of calculus implies
∂k f (xk−1 + thk ek ) dt .
With this, we find the representation
f (x + h) − f (x) − A(x)h =
∂k f (xk−1 + thk ek ) − ∂k f (x) dt ,
which gives the bound
k=1 |y−x|∞ ≤|h|∞
∂k f (y) − ∂k f (x)
The continuity of ∂k f implies
f (x + h) − f (x) − A(x)h = o(|h|∞ ) (h → 0) .
Thus, from Remarks 2.2(a) and 2.2(c), f is differentiable at x, and ∂f (x) = A(x).
∂f (x) − ∂f (y) h
∂k f (x) − ∂k f (y)
and the equivalence of the norms of Rn , the continuity of ∂f follows from that of
∂k f for 1 ≤ k ≤ n.
2.11 Corollary Let X be open in Rn . Then f : X → Rm is continuously differentiable if and only if every coordinate function f j : X → R has continuous partial
derivatives, and then
∂f (x) = ∂k f j (x) ∈ Rm×n .
2.12 Example
The function
f : R3 → R2 ,
(x, y, z) → ex cos y, sin(xz)
is continuously differentiable, and
∂f (x, y, z) =
ex cos y
z cos(xz)
−ex sin y
x cos(xz)
for (x, y, z) ∈ R3 .
The derivative of a real-valued function of several real variables has an important geometric interpretation, which we prepare for next.
158
Let E be a normed vector space on K. We call E := L(E, K) the (continuous)
dual space of E. The elements of E are the continuous linear forms on E.
2.13 Remarks (a) By convention, we give E the operator norm on L(E, K), that
f := sup |f (x)| for f ∈ E .
Then Corollary 1.2 guarantees that E := (E , · ) is a Banach space. We also
say that the norm for E is the norm dual to the norm for E (or simply the dual
(b) Let E, (·|·) be an inner product space. For y ∈ E, we set
fy (x) := (x|y)
Then fy is a continuous linear form on E, and fy
Proof Because the inner product is linear in its first argument, we have fy ∈ Hom(E, K).
Because f0 = 0, it suﬃces to consider the case y ∈ E \{0}.
From the Cauchy–Schwarz inequality, we get
|fy (x)| = |(x | y)| ≤ x
Therefore fy ∈ L(E, K) = E with fy ≤ y . From fy (y/ y ) = y/ y
fy = sup |fy (x)| ≥ fy (y/ y ) = y .
y = y , it
Altogether, we find fy = y .
(c) Let E, (·|·) be an inner product space. Then the map
T:E→E ,
y → fy := (·|y)
is conjugate linear and an isometry.
Proof This follows from T (y + λz) = (· | y + λz) = (· | y) + λ(· | z) = T y + λT z and
from (b).
According to Remark 2.13(b), every y ∈ E has a corresponding continuous
linear form fy on E. The next theorem shows for finite-dimensional Hilbert spaces
that there are no other continuous linear forms.4 In other words, every f ∈ E
determines a unique y ∈ E such that f = fy .
4 From Theorem 1.6 we know that every linear form on a finite-dimensional Hilbert space
is continuous. Moreover, Corollary 1.5 guarantees that every finite-dimensional inner product
space is a Hilbert space.
2.14 Theorem (Riesz representation theorem) Suppose E, (·|·) is a finitedimensional Hilbert space. Then, for every f ∈ E , there is a unique y ∈ E such
that f (x) = (x|y) for all x ∈ E. Therefore
T: E→E ,
y → (·|y)
is a bijective conjugate linear isometry.
Proof From Remark 2.13(c), we know that T : E → E is a conjugate linear
isometry. In particular, T is injective. Letting n := dim(E), it follows from
Theorem 1.9 and Proposition 1.8(ii) that
E = L(E, K) ∼ K1×n ∼ Kn ,
and thus dim(E ) = dim(E). The rank formula of linear algebra, namely,
dim ker(T ) + dim im(T ) = dim E
(see, for example, [Gab96, ????? D.5.4]), now implies the surjectivity of T .
2.15 Remarks
(a) With the notation of Theorem 2.14, we set
[· | ·] : E × E → K ,
[f |g] := (T −1 g |T −1 f ) .
Then E , [· | ·] is a Hilbert space.
We leave the simple check to you.
(b) Suppose E, (·|·) is a real finite-dimensional Hilbert space. Then the map
E → E , y → (·|y) is an isometric isomorphism. In particular, (Rm ) is isometrically isomorphic to Rm by the canonical isomorphism, where (·|·) denotes the
Euclidean inner product on Rm . Very often, one identifies the spaces Rm and (Rm )
using this isomorphism.
(c) The Riesz representation theorem also holds for infinite-dimensional Hilbert
spaces. For a proof in full generality, we direct you to the literature on functional
analysis or encourage you to take a course in the subject.

[ _to('140520153657') ]
The gradient
Suppose X is open in Rn and f : X → R is differentiable at x0 ∈ X. Then we also call the derivative $\partial \operatorname{ f }(x0)$ the differential of f at x0 , and write it as $\operatorname{d} \operatorname{f}(x0)$.  The differential of f at x0 is therefore a (continuous) linear form on Rn . Using the Riesz representation theorem, there is a unique y ∈ Rn such that
df (x0 )h = (h|y) = (y |h)
[ 5 In Section VIII.3, we will clarify how this notation relates to the same notation in Remark VI.5.2 for the induced differential. ]
This y ∈ Rn uniquely determined by f and x0 is called the gradient of f at x0 .
We denote it by \nabla f (x0 ) or grad f (x0 ). The differential and the gradient of f at x0 are therefore linked by the fundamental relationship
df (x0 )h = \nabla f (x0 )| h
[ in this book the inner product is denoted $\left . | . \right)$ ]
Note that the differential df (x0 ) is a linear form on Rn , while the gradient \nabla f (x0 ) is a vector in Rn .

2.16 Proposition We have
\nabla f (x0 ) = ∂1 f (x0 ), . . . , ∂n f (x0 ) ∈ Rn .
Proof From Proposition 2.5, we have df (x0 )ek = ∂k f (x0 ) for k = 1, . . . , n. Now
it follows that
\nabla f (x0 )|h = df (x0 )h = df (x0 )
∂k f (x0 )hk = (y |h)
hk e k =
for h = (h1 , . . . , hn ) ∈ Rn with y := ∂1 f (x0 ), . . . , ∂n f (x0 ) ∈ Rn . The theorem
follows because this is true for every h ∈ Rn .
2.17 Remarks
(a) We call the point x0 a critical point of f if \nabla f (x0 ) = 0.
(b) Suppose x0 is not a critical point of f . We set h0 := \nabla f (x0 ) |\nabla f (x0 )|. The
proof of Remark 2.13(b) then shows that
df (x0 )h0 = |\nabla f (x0 )| = max df (x0 )h .
|h|≤1
Because df (x0 )h is the directional derivative of f
at x0 in the direction h, we have
f (x0 + th0 ) = f (x0 ) + tdf (x0 )h0 + o(t)
= f (x0 ) + t max df (x0 )h + o(t)
|h|=1
as t → 0, that is, the vector \nabla f (x0 ) points in the
direction along which f has the largest directional
derivative f , that is, in the direction of steepest
ascent of f .
If n = 2, the graph of f ,
x, f (x) ; x ∈ X
⊂ R 2 × R ∼ R3 ,
can be interpreted as a surface in R3 .6 We now imagine taking a point m on a path
in M as follows: We start at a point x0 , f (x0 ) ∈ M
such that x0 is not a critical point of f . We then
move m so that the projection of its “velocity vector”
is always along the gradient of f . The route m takes
when moved in this way is called the curve of steepest
ascent. If we move m against the gradient, its route
is called the curve of steepest descent.
(c) The remark above shows that \nabla f (x0 ) has acquired a geometric interpretation
independent of the special choice of coordinates or scalar product. However, the
representation of \nabla f (x0 ) does depend on the choice of scalar product. In particular, the representation from Proposition 2.16 holds only when Rn is given the
Euclidean scalar product.
Proof Suppose [gjk ] ∈ Rn×n is a symmetric, positive definite matrix, that is, gjk = gkj
for 1 ≤ j, k ≤ n and there is a γ > 0 such that
defines a scalar product on R (see Exercise 1.8).7 Thus Theorem 2.14 says there exists a
unique y ∈ Rn such that df (x0 )h = (y | h)g for h ∈ Rn . We call \nabla g f (x0 ) := y the gradient
of f at x0 with respect to the scalar product (· | ·)g . To determine its components, we
∂j f (x0 )hj = df (x0 )h = (y | h)g =
Therefore,
gjk y k = ∂j f (x0 )
for j = 1, . . . , n .
By assumption, the matrix g is invertible, and its inverse is also symmetric and positive
definite.8 We denote the entries of the inverse [gjk ] by g jk , that is [g jk ] = [gjk ]−1 . From
(2.5), it then follows that
g kj ∂j f (x0 )
for k = 1, . . . , n .
Corollary 8.9.
converse also holds: Every scalar product on Rn is of the form (· | ·)g (see, for example,
[Art93, Section VII.1]).
8 See Exercise 5.18.
7 The
162
This means that
\nabla g f (x0 ) =
g 1k ∂k f (x0 ), . . . ,
g nk ∂k f (x0 )
is the gradient of f with respect to the scalar product induced by g = [gjk ].
Complex differentiability
With the identification L(C) = C and C1×1 = C, we can identify A ∈ L(C) with
its matrix representation. Therefore, Az = a · z for z ∈ C and some a ∈ C. As
usual, we identify C = R + i R with R2 :
C = R + iR
z = x + i y ← (x, y) ∈ R2 , where x = Re z and y = Im z .
For the action of A with respect to this identification, we find by the identification
a = α + i β ← (α, β) that
Az = a · z = (α + i β)(x + iy) = (αx − βy) + i(βx + αy) ,
Az ←
Suppose X is open at C. For f : X → C, we set u := Re f and v := Im f .
CX f = u + i v ← (u, v) =: F ∈ R2
where, in the last term, X is understood to be a subset of R2 . In this notation, the
following fundamental theorem gives the connection between complex9 and total
differentiability.
2.18 Theorem The function f is complex differentiable at z0 = x0 +iy0 if and only
if F := (u, v) is totally differentiable at (x0 , y0 ) and satisfies the Cauchy–Riemann
equations10
at (x0 , y0 ). In that case,
f (z0 ) = ux (x0 , y0 ) + i vx (x0 , y0 ) .
(i) Suppose f is complex differentiable at z0 . We set
9 Recall that f is complex differentiable at a point z if and only if f (z + h) − f (z ) h has
a limit in C as h → 0 in C.
10 For functions f in two or three real variables, common notations are f := ∂ f , f := ∂ f
and fz := ∂3 f .
where α := Re f (z0 ) and β := Im f (z0 ). Then for h = ξ + i η ← (ξ, η), we have
(ξ,η)→(0,0)
|F (x0 + ξ, y0 + η) − F (x0 , y0 ) − A(ξ, η)|
|(ξ, η)|
f (z0 + h) − f (z0 ) − f (z0 )h
Therefore F is totally differentiable at (x0 , y0 ) (see Remark II.2.1(a)) with
∂F (x0 , y0 ) =
∂1 u(x0 , y0 )
∂1 v(x0 , y0 )
∂2 u(x0 , y0 )
∂2 v(x0 , y0 )
Thus we have the Cauchy–Riemann equations
∂1 u(x0 , y0 ) = ∂2 v(x0 , y0 ) ,
∂2 u(x0 , y0 ) = −∂1 v(x0 , y0 ) .
(ii) If F is totally differentiable at (x0 , y0 ) and (2.8) holds, we may set
a := ∂1 u(x0 , y0 ) + i ∂1 v(x0 , y0 ) .
Then because of (2.7), we have
f (z0 + h) − f (z0 ) − ah
|F (x0 + ξ, y0 + η) − F (x0 , y0 ) − ∂F (x0 , y0 )(ξ, η)|
Consequently, f is complex differentiable at z0 with f (z0 ) = a.
2.19 Examples (a) According to Example IV.1.13(b), f : C → C, z → z 2 is
everywhere complex differentiable with f (z) = 2z. Because
f (x + i y) = (x + iy)2 = x2 − y 2 + i (2xy) ← u(x, y), v(x, y) ,
ux = vy = 2x and uy = −vx = −2y .
Therefore the Cauchy–Riemann equations are satisfied (as they must be), and
f (z) = f (x + i y) = 2x + i 2y = 2(x + i y) = 2z.
(b) The map f : C → C for z → z is complex differentiable nowhere,11 because,
f (x + i y) = x − i y ← u(x, y), v(x, y) ,
the Cauchy–Riemann equations are never satisfied:
ux = 1 ,
11 See
Exercise IV.1.4.
uy = 0 ,
vx = 0 ,
vy = −1 .
164
The above map gives a very simple example of a continuous but nowhere
differentiable complex-valued function of one complex variable. Note, though,
that the “same” function
F = (u, v) : R2 → R2 ,
(x, y) → (x, −y)
is totally differentiable everywhere with the constant derivative
for (x0 , y0 ) ∈ R2 . Therefore F is actually continuously differentiable.
Calculate ∂2 f (1, y) for f : (0, ∞)2 → R with
(xx )x
+ log(x) arctan arctan arctan sin cos(xy) − log(x + y)
Suppose f : R2 → R is defined by
x2 + y 2 ,
y>0,
y=0,
y<0.
(a) f is not differentiable at (0, 0);
(b) every directional derivative of f exists at (0, 0).
At which points is
R2 → R ,
(x, y) →
differentiable?
Let f : R2 → R be defined by
(a) ∂1 f (0, 0) and ∂2 f (0, 0) exist.
(b) Dv f (0, 0) does not exist for v ∈ R2 \{e1 , e2 }.
(c) f is not differentiable at (0, 0).
Calculate the Jacobi matrix of
R3 → R ,
(x, y, z) → 3x2 y + exz + 4z 3 ;
R2 → R3 ,
(x, y) → xy, cosh(xy), log(1 + x2 ) ;
(x, y, z) → log(1 + x2 + z 2 ), z 2 + y 2 − x2 , sin(xz) ;
R3 → R3 ,
(x, y, z) → (x sin y cos z, x sin y sin z, x cos y) .
6 Suppose X is open at Rn , F is a Banach space, and f : X → F . Also suppose x0 ∈ X,
and choose ε > 0 such that B(x0 , ε) ⊂ X. Finally, let
xk (h) := x0 + h1 e1 + · · · + hk ek
for k = 1, . . . , n and h ∈ B(x0 , ε) .
Prove that f is differentiable at x0 if and only if, for every h ∈ B(x0 , ε) such that hk = 0
for 1 ≤ k ≤ n, the limit
hk →0
hk =0
f xk (h) − f xk−1 (h)
for 1 ≤ k ≤ n
exists in F .
Prove Remark 2.15(a). (Hint: Theorem 2.14.)
Determine the gradients of these functions:
R \{0} → R ,
x → (x0 | x) ;
x → |(x0 | x)|2 ;
x → 1/|x| .
At which points is C → C, z → z |z| differentiable? If possible, find the derivative.
10 At which points is Rm → Rm , x → x |x|k with k ∈ N differentiable? Calculate the
derivative wherever it exists.
Find all differentiable functions f : C → C such that f (C) ⊂ R.
12 For p ∈ [1, ∞], let fp : Rm → R, x → |x|p . Where is fp differentiable? If possible,
find \nabla fp (x).
13 Suppose X is open in C and f : X → C is differentiable. Also define f ∗ (z) := f (z)
for z ∈ X ∗ := { z ∈ C ; z ∈ X }. Show that f ∗ : X ∗ → C is differentiable.
166
3 Multivariable differentiation rules
We collect here the most important rules for multidimensional differentiation. As
usual, we assume
• $E$ and $F$ are Banach spaces over the field $\mathbb{K}$;
Linearity
The next theorem shows that — as in the one-variable case — differentiation is
linear.
3.1 Proposition Suppose f, g : X → F are differentiable at x0 and α ∈ K. Then
f + αg is also differentiable at x0 , and
∂(f + αg)(x0 ) = ∂f (x0 ) + α∂g(x0 ) .
By assumption, we can write
g(x) = g(x0 ) + ∂g(x0 )(x − x0 ) + sx0 (x) x − x0
where the functions rx0 , sx0 : X → F are continuous and vanish at x0 . Thus,
(f + αg)(x) = (f + αg)(x0 ) + ∂f (x0 ) + α∂g(x0 ) (x − x0 ) + tx0 (x) x − x0
where tx0 := rx0 + αsx0 . Proposition 2.1 then implies the theorem.
3.2 Corollary C 1 (X, F ) is a vector subspace of C(X, F ), and
∂ : C 1 (X, F ) → C X, L(E, F ) ,
is linear.
The chain rule
In Chapter IV, we saw the great importance of the chain rule for functions of one
variable, and, as we shall see here, the same is true in the multivariable case.
3.3 Theorem (chain rule) Suppose Y is open in F and G is a Banach space.
Also suppose f : X → F is differentiable at x0 and g : Y → G is differentiable at
y0 := f (x0 ) and that f (X) ⊂ Y . Then g ◦ f : X → G is differentiable at x0 , and
the derivative is given by
∂(g ◦ f )(x0 ) = ∂g f (x0 ) ∂f (x0 ) .
VII.3 Multivariable differentiation rules
Proof 1
If A := ∂g f (x0 ) ∂f (x0 ), then A ∈ L(E, G). Proposition 2.1 implies
f (x) = f (x0 ) + ∂f (x0 )(x − x0 ) + r(x) x − x0
g(y) = g(y0 ) + ∂g(y0 )(y − y0 ) + s(y) y − y0
where r : X → F and s : Y → G are continuous at x0 and at y0 , respectively, and
also r(x0 ) = 0 and s(y0 ) = 0. We define t : X → G through t(x0 ) := 0 and
t(x) := ∂g f (x0 ) r(x) + s f (x)
∂f (x0 )
+ r(x)
for x = x0 .
Then t is continuous at x0 . From (3.1) and with y := f (x), we derive the relation
(g ◦ f )(x) = g f (x0 ) + A(x − x0 ) + ∂g f (x0 ) r(x) x − x0
+ s f (x) ∂f (x0 )(x − x0 ) + r(x) x − x0
= (g ◦ f )(x0 ) + A(x − x0 ) + t(x) x − x0 .
The theorem then follows from Proposition 2.1.
3.4 Corollary (chain rule in coordinates) Suppose X is open in Rn and Y is
open in Rm . Also suppose f : X → Rm is differentiable at x0 and g : Y → R is
differentiable at y0 := f (x0 ) and also f (X) ⊂ Y . Then h := g ◦ f : X → R is
∂h(x0 ) = ∂g f (x0 )
∂f (x0 ) ,
that is, the Jacobi matrix of the composition h = g ◦ f is the product of the Jacobi
matrices of g and f .
This follows from Theorem 3.3, Corollary 2.9, and Theorem 1.9(ii).
3.5 Remark With the notation of Corollary 3.4, the entries of the Jacobi matrix
of h are
∂hj (x0 )
∂g j f (x0 ) ∂f i (x0 )
∂y i
for 1 ≤ j ≤
and 1 ≤ k ≤ n .
With this fundamental formula, we can calculate partial derivatives of composed
functions in many concrete cases.
1 See
Using the rule for multiplying matrices, this follows immediately from (3.2).
the proof of Theorem IV.1.7.
168
3.6 Examples
(a) We consider the map
f : R2 → R3 ,
g : R3 → R2 ,
(x, y) → (x2 , xy, xy 2 ) ,
(ξ, η, ζ) → sin ξ, cos(ξηζ) .
For h := g ◦ f : R2 → R2 , we then have h(x, y) = sin x2 , cos(x4 y 3 ) . Thus h is
∂h(x, y) =
2x cos x2
−4x y sin(x y ) −3x y sin(x4 y 3 )
As for the Jacobi matrices of g and f , we respectively find
y 2 2xy
−ηζ sin(ξηζ)
−ξζ sin(ξηζ)
−ξη sin(ξηζ)
Now we can easily verify that the product of these two matrices agrees with (3.3)
at the position (ξ, η, ζ) = f (x, y).
(b) Suppose X is open in Rn and f ∈ C 1 (X, R). Also let I be an open interval
in R, and suppose ϕ ∈ C 1 (I, Rn ) with ϕ(I) ⊂ X. Then f ◦ ϕ belongs to C 1 (I, R),
(f ◦ ϕ) (t) = \nabla f ϕ(t) ϕ(t) for t ∈ I .
From the chain rule, we get
(f ◦ ϕ) (t) = df ϕ(t) ϕ(t) = \nabla f ϕ(t)
ϕ(t)
for t ∈ I .
(c) We use the above notation and consider a path ϕ : I → X, which stays
within a “level set” of f , that is, there is a y ∈ im(f ) such that f ϕ(t) = y
for each t ∈ I. From (b), it then follows
\nabla f ϕ(t) ϕ(t) = 0
for t ∈ I. This shows the gradient \nabla f (x)
at the point x = ϕ(t) is orthogonal to
the path ϕ and, therefore, also to the
tangent of ϕ through t, ϕ(t) (see Remark IV.1.4(a)). Somewhat imprecisely,
we say, “The gradient is orthogonal to
level sets”.
Level sets of the function
(x, y, z) → x2 + y 2 − z 2

[ _to('140526160736') ]
The product rule
The product rule for real-valued functions is another application of the chain rule.2
3.7 Proposition Suppose f, g ∈ C 1 (X, R). Then f g also belongs to C 1 (X, R),
and the product rule says
∂(f g) = g∂f + f ∂g .
m : R2 → R ,
(α, β) → αβ
we have m ∈ C 1 (R , R) and \nabla m(α, β) = (β, α). Setting F := m ◦ (f, g), we get
F (x) = f (x)g(x) for x ∈ X. From the chain rule, it follows that F ∈ C 1 (X, R) with
∂F (x) = ∂m f (x), g(x) ◦ ∂f (x), ∂g(x) = g(x)∂f (x) + f (x)∂g(x) .

[ _to('140526160631') ]
3.8 Corollary [ chain rule for the gradient ] In the case $E$ = Rn , we have
d(f g) = gdf + f dg
and \nabla (f g) = g\nabla f + f \nabla g .
The first formula is another way of writing (3.4). Because
\nabla (f g)(x) h = d(f g)(x)h = f (x)dg(x)h + g(x)df (x)h
= f (x)\nabla g(x) + g(x)\nabla f (x) h
for x ∈ X and h ∈ Rn , the second formula also holds.

The mean value theorem
As in the one-variable case, the multivariable case has a mean value theorem, which
means we can use the derivative to estimate the difference of function values.
In what follows, we use the notation [[x, y]] introduced in Section III.4 for the
straight path x + t(y − x) ; t ∈ [0, 1] between the points x, y ∈ E.
3.9 Theorem (mean value theorem) Suppose f : X → F is differentiable. Then
f (x) − f (y) ≤ sup ∂f x + t(y − x)
0≤t≤1
for all x, y ∈ X such that [[x, y]] ⊂ X.
also Example 4.8(b).
170
Proof We set ϕ(t) := f x + t(y − x) for t ∈ [0, 1]. Because [[x, y]] ⊂ X, we know
ϕ is defined. The chain rule shows that ϕ is differentiable with
ϕ(t) = ∂f x + t(y − x) (y − x) .
From Theorem IV.2.18, the mean value theorem for vector-valued functions of one
variable, it follows that
f (y) − f (x) = ϕ(1) − ϕ(0) ≤ sup
ϕ(t) .
We finish the theorem by also considering
ϕ(t) ≤ ∂f x + t(y − x)
for t ∈ [0, 1] .
Under the somewhat stronger assumption of continuous differentiability, we
can prove a very useful variant of the mean value theorem:
3.10 Theorem (mean value theorem in integral form) Let f ∈ C 1 (X, F ). Then
f (y) − f (x) =
∂f x + t(y − x) (y − x) dt
(3.6)
for x, y ∈ X such that [[x, y]] ⊂ X.
Proof The auxiliary function ϕ from the previous proof is now continuously
differentiable. Thus we can apply the fundamental theorem of calculus (Corollary VI.4.14), and it gives
f (y) − f (x) = ϕ(1) − ϕ(0) =
3.11 Remarks
∂f x + t(y − x) (y − x) dt .
ϕ(t) dt =
Suppose f : X → F is differentiable.
(a) If ∂f is continuous, the representation (3.6) gives the estimate (3.5).
Apply Proposition VI.4.3 and the definition of the operator norm.
(b) If X is convex and ∂f : X → L(E, F ) is bounded, then f is Lipschitz continuous.
Let α := supx∈X ∂f (x) . Then from Theorem 3.9, we get
f (y) − f (x) ≤ α y − x
(c) If X is connected and ∂f = 0, then f is constant.
(3.7)
Proof Suppose x0 ∈ X and r > 0 such that B(x0 , r) ⊂ X. Letting y0 := f (x0 ), it then
follows from (3.7) that f (x) = y0 for all x ∈ B(x0 , r). Because x0 was arbitrary, f is
locally constant. Therefore f −1 (y0 ) is nonempty and open in X. Also, from Proposition 2.1(ii) and Example III.2.22(a), f −1 (y0 ) is closed in X. The claim then follows from
Remark III.4.3.
The theorem of the differentiability of limits of sequences of functions (Theorem V.2.8) can now be easily extended to the general case.
3.12 Theorem Let fk ∈ C 1 (X, F ) for k ∈ N. Also assume there are functions
f ∈ F X and g : X → L(E, F ) such that
(i) (fk ) converges pointwise to f ;
(ii) (∂fk ) locally converges uniformly to g.
Then f belongs to C 1 (X, F ), and we have ∂f = g.
Proof Under the application of Theorem 3.9, the proof of Theorem V.2.8 remains
valid word-for-word.
Necessary condition for local extrema
In the important Theorem IV.2.1, we gave a necessary criterion for the existence of
a local extremum of a function of one real variable. With the methods developed
here, we can now treat many variables.
3.13 Theorem Suppose the map f : X → R has a local extremal point at x0 and
all its directional derivatives exist there. Then
Dv f (x0 ) = 0
for v ∈ E \{0} .
Proof Let v ∈ E \{0}. We choose r > 0 such that x0 + tv ∈ X for t ∈ (−r, r)
and consider a function of one real variable:
ϕ : (−r, r) → R ,
t → f (x0 + tv) .
Then ϕ is differentiable at 0 and has a local extremum at 0. From Theorem IV.2.1,
we have ϕ(0) = Dv f (x0 ) = 0.
3.14 Remarks (a) Suppose f : X → R is differentiable at x0 . Then x0 is called
a critical point of f if df (x0 ) = 0. If f has a local extremum at x0 , then x0 is a
critical point. When E = Rn , this definition agrees with that of Remark 2.17(a).
This follows from Proposition 2.5 and Theorem 3.13.
172
(b) A critical point x0 is not necessarily a local extremal point.3
f : R2 → R ,
(x, y) → x2 − y 2 .
Then \nabla f (0, 0) = 0, but (0, 0) is not
an extremal point of f . Instead, it is
a “saddle point”.
We say a function f : E → F is positively homogeneous of degree α ∈ R if
f (tx) = tα f (x)
for t > 0 and x ∈ E \{0} .
Show that if f ∈ C 1 (E, F ) is positively homogeneous of degree 1, then f ∈ L(E, F ).
2 Suppose f : Rm → R is differentiable at Rm \{0}. Prove that f is positively homogeneous of degree α if it satisfies the Euler homogeneity relation
\nabla f (x) x = αf (x)
for x ∈ Rm \{0} .
Suppose X is open in Rm and f ∈ C 1 (X, Rn ). Show that
x → sin |f (x)|2
is continuously differentiable, and determine \nabla g.
For f ∈ C 1 (Rn , R) and A ∈ L(Rn ), show
\nabla (f ◦ A)(x) = A∗ \nabla f (Ax)
5 Suppose X is open in Rk and Y is open and bounded in Rn . Further suppose f ∈
C X × Y , R is differentiable in X ×Y and there is a ξ ∈ C 1 (X, Rn ) such that im(ξ) ⊂ Y
m(x) := min f (x, y) = f x, ξ(x) for x ∈ X .
Calculate the gradient of m : X → R.
For g ∈ C 1 (Rm , R) and fj ∈ C 1 (R, R) with j = 1, . . . , m, calculate the gradient of
(x1 , . . . , xm ) → g f1 (x1 ), . . . , fm (xm ) .
7 The function f ∈ C 1 (R2 , R) satisfies ∂1 f = ∂2 f and f (0, 0) = 0. Show that there is a
g ∈ C(R2 , R) such that f (x, y) = g(x, y)(x + y) for (x, y) ∈ R2 .
Verify for the exponential map that
exp ∈ C 1 L(E), L(E)
3 Compare
this also with Remark IV.2.2(c).
∂ exp(0) = idL(E) .
VII.4 Multilinear maps
4 Multilinear maps
Let E and F be Banach spaces, and let X be an open subset of E. For f ∈
C 1 (X, F ), we have ∂f ∈ C X, L(E, F ) . We set g := ∂f and F := L(E, F ).
According to Theorem 1.1, F is a Banach space, and we can therefore study the
differentiability of the map g ∈ C(X, F). If g is [continuously] differentiable, we
say f is twice [continuously] differentiable, and ∂ 2 f := ∂g is the second derivative
∂ 2 f (x) ∈ L E, L(E, F ) for x ∈ X .
Theorem 1.1 says that L E, L(E, F ) is also a Banach space. Thus we can study
the third derivative ∂ 3 f := ∂(∂ 2 f ) (if it exists) and likewise find
∂ 3 f (x) ∈ L E, L E, L(E, F )
In this notation, the spaces occupied by ∂ n f evidently become increasingly complex with increasing n. However, things are not as bad as they seem. We will show
that the complicated-looking space L E, L E, . . . , L(E, F ) · · · (with E occurring
n-times) is isometrically isomorphic to the space of all continuous multilinear maps
from E ×· · ·×E (again with n E’s) to F . Multilinear maps are therefore the proper
setting for understanding higher derivatives.
Continuous multilinear maps
In the following, E1 , . . . , Em for m ≥ 2, E, and F are Banach spaces over the field
K. A map ϕ : E1 × · · · × Em → F is multilinear or, equivalently, m-linear1 if for
every k ∈ {1, . . . , m} and every choice of xj ∈ Ej for j = 1, . . . , m with j = k, the
ϕ(x1 , . . . , xk−1 , ·, xk+1 , . . . , xm ) : Ek → F
is linear, that is, ϕ is multilinear if it is linear in every variable.
First we show that a multilinear map is continuous if and only if it is
bounded.2
4.1 Proposition For the m-linear map ϕ : E1 × · · · × Em → F , these statements
are equivalent:
(i) ϕ is continuous.
(ii) ϕ is continuous at 0.
(iii) ϕ is bounded on bounded sets.
(iv) There is an α ≥ 0 such that
ϕ(x1 , . . . , xm ) ≤ α x1 · · · · · xm
for xj ∈ Ej ,
m = 2 [or m = 3] one speaks of bilinear [or trilinear] maps.
with Theorem VI.2.5.
2 Compare
1≤j≤m.
174
Proof The implication “(i)=
⇒(ii)” is clear.
⇒(iii)” Supposing B ⊂ E1 × · · ·× Em is bounded, there exists, according
to Example II.3.3(c) and Remark II.3.2(a), a β > 0 such that xj ≤ β for
(x1 , . . . , xm ) ∈ B and 1 ≤ j ≤ m. Because ϕ is continuous at 0, there is a
ϕ(y1 , . . . , ym ) ≤ 1
for yj ∈ Ej ,
For (x1 , . . . , xm ) ∈ B and 1 ≤ j ≤ m, we set yj := δxj /β. Then yj ≤ δ, and
(δ/β)m ϕ(x1 , . . . , xm ) = ϕ(y1 , . . . , ym ) ≤ 1 .
Therefore ϕ(B) is bounded.
“(iii)=
⇒(iv)” By assumption, there is an α > 0 such that
ϕ(x1 , . . . , xm ) ≤ α for (x1 , . . . , xm ) ∈ B .
For yj ∈ Ej \{0}, we set xj := yj / yj . Then (x1 , . . . , xm ) belongs to B, and we
ϕ(y1 , . . . , ym ) = ϕ(x1 , . . . , xm ) ≤ α ,
y1 · · · · · ym
“(iv)=
⇒(i)” Suppose y = (y1 , . . . , ym ) is a point of E1 × · · · × Em and (xn )
is a sequence in E1 × · · · × Em with limn xn = y. Letting (xn , . . . , xn ) := xn , we
have by assumption that
ϕ(y1 , . . . , ym ) − ϕ(xn , . . . , xn )
≤ ϕ(y1 − xn , y2 , . . . , ym ) + ϕ(xn , y2 − xn , y3 , . . . , ym )
+ · · · + ϕ(xn , xn , . . . , ym − xn )
y1 − xn
y2 · · · · · ym + xn
y2 − xn · · · · · ym
Because the sequence (xn ) is bounded in E1 × · · · × Em , this estimate shows,
together with (the analogue of) Proposition II.3.14, that ϕ(xn ) converges to ϕ(y).
It is useful to introduce a shorthand. We denote by
L(E1 , . . . , Em ; F )
the set of all continuous multilinear maps from E1 × · · · × Em to F . Evidently
L(E1 , . . . , Em ; F ) is a vector subspace of C(E1 × · · · × Em , F ). Often, all the Ej
are the same, and we use the notation
Lm (E, F ) := L(E, . . . , E; F ) .
In addition, we set L1 (E, F ) := L(E, F ) and L0 (E, F ) := F . Finally, let
ϕ := inf α ≥ 0 ;
ϕ(x1 , . . . , xm ) ≤ α x1 · · · · · xm , xj ∈ Ej
for ϕ ∈ L(E1 , . . . , Em ; F ).
4.2 Theorem
(i) For ϕ ∈ L(E1 , . . . , Em ; F ), we find
ϕ(x1 , . . . , xm ) ;
xj ≤ 1, 1 ≤ j ≤ m
ϕ(x1 , . . . , xm ) ≤ ϕ
x1 · · · · · xm
for (x1 , . . . , xm ) ∈ E1 × · · · × Em .
(ii) The relation (4.1) defines a norm and
L(E1 , . . . , Em ; F ) := L(E1 , . . . , Em ; F ), ·
(iii) When dim Ej < ∞ for 1 ≤ j ≤ m, every m-linear map is continuous.
Proof For m = 1, these are implied by Proposition VI.2.3, Conclusion VI.2.4(e),
and Theorems 1.1 and 1.6. The general case can be verified through the obvious
modification of these proofs, and we leave this to you as an exercise.
The canonical isomorphism
The norm on L(E1 , . . . , Em ; F ) is a natural extension of the operator norm on
L(E, F ). The next theorem shows this norm is also natural in another way.
4.3 Theorem The spaces L(E1 , . . . , Em ; F ) and L E1 , L E2 , . . . , L(Em , F ) · · ·
are isometrically isomorphic.
Proof We verify the statement for m = 2. The general case obtains via a simple
induction argument.
(i) For T ∈ L E1 , L(E2 , F ) we set
for (x1 , x2 ) ∈ E1 × E2 .
ϕT (x1 , x2 ) := (T x1 )x2
Then ϕT : E1 × E2 → F is bilinear, and
ϕT (x1 , x2 ) ≤ T
Therefore ϕT belongs to L(E1 , E2 ; F ), and ϕT ≤ T .
176
(ii) Suppose ϕ ∈ L(E1 , E2 ; F ). Then we set
Tϕ (x1 )x2 := ϕ(x1 , x2 ) for (x1 , x2 ) ∈ E1 × E2 .
Tϕ (x1 )x2 = ϕ(x1 , x2 ) ≤ ϕ
for (x1 , x2 ) ∈ E1 × E2 ,
Tϕ (x1 ) ∈ L(E2 , F ) for Tϕ (x1 ) ≤ ϕ
for every x1 ∈ E1 . Therefore
Tϕ := x1 → Tϕ (x1 ) ∈ L E1 , L(E2 , F ) and Tϕ ≤ ϕ .
(iii) Altogether, we have proved that the maps
T → ϕT : L E1 , L(E2 , F ) → L(E1 , E2 ; F )
ϕ → Tϕ : L(E1 , E2 ; F ) → L E1 , L(E2 , F )
are linear, continuous, and inverses of each other. Thus they are topological isomorphisms. In particular, TϕT = T , and we get
Thus the map T → ϕT is an isometry.
Convention L(E1 , . . . , Em ; F ) and L E1 , L E2 , . . . , L(Em , F ) · · ·
tified using the isometric isomorphism (4.2).
are iden-
4.4 Conclusions (a) For m ∈ N, we have
L E, Lm (E, F ) = Lm+1 (E, F ) .
This follows immediately from Theorem 4.3 and the above convention.
(b) L (E, F ) is a Banach space.
This follows from Theorem 4.3 and Remark 1.3(c).
Symmetric multilinear maps
Suppose m ≥ 2 and ϕ : E m → F is m-linear. We say ϕ is symmetric if
ϕ(xσ(1) , . . . , xσ(m) ) = ϕ(x1 , . . . , xm )
for every (x1 , . . . , xm ) and every permutation σ of {1, . . . , m}. We set
Lm (E, F ) :=
ϕ ∈ Lm (E, F ) ; ϕ is symmetric
4.5 Proposition Lm (E, F ) is a closed vector subspace of Lm (E, F ) and is theresym
fore itself a Banach space.
Proof Suppose (ϕk ) is a sequence in Lm (E, F ) that converges in Lm (E, F )
to ϕ. For every (x1 , . . . , xm ) ∈ E m and every permutation3 σ ∈ Sm , we then have
ϕ(xσ(1) , . . . , xσ(m) ) = lim ϕk (xσ(1) , . . . , xσ(m) ) = lim ϕk (x1 , . . . , xm )
= ϕ(x1 , . . . , xm ) .
Therefore ϕ is symmetric.
The next theorem shows that m-linear maps are actually continuously differentiable and that the derivatives are sums of (m−1)-linear functions.
4.6 Proposition L(E1 , . . . , Em ; F ) is a vector subspace of C 1 (E1 × · · · × Em , F ).
And, for ϕ ∈ L(E1 , . . . , Em ; F ) and (x1 , . . . , xm ) ∈ E1 × · · · × Em , we have
∂ϕ(x1 , . . . , xm )(h1 , . . . , hm ) =
ϕ(x1 , . . . , xj−1 , hj , xj+1 , . . . , xm )
for (h1 , . . . , hm ) ∈ E1 × · · · × Em .
We denote by Ax the map
(h1 , . . . , hm ) →
from E1 × · · · × Em to F . Then it is not hard to verify that
(x → Ax ) ∈ C E1 × · · · × Em , L(E1 × · · · × Em , F ) .
Letting yk := xk + hk , the equation
ϕ(y1 , . . . , ym ) = ϕ(x1 , . . . , xm ) +
ϕ(x1 , . . . , xk−1 , hk , yk+1 , . . . , ym )
follows from the multilinearity of ϕ. Because each of the maps
Ek+1 × · · · × Em → F ,
3 Recall
(zk+1 , . . . , zm ) → ϕ(x1 , . . . , xk−1 , hk , zk+1 , . . . , zm )
that Sn is the group of permutations of the set {1, . . . , n} (see the end of Section I.7).
178
is (m−k)-linear, we likewise get
= ϕ(x1 , . . . , xk−1 , hk , xk+1 , . . . , xm )
ϕ(x1 , . . . , xk−1 , hk , xk+1 , . . . , xk+j−1 , hk+j , yk+j+1 , . . . , ym ) .
Consequently, we find
ϕ(x1 + h1 , . . . , xm + hm ) − ϕ(x1 , . . . , xm ) − Ax h = r(x, h) ,
where r(x, h) is a sum of function-valued multilinear maps, in which every summand has at least two different hi for suitable i ∈ {1, . . . , m}. Thus Theorem 4.2(i)
implies that r(x, h) = o( h ) as h → 0.
4.7 Corollary Suppose X is open in K and ϕ ∈ L(E1 , . . . , Em ; F ) and fj ∈
C 1 (X, Ej ) for 1 ≤ j ≤ m. Then the map
ϕ(f1 , . . . , fm ) : X → F ,
x → ϕ f1 (x), . . . , fm (x)
belongs to C 1 (X, F ), and we have
∂ ϕ(f1 , . . . , fm ) =
ϕ(f1 , . . . , fj−1 , fj , fj+1 , . . . , fm ) .
This follows from fj (x) ∈ Ej (Proposition 4.6) and the chain rule.
The signature formula
det[aj ] =
shows that det[aj ] is an m-linear function of the row vectors aj := (aj , . . . , aj )
(or, alternatively, of the column vectors a• := (a1 , . . . , am )).
For a1 , . . . , am ∈ Km with ak = (a1 , . . . , am ), we set
[a1 , . . . , am ] := [aj ] ∈ Km×m .
In other words, [a1 , . . . , am ] is the square matrix with the column vectors a• := ak
for 1 ≤ k ≤ m. Therefore, the determinant function
det : Km × · · · × Km → K ,
is a well-defined m-linear map.
(a1 , . . . , am ) → det[a1 , . . . , am ]
4.8 Examples Suppose X is open in K.
(a) For a1 , . . . , am ∈ C 1 (X, Km ), we have det[a1 , . . . , am ] ∈ C 1 (X, K) and
det[a1 , . . . , am ]
det[a1 , . . . , aj−1 , aj , aj+1 , . . . , am ] .
(b) Suppose ϕ ∈ L(E1 , E2 ; F ) and (f, g) ∈ C 1 (X, E1 ×E2 ). Then the (generalized)
product rule is
∂ϕ(f, g) = ϕ(∂f, g) + ϕ(f, ∂g) .
Let H := H, (· | ·) be a finite-dimensional Hilbert space. Show these facts:
(a) For every a ∈ L2 (H, K), there is exactly one A ∈ L(H), the linear operator induced
by a, satisfying
a(x, y) = (Ax | y)
The map
L2 (H, K) → L(H) ,
a→A
thus induced is an isometric isomorphism.
(b) In the real case, we have a ∈ L2 (H, R) ⇐ A = A∗ .
(c) Suppose H = Rm and (· | ·) is the Euclidean scalar product. Then show
ajk xj y k
a(x, y) =
for x = (x1 , . . . , xm ) ,
y = (y 1 , . . . , y m ) ,
where [ajk ] is the matrix representation of the linear operator induced by a.
(Hint for (a): the Riesz representation theorem.4 )
2 Suppose X is open in E and f ∈ C 1 X, Lm (E, F ) satisfies f (X) ⊂ Lm (E, F ).
Then show ∂f (x)h ∈ Lm (E, F ) for x ∈ X and h ∈ E.
3 For A ∈ Kn×n , show det eA = etr(A) . (Hint: Let h(t) := det etA − et tr(A) for t ∈ R.
With the help of Example 4.8(a), conclude h = 0.)
For T1 , T2 ∈ L(F, E) and S ∈ L(E, F ), let g(T1 , T2 )(S) := T1 ST2 . Verify
(a) g(T1 , T2 ) ∈ L L(E, F ), L(F, E) ;
(b) g ∈ L2 L(F, E); L L(E, F ), L(F, E) .
Suppose H is a Hilbert space A ∈ L(H) and a(x) := (Ax | x) for x ∈ H. Show
(a) ∂a(x)y = (Ax | y) + (Ay | x) for x, y ∈ H;
(b) if H = Rn , then \nabla a(x) = (A + A∗ )x for x ∈ Rn .
4 Because the Riesz representation theorem also holds for infinite-dimensional Hilbert spaces,
(4.3) is also true in such spaces.
180
5 Higher derivatives
We can now define higher derivatives. We will see that if a function is continuously
differentiable, its higher derivatives are symmetric multilinear maps.
Primarily, this section generalizes the Taylor expansion to functions of more
than one variable. As one of its consequences, it will give, as in the one-dimensional
case, a suﬃcient criterion of the existence of local extrema.
• E and F are Banach spaces and X is an open subset of E.
Definitions
As mentioned previously, we will define higher derivatives inductively, as we did
in the one variable case.
Suppose f : X → F and x0 ∈ X. We then set ∂ 0 f := f . Therefore
∂ f (x0 ) belongs to F = L0 (E, F ). Suppose now that m ∈ N× and ∂ m−1 f : X →
Lm−1 (E, F ) is already defined. If
∂ m f (x0 ) := ∂(∂ m−1 f )(x0 ) ∈ L E, Lm−1 (E, F ) = Lm (E, F )
exists, we call ∂ m f (x0 ) the m-th derivative of f at x0 , and f is m-times differentiable at x0 . When ∂ m f (x) exists for every x ∈ X, we call
∂ m f : X → Lm (E, F )
the m-th derivative of f and say f is m-times differentiable. If ∂ m f is continuous
as well, we say f is m-times continuously differentiable. We set
C m (X, F ) := { f : X → F ; f is m-times continuously differentiable }
C ∞ (X, F ) :=
C m (X, F ) .
Evidently C (X, F ) = C(X, F ), and f is smooth or infinitely continuously differentiable if f belongs to C ∞ (X, F ).
Instead of ∂ m f , we sometimes write Dm f or f (m) ; we also set f := f (1) ,
f := f (2) , f := f (3) etc.
5.1 Remarks Suppose m ∈ N.
(a) The m-th derivative is linear, that is,
∂ m (f + αg)(x0 ) = ∂ m f (x0 ) + α∂ m g(x0 )
for α ∈ K and f, g : X → F if f and g are m-times differentiable at x0 .
VII.5 Higher derivatives
This follows by induction from Proposition 3.1.
f : X → F ; ∂ j f ∈ C X, Lj (E, F ) , 0 ≤ j ≤ m .
(b) C (X, F ) =
This is implied by Proposition 2.1(ii) and the definition of the m-th derivative.
(c) One can now easily verify that C m+1 (X, F ) is a vector subspace of C m (X, F ).
Also C ∞ (X, F ) is a vector subspace of C m (X, F ). In particular, we have the chain
of inclusions
C ∞ (X, F ) ⊂ · · · ⊂ C m+1 (X, F ) ⊂ C m (X, F ) ⊂ · · · ⊂ C(X, F ) .
In addition, for k ∈ N the map
∂ k : C m+k (X, F ) → C m (X, F )
is defined and linear.
5.2 Theorem Suppose f ∈ C 2 (X, F ). Then
∂ 2 f (x) ∈ L2 (E, F ) for x ∈ X ,
that is,1
∂ 2 f (x)[h, k] = ∂ 2 f (x)[k, h] for x ∈ X and h, k ∈ E .
Proof (i) Suppose x ∈ X and r > 0 such that B(x, 2r) ⊂ X and h, k ∈ rB. We
set g(y) := f (y + h) − f (y) and
∂ 2 f (x + sh + tk) − ∂ 2 f (x) h ds k dt .
r(h, k) :=
From the mean value theorem (Theorem 3.10) and from the linearity of the derivative, we get
f (x + h + k) − f (x + k) − f (x + h) + f (x) = g(x + k) − g(x)
∂f (x + h + tk) − ∂f (x + tk) k dt
∂g(x + tk)k dt =
∂ 2 f (x + sh + tk)h ds k dt = ∂ 2 f (x)[h, k] + r(h, k) ,
because x + sh + tk belongs to B(x, 2r) for s, t ∈ [0, 1] .
(ii) Setting g(y) := f (y + k) − f (y) and
∂ 2 f (x + sk + th) − ∂ 2 f (x) k ds h dt ,
r(k, h) :=
clarity, we have enclosed the arguments of multilinear maps in square brackets.
182
it follows analogously that
f (x + h + k) − f (x + h) − f (x + k) + f (x) = ∂ 2 f (x)[k, h] + r(k, h) .
∂ 2 f (x)[h, k] − ∂ 2 f (x)[k, h] = r(k, h) − r(h, k) .
Noting also the estimate
r(k, h) ∨ r(h, k) ≤
0≤s,t≤1
∂ 2 f (x + sh + tk) − ∂ 2 f (x)
we find
∂ 2 f (x)[h, k] − ∂ 2 f (x)[k, h]
≤ 2 sup
In this inequality, we can replace h and k by τ h and τ k with τ ∈ (0, 1]. Then we
∂ 2 f x + τ (sh + tk) − ∂ 2 f (x)
Finally, the continuity of ∂ 2 f implies
∂ 2 f x + τ (sh + tk) − ∂ 2 f (x) → 0 (τ → 0) .
Therefore ∂ 2 f (x)[h, k] = ∂ 2 f (x)[k, h] for h, k ∈ rB, and we are done.
5.3 Corollary For f ∈ C m (X, F ) such that m ≥ 2, we have
∂ m f (x) ∈ Lm (E, F ) for x ∈ X .
Proof Taking cues from Theorem 5.2, we now construct an induction proof in
m. The induction step m → m + 1 goes as follows. Because Lm (E, F ) is a closed
vector subspace or L(E, F ), it follows from the induction hypothesis that
∂ m+1 f (x)h1 ∈ Lm (E, F ) for h1 ∈ E ,
(see Exercise 4.2). In particular, we get
∂ m+1 f (x)[h1 , hσ(2) , hσ(3) , . . . , hσ(m+1) ] = ∂ m+1 f (x)[h1 , h2 , h3 , . . . , hm+1 ]
for every (h1 , . . . , hm+1 ) ∈ E m+1 and every permutation σ of {2, . . . , m + 1}.
Because every τ ∈ Sm+1 can be represented as a composition of transpositions, it
suﬃces to verify that
∂ m+1 f (x)[h1 , h2 , h3 , . . . , hm+1 ] = ∂ m+1 f (x)[h2 , h1 , h3 , . . . , hm+1 ] .
Because ∂ m+1 f (x) = ∂ 2 (∂ m−1 f )(x), this follows from Theorem 5.2.
Higher order partial derivatives
We consider first the case E = Rn with n ≥ 2.
For q ∈ N× and indices j1 , . . . , jq ∈ {1, . . . , n}, we call
∂ q f (x)
:= ∂j1 ∂j2 · · · ∂jq f (x)
∂xj1 ∂xj2
the q-th order partial derivative2 for f : X → F at x. The map f is said to be
q-times [continuously] partially differentiable if its derivatives up to and including
order q exist [and are continuous].
5.4 Theorem Suppose X is open in Rn , f : X → F , and m ∈ N× . Then the
following statements hold:
(i) f belongs to C m (X, F ) if and only if f is m-times continuously partially
differentiable.
(ii) For f ∈ C m (X, F ), we have
for 1 ≤ q ≤ m ,
∂xj1 · · · ∂xjq
∂xjσ(1) · · · ∂xjσ(q)
for every permutation σ ∈ Sq , that is, the partial derivatives are independent
of the order of differentiation.3
First, we observe that for h1 , . . . , hq ∈ Rn such that q ≤ m, we have
∂ q f (x)[h1 , . . . , hq ] = ∂ · · · ∂ ∂f (x)h1 h2 · · · hq
In particular, it follows that
∂ q f (x)[ej1 , . . . , ejq ] = ∂jq ∂jq−1 · · · ∂j1 f (x) for x ∈ X .
∂jq ∂jq−1 · · · ∂j1 f (x)hj1 · · · hjq
∂ q f (x)[h1 , . . . , hq ] =
j1 ,...,jq =1
for x ∈ X and hi = (h1 , . . . , hn ) ∈ Rn with 1 ≤ i ≤ q.
2 In an obvious notation, we simplify writing multiple successive derivatives of the same variable as, for example,
∂3f
∂x1 ∂x4 ∂x4
∂x1 (∂x4 )2
∂4f
∂x2 ∂x3 ∂x3 ∂x2
∂x2 (∂x3 )2 ∂x2
etc. Note though, that one should generally maintain the differentiation order between different
variables, because generally
∂2f
∂x1 ∂x2
∂x2 ∂x1
(see Remark 5.6).
3 One can show that the partial derivatives are already independent of that order when f : X ⊂
Rn → F is m-times totally differentiable (see Exercise 17). In general, the sequence of partial
derivatives can not be exchanged when f is only m-times partially differentiable (see Remark 5.6).
184
(i) Formula (5.1) immediately implies that every element of C m (X, F ) is
m-times continuously partially differentiable.
Conversely, suppose that f is m-times continuously partially differentiable.
In the case m = 1, we know from Theorem 2.10 that f belongs to C 1 (X, F ). We
assume that the statement is correct for m−1 ≥ 1. From this assumption and (5.2),
we see that ∂j (∂ m−1 f ) exists and is continuous for j ∈ {1, . . . , n}. Theorem 2.10
then says
∂(∂ m−1 f ) : X → L Rn , Lm−1 (Rn , F ) = Lm (Rn , F )
exists and is continuous. Therefore f ∈ C m (X, F ).
(ii) This follows immediately from Corollary 5.3 and (5.1).
5.5 Corollary Suppose f : X ⊂ Rn → K. Then we have these:
(i) f belongs to C 2 (X, K) if and only if
f, ∂j f, ∂j ∂k f ∈ C(X, K) for 1 ≤ j, k ≤ n .
(ii) (Schwarz theorem) If f belongs to C 2 (X, K), then
∂j ∂k f (x) = ∂k ∂j f (x)
5.6 Remark
entiable.
The Schwarz theorem is false when f is only twice partially differ-
⎪ xy(x2 − y 2 )
x2 + y 2
1 ≤ j, k ≤ n .
⎪ y(x4 + 4x2 y 2 − y 4 )
∂1 f (x, y) =
∂2 ∂1 f (0, 0) = lim
∂1 f (0, h) − ∂1 f (0, 0)
= −1 .
Using f (y, x) = −f (x, y), we find
∂2 f (y, x) = lim
f (y, x + h) − f (y, x)
f (x + h, y) − f (x, y)
= − lim
= −∂1 f (x, y) .
Therefore ∂1 ∂2 f (y, x) = −∂2 ∂1 f (x, y) for (x, y) ∈ R2 . Then ∂2 ∂1 f (0, 0) = 0 gives
∂1 ∂2 f (0, 0) = −∂2 ∂1 f (0, 0).
The chain rule is also important for higher derivatives, though there is generally
no simple representation for higher derivatives of composed functions.
5.7 Theorem (chain rule) Suppose Y is open in F and G is a Banach space. Also
suppose m ∈ N× and f ∈ C m (X, F ) with f (X) ⊂ Y and g ∈ C m (Y, G). Then we
have g ◦ f ∈ C m (X, G).
Proof Using Theorem 3.3, the statement follows from an induction in m. We
leave the details to you as an exercise.
Taylor’s formula
We now extend Taylor’s theorem to functions of multiple variables. For that, we
use the following notation:
1≤k≤q ,
⎪ ∂ k f (x)[ h, . . . , h ] ,
∂ f (x)[h] :=
k-times
⎩ f (x) ,
k=0,
for x ∈ X, h ∈ E, and f ∈ C q (X, F ).
5.8 Theorem (Taylor’s formula) Suppose X is open in E, q ∈ N× , and f belongs
to C q (X, F ). Then
f (x + h) =
∂ f (x)[h]k + Rq (f, x; h)
for x ∈ X and h ∈ E such that [[x, x + h]] ⊂ X. Here
Rq (f, x; h) :=
(1 − t)q−1 q
∂ f (x + th) − ∂ q f (x) [h]q dt ∈ F
(q − 1)!
is the q-th order remainder of f at the point x.
Proof When q = 1, the formula reduces to the mean value theorem in integral
form (Theorem 3.10):
∂f (x + th)h dt
f (x + h) = f (x) +
∂f (x + th) − ∂f (x) h dt .
= f (x) + ∂f (x)h +
186
Suppose now q = 2. Letting u(t) := ∂f (x+th)h and v(t) := t−1 for t ∈ [0, 1],
we have u (t) = ∂ 2 f (x + th)[h]2 and v = 1. Therefore, by the generalized product
rule (Example 4.8(b)) and by integrating the above formula, we get4
(1 − t)∂ 2 f (x + th)[h]2 dt
f (x + h) = f (x) + ∂f (x)h +
∂ f (x) [h]k + R2 (f, x; h) .
For q = 3, we set u(t) := ∂ 2 f (x + th)[h]2 and v(t) := −(1 − t)2 /2. We then
get in similar fashion that
f (x + h) = f (x) + ∂f (x)h + ∂ 2 f (x)[h]2 +
(1 − t)2 3
∂ f (x + th)[h]3 dt .
In the general case q > 3, the claim follows using simple induction arguments.
5.9 Remark For f ∈ C q (X, F ) and x, h ∈ E such that [[x, x + h]] ⊂ X, we have
Rq (f, x; h) ≤
max ∂ q f (x + th) − ∂ q f (x)
q! 0≤t≤1
In particular, we get
Rq (f, x; h) = o( h q ) (h → 0) .
This follows from Proposition VI.4.3 and the continuity of ∂ q f .
5.10 Corollary Suppose f ∈ C q (X, F ) with q ∈ N× and x ∈ X. Then we have 5
∂ f (x)[h]k + o( h q ) (h → 0) .
Functions of m variables
In the remainder of this section, we consider functions of m real variables, that is,
we set E = Rm . In this case, it is convenient to represent partial derivatives using
multiindices.
5 For
the proof of Proposition VI.5.4.
suﬃciently small h, [[x, x + h]] lies in X.
Suppose α = (α1 , . . . , αm ) ∈ Nm is a multiindex of length6 |α| =
and let f ∈ C |α| (X, F ). Then we write
∂ α f := ∂1 1 ∂2 2 · · · ∂mm f =
· · · (∂xm )αm
(∂x1 )α1 (∂x2 )α2
for α = 0
and ∂ 0 f := f .
5.11 Theorem (Taylor’s theorem) Suppose f ∈ C q (X, F ) with q ∈ N× and x ∈ X.
∂ α f (x)
(y − x)α + o(|x − y|q ) (x → y) .
f (y) =
We set h := y − x and write h =
hj ej . Then
∂ k f (x)[h]k = ∂ k f (x)
hj1 ej1 , . . . ,
j1 =1
hjk ejk
jk =1
∂ f (x)[ej1 , . . . , ejk ]hj1 · · · · · hjk
for 1 ≤ k ≤ q. The number of k-tuples (j1 , . . . , jk ) of numbers 1 ≤ ji ≤ m for
which all of the numbers ∈ {1, . . . , m} occur exactly α -times is equal to
(α1 )! (α2 )! · · · · · (αm )!
From (5.1) and Corollary 5.3, the claim follows from
m−1
∂ k f (x)[ej1 , ej2 , . . . , ejk ] = ∂mm ∂m−1 · · · ∂1 1 f (x) = ∂ α f (x) .
Summarizing, we get from (5.3)–(5.5) that
∂ k f (x)[h]k =
∂ f (x)hα
and the theorem follows from Corollary 5.10.
5.12 Remarks
Suppose f ∈ C q (X, R) for some q ∈ N× .
(a) If q = 1, we have
f (y) = f (x) + df (x)(y − x) + o(|y − x|) = f (x) + \nabla f (x) y − x + o(|y − x|)
as y → x.
Section I.8.
188
(b) If q = 2, we have
f (y) = f (x) + df (x)(y − x) + ∂ 2 f (x)[y − x]2 + o(|y − x|2 )
= f (x) + \nabla f (x) y − x + Hf (x)(y − x) y − x + o(|y − x|2 )
as y → x. Here,7
Hf (x) := ∂j ∂k f (x) ∈ Rm×m
denotes the Hessian matrix of f at x. In other words, Hf (x) is the representation
matrix of the linear operator induced by the bilinear form ∂ 2 f (x)[ · ·] at Rm (see
Exercise 4.1).
Suﬃcient criterion for local extrema
In the following remarks, we gather several results from linear algebra.
5.13 Remarks (a) Let H := H, (·|·) be a finite-dimensional real Hilbert space.
The bilinear form b ∈ L2 (H, R) is said to be positive [semi]definite if it is symmetric
b(x, x) > 0 b(x, x) ≥ 0 for x ∈ H \{0} .
It is negative [semi]definite if −b is positive [semi]definite. If b is neither positive
nor negative semidefinite, then b is indefinite. The form b induces the linear
operator8 B ∈ L(H) with (Bx|y) = b(x, y) for x, y ∈ H, which is said to be
positive or negative [semi]definite if b has the corresponding property.
Suppose A ∈ L(H). Then these statements are equivalent:
(i) A is positive [semi]definite.
(ii) A = A∗ and (Ax|x) > 0 [≥ 0] for x ∈ H \{0}.
(iii) A = A∗ and there is an α > 0 [α ≥ 0] such that (Ax|x) ≥ α |x|2 for x ∈ H.
This follows from Exercises 4.1 and III.4.11.
(b) Suppose H = H, (·|·) is a Hilbert space of dimension m and A ∈ L(H) is
self adjoint, that is, A = A∗ . Then all eigenvalues of A are real and semisimple.
In addition, there is an ONB of eigenvectors h1 , . . . , hm which give A the spectral
λj (·|hj )hj ,
7 We write Rm×m for the vector subspace of Rm×m that consists of all symmetric (m × m)
matrices.
8 See Exercise 4.1.
where, in λ1 , . . . , λm , the eigenvalues of A appear according to their multiplicity
and Ahj = λj hj for 1 ≤ j ≤ m. This means in particular that the matrix of A is
diagonal in this basis, that is, [A] = diag(λ1 , . . . , λm ).
Finally, A is positive definite if and only if all its eigenvalues are positive; A
is positive semidefinite if and only if none of its eigenvalues are negative.
Proof The statements that σ(A) ⊂ R and that the spectral representation exists are
standard in linear algebra (for example, [Art93, Section VII.5]).9
From (5.6), we read off
λj |(x | hj )|2
(Ax | x) =
The claimed characterization of A is now an easy consequence of the assumed positive
[semi]definiteness of the eigenvalues.
We can now derive — in a (partial) generalization of the results of Application IV.3.9 — a suﬃcient criteria for maxima and minima of real-valued functions
of multiple real variables.
5.14 Theorem Suppose X is open in Rm and x0 ∈ X is a critical point of
f ∈ C 2 (X, R). Then
(i) if ∂ 2 f (x0 ) is positive definite, then f has an isolated local minimum at x0 ;
(ii) if ∂ 2 f (x0 ) is negative definite, then f has an isolated local maximum at x0 ;
(iii) if ∂ 2 f (x0 ) is indefinite, then f does not have an local extremum at x0 .
Because x0 is a critical point of f , according to Remark 5.12(b), we have
f (x0 + ξ) = f (x0 ) + ∂ 2 f (x0 )[ξ]2 + o(|ξ|2 ) (ξ → 0) .
(i) Because ∂ 2 f (x0 ) is positive definite, there is an α > 0 such that
∂ 2 f (x0 )[ξ]2 ≥ α |ξ|2
We fix an δ > 0 such that o(|ξ|2 ) ≤ α |ξ| /4 for ξ ∈ Bm (0, δ) and then find the
estimate
α 2 α 2
α 2
f (x0 + ξ) ≥ f (x0 ) + |ξ| − |ξ| = f (x0 ) + |ξ|
Therefore f has a local minimum at x0 .
(ii) The statement follows by applying (i) to −f .
(iii) Because ∂ 2 f (x0 ) is indefinite, there are ξ1 , ξ2 ∈ Rm \{0} such that
α := ∂ 2 f (x0 )[ξ1 ]2 > 0 and β := ∂ 2 f (x0 )[ξ2 ]2 < 0 .
9 See
also Example 10.17(b).
190
Also, we find tj > 0 such that [[x0 , x0 + tj ξj ]] ⊂ X and
α o(t2 |ξ1 | )
|ξ1 |2 > 0
t2 |ξ1 |2
o(t2 |ξ2 | )
|ξ2 |2 < 0
t2 |ξ2 |2
for 0 < t < t1 and 0 < t < t2 , respectively. Consequently,
f (x0 + tξ1 ) = f (x0 ) + t2
> f (x0 )
2 |ξ1 |
t2 |ξ1 |
f (x0 + tξ2 ) = f (x0 ) + t2
|ξ2 |2 < f (x0 ) for 0 < t < t2 .
2 |ξ |2
for 0 < t < t1 ,
Therefore x0 is not a local extremal point of f .
5.15 Examples
We consider
(x, y) → c + δx2 + εy 2
with c ∈ R and δ, ε ∈ {−1, 1}. Then
\nabla f (x, y) = 2(δx, εy) and Hf (x, y) = 2
δ 0
0 ε
Thus (0, 0) is the only critical point of f .
(a) Taking f (x, y) = c + x2 + y 2 so that δ = ε = 1, we find Hf (0, 0) is positive
definite. Thus f has an isolated (absolute) minimum at (0, 0).
(b) Taking f (x, y) = c − x2 − y 2 , we find Hf (0, 0) is negative definite, and f has
an isolated (absolute) maximum at (0, 0).
(c) Taking f (x, y) = c + x2 − y 2 makes Hf (0, 0) indefinite. Therefore f does not
have an extremum at (0, 0); rather (0, 0) is a “saddle point”.
Minimum
Maximum
Saddle
(d) When ∂ 2 f is semidefinite at a critical point of f , we cannot learn anything
more from studying the second derivative. For consider the maps fj : R2 → R,
j = 1, 2, 3, with
f1 (x, y) := x2 + y 4 ,
f2 (x, y) := x2 ,
f3 (x, y) := x2 + y 3 .
In each case (0, 0) is a critical point of fj , and the Hessian matrix Hfj (0, 0) is
positive semidefinite.
However, we can easily see that (0, 0) is an isolated minimum for f1 , not an isolated
minimum for f2 , and not a local extremal point for f3 .
If A ∈ L(E, F ), show A ∈ C ∞ (E, F ) and ∂ 2 A = 0.
If ϕ ∈ L(E1 , . . . , Em ; F ), show ϕ ∈ C ∞ (E1 × · · · × Em , F ) and ∂ m+1 ϕ = 0.
Suppose X is open in Rm . The (m-dimensional) Laplacian Δ is defined by
Δ : C 2 (X, R) → C(X, R) ,
We say the function u ∈ C 2 (X, R) is harmonic in X if Δu = 0. The harmonic functions
thus comprise the kernel of the linear map Δ. Verify that gm : Rm \{0} → R defined by
gm (x) :=
|x|2−m ,
m=2,
m>2,
is harmonic in Rm \{0}.
For f, g ∈ C 2 (X, R), show
Δ(f g) = gΔf + 2(\nabla f | \nabla g) + f Δg .
Suppose g : [0, ∞) × R → R2 , (r, ϕ) → (r cos ϕ, r sin ϕ). Verify these:
(a) g | (0, ∞) × (−π, π) → R2 \H for H :=
is topological.
(x, y) ∈ R2 ; x ≤ 0, y = 0
(b) im(g) = R2 .
(c) If X is open in R2 \H and f ∈ C 2 (X, R), then
(Δf ) ◦ g =
1 ∂ 2 (f ◦ g)
1 ∂(f ◦ g)
∂ 2 (f ◦ g)
∂r 2
∂ϕ2
on g −1 (X).
6 Let X := Rn × (0, ∞) and p(x, y) := y (|x|2 + y 2 ) for (x, y) ∈ X.
Calculate Δp.
Verify
(Δu) ◦ A = Δ(u ◦ A)
for u ∈ C 2 (Rn , R)
if A ∈ L(R ) is orthogonal, that is, if A A = 1.
= (−R+ ) × {0}
192
Suppose X is open in Rm and E is a Banach space. For k ∈ N, let
BC k (X, E) :=
u ∈ BC(X, E) ; ∂ α u ∈ BC(X, E), |α| ≤ k ,
:= max ∂ α u
(a) BC k (X, E) is a Banach space;
(b) BUC k (X, E) := u ∈ BC(X, E) ; ∂ α u ∈ BUC(X, E), |α| ≤ k
subspace of BC k (X, E) and is therefore itself a Banach space.
is a closed vector
9 Let q ∈ N and aα ∈ K for α ∈ Nm with |α| ≤ q. Also suppose aα = 0 for some
α ∈ Nm with |α| = q. Then we call
A(∂) : C q (X, K) → C(X, K) ,
aα ∂ α u
u → A(∂)u :=
a linear differential operator of order q with constant coeﬃcients.
Show for k ∈ N that
(a) A(∂) ∈ L BC k+q (X, K), BC k (X, K) ;
(b) A(∂) ∈ L BUC k+q (X, K), BUC k (X, K) .
For u ∈ C 2 (R × Rm , R) and (t, x) ∈ R × Rm we set
2u := ∂t u − Δx u
(∂t − Δ)u := ∂t u − Δx u ,
where Δx is the Laplacian in x ∈ Rm . We call 2 the wave operator (or d’Alembertian).
We call ∂t − Δ the heat operator.
(a) Calculate (∂t − Δ)k in (0, ∞) × Rm when
k(t, x) := t−m/2 exp −|x|2 /4t
for (t, x) ∈ (0, ∞) × Rm .
(b) Suppose g ∈ C 2 (R, R) c > 0, and v ∈ S m−1 . Calculate 2w when
w(t, x) := g(v · x − tc)
for (t, x) ∈ R × Rm .
11 Suppose X is an open convex subset of a Banach space E and f ∈ C 2 (X, R). Show
these statements are equivalent:
(i) f is convex;
(ii) f (x) ≥ f (a) + ∂f (a)(x − a), for a, x ∈ X;
(iii) ∂ 2 f (a)[h]2 ≥ 0, for a ∈ X and h ∈ E.
If E = Rm , these statements are also equivalent to
(iv) Hf (a) is positive semidefinite for a ∈ X.
For all α ∈ R, classify the critical points of the function
fα : R2 → R ,
(x, y) → x3 − y 3 + 3αxy ,
as maxima, minima, or saddle points.
Suppose f : R2 → R, (x, y) → (y − x2 )(y − 2x2 ). Prove these:
(a) (0, 0) is not a local minimum for f .
(b) For every (x0 , y0 ) ∈ R2
minimum at 0.
(0, 0) , R → R t → f (tx0 , ty0 ) has an isolated local
Determine, up to (including) second order, the Taylor expansion of
(0, ∞) × (0, ∞) → R ,
(x, y) → (x − y)/(x + y)
at the point (1, 1).
Suppose X is open in Rm . For f, g ∈ C 1 (X, Rm ), define [f, g] ∈ C(X, Rm ) by
[f, g](x) := ∂f (x)g(x) − ∂g(x)f (x)
We call [f, g] the Lie bracket of f and g.
Verify these statements:
(i) [f, g] = −[g, f ];
(ii) [αf + βg, h] = α[f, h] + β[g, h] for α, β ∈ R and h ∈ C 1 (X, Rm );
(iii) [ϕf, ψg] = ϕψ[f, g] + (\nabla ϕ | f )ψf − (\nabla ψ | g)ϕg for ϕ, ψ ∈ C 1 (X, R);
(iv) (Jacobi identity)
If f, g, h ∈ C 2 (X, Rm ), then [f, g], h + [g, h], f + [h, f ], g = 0.
exp 1 (|x|2 − 1) ,
|x| < 1 ,
|x| ≥ 1 ,
is smooth.
Suppose X is open in Rm and f : X → F is m-times differentiable. Then show
∂ q f (x) ∈ Lq (X, F )
for x ∈ X and 1 ≤ q ≤ m .
In particular, show for every permutation σ ∈ Sq that
∂xj1 · · · · · ∂xjq
∂xjσ(1) · · · · · ∂xjσ(q)
(Hint: It suﬃces to consider the case q = m = 2. Apply the mean value theorem on
[0, 1] → F ,
t → f (x + tsh1 + tsh2 ) − f (x + tsh1 ) ,
and verify that
s→0
f (x + sh1 + sh2 ) − f (x + sh1 ) − f (x + sh2 ) + f (x)
= ∂ 2 f (x)[h1 , h2 ] .)
194
18 Suppose H is a finite-dimensional Hilbert space, and A ∈ L(H) is positive [or negative] definite. Show then A that belongs to Laut(H), and A−1 is positive [or negative]
definite.
19 Suppose H is a finite-dimensional Hilbert space and A ∈ C 1 [0, T ], L(H) . Also
suppose A(t) symmetric for every t ∈ [0, T ]. Prove these:
(a) ∂A(t) is symmetric for t ∈ [0, T ].
(b) If ∂A(t) is positive definite for every t ∈ [0, T ], then, for 0 ≤ σ < τ ≤ T , we have
(i) A(τ ) − A(σ) is positive definite;
(ii) A−1 (τ ) − A−1 (σ) is negative definite if A(t) is an automorphism for every t ∈ [0, T ].
(Hint for (ii): Differentiate t → A−1 (t)A(t).)
20 Let H be a finite-dimensional Hilbert space, and let A, B ∈ L(H). Show that if A,
B, and A − B are positive definite, then A−1 − B −1 is negative definite.
(Hint: Consider t → B + t(A − B) for t ∈ [0, 1], and study Exercise 19.)
21 Suppose X is open in Rm and f, g ∈ C q (X, K). Show that f g belongs to C q (X, K)
and prove the Leibniz rule:
∂ α (f g) =
for α ∈ Nm and |α| ≤ q .
Here β ≤ α means that βj ≤ αj for 1 ≤ j ≤ m, and
for α, β ∈ Nm with α = (α1 , . . . , αm ) and β = (β1 , . . . , βm ). (Hint: Theorem IV.1.12(ii)
and induction.)
VII.6 Nemytskii operators and the calculus of variations
6 Nemytskii operators and the calculus of variations
Although we developed the differential calculus for general Banach spaces, we
have so far almost exclusively demonstrated it with finite-dimensional examples.
In this section, we remedy that by giving you a glimpse of the scope of the general
theory. To that end, we consider first the simplest nonlinear map between function
spaces, namely, the “covering operator”, which connects the two function spaces
using a fixed nonlinear map. We restrict to covering operators in Banach spaces
of continuous functions, and seek their continuity and differentiability properties.
As an application, we will study the basic problem of the calculus of variations, namely, the problem of characterizing and finding the extrema of realvalued functions of “infinitely many variables”. In particular, we derive the Euler–
Lagrange equation, whose satisfaction is necessary for a variational problem to
have a local extremum.
Nemytskii operators
Suppose T , X, and Y are nonempty sets and ϕ is a map from T × X to Y . Then
we define the induced Nemytskii or covering operator ϕ through
u → ϕ ·, u(·) .
This means ϕ is the map that associates to every function u : T → X the function
ϕ (u) : T → Y , t → ϕ t, u(t) .
In the following suppose
• T is a compact metric space;
E and F are Banach spaces;
X is open in E.
When we do not expect any confusion, we denote both norms on E and F by |·|.
6.1 Lemma C(T, X) is open in the Banach space C(T, E).
Proof From Theorem V.2.6, we know that C(T, E) is a Banach space.
It suﬃces to consider the case X = E. Hence let u ∈ C(T, X). Then, because
u(T ) is the continuous image of a compact metric space, it is a compact subset of
X (see Theorem III.3.6). Then according to Example III.3.9(c), there is an r > 0
such that d u(T ), X c ≥ 2r. Then, for v ∈ u + rBC(T,E) , that |u(t) − v(t)| < r
for t ∈ T implies that v(T ) lies in the r-neighborhood of u(T ) in E and, therefore,
in X. Thus v belongs to C(T, X).
The continuity of Nemytskii operators
We first show that continuous maps induce continuous Nemytskii operators in
spaces of continuous functions.
196
6.2 Theorem Suppose ϕ ∈ C(T × X, F ). Then
(i) ϕ ∈ C C(T, X), C(T, F ) ;
(ii) if ϕ is bounded on bounded sets, this is also true of ϕ .
Proof For u ∈ C(T, X), suppose u(t) := t, u(t) for t ∈ T . Then u clearly
belongs to C(T, T × X) (see Proposition III.1.10). Thus Theorem III.1.8 implies
that ϕ (u) = ϕ ◦ u belongs to C(T, F ).
(i) Suppose (uj ) is a sequence in C(T, X) with uj → u0 in C(T, X). Then
the set M := { uj ; j ∈ N× } ∪ {u0 } is compact in C(T, X), according to Example III.3.1(a).
We set M (T ) := { m(T ) ; m ∈ M } and want to show M (T ) is compact in
X. To see this, let (xj ) be a sequence in M (T ). Then there are tj ∈ T and mj ∈ M
such that xj = mj (tj ). Because T and M are sequentially compact and because
of Exercise III.3.1, we find (t, m) ∈ T × M and a subsequence (tjk , mjk ) k∈N that
converges in T × M to (t, m). From this, we get
|xjk − m(t)| ≤ |mjk (tjk ) − m(tjk )| + |m(tjk ) − m(t)|
≤ mjk − m ∞ + |m(tjk ) − m(t)| → 0
as k → ∞ because m is continuous. Thus (xjk ) converges to the element m(t) in
M (T ), which proves the sequential compactness, and therefore the compactness,
of M (T ).
Thus, according to Exercise III.3.1, T × M (T ) is compact. Now Theorem III.3.13 shows that the restriction of ϕ to T × M (T ) is uniformly continuous.
From this, we get
ϕ (uj ) − ϕ (u0 )
= max ϕ t, uj (t) − ϕ t, u0 (t)
→0
as j → ∞ because uj − u0 ∞ → 0. We also find that t, uj (t) and t, u0 (t)
belong to T × M (T ) for t ∈ T and j ∈ N. Therefore ϕ is sequentially continuous
and hence, because of Theorem III.1.4, continuous.
(ii) Suppose B ⊂ C(T, X) and there is an R > 0 such that u ∞ ≤ R for
u ∈ B. Then
u(T ) ; u ∈ B ⊂ X ,
B(T ) :=
and |x| ≤ R for x ∈ B(T ). Therefore B(T ) is bounded in X. Thus T × B(T ) is
bounded in T × X. Because ϕ is bounded on bounded sets, there is an r > 0 such
that |ϕ(t, x)| ≤ r for (t, x) ∈ T × B(T ). This implies ϕ (u) ∞ ≤ r for u ∈ B.
Therefore ϕ is bounded on bounded sets because this is the case for ϕ.
6.3 Remark When E is finite-dimensional, every ϕ ∈ C(T × X, F ) is bounded on
sets of the form T × B, where B is bounded in E and where B, the closure of B
in E, lies in X.
This follows from Theorem III.3.5, Corollary III.3.7 and Theorem 1.4.
The differentiability of Nemytskii operators
Suppose p ∈ N ∪ {∞}. Then we write
ϕ ∈ C 0,p (T × X, F )
if, for every t ∈ T , the function ϕ(t, ·) : X → F is p-times differentiable and if its
derivatives, which we denote by ∂2 ϕ, satisfy
∂2 ϕ ∈ C T × X, Lq (E, F )
for q ∈ N and q ≤ p .
Consequently, C 0,0 (T × X, F ) = C(T × X, F ).
6.4 Theorem For ϕ ∈ C 0,p (T × X, F ), ϕ belongs to C p C(T, X), C(T, F ) , and 1
∂ϕ (u)h (t) = ∂2 ϕ t, u(t) h(t) for t ∈ T ,
u ∈ C(T, X), and h ∈ C(T, E).
Because of Theorem 6.2, we may assume p ≥ 1. First we make
G : C(T, X) × C(T, E) → F T ,
(u, h) → ∂2 ϕ ·, u(·) h(·)
the Nemytskii operator induced by the map2
t, (x, ξ) → ∂2 ϕ(t, x)ξ ∈ C T × (X × E), F .
Then Theorem 6.2 implies that G(u, h) belongs to C(T, F ). Obviously
G(u, h)
C(T,F )
≤ max ∂2 ϕ t, u(t)
C(T,E)
and G(u, ·) is linear. Consequently, we see that
A(u) := G(u, ·) ∈ L C(T, E), C(T, F )
for u ∈ C(T, X) .
The function ∂2 ϕ ∈ C T × X, L(E, F ) induces a Nemytskii operator A.
Then, from Theorems 6.2 and 1.1,
A ∈ C C(T, X), C T, L(E, F )
Suppose now u ∈ C(T, X). We choose ε > 0 such that u(T ) + εBE ⊂ X.
Then u + h belongs to C(T, X) for every h ∈ C(T, E) such that h ∞ < ε, and
and in like situations, all statements about derivatives apply to the natural case p ≥ 1.
× C(T, E) and C T, X × E) are identified in the obvious way.
2 C(T, X)
198
the mean value theorem in integral form (Theorem 3.10) implies
ϕ (u + h)(t) − ϕ (u)(t) − A(u)h (t)
= ϕ t, u(t) + h(t) − ϕ t, u(t) − ∂2 ϕ t, u(t) h(t)
∂2 ϕ t, u(t) + τ h(t) − ∂2 ϕ t, u(t) h(t) dτ
A(u + τ h) − A(u)
C(T,L(E,F ))
Consequently, it follows from (6.2) that
ϕ (u + h) − ϕ (u) − A(u)h = o( h
C(T,E) )
(h → 0) .
Thus, ∂ϕ (u) exists and equals A(u). Analogously to (6.1), we consider
∂ϕ (u) − ∂ϕ (v)
L(C(T,E),C(T,F ))
≤ A(u) − A(v)
which, because of (6.2), implies
∂ϕ ∈ C C(T, X), L C(T, E), C(T, F )
This proves the theorem for p = 1.
The general case now follows simply by induction, which we leave to you.
6.5 Corollary If ∂2 ϕ is bounded on bounded sets, so is ∂ϕ .
This follows from (6.1) and Theorem 6.2(ii).
6.6 Examples (a) Let ϕ(ξ) := sin ξ for ξ ∈ R and T := [0, 1]. Then the Nemytskii
operator
ϕ : C(T ) → C(T ) , u → sin u(·)
induced by ϕ is a C ∞ map,
∂ϕ (u)h (t) = cos u(t) h(t)
and u, h ∈ C(T ). This means that in this case the linear map ∂ϕ (u) ∈ L C(T )
is the function cos u(·) understood as a multiplication operator, that is,
C(T ) → C(T ) ,
h → cos u(·) h .
(b) Suppose −∞ < α < β < ∞ and ϕ ∈ C 0,p [α, β] × X, F . Also let
ϕ t, u(t) dt
f (u) :=
Then f ∈ C p C [α, β], X , F and
∂f (u)h =
∂2 ϕ t, u(t) h(t) dt
h ∈ C [α, β], E .
Proof With T := [α, β], Theorem 6.4 implies that ϕ belongs to C p C(T, X), C(T, F )
and that ∂ϕ (u) is the multiplication operator
C(T, E) → C(T, F ) ,
h → ∂2 ϕ ·, u(·) h .
From Section VI.3, we know that α belongs to L C(T, F ), F . Therefore, it follows
from chain rule (Theorem 3.3) and Example 2.3(a) that
◦ ϕ ∈ C p C(T, X), F
◦ ∂ϕ (u)
∂f (u) =
(c) Suppose −∞ < α < β < ∞ and ϕ ∈ C 0,p [α, β] × (X × E), F . Then
C 1 [α, β], X is open in C 1 [α, β], E , and, for the map defined through
Φ(u)(t) := ϕ t, u(t), u(t)
for u ∈ C 1 [α, β], X and α ≤ t ≤ β ,
Φ ∈ C p C 1 [α, β], X , C [α, β], F
∂Φ(u)h (t) = ∂2 ϕ t, u(t), u(t) h(t) + ∂3 ϕ t, u(t), u(t) h(t)
for t ∈ [α, β], u ∈ C 1 [α, β], X , and h ∈ C 1 [α, β], E .
The canonical inclusion
i : C 1 [α, β], E → C [α, β], E ,
is linear and continuous. This is because the maximum norm that C [α, β], E induces
on C 1 [α, β], E is weaker than the C 1 norm. Therefore, because of Lemma 6.1 and
Theorem III.2.20,
C 1 [α, β], X = i−1 C [α, β], X
is open in C 1 [α, β], E .
From (6.4), it follows that
Ψ := u → (u, u) ∈ L C 1 (T, X), C(T, X) × C(T, E)
for T := [α, β]. Because Φ = ϕ ◦ Ψ, we conclude using Theorem 6.4 and the chain rule.
200
(d) Suppose the assumptions of (c) are satisfied and
ϕ t, u(t), u(t) dt
for u ∈ C 1 [α, β], X .
f ∈ C p C 1 [α, β], X , F
∂2 ϕ t, u(t), u(t) h(t) + ∂3 ϕ t, u(t), u(t) h(t) dt
for u ∈ C 1 [α, β], X and h ∈ C 1 [α, β], E .
Because f =
◦ Φ, this follows from (c) and the chain rule.
As a simple consequence of the examples above, we will study the parameterdependent integral
ϕ(t, x) dt .
In Volume III, we will prove, in the framework of Lebesgue integration theory, a
general version of the following theorem about the continuity and the differentiability of parameter-dependent integrals.
6.7 Proposition Suppose −∞ < α < β < ∞ and p ∈ N ∪ {∞}. Also assume ϕ
belongs to C 0,p [α, β] × X, F , and let
Φ(x) :=
ϕ(t, x) dt
Then we have Φ ∈ C p (X, F ) and
∂2 ϕ(t, ·) dt .
Proof 3 For x ∈ X, let ux be the constant map [α, β] → E, t → x. Then, for the
function ψ defined through x → ux , we clearly have
recommend also that you devise a direct proof that does not make use of Examples 6.6.
where ∂ψ(x)ξ = uξ for ξ ∈ E. With (6.3), we get Φ(x) = f (ux ) = f ◦ ψ(x). Therefore, from Example 6.6(b) and the chain rule, it follows that Φ ∈ C p (X, F ) and4
∂Φ(x)ξ = ∂f ψ(x) ∂ψ(x)ξ = ∂f (ux )uξ =
∂2 ϕ(t, x)ξ dt
for x ∈ X and ξ ∈ E. Because ∂2 ϕ(·, x) ∈ C [α, β], L(E, F ) for x ∈ X, we have
∂2 ϕ(t, x) dt ∈ L(E, F ) ,
∂2 ϕ(t, x) dt ξ =
for ξ ∈ E
(see Exercise VI.3.3). From this, we get
for x ∈ X and ξ ∈ E
∂ϕ(t, x) dt ξ
∂Φ(x)ξ =
and therefore (6.5).
6.8 Corollary Suppose the assumptions of Proposition 6.7 are satisfied with p ≤ 1
and E := Rm and also
b(x)
Ψ(x) :=
a(x)
and a, b ∈ C X, (α, β) . Then Ψ belongs to C p (X, F ), and for x ∈ X we have
∂Ψ(x) =
∂2 ϕ(t, x) dt + ϕ b(x), x ∂b(x) − ϕ a(x), x ∂a(x) .
Φ(x, y, z) :=
for (x, y, z) ∈ X × (α, β) × (α, β) .
Proposition 6.7 secures a Φ(·, y, z) ∈ C p (X, F ) such that
∂1 Φ(x, y, z) :=
∂2 ϕ(t, x) dt
for every choice of (x, y, z) ∈ X × (α, β) × (α, β), if p = 1.
4 Here and in the following statements we understand that statements about derivatives apply
to the case p ≥ 1.
202
Suppose (x, y) ∈ X × (α, β). Then it follows from Theorem VI.4.12 that
Φ(x, y, ·) belongs to C p (α, β), F and
∂3 Φ(x, y, z) = ϕ(z, x)
for z ∈ (α, β) .
Analogously, by noting that (x, z) ∈ X × (α, β) because of (VI.4.3), we find
Φ(x, ·, z) belongs to C p (α, β), F and
∂2 Φ(x, y, z) = −ϕ(y, x)
for y ∈ (α, β) .
From this, we easily derive
∂k Φ ∈ C X × (α, β) × (α, β), F
for 1 ≤ k ≤ 3
and get Φ ∈ C p X × (α, β) × (α, β), F from Theorem 2.10. The chain rule now
Ψ = Φ ·, a(·), b(·) ∈ C p (X, F )
∂Ψ(x) = ∂1 Φ x, a(x), b(x) + ∂2 Φ x, a(x), b(x) ∂a(x)
+ ∂3 Φ x, a(x), b(x) ∂b(x)
∂2 ϕ(t, x) dt + ϕ b(x), x ∂b(x) − ϕ a(x), x ∂a(x)
From the formula for ∂Ψ, an easy induction shows Ψ ∈ C p (X, F ) for p ≥ 2
if ϕ belongs to C p−1,p [0, 1] × X, F , where C p−1,p is defined in the obvious way.
Variational problems
In the remaining part of this section, we assume that5
X is open in Rm ;
a, b ∈ X;
L ∈ C 0,1 [α, β] × (X × Rm ), R .
Then the integral
L t, u(t), u(t) dt
exists for every u ∈ C 1 [α, β], X . The definition (6.6) gives rise to a variational
problem with free boundary conditions when we seek to minimize the integral over
5 In most applications L is only defined on a set of the form [α, β] × X × Y , where Y is open
in Rm (see Exercise 10). We leave you the simple modifications needed to include this case.
all C 1 functions u in X. We write that problem as
L(t, u, u) dt = Min
Thus, problem (6.7) becomes one of minimizing f over the set U := C 1 [α, β], X .
The definition (6.6) can also be a part of a variational problem with fixed
boundary conditions, when we minimize the function f over a constrained set
Ua,b :=
u ∈ C 1 [α, β], X ; u(α) = a, u(β) = b
We write this problem as
for u ∈ C 1 [α, β], X ,
u(α) = a ,
u(β) = b . (6.8)
Every solution of (6.7) and (6.8) is said to be extremal6 for the variational problems (6.7) and (6.8), respectively.
To treat this problem eﬃciently, we need this:
6.9 Lemma Let E := C 1 [α, β], Rm and
u ∈ E ; u(α) = u(β) = 0
E0 :=
=: C0 [α, β], Rm .
Then E0 is a closed vector subspace of E and is therefore itself a Banach space.
For u ∈ Ua,b , Ua,b − u is open in E0 .
Proof It is clear that E0 is a closed vector subspace of E. Because, according to
Example 6.6(c), U is open in E and because Ua,b = U ∩ (u + E0 ), we know Ua,b is
open in the aﬃne space u + E0 . Now it is clear that Ua,b − u is open in E0 .
6.10 Lemma
(i) f ∈ C 1 (U, R) and
∂2 L t, u(t), u(t) h(t) + ∂3 L t, u(t), u(t) h(t) dt
for u ∈ U and h ∈ E.
(ii) Suppose u ∈ Ua,b , and let g(v) := f (u + v) for v ∈ V := Ua,b − u. Then g
belongs to C 1 (V, R) with
∂g(v)h = ∂f (u + v)h
6 We
for v ∈ V and h ∈ E0 .
(i) is a consequence of Example 6.6(d), and (ii) follows from (i).
will also use the term extremal in the calculus of variations to describe solutions to the
Euler–Lagrange equation.
204
6.11 Lemma
For u ∈ C [α, β], Rm , suppose
u(t) v(t) dt = 0 for v ∈ C0 [α, β], Rm .
Then u = 0.
Proof Suppose u = 0. Then there exists a k ∈ {1, . . . , m} and a t0 ∈ (α, β) such
that uk (t0 ) = 0. It suﬃces to consider the case uk (t0 ) > 0. From the continuity
of uk , there are α < α < β < β such that uk (t) > 0 for t ∈ (α , β ) (see
Example III.1.3(d)).
Choosing v ∈ C0 [α, β], Rm such that vj = 0 for j = k, it follows from (6.10)
for w ∈ C0 [α, β], R .
uk (t)w(t) dt = 0
Now suppose w ∈ C0 [α, β], R such that w(t) > 0 for t ∈ (α , β ) and w(t) = 0
for t outside of the interval (α , β ) (see Exercise 7). Then we have
uk (t)w(t) dt =
uk (t)w(t) dt > 0 ,
which contradicts (6.11).
The Euler–Lagrange equation
We can now prove the fundamental result of the calculus of variations:
6.12 Theorem Suppose u is extremal for the variational problem (6.7) with free
boundary conditions or (alternatively) (6.8) with fixed boundary conditions. Also
suppose
t → ∂3 L t, u(t), u(t) ∈ C 1 [α, β], R .
Then u satisfies the Euler–Lagrange equation
∂3 L(·, u, u)
= ∂2 L(·, u, u) .
In the problem with free boundary conditions, u also satisfies the natural boundary
conditions
∂3 L α, u(α), u(α) = ∂3 L β, u(β), u(β) = 0 .
For fixed boundary conditions, u satisfies
u(α) = a and u(β) = b .
Proof (i) We consider first the problem (6.7). By assumption, the function f defined in (6.6) assumes its minimum at u ∈ U . Thus it follows from Lemma 6.10(i),
Theorem 3.13, and Proposition 2.5 that
∂2 L t, u(t), u(t) h(t) + ∂3 L t, u(t), u(t) h(t) dt = 0
(6.16)
for h ∈ E = C 1 [α, β], Rm . Using the assumption (6.12), we can integrate by
parts to get
∂3 L(·, u, u)h dt = ∂3 L(·, u, u)h
∂3 L(·, u, u) h dt .
We then get from (6.16) that
∂2 L(·, u, u) − ∂3 L(·, u, u)
h dt + ∂3 L(·, u, u)h
(6.17)
for h ∈ E. In particular, we have
h dt = 0 for h ∈ E0 ,
(6.18)
∂3 L(·, u, u)h
for h ∈ E0 .
(6.19)
Now the Euler–Lagrange equations (6.13) is implied by (6.18) and Lemma 6.11.
Consequently, it follows from (6.17) that
(6.20)
For ξ, η ∈ R , we set
hξ,η (t) :=
Then hξ,η belongs to E, and from (6.20) we get for h = hξ,η that
∂3 L β, u(β), u(β) η − ∂3 L α, u(α), u(α) ξ = 0 .
(6.21)
Because (6.21) holds for every choice of ξ, η ∈ Rm , the natural boundary conditions
(6.14) are satisfied.
(ii) Now we consider the problem (6.8). We fix a u ∈ Ua,b and set v := u − u.
Then it follows from Lemma 6.10(ii) that g assumes its minimum at the point v
of the open set V := Ua,b − u of the Banach space E0 . It therefore follows from
Lemma 6.10(ii), Theorem 3.13, and Proposition 2.5 that (6.16) also holds in this
case, though only for h ∈ E0 . On the other hand, because of (6.19), we get from
(6.17) the validity of (6.18) and, consequently, the Euler–Lagrange equation. The
statement (6.15) is trivial.
206
6.13 Remarks (a) In Theorem 6.12, it is assumed that the variational problem
has a solution, that is, that there is an extremal u. In (6.12) we made an additional
assumption placing an implicit regularity condition on the extremal u. Only under
this extra assumption is the Euler–Lagrange equation a necessary condition for the
presence of an extremal u.
The indirect method of the calculus of variations gives the Euler–Lagrange
equation a central role. In it, we seek to verify that there is a function u satisfying
equation (6.13) and the boundary conditions (6.14) or (6.15). Because the Euler–
Lagrange equation is a (generally nonlinear) differential equation, this leads to the
subject of boundary value problems for differential equations. If it can be proved,
using methods of boundary value problem, that there are solutions, we then select
in a second step any that are actually extremal.
In contrast is the direct method of the calculus of variations, in which one
seeks to directly show that the function f has a minimum at a point u ∈ U (or the
function g at a point v ∈ V ). Then Theorem 3.13 and Proposition 2.5 says that
∂f (u) = 0 (or ∂g(v) = 0) and therefore that the relation (6.16) is satisfied for all
h ∈ E1 (or all h ∈ E0 ). If the auxiliary “regularity problem” can be solved — that
is, if it can be proved that the extremal u satisfies the condition (6.12) — then it
also satisfies the Euler–Lagrange differential equation.
Thus Theorem 6.12 has two possible applications. In the first case, we may
use the theory of boundary value problems for differential equations to solve the
Euler–Lagrange equation, that is, to minimize the integral. This is the classical
method of calculus of variations.
In general, however, it is very diﬃcult to prove existence results for boundary
value problems. The second case thus takes another point of view: We take a
boundary value problem as given and then try to find the variational problem
that gives rise to it, so that the differential equation can be interpreted as an
Euler–Lagrange equation. If such a variational problem exists, then we try to
prove it has an extremal solution that satisfies the regularity condition (6.12).
Such a proof then implies that the original boundary value problem also has a
solution. This turns out to be a powerful method for studying boundary value
problems.
Unfortunately, the details surpass the scope of this text. For more, you
should consult the literature or take courses on variational methods, differential
equations, and (nonlinear) functional analysis.
(b) Clearly Theorem 6.12 also holds if u is only a critical point of f (or g), that
is, if ∂f (u) = 0 (or ∂g(v) = 0 with v := u − u). In this case, one says that u is a
stationary value for the integral (6.6).
(c) Suppose E is a Banach space, Z is open in E, and F : Z → R. If the directional
derivative of F exists for some h ∈ E \ {0} in z0 ∈ Z, then (in the calculus of
variations) we call Dh F (z0 ) the first variation of F in the direction h and write
it as δF (z0 ; h). If the first variation of F exists in every direction, we call
δF (z0 ) := δF (z0 ; ·) : E → R
the first variation of F at z0 .
If F has a local extremum at z0 ∈ Z at which the first variation exists, then
δF (z0 ) = 0. In other words: the vanishing of the first variation is a necessary
condition for the presence of a local extremum.
This is a reformulation of Theorem 3.13.
(d) Under our assumptions, f (or g) is continuously differentiable. Hence, the
first variation exists and agrees with ∂f (or ∂g). In many cases, however, the
Euler–Lagrange equation can already be derived from weaker assumptions which
only guarantee the existence of the first variation.
(e) In the calculus of variations, it is usual to use write ∂2 L as Lu and ∂3 L as Lu ,
respectively. Then the Euler–Lagrange equation takes the abbreviated form
(Lu ) = Lu ,
(6.22)
that is, it loses its arguments. If also we assume L and the extremal u are twice
differentiable, we can write (6.22) as Lt,u + Lu,u u + Lu,u u = Lu , where Lt,u =
∂1 ∂3 L etc.
Classical mechanics
The calculus of variations is important in physics. To demonstrate this, we consider
a system of massive point particles in m degrees of freedom, which are specified
by the (generalized) coordinates q = (q 1 , . . . , q m ). The fundamental problem in
classical mechanics is to determine the state q(t) of the system at a given time t,
given knowledge of its initial position q0 at time t0 .
We denote by q := dq/dt the (generalized) velocity coordinates, by T (t, q, q)
the kinetic energy, and by U (t, q) the potential energy. Then we make the fundamental assumption of Hamilton’s principle of least action. This says that between
each pair of times t0 and t1 , the system goes from q0 to q1 in a way that minimizes
T (t, q, q) − U (t, q) dt ,
that is, the actual path chosen by the system is the one that minimizes this integral
over all (virtual) paths starting from q0 at t0 and arriving at q1 at t1 . In this
context, L := T − U is called the Lagrangian of the system.
Hamilton’s principle means that the path q(t) ; t0 ≤ t ≤ t1 that the
system takes is an extremum of the variational problem with fixed boundary con-
208
ditions
L(t, q, q) dt = Min for q ∈ C 1 [t0 , t1 ], Rm ,
q(t0 ) = q0 ,
q(t1 ) = q1 .
It then follows from Theorem 6.12 that q satisfies the Euler–Lagrange equation
(Lq ) = Lq
if the appropriate regularity conditions are also satisfied.
In physics, engineering, and other fields that use variational methods (for
example, economics), it is usual to presume the regularity conditions are satisfied
and to postulate the validity of the Euler–Lagrange equation (and to see the existence of an extremal path as physically obvious). The Euler–Lagrange equation is
then used to extract the form of the extremal path and to subsequently understand
the behavior of the system.
6.14 Examples (a) We consider the motion of an unconstrained point particle
of (positive) mass m moving in three dimensions acted on by a time-independent
potential field U (t, q) = U (q). We assume the kinetic energy depends only on q
and is given by
T (q) = m |q| 2 .
The Euler–Lagrange differential equation takes the form
m¨ = −\nabla U (q) .
(6.23)
Because q is the acceleration of the system, (6.23) is the same as Newton’s equation
of motion for the motion of a particle acted on by a conservative7 force −\nabla U .
Proof We identify (R3 ) = L(R3 , R) with R3 using the Riesz representation theorem.
We then get from L(t, q, q) = T (q) − U (q) the relations
∂2 L(t, q, q) = −\nabla U (q) and ∂3 L(t, q, q) = \nabla T (q) = mq ,
which prove the claim.
(b) In a generalization of (a), we consider the motion of N unconstrained, massive
point particles in a potential field. We write x = (x1 , . . . , xN ) for the coordinates,
where xj ∈ R3 specifies the position of the j-th particle. Further suppose X is
open in R3N and U ∈ C 1 (X, R). Letting mj be the mass of the j-th particle, the
kinetic energy of the system is
T (x) :=
force field is called conservative if it has a potential (see Remark VIII.4.10(c)).
Then we get a system of N Euler–Lagrange equations:
−mj xj = \nabla xj U (x)
for 1 ≤ j ≤ N .
Here, \nabla xj denotes the gradient of U in the variables xj ∈ R3 .
1 Suppose I = [α, β], where −∞ < α < β < ∞, k ∈ C(I × I, R), ϕ ∈ C 0,1 (I × E, F ).
Also let
Φ(u)(t) :=
for t ∈ I
k(t, s)ϕ s, u(s) ds
and u ∈ C(I, E). Prove that Φ ∈ C
C(I, E), C(I, F ) and
∂Φ(u)h (t) =
k(t, s)∂2 ϕ s, u(s) h(s) ds
and u, h ∈ C(I, E).
2 Suppose I and J := [α, β] are compact perfect intervals and f ∈ C(I × J, E). Show
f (s, t) ds dt =
f (s, t) dt ds .
(Hint: Consider
J →E ,
f (s, t) dt ds
and apply Proposition 6.7 and Corollary VI.4.14.)
For f ∈ C [α, β], E , show
(s − t)f (t) dt
f (τ ) dτ dt =
Suppose I and J are compact perfect intervals and ρ ∈ C(I × J, R). Verify that
log ((x − s)2 + (y − t)2 ρ(s, t) dt ds
u(x, y) :=
for (x, y) ∈ R2 \(I × J)
is harmonic.
Define
h : R → [0, 1) ,
e−1/s ,
s>0,
s≤0,
and, for −∞ < α < β < ∞, define k : R → R, s → h(s − α)h(β − s) and
Then show that
∈ C ∞ (R, R);
k(s) ds
k(s) ds .
210
is increasing;
(c) (t) = 0 for t ≤ α and (t) = 1 for t ≥ β.
6 Suppose −∞ < αj < βj < ∞ for j = 1, . . . , m and A := m (αj , βj ) ⊂ Rm . Show
that there is a g ∈ C ∞ (Rm , R) such that g(x) > 0 for x ∈ A and g(x) = 0 for x ∈ Ac .
g(x1 , . . . , xm ) := k1 (x1 ) · · · · · km (xm )
for (x1 , . . . , xm ) ∈ Rm ,
where kj (t) := h(t − αj )h(βj − t) for t ∈ [αj , βj ] and j = 1, . . . , m.)
7 Suppose K ⊂ Rm is compact and U is an open neighborhood of K. Then show there
is an f ∈ C ∞ (Rm , R) such that
(a) f (x) ∈ [0, 1] for x ∈ Rm ;
(b) f (x) = 1 for x ∈ K;
(c) f (x) = 0 for x ∈ U c .
(Hint: For x ∈ K, there exists an εx > 0 such that Ax := B(x, εx ) ⊂ U . Then choose
gx ∈ C ∞ (Rm , R) with gx (y) > 0 for y ∈ Ax (see Exercise 6) and choose x0 , . . . , xn ∈ K
such that K ⊂ n Axj . Then G := gx0 + · · · + gxn belongs to C ∞ (Rm , R), and the
number δ := minx∈K G(x) > 0. Finally, let be as in Exercise 5 with α = 0, β = δ, and
8 Define E0 := u ∈ C 1 [0, 1], R ; u(0) = 0, u(1) = 1 and f (u) := 0 tu(t) dt for
u ∈ E0 . Then show inf f (u) ; u ∈ E0 = 0, although there is no u0 ∈ E0 such that
f (u0 ) = 0.
For a, b ∈ Rm , consider the variational problem with fixed boundary conditions
|u(t)| dt = Min
for u ∈ C 1 [α, β], Rm ,
u(β) = b .
(6.24)
(a) Find all solutions of the Euler–Lagrange equations for (6.24) with the property that
|u(t)| = const.
(b) How do the Euler–Lagrange equations for (6.24) read when m = 2 under the assump˙
tion u1 (t) = 0 and u2 (t) = 0 for t ∈ [α, β]?
Find the Euler–Lagrange equations when
(a) α u 1 + u2 dt = Min for u ∈ C 1 [α, β], R ;
(1 + u2 )/u dt = Min for u ∈ C 1 [α, β], (0, ∞) .
11 Suppose f is defined through (6.6) with L ∈ C 0,2 [α, β] × (X × Rm ), R .
Show f ∈ C 2 (U, R) and
∂ 2 f (u)[h, k] =
∂2 L(·, u, u)[h, k] + ∂2 ∂3 L(·, u, u)[h, k]
+ ∂2 ∂3 L(·, u, u)[h, k] + ∂3 L(·, u, u)[h, k] dt
for h, k ∈ E1 . Derive from this a condition suﬃcient to guarantee that a solution u to
Euler–Lagrange equation is a local minimum of f .
12 Suppose T (q) := m |q|2 /2 and U (t, q) = U (q) for q ∈ R3 . Prove that the total energy
E := T + U is constant along every solution q of the Euler–Lagrange equation for the
variational problem
T (q) − U (q) dt = Min
for q ∈ C 2 [t0 , t1 ], R3 .
212
7 Inverse maps
Suppose J is an open interval in R and f : J → R is differentiable. Also suppose
a ∈ J with f (a) = 0. Then the linear approximation
x → f (a) + f (a)(x − a)
to f is invertible at a. This also stays true locally, that is, there is an ε > 0 such
that f is invertible on X := (a − ε, a + ε). Additionally, Y = f (X) is an open
interval, and the inverse map f −1 : Y → R is differentiable with
(f −1 ) f (x) = f (x)
(Compare with Theorem IV.1.8).
In this section, we develop the natural generalization of this for functions of
multiple variables.
The derivative of the inverse of linear maps
• E and F are Banach spaces over the field K.
We want to determine the differentiability of the map
inv : Lis(E, F ) → L(F, E) ,
A → A−1
and, if necessary, find its derivative. To make sense of this question, we must first
verify that Lis(E, F ) is an open subset of the Banach space L(E, F ). To show this,
we first prove a theorem about “geometric series” in the Banach algebra L(E).
Here we let I := 1E .
7.1 Proposition Suppose A ∈ L(E) with A < 1. Then I −A belongs to Laut(E),
(I − A)−1 = k=0 Ak , and (I − A)−1 ≤ (1 − A )−1 .
Ak in the Banach algebra L(E). Be-
Proof We consider the geometric series
cause Ak ≤ A k and
= 1 (1 − A ) ,
it follows from the majorant criterion (Theorem II.8.3) that
solutely in L(E). In particular, the value of the series
Ak converges ab-
VII.7 Inverse maps
is a well-defined element of L(E). Clearly AB = BA = ∞ Ak . Therefore (I −
A)B = B(I − A) = I, and consequently (I − A)−1 = B. Finally, Remark II.8.2(c)
and (7.1) gives the estimate
(I − A)−1 = B ≤ (1 − A )−1 .
7.2 Proposition
(i) Lis(E, F ) is open in L(E, F ).
is infinitely many times continuously differentiable, and1
∂ inv(A)B = −A−1 BA−1
for A ∈ Lis(E, F ) and B ∈ L(E, F ) .
Proof (i) Suppose A0 ∈ Lis(E, F ) and A ∈ L(E, F ) such that A − A0
1/ A−1 . Because
A = A0 + A − A0 = A0 I + A−1 (A − A0 ) ,
it suﬃces to verify that I + A−1 (A − A0 ) belongs to Laut(E). Since
− A−1 (A − A0 ) ≤ A−1
A − A0 < 1 ,
this follows from Proposition 7.1. Therefore the open ball in L(E, F ) with center
A0 and radius 1/ A−1 belongs to Lis(E, F ).
(ii) We have 2 A − A0 < 1/ A−1 . From (7.3), we get
A−1 = I + A−1 (A − A0 )
A−1 ,
A−1 − A−1 = I + A−1 (A − A0 )
I − I + A−1 (A − A0 )
that is,
inv(A) − inv(A0 ) = − I + A−1 (A − A0 )
A−1 (A − A0 )A−1 .
From this and Proposition 7.1, we derive
inv(A) − inv(A0 ) ≤
1 Because
A−1 2 A − A0
< 2 A−1
1 − A−1 (A − A0 )
A − A0
of Remark 2.2(e), this formula reduces when E = F = K to (1/z) = −1/z 2 .
214
Therefore inv is continuous.
(iii) We next prove that inv is differentiable. So let A ∈ Lis(E, F ). For
B ∈ L(E, F ) such that B < 1/ A−1 we have according to (i) that A + B
belongs to Lis(E, F ) and
(A + B)−1 − A−1 = (A + B)−1 A − (A + B) A−1 = −(A + B)−1 BA−1 .
This implies
inv(A + B) − inv(A) + A−1 BA−1 = inv(A) − inv(A + B) BA−1 .
inv(A + B) − inv(A) + A−1 BA−1 = o( B ) (B → 0)
follows from the continuity of inv. Therefore inv : Lis(E, F ) → L(F, E) is differentiable, and (7.2) holds.
(iv) We express
g : L(F, E)2 → L L(E, F ), L(F, E)
g(T1 , T2 )(S) := −T1 ST2
for T1 , T2 ∈ L(F, E) and S ∈ L(E, F ) .
Then is it easy to see that g is bilinear and continuous, and therefore
g ∈ C ∞ L(F, E)2 , L L(E, F ), L(F, E)
(see Exercises 4.4 and 5.2). In addition,
∂ inv(A) = g inv(A), inv(A)
for A ∈ Lis(E, F ) .
Therefore we get from (ii) the continuity of ∂ inv. Thus the map inv belongs to
C 1 Lis(E, F ), L(F, E) . Finally, it follows from (7.6) with the help of the chain
rule and a simple induction argument that this map is smooth.
The inverse function theorem
We can now prove a central result about the local behavior of differentiable maps,
which — as we shall see later — has extremely far-reaching consequences.
7.3 Theorem (inverse function) Suppose X is open in E and x0 ∈ X. Also
suppose for q ∈ N× ∪ {∞} that f ∈ C q (X, F ). Finally, suppose
∂f (x0 ) ∈ Lis(E, F ) .
Then there is an open neighborhood U of x0 in X and an open neighborhood V
of y0 := f (x0 ) with these properties:
(i) f : U → V is bijective.
(ii) f −1 ∈ C q (V, E), and for every x ∈ U , we have
∂f (x) ∈ Lis(E, F ) and
∂f −1 f (x) = ∂f (x)
Proof (i) We set A := ∂f (x0 ) and h := A−1 f : X → E. Then it follows from
Exercise 5.1 and the chain rule (Theorem 5.7), that h belongs C q (X, E) and
∂h(x0 ) = A−1 ∂f (x0 ) = I. Thus we lose no generality in considering the case
E = F and ∂f (x0 ) = I (see Exercise 5.1).
(ii) We make another simplification by setting
h(x) := f (x + x0 ) − f (x0 )
for x ∈ X1 := X − x0 .
Then X1 is open in E, and h ∈ C q (X1 , E) with h(0) = 0 and ∂h(0) = ∂f (x0 ) = I.
Thus it suﬃces to consider the case
x0 = 0 ,
E=F ,
f (0) = 0 ,
∂f (0) = I .
(iii) We show that f is locally bijective around 0. Precisely, we prove that
there are null neighborhoods2 U and V such that the equation f (x) = y has a
unique solution in U for every y ∈ V and such that f (U ) ⊂ V . This problem is
clearly equivalent to determining the null neighborhoods U and V such that the
gy : U → E , x → x − f (x) + y
has exactly one fixed point for every y ∈ V . Starting, we set g := g0 . Because
∂f (0) = I, we have ∂g(0) = 0, and we find using the continuity of ∂g an r > 0 such
∂g(x) ≤ 1/2 for x ∈ 2rB .
Because g(0) = 0, we get g(x) = 0 ∂g(tx)x dt by applying the mean value theorem
in integral form (Theorem 3.10). In turn, we get
g(x) ≤
∂g(tx)
2 That
is, neighborhoods of 0.
x dt ≤ x /2 for x ∈ 2rB .
216
Thus for every y ∈ rB, we have the estimate
gy (x) ≤ y + g(x) ≤ 2r
for x ∈ 2rB ,
that is, gy maps 2rB within itself for every y ∈ rB.
For x1 , x2 ∈ 2rB, we find with the help of the mean value theorem that
gy (x1 ) − gy (x2 ) =
∂g x2 + t(x1 − x2 ) (x1 − x2 ) dt ≤
x1 − x2
Consequently, gy is a contraction on 2rB for every y ∈ rB. From this, the Banach
fixed point theorem (Theorem IV.4.3) secures the existence of a unique x ∈ 2rB
such that gy (x) = x and therefore such that f (x) = y.
We set V := rB and U := f −1 (V ) ∩ 2rB. Then U is an open null neighborhood, and f |U : U → V is bijective.
(iv) Now we show that f −1 : V → E is continuous. So we consider
x = x − f (x) + f (x) = g(x) + f (x)
x1 − x2 ≤
x1 − x2 + f (x1 ) − f (x2 )
for x1 , x2 ∈ U ,
and consequently
f −1 (y1 ) − f −1 (y2 ) ≤ 2 y1 − y2
for y1 , y2 ∈ V .
Thus f −1 : V → E is Lipschitz continuous.
(v) We show the differentiability of f −1 : V → E and prove that the derivative
is given by
∂f −1 (y) = ∂f (x)
with x := f −1 (y) .
First we want to show ∂f (x) belongs to Laut(E) for x ∈ U . From f (x) =
x − g(x), it follows that ∂f (x) = I − ∂g(x) for x ∈ U . Noting (7.7), we see
∂f (x) ∈ Laut(E) follows from Proposition 7.1.
Suppose now y, y0 ∈ V , and let x := f −1 (y) and x0 := f −1 (y0 ). Then
f (x) − f (x0 ) = ∂f (x0 )(x − x0 ) + o( x − x0 ) (x → x0 ) ,
and we get as x → x0 that
f −1 (y) − f −1 (y0 ) − ∂f (x0 )
= x − x0 − ∂f (x0 )
(y − y0 )
f (x) − f (x0 )
≤ c o( x − x0 )
where c = ∂f −1 (x0 )
we finally have
. Because (7.9) gives the estimate 2 y − y0 ≥ x−x0 ,
y − y0
2c o( x − x0 )
as x → x0 . We now pass the limit y → y0 so that x → x0 follows from (iv), and
(7.11) shows that f −1 is differentiable at y0 , with derivative ∂f (x0 ) .
(vi) It remains to verify that f −1 belongs to C q (V, E). So consider what
(7.10) shows, that is,
∂f −1 = (∂f ◦ f −1 )−1 = inv ◦ ∂f ◦ f −1 .
We know from (iv) that f −1 belongs to C(V, E) with f −1 (V ) ∩ B(x0 , 2r) = U
and, by assumption, that ∂f ∈ C U, L(E) . Using Proposition 7.2 it follows that
∂f −1 belongs to C V, L(E) , which proves f −1 ∈ C 1 (V, E). For q > 1, one proves
f −1 ∈ C p (V, E) by induction from (7.12) using the chain rule.
Diffeomorphisms
Suppose X is open in E, Y is open in F , and q ∈ N ∪ {∞}. We call the map
f : X → Y a C q diffeomorphism from X to Y if it is bijective,
f ∈ C q (X, Y ) ,
and f −1 ∈ C q (Y, E) .
We may call a C 0 diffeomorphism a homeomorphism or a topological map. We
Diff q (X, Y ) := { f : X → Y ; f is a C q diffeomorphism } .
The map g : X → F is a locally C q diffeomorphism3 if every x0 ∈ X has
open neighborhoods U ∈ UX (x0 ) and V ∈ UF g(x0 ) such that g |U belongs
to Diff q (U, V ). We denote the set of all locally C q diffeomorphisms from X to F
by Diff q (X, F ).
7.4 Remarks Suppose X is open in E and Y is open in F .
(a) For every q ∈ N, we have the inclusions
Diff ∞ (X, Y ) ⊂ Diff q+1 (X, Y ) ⊂ Diff q (X, Y ) ⊂ Diff q (X, F ) ,
Diff ∞ (X, F ) ⊂ Diff q+1 (X, F ) ⊂ Diff q (X, F ) .
(b) Suppose f ∈ C q+1 (X, Y ) and f ∈ Diff q (X, Y ). Then it does not follow
generally that f belongs to Diff q+1 (X, Y ).
3 When
q = 0, one speaks of a local homeomorphism or a locally topological map.
218
Proof We set E := F := R and X := Y := R and consider f (x) := x3 . Then f is
a smooth topological map, that is, f ∈ C ∞ (X, Y ) ∩ Diff 0 (X, Y ). However, f −1 is not
differentiable at 0.
(c) Local topological maps are open, that is, they map open sets to open sets (see
Exercise III.2.14).
This follows easily from Theorems I.3.8(ii) and III.2.4(ii).
(d) For f ∈ Diff 1 (X, Y ), we have ∂f (x) ∈ Lis(E, F ) for x ∈ X.
This is a consequence of the chain rule (see Exercise 1).
7.5 Corollary Suppose X is open in E, q ∈ N× ∪ {∞}, and f ∈ C q (X, F ). Then
f ∈ Diff q (X, F ) ⇐ ∂f (x) ∈ Lis(E, F ) for x ∈ X .
Proof “= From Remark 7.4(a), it follows by induction that f belongs to
Diff 1 (X, F ). The claim then follows from Remark 7.4(d).
“⇐ This follows from the inverse function theorem
7.6 Remark Under the assumptions of Corollary 7.5, ∂f (x) belongs to Lis(E, F )
for x ∈ X. Then f is locally topological, and thus Y := f (X) is open. In general,
however, f is not a C q diffeomorphism from X to Y .
Proof Let X := E := F := C and f (z) := ez . Then f is smooth and ∂f (z) = ez = 0
for z ∈ C. Because of its 2πi -periodicity, f is not injective.
We will now formulate the inverse function theorem for the case E = F = Rm .
We will exploit that every linear map on Rm is continuous and that a linear map
is invertible if its determinant does not vanish.
In the rest of this section, suppose X is open in Rm and x0 ∈ X. Also
suppose q ∈ N× ∪ {∞} and f = (f 1 , . . . , f m ) ∈ C q (X, Rm ).
7.7 Theorem If det ∂f (x0 ) = 0, then there are open neighborhoods U of x0
and V of f (x0 ) such that f |U belongs to Diff q (U, V ).
Proof From Theorem 1.6 we know that Hom(Rm , Rm ) = L(Rm ). Linear algebra
teaches us that
∂f (x0 ) ∈ Laut(Rm ) ⇐ det ∂f (x0 ) = 0
(for example, [Gab96, A3.6(a)]). The claim then follows from Theorem 7.3.
7.8 Corollary If det ∂f (x0 ) = 0, then there are open neighborhoods U of x0
and V of f (x0 ) such that the system of equations
f 1 (x1 , . . . , xm ) = y 1 ,
f m (x1 , . . . , xm ) = y m
has exactly one solution
x1 = x1 (y 1 , . . . , y m ), . . . , xm = xm (y 1 , . . . , y m )
for every m-tuple (y 1 , . . . , y m ) ∈ V in U . The functions x1 , . . . , xm belong to
C q (V, R).
7.9 Remarks (a) According to Remark 1.18(b) and Corollary 2.9, the determinant
of the linear map ∂f (x) can be calculated using the Jacobi matrix ∂k f j (x) , that
is, det ∂f (x) = det ∂k f j (x) . This is called the Jacobian determinant of f (at
the point x). We may also simply call it the Jacobian and write it as
∂(f 1 , . . . , f m )
(x) .
∂(x1 , . . . , xm )
(b) Because the proof of the inverse function theorem is constructive, it can in
principle be used to calculate (approximately) f −1 (x). In particular, we can approximate the solution in the finite-dimensional case described in Corollary 7.8 by
looking at the equation system (7.13) in the neighborhood of x0 .
Proof This follows because the proof of Theorem 7.3 is based on the contraction theorem
and therefore on the method of successive approximation.
1 Suppose X is open in E, Y is open in F , and f ∈ Diff 1 (X, Y ). Show that ∂f (x0 )
, where y0 := f (x0 ).
belongs to Lis(E, F ) for x0 ∈ X, and ∂f −1 (y0 ) = ∂f (x0 )
2 Suppose m, n ∈ N× , and X is open in Rn . Show that if Diff 1 (X, Rm ) is not empty,
then m = n.
3 For the maps defined in (a)–(d), find Y := f (X) and f −1 . Also decide whether
f ∈ Diff q (X, R2 ) or f ∈ Diff q (X, Y ).
(a) X := R2 and f (x, y) := (x + a, y + b) for (a, b) ∈ R2 ;
(b) X := R2 and f (x, y) := (x2 − x − 2, 3y);
(c) X := R2 {(0, 0)} and f (x, y) := (x2 − y 2 , 2xy);
(d) X :=
(x, y) ∈ R2 ; 0 < y < x
(Hint for (c): R ← C.)
and f (x, y) := log xy, 1/(x2 + y 2 ) .
220
Suppose
f : R2 → R 2 ,
(x, y) ∈ R ; x > 0
and X :=
(a) f | X ∈
(x, y) → (cosh x cos y, sinh x sin y)
and Y := f (X). Show that
Diff ∞ (X, R2 );
(b) f | X ∈ Diff (X, Y );
(c) for U := (x, y) ∈ X ; 0 < y < 2π
Diff ∞ (U, V );
(d) Y = R2
and V := Y
[0, ∞) × {0} , f | U belongs to
[−1, 1] × {0} .
Suppose X is open in Rm and f ∈ C 1 (X, Rm ). Show that
(a) if ∂f (x) ∈ Lis(Rm ) for x ∈ X, then x → |f (x)| does not have a maximum in X;
(b) if ∂f (x) ∈ Lis(Rm ) and f (x) = 0 for x ∈ X, then x → |f (x)| has no minimum.
Suppose H is a real Hilbert space and
1 + |x|2 .
Letting Y := im(f ), show f ∈ Diff ∞ (H, Y ). What are f −1 and ∂f ?
7 Suppose X is open in Rm and f ∈ C 1 (X, Rm ). Also suppose there is an α > 0 such
|f (x) − f (y)| ≥ α |x − y| for x, y ∈ X .
Show that Y := f (X) is open in Rm and f ∈ Diff 1 (X, Y ). Also show for X = Rm that
(Hint: From (7.14), it follows that ∂f (x) ∈ Lis(X, Y ) for x ∈ X. If X = Rm , then (7.14)
implies that Y is closed in Rm .)
8 Suppose f ∈ Diff 1 (Rm , Rm ) and g ∈ C 1 (Rm , Rm ), and either of these assumptions is
satisfied:
(a) f −1 and g is Lipschitz continuous;
(b) g vanishes outside of a bounded subset of Rm .
Show then there is an ε0 > 0 such that f + εg ∈ Diff 1 (Rm , Rm ) for ε ∈ (−ε0 , ε0 ).
(Hint: Consider idRm + f −1 ◦ (εg) and apply Exercise 7.)
VII.8 Implicit functions
8 Implicit functions
In the preceding sections, we have studied (in the finite-dimensional case) the
solvability of nonlinear systems of equations. We concentrated on the case in
which the number of equations equals the number of variables. Now, we explore
the solvability of nonlinear system having more unknowns than equations.
We will prove the main result, the implicit function theorem, without troubling ourselves with the essentially auxiliary considerations needed for proving it
on general Banach spaces. To illustrate the fundamental theorem, we prove the
fundamental existence and continuity theorems for ordinary differential equations.
Finite-dimensional applications of the implicit function theorem will be treated in
the two subsequent sections.
For motivation, we consider the function f : R2 → R, (x, y) → x2 + y 2 − 1.
Suppose (a, b) ∈ R2 where a = ±1, b > 0 and f (a, b) = 0. Then there are open
intervals A and B with a ∈ A and b ∈ B such that for every x ∈ A there exists
exactly one y ∈ B such that f (x, y) = 0. By the assignment x → y, we define
a map g : A → B with f x, g(x) = 0 for
x ∈ A. Clearly, we have g(x) = 1 − x2 .
In addition, there is an open interval B
such that −b ∈ B and a g : A → B such
that f x, g(x) = 0 for x ∈ A. Here, we
have g(x) = − 1 − x2 . The function g is
uniquely determined through f and (a, b)
on A, and the function g is defined by f
and (a, −b). We say therefore that g and g
are implicitly defined by f near (a, b) and
(a, −b), respectively. The functions g and
g are solutions for y (as a function of x)
of the equation f (x, y) = 0 near (a, b) and
(a, −b), respectively. We say f (x, y) = 0
defines g implicitly near (a, b).
Such solutions can clearly not be given in neighborhoods of (1, 0) or (−1, 0).
To hint at the obstruction, we note that for a = ±1, we have ∂2 f (a, b) = 0, whereas
for a = ±1 this quantity does not vanish, as ∂2 f (a, b) = 2b.
Here, suppose
• E1 , E2 and F are Banach spaces over K;
Suppose Xj is open in Ej for j = 1, 2, and f : X1 × X2 → F is differentiable
at (a, b). Then the functions f (·, b) : X1 → F and f (a, ·) : X2 → F are also
222
differentiable at a and b, respectively. To avoid confusion with the standard partial
derivatives, we write D1 f (a, b) for the derivative of f (·, b) at a, and we write
D2 f (a, b) for the derivative of f (a, ·) at b,
8.1 Remarks (a) Obviously Dj f : X1 × X2 → L(Ej , F ) for j = 1, 2.
(b) The statements
(i) f ∈ C q (X1 × X2 , F ) and
(ii) Dj f ∈ C q−1 X1 × X2 , L(Ej , F ) for j = 1, 2
are equivalent. Because of them, we have
∂f (a, b)(h, k) = D1 f (a, b)h + D2 f (a, b)k
for (a, b) ∈ X1 × X2 and (h, k) ∈ E1 × E2 (see Theorem 5.4 and Proposition 2.8).
The implication “(i)=
⇒(i)” Suppose (a, b) ∈ X1 × X2 and
A(h, k) := D1 f (a, b)h + D2 f (a, b)k
for (h, k) ∈ E1 × E2 .
One easily verifies that A belongs to L(E1 × E2 , F ). The mean value theorem in integral
form (Theorem 3.10) gives
f (a + h, b + k) − f (a, b) − A(h, k)
= f (a + h, b + k) − f (a, b + k) + f (a, b + k) − f (a, b) − A(h, k)
D1 f (a + th, b + k) − D1 f (a, b) h dt + f (a, b + k) − f (a, b) − D2 f (a, b)k
if max{ h , k } is suﬃciently small. From this, we get the estimate
f (a + h, b + k) − f (a, b) − A(h, k) ≤ ϕ(h, k) max{ h , k }
ϕ(h, k) := max D1 f (a + th, b + k) − D1 f (a, b)
f (a, b + k) − f (a, b) − D2 f (a, b)k
Now the continuity of D1 f gives ϕ(h, k) → 0 for (h, k) → (0, 0). Thus we see that
f (a + h, b + k) − f (a, b) − A(h, k) = o (h, k)
(h, k) → (0, 0) .
Therefore f is differentiable in (a, b) with ∂f (a, b) = A. Finally, the regularity assumption
on Dj f and the definition of A implies that ∂f ∈ C q−1 (X1 × X2 , F ) and therefore that
f ∈ C q (X1 × X2 , F ).
(c) In the special case E1 = Rm ,
∂1 f 1 · · · ∂m f 1
D1 f = ⎣ .
∂1 f
with f = (f 1 , . . . , f ).
E2 = Rn , and F = R , we have
∂m+1 f 1 · · · ∂m+n f 1
D2 f = ⎣
∂m+1 f
This follows from (b) and Corollary 2.9.
The implicit function theorem
With what we just learned, we can turn to the solvability of nonlinear equations.
The result is fundamental.
8.2 Theorem (implicit function) Suppose W is open in E1 ×E2 and f ∈ C q (W, F ).
Further, suppose (x0 , y0 ) ∈ W such that
f (x0 , y0 ) = 0
D2 f (x0 , y0 ) ∈ Lis(E2 , F ) .
Then there are open neighborhoods U ∈ UW (x0 , y0 ) and V ∈ UE1 (x0 ), and also a
unique g ∈ C q (V, E2 ) such that
(x, y) ∈ U and f (x, y) = 0 ⇐ x ∈ V and y = g(x) .
In addition,
∂g(x) = − D2 f x, g(x)
D1 f x, g(x)
(i) Let A := D2 f (x0 , y0 ) ∈ Lis(E2 , F ) and f := A−1 f ∈ C q (W, E2 ). Then
and D2 f (x0 , y0 ) = IE2 .
Thus because of Exercise 5.1 we lose no generality in considering the case F = E2
and D2 f (x0 , y0 ) = IE2 . Also, we can assume W = W1 ×W2 for open neighborhoods
W1 ∈ UE1 (x0 ) and W2 ∈ UE2 (y0 ).
(ii) For the map
ϕ : W1 × W2 → E1 × E2 ,
(x, y) → x, f (x, y)
we have ϕ ∈ C q (W1 × W2 , E1 × E2 ) with1
∂ϕ(x0 , y0 ) =
D1 f (x0 , y0 ) IE2
∈ L(E1 × E2 ) .
We verify at once that
−D1 f (x0 , y0 ) IE2
∈ L(E1 × E2 )
is the inverse of ∂ϕ(x0 , y0 ). Consequently, we have ∂ϕ(x0 , y0 ) ∈ Laut(E1 ×E2 ), and
because ϕ(x0 , y0 ) = (x0 , 0), the inverse function theorem (Theorem 7.3) guarantees
and in similar situations, we use the natural matrix notation.
224
the existence of open neighborhoods U ∈ UE1 ×E2 (x0 , y0 ) and X ∈ UE1 ×E2 (x0 , 0)
on which ϕ|U ∈ Diff q (U, X). We set ψ := (ϕ|U )−1 ∈ Diff q (X, U ) and write ψ in
the form
ψ(ξ, η) = ψ1 (ξ, η), ψ2 (ξ, η)
for (ξ, η) ∈ X .
Then ψj ∈ C q (X, E2 ) for j = 1, 2, and the definition of ϕ shows
(ξ, η) = ϕ ψ(ξ, η) = ψ1 (ξ, η), f ψ1 (ξ, η), ψ2 (ξ, η)
Therefore, we recognize
ψ1 (ξ, η) = ξ and η = f ξ, ψ2 (ξ, η)
(8.3)
Furthermore, V := x ∈ E1 ; (x, 0) ∈ X is an open neighborhood of x0 in E1 ,
and for g(x) := ψ2 (x, 0) with x ∈ V , we have g ∈ C q (V, E2 ) and
x, f x, g(x)
= ψ1 (x, 0), f ψ1 (x, 0), ψ2 (x, 0)
= ϕ ψ1 (x, 0), ψ2 (x, 0) = ϕ ◦ ψ(x, 0) = (x, 0)
for x ∈ V . This together with (8.3) implies (8.1). The uniqueness of g is clear.
(iii) We set h(x) := f x, g(x) for x ∈ V . Then h = 0 and the chain rule
together with Remark 8.1(b), imply
∂h(x) = D1 f x, g(x) IE1 + D2 f x, g(x) ∂g(x) = 0 for x ∈ V .
(8.4)
From q ≥ 1, it follows that
D2 f ∈ C q−1 U, L(E2 ) ⊂ C(U, L(E2 ) .
According to Proposition 7.2(i), Laut(E2 ) is open in L(E2 ). Therefore
(D2 f )−1 Laut(E2 )
(x, y) ∈ U ; D2 f (x, y) ∈ Laut(E2 )
is an open neighborhood of (x0 , y0 ) in U . By making U smaller, we can therefore
assume D2 f (x, y) ∈ Laut(E2 ) for (x, y) ∈ U . Then from (8.4), we get (8.2).
8.3 Remark Theorem 8.2 says that near (x0 , y0 ) the fiber f −1 (0) is the graph of
a C q function.
By formulating the implicit function theorem in the special case E1 = Rm
and E2 = F = Rn , we get a statement about the “local solvability of nonlinear
systems of equations depending on parameters”.
8.4 Corollary Suppose W is open in Rm+n and f ∈ C q (W, Rn ). Also suppose
(a, b) ∈ W such that f (a, b) = 0, that is
f 1 (a1 , . . . , am , b1 , . . . , bn ) = 0 ,
f n (a1 , . . . , am , b1 , . . . , bn ) = 0 .
Then if
∂(f 1 , . . . , f n )
(a, b) := det ∂m+k f j (a, b)
∂(xm+1 , . . . , xm+n )
1≤j,k≤n
there is an open neighborhood U of (a, b) in W , an open neighborhood V of a
in Rm , and a g ∈ C q (V, Rn ) such that
That is, there is a neighborhood V of a in Rm such that the system of equations
f 1 (x1 , . . . , xm , y 1 , . . . , y n ) = 0 ,
f n (x1 , . . . , xm , y 1 , . . . , y n ) = 0
y 1 = g 1 (x1 , . . . , xm ) ,
y n = g n (x1 , . . . , xm )
near b = (b1 , . . . , bn ) for every m-tuple (x1 , . . . , xm ) ∈ V . Also, the solutions
g 1 , . . . , g n are C q functions in the “parameters” (x1 , . . . , xm ) from V , and
∂m+1 f 1
∂g(x) = − ⎣
∂m+1 f n
⎤−1 ⎡
∂m+n f 1
∂1 f 1
∂1 f n
∂m f 1
for x ∈ V , where the derivatives ∂k f j are evaluated at x, g(x) .
Because D2 f (x, y) belongs to Laut(Rn ) if and only if
(x, y) = 0 ,
we verify all the claims from Theorem 8.2 and Remark 8.1(c).
226
Regular values
Suppose X is open in Rm and f : X → Rn is differentiable. Then x ∈ X is called
a regular point of f if ∂f (x) ∈ L(Rm , Rn ) is surjective. The map is called regular
or a submersion if every point in X is regular. Finally, we say y ∈ Rn is a regular
value of f if the fiber f −1 (y) consists only of regular points.
8.5 Remarks (a) If m < n, then f has no regular points.
This follows from rank ∂f (x) ≤ m.
(b) If n ≤ m, then x ∈ X is a regular point of f if and only if ∂f (x) has rank2 n.
(c) If n = 1, then x is a regular point of f if and only if \nabla f (x) = 0.
(d) Every y ∈ Rn \im(f ) is a regular value of f .
(e) Suppose x0 ∈ X is a regular point of f with f (x0 ) = 0. Then there are n
variables that uniquely solve the system of equations
f 1 (x1 , . . . , xm ) = 0 ,
f n (x1 , . . . , xm ) = 0
in a neighborhood of x0 as functions of then m − n other variables. When f is in
the class C q , the solutions are also C q functions.
From (a), we have m ≥ n. Also, we can make
(x0 ) = 0
∂(xm−n+1 , . . . , xm )
by suitably permuting the coordinates in Rm , that is, by applying an appropriate orthogonal transformation of Rm . The remark then follows from Corollary 8.4.
(f ) Suppose 0 ∈ im(f ) is a regular value of f ∈ C q (X, Rn ). Then for every
x0 ∈ f −1 (0), there is a neighborhood U in Rm such that f −1 (0) ∩ U is the graph
of a C q function of m − n variables.
This follows from (e) and Remark 8.3.
Ordinary differential equations
In Section 1, we used the exponential map, to help address existence and uniqueness questions for linear differential equations. We next treat differential equations
of the form x = f (t, x), where now f may be nonlinear.
2 Suppose E and F are finite-dimensional Banach spaces and A ∈ L(E, F ). Then the rank
of A is defined by rank(A) := dim im(A) . Obviously, rank(A) is same as the linear algebraic
rank of the representation matrix [A]E,F in any bases E of E and F of F .
In the rest of this section, suppose
• J is an open interval in R;
E is a Banach space;
D is an open subset of E;
f ∈ C(J × D, E).
The function u : Ju → D is said to be the solution of the differential equation
x = f (t, x) in E if Ju is a perfect subinterval of J, u ∈ C 1 (Ju , D), and
u(t) = f t, u(t)
By giving (t0 , x0 ) ∈ J × D, the pair
x = f (t, x) and x(t0 ) = x0
(8.5)(t0 ,x0 )
becomes an initial value problem for x = f (t, x). The map u : Ju → D is a solution
to (8.5)(t0 ,x0 ) if u satisfies the differential equation x = f (t, x) and u(t0 ) = x0 . It
is a noncontinuable (or maximal), if there is no solution v : Jv → D of (8.5)(t0 ,x0 )
such that v ⊃ u and v = u. In this case Ju is a maximal existence interval for
(8.5)(t0 ,x0 ) . When Ju = J, we call u a global solution of (8.5)(t0 ,x0 ) .
8.6 Remarks (a) Global solutions are noncontinuable.
(b) Suppose Ju is a perfect subinterval of J. Then u ∈ C(Ju , D) is a solution of
(8.5)(t0 ,x0 ) if and only if u satisfies the integral equation
u(t) = x0 +
f s, u(s) ds
Proof Because s → f s, u(s) is continuous, the remark follows easily from the fundamental theorem of calculus.
(c) Suppose A ∈ L(E) and g ∈ C(R, E). Then the initial value problem
x = Ax + g(t) and x(t0 ) = x0
has the unique global solution
u(t) = e(t−t0 )A x0 +
e(t−s)A g(s) ds
This is a consequence of Theorem 1.17.
(d) When E = R, one says that x = f (t, x) is a scalar differential equation.
(e) The initial value problem
x = x2 and x(t0 ) = x0
has at most one solution for (t0 , x0 ) ∈ R .
(8.6)
228
Proof Suppose u ∈ C 1 (Ju , R) and v ∈ C 1 (Jv , R) are solutions of (8.6). For t ∈ Ju ∩ Jv ,
let w(t) := u(t) − v(t), and let I be a compact subinterval of Ju ∩ Jv with t0 ∈ I. Then
it suﬃces to verify that w vanishes on I. From (b), we have
w(t) = u(t) − v(t) =
u2 (s) − v 2 (s) ds =
u(s) + v(s) w(s) ds
Letting α := maxs∈I |u(s) + v(s)|, it follows that
|w(t)| ≤ α
|w(s)| ds
for t ∈ I ,
and the claim is implied by Gronwall’s lemma.
(f ) The initial value problem (8.6) has no global solution for x0 = 0.
Proof Suppose u ∈ C 1 (Ju , R) is a solution of (8.6) with x0 = 0. Because 0 is the unique
global solution for the initial value (t0 , 0), it follows from (e) that u(t) = 0 for t ∈ Ju .
Therefore (8.6) implies
on Ju . By integration, we find
= t − t0
u(t)
u(t) = 1/(t0 − t + 1/x0 )
Thus we get Ju = J = R.
(g) The initial value problem (8.6) has, for every (t0 , x0 ) ∈ R2 , the unique noncontinuable solution u(·, t0 , x0 ) ∈ C 1 J(t0 , x0 ), R with
⎪ (t0 + 1/x0 , ∞) ,
J(t0 , x0 ) =
⎩ (−∞, t + 1/x ) ,
x0 < 0 ,
x0 > 0 ,
u(t, t0 , x0 ) =
1/(t0 − t + 1/x0 ) ,
x0 = 0 .
This follows from (e) and the proof of (f).
(h) The scalar initial value problem
x = 2 |x| ,
has uncountably many global solutions.
x(0) = 0
(8.7)
Proof Clearly u0 ≡ 0 is a global solution.
For α < 0 < β, let
⎪ −(t − α)2 ,
t ∈ (−∞, α] ,
t ∈ (α, β) ,
uα,β (t) :=
⎩ (t − β)2 ,
One verifies easily that uα,β is a global solution of (8.7).
Separation of variables
It is generally not possible to solve a given differential equation “explicitly”. For
certain scalar differential equations, though, a unique local solution to an initial
value problem can be given with the help of the implicit function theorem.
The function Φ : J × D → R is said to a first integral of x = f (t, x) if, for
every solution u : Ju → D of x = f (t, x), the map
t → Φ t, u(t)
is constant. If in addition
Φ ∈ C 1 (J × D, R) and ∂2 Φ(t, x) = 0
for (t, x) ∈ J × D ,
then Φ is a regular first integral.
8.7 Proposition Suppose Φ is a regular first integral of the scalar differential
equation x = f (t, x) and
f (t, x) = −∂1 Φ(t, x)/∂2 Φ(t, x)
for (t, x) ∈ J × D .
Then for every (t0 , x0 ) ∈ J × D, there is an open interval I of J and U from D
with (t0 , x0 ) ∈ I × U such that the initial value problem
x = f (t, x) ,
x(t0 ) = x0
on I has exactly one solution u with u(I) ⊂ U . It is achieved by solving the
equation Φ(t, x) = Φ(t0 , x0 ) for x.
This follows immediately from Theorem 8.2.
By suitably choosing Φ, we get an important class of scalar differential equations with “separated variables”. As the next corollary shows, we can solve these
equations by quadrature.
230
8.8 Proposition Suppose g ∈ C(J, R) and h ∈ C(D, R) with h(x) = 0 for x ∈ D.
Also suppose G ∈ C 1 (J, R) and H ∈ C 1 (D, R) are antiderivatives of g and h,
respectively. Then
Φ(t, x) := G(t) − H(x) for (t, x) ∈ J × D ,
is a regular first integral of x = g(t)/h(x).
Proof Obviously Φ belongs to C 1 (J × D, R), and ∂2 Φ(t, x) = −h(x) = 0. Now
suppose u ∈ C 1 (Ju , D) is a solution of x = g(t)/h(x). Then it follows from the
chain rule that
Φ t, u(t) = ∂1 Φ t, u(t) + ∂2 Φ t, u(t) u(t)
= g(t) − h u(t) ·
g(t)
h u(t)
Therefore t → Φ t, u(t) is constant on Ju .
8.9 Corollary (separation of variables) Suppose g ∈ C(J, R) and h ∈ C(D, R)
with h(x) = 0 for x ∈ D. Then the initial value problem
x = g(t)/h(x) ,
has a unique local solution for every (t0 , x0 ) ∈ J × D. That solution can be
obtained by solving the equation
h(ξ) dξ =
g(τ ) dτ
(8.8)
This is an immediate consequence of Theorems 8.7 and 8.8.
8.10 Remark
Under the assumptions of Corollary 8.9,
implies formally h(x) dx = g(t) dt,the “equation with separated variables”. Formal
integration then gives (8.8).
8.11 Examples
(a) We consider the initial value problem
x = 1 + x2 ,
x(t0 ) = x0 .
(8.9)
Letting g := 1 and h := 1/(1 + X 2 ), it follows from Corollary 8.9 that
arctan x − arctan x0 =
1 + ξ2
dτ = t − t0 .
x(t) = tan(t − α)
for t ∈ (α − π/2, α + π/2)
and α := t0 − arctan x0 is the unique maximal solution of (8.9). In particular,
(8.9) has no global solutions.
(b) Suppose α > 0, D := (0, ∞) and x0 > 0. For
x = x1+α ,
Corollary 8.9 implies
x(0) = x0 ,
(8.10)
ξ −1−α dξ = t .
x(t) = (x−α − αt)−1/α
is the unique maximal solution of (8.10), which is therefore not globally solvable.
(c) On D := (1, ∞), we consider the initial value problem
x(0) = x0 .
(8.11)
Because log ◦ log is an antiderivative on D of x → 1/(x log x), we get from Corollary 8.9 that
log(log x) − log(log x0 ) =
From this we derive that
(et )
x(t) = x0
is the unique global solution of (8.11).
(d) Let −∞ < a < b < ∞ and f ∈ C (a, ∞), R such that f (x) > 0 for x ∈ (a, b)
and f (b) = 0. For x0 ∈ (a, b) and t0 ∈ R, we consider the initial value problem
x = f (x) ,
According to Corollary 8.9, we get the unique local solution u : Ju → (a, b) by
solving
=: H(x)
t = t0 +
f (ξ)
for x. Because H = 1/f , H is strictly increasing. Therefore u = H −1 , and Ju
agrees with H (a, b) . Also
T ∗ := lim H(x) = sup Ju
x→b−0
232
exists in R, and
Now limt→T ∗ −0 u(t) = b implies
t→T ∗ −0
u(t) =
f u(t) = 0 .
In the case T ∗ < ∞, it then follows from Proposition IV.1.16 that
u(t) ,
gives a continuation of u. Analogous considerations for
T∗ := lim H(x) = inf Ju
x→a+0
are left to you.
(e) For a ∈ C(J, R) and (t0 , x0 ) ∈ J × R, we consider the initial value problem
x = a(t)x ,
(8.12)
for the scalar linear homogeneous differential equation x = a(t)x with time˙
dependent coeﬃcients a. Therefore (8.12) has the unique global solution
x(t) = x0 e
a(τ ) dτ
Proof From Gronwall’s lemma, it follows that (8.12) has at most one solution. If u ∈
C 1 (Ju , R) is a solution and there is a t1 ∈ Ju such that u(t1 ) = 0, then the uniqueness
result for the initial value problem x = a(t)x, x(t1 ) = 0 implies that u = 0.
Suppose therefore x0 = 0. Then we get by separation of variables that
log |x| − log |x0 | =
|x(t)| = |x0 | e
Because x(t) = 0 for t ∈ J, the claim follows.
Lipschitz continuity and uniqueness
Remark 8.6(h) shows that the initial value problem (8.5)(t0 ,x0 ) is generally not
uniquely solvable. We will see next that we can guarantee uniqueness if we require
that f is somewhat regular than merely continuous.
Suppose E and F are Banach spaces, X ⊂ E and I ⊂ R. Then f ∈ C(I ×
X, F ) is said to be locally Lipschitz continuous in x ∈ X, if every (t0 , x0 ) in I × X
has a neighborhood U × V such that
f (t, x) − f (t, y) ≤ L x − y
for t ∈ U and x, y ∈ V
for some L ≥ 0. We set
C 0,1- (I×X, F ) := f ∈ C(I×X, F ) ; f is locally Lipschitz continuous in x ∈ X .
If I is a single point, and thus f has the form f : X → F , we call f locally
Lipschitzcontinuous, and
C 1- (X, F ) := { f : X → F ; f is locally Lipschitz continuous } .
8.12 Remarks
(a) Obviously, we have C 0,1- (I × X, F ) ⊂ C(I × X, F ).
(b) Suppose X is open in E, and f ∈ C(I × X, F ). Assume ∂2 f exists and belongs
to C I × X, L(E, F ) . Then f ∈ C 0,1- (I × X, F ).
Proof Let (t0 , x0 ) ∈ I × X. Because ∂2 f belongs to C I × X, L(E, F ) and X open is
in E, there is an ε > 0 such that B(x0 , ε) ⊂ X and
∂2 f (t0 , x0 ) − ∂2 f (t, x) ≤ 1
for (t, x) ∈ U × V ,
(8.13)
where we set U := (t0 − ε, t0 + ε) ∩ I and V := B(x0 , ε). Putting L := 1 + ∂2 f (t0 , x0 ) ,
it follows from the mean value theorem (Theorem 3.9) and (8.13) that
f (t, x) − f (t, y) ≤ sup
0≤s≤1
∂2 f t, x + s(y − x)
for (t, x), (t, y) ∈ U × V . Therefore f is locally Lipschitz continuous in x.
(c) Polynomials of degree ≥ 2 are locally Lipschitz continuous but not Lipschitz
(d) Suppose I × X is compact and f ∈ C 0,1- (I × X, F ). Then f is uniformly
Lipschitz continuous in x ∈ X, that is, there is an L ≥ 0 such that
for x, y ∈ X and t ∈ I .
For every (t, x) ∈ I × X, there are εt > 0, εx > 0, and L(t, x) ≥ 0 such that
f (s, y) − f (s, z) ≤ L(t, x) y − z
for (s, y), (s, z) ∈ B(t, εt ) × B(x, εx ) .
234
Because I × X is compact, there exist (t0 , x0 ), . . . , (tm , xm ) ∈ I × X such that
I ×X ⊂
B(tj , εtj ) × B(xj , εxj /2) .
Because f (I × X) is compact, there is a R > 0 such that f (I × X) ⊂ RB. We set
δ := min{εx0 , . . . , εxm }/2 > 0
L := max L(t0 , x0 ), . . . , L(tm , xm ), 2R/δ > 0 .
Suppose now (t, x), (t, y) ∈ I × X. Then there is an k ∈ {0, . . . , m} such that
(t, x) ∈ B(tk , εtk ) × B(xk , εxk /2) .
If x − y < δ, we find (t, y) lies in B(tk , εtk ) × B(xk , εxk ), and we get
f (t, x) − f (t, y) ≤ L(tk , xk ) x − y ≤ L x − y
On the other hand, if x − y ≥ δ, it follows that
f (t, x) − f (t, y) ≤
Therefore f is uniformly Lipschitz continuous in x ∈ X.
(e) Suppose X is compact in E and f ∈ C 1- (X, F ). Then f is Lipschitz continuous.
This is a special case of (d).
8.13 Theorem Let f ∈ C 0,1- (J × D, E), and suppose u : Ju → D and v : Jv → D
are solutions of x = f (t, x) with u(t0 ) = v(t0 ) for some t0 ∈ Ju ∩ Jv . Then
u(t) = v(t) for every t ∈ Ju ∩ Jv .
Proof Suppose I ⊂ Ju ∩ Jv is a compact interval with t0 ∈ I and w := u − v. It
suﬃces to verify that w |I = 0.
Because K := u(I) ∪ v(I) ⊂ D is compact, Remark 8.12(d) guarantees the
existence of an L ≥ 0 such that
f s, u(s) − f s, v(s)
≤ L u(s) − v(s) = L w(s)
for s ∈ I .
Therefore from
(see Remark 8.6(b)) the inequality
w(t) ≤ L
w(s) ds
follows, and Gronwall’s lemma implies w |I = 0.
The Picard–Lindel¨f theorem
• J ⊂ R is an open interval;
E is a finite-dimensional Banach space;
D is open in E;
f ∈ C 0,1- (J × D, E).
We now prove the fundamental local existence and uniqueness theorem for of
ordinary differential equations.
8.14 Theorem (Picard–Lindel¨f) Suppose (t0 , x0 ) ∈ J × D. Then there is an
α > 0 such that the initial value problem
(8.14)
has a unique solution on I := [t0 − α, t0 + α].
(i) Because J × D ⊂ R × E is open, there are a, b > 0 such that
R := [t0 − a, t0 + a] × B(x0 , b) ⊂ J × D .
From the local Lipschitz continuity of f in x, we find an L > 0 such that
for (t, x), (t, y) ∈ R .
Finally, it follows from the compactness of R and Remark 8.12(a) that there is an
M > 0 such that
f (t, x) ≤ M for (t, x) ∈ R .
(ii) Suppose α := min a, b/M, 1/(2L) > 0 and I := [t0 − α, t0 + α]. We set
T (y)(t) := x0 +
f τ, y(τ ) dτ
for y ∈ C(I, D) ,
t∈I .
If we can show that the map T : C(I, D) → C(I, E) has exactly one fixed point u,
it will follows from Remark 8.6(b) that u is the unique solution of (8.14).
(iii) Let
y ∈ C(I, E) ; y(t0 ) = x0 , maxt∈I y(t) − x0 ≤ b
Then X is a closed subset of the Banach space C(I, E) and therefore a complete
metric space (see Exercise II.6.4).
For y ∈ X, we have y(t) ∈ B(x0 , b) ⊂ D for t ∈ I. Therefore T (y) is defined,
and T (y)(t0 ) = x0 . Also
T (y)(t) − x0 =
f τ, y(τ ) dτ ≤ α sup
(t,ξ)∈R
f (t, ξ) ≤
236
This shows that T maps the space X into itself.
(iv) For y, z ∈ X, we find
T (y)(t) − T (z)(t) =
f τ, y(τ ) − f τ, z(τ )
≤ α max f t, y(t) − f t, z(t)
t∈I
≤ αL max y(t) − z(t)
for t ∈ I. Using the definition of α, it follows that
T (y) − T (z)
C(I,E)
Therefore T : X → X is a contraction. Then the contraction theorem (Theorem IV.4.3) gives a unique fixed point u in X.
8.15 Remarks (a) The solution of (8.14) on I can be calculated using the method
of successive approximation (or iteratively) by putting
um+1 (t) := x0 +
f τ, um (τ ) dτ
for m ∈ N and t ∈ I ,
with u0 (t) = x0 for t ∈ I. The sequence (um ) converges uniformly on I to u, and
we have the error estimate
≤ αM/2m−1
(8.15)
Proof The first statement follows immediately from the last proof and from Theorem IV.4.3(ii). Statement (iii) of that theorem gives also the error estimate
≤ 2−m+1 u1 − u0
The claim now follows because for t ∈ I we have
u1 (t) − u0 (t) =
f τ, u0 (τ ) dτ ≤ αM .
(b) The error estimate (8.15) be improved to
Mα e
um − u C(I,E) < m
2 (m + 1)!
(see Exercise 11).
(c) The assumption that E is finite-dimensional was only used to prove the existence of the bound M on f (R). Therefore Theorem 8.14 and its proof stays correct
if the assumption that “E is finite-dimensional” is replaced with “f is bounded on
bounded sets”.
(d) Although the continuity of f is not enough to guarantee the uniqueness of
the solution (8.14), it is enough to prove the existence of a solution (see [Ama95,
Theorem II.7.3]).
Finally we show that the local solution of Theorem 8.14 can be extended to
a unique noncontinuable solution.
8.16 Theorem For every (t0 , x0 ) ∈ J × D, there is exactly one noncontinuable
solution u(·, t0 , x0 ) : J(t0 , x0 ) → D of the initial value problem
The maximal existence interval is open, that is, J(t0 , x0 ) = t− (t0 , x0 ), t+ (t0 , x0 ) .
Proof Suppose (t0 , x0 ) ∈ J × D. According to Theorem 8.14, there is an α0
and a unique solution u of (8.5)(t0 ,x0 ) on I0 := [t0 − α0 , t0 + α0 ]. We set x1 :=
u(t0 + α0 ) and t1 := t0 + α0 and apply Theorem 8.14 to the initial value problem
(8.5)(t1 ,x1 ) . Then there is an α1 > 0 and a unique solution v of (8.5)(t1 ,x1 ) on
I1 := [t1 − α1 , t1 + α1 ]. Theorem 8.13 shows that u(t) = v(t) for t ∈ I0 ∩ I1 .
t ∈ I0 ,
u1 (t) :=
v(t) ,
t ∈ I1 ,
is a solution of (8.5)(t0 ,x0 ) on I0 ∪ I1 . An analogous argument shows that u can
be continued to the left past t0 − α0 . Suppose now
t+ := t+ (t0 , x0 ) := sup β ∈ R ; (8.5)(t0 ,x0 ) has a solution on [t0 , β]
t− := t− (t0 , x0 ) := inf γ ∈ R ; (8.5)(t0 ,x0 ) has a solution on [γ, t0 ]
The considerations above show that t+ ∈ (t0 , ∞] and t− ∈ [−∞, t0 ) are well defined
and that (8.5)(t0 ,x0 ) has a noncontinuable solution u on (t− , t+ ). Now it follows
from Theorem 8.13 that u is unique.
8.17 Examples (a) (ordinary differential equations of m-th order)
E := Rm and g ∈ C 0,1- (J × D, R). Then
x(m) = g(t, x, x, . . . , x(m−1) )
(8.16)
is an m-th order ordinary differential equation. The function u : Ju → R is a
solution of (8.14) if Ju ⊂ J is a perfect interval, u belongs to C m (Ju , R),
u(t), u(t), . . . , u(m−1) (t) ∈ D
u(m) (t) = g t, u(t), u(t), . . . , u(m−1) (t)
238
For t0 ∈ J and x0 := (x0 , . . . , xm−1 ) ∈ D, the pair
x(m) = g t, x, x, . . . , x(m−1)
x(t0 ) = x0 , . . . , x(m−1) (t0 ) = xm−1
(8.17)(t0 ,x0 )
is called an initial value problem for (8.16) with initial value x0 and initial time t0 .
In this language we have, For every (t0 , x0 ) ∈ J ×D, there is a unique maximal
solution u : J(t0 , x0 ) → R of (8.17)(t0 ,x0 ) . The maximal existence interval J(t0 , x0 )
is open.
We define f : J × D → Rm by
f (t, y) := y 2 , y 3 , . . . , y m , g(t, y)
for t ∈ J and y = (y 1 , . . . , y m ) ∈ D .
(8.18)
Then f belongs to C 0,1- (J × D, Rm ). Using Theorem 8.16, there is a unique noncontinuable solution z : J(t0 , x0 ) → Rm of (8.5)(t0 ,x0 ) . One can then verify easily that
u := pr1 ◦ z : J(t0 , x0 ) → R is a solution of (8.17)(t0 ,x0 ) .
Conversely, if v : J(t0 , x0 ) → R is a solution of (8.17)(t0 ,x0 ) , then the vector function
(v, v, . . . , v (m−1) ) solves the initial value problem (8.5)(t0 ,x0 ) on J(t0 , x0 ), where f is
defined through (8.18). From this follows v = u. An analogous argument shows that u
is noncontinuable.
(b) (Newton’s equation of motion in one dimension) Suppose V is open in R and
f ∈ C 1 (V, R). For some x0 ∈ V , suppose
U (x) :=
f (ξ) dξ for x ∈ V
and T (y) := y 2 /2 for y ∈ R .
Finally let D := V × R and L := T − U . According to Example 6.14(a), the
differential equation
−¨ = f (x)
(8.19)
is Newton’s equation of motion for the (one-dimensional) motion of a massive
particle acted on by the conservative force −\nabla U = f . From (a), we know that
(8.19) is equivalent to the system
y = −f (x)
(8.20)
and therefore to a first order differential equation
u = F (u) ,
(8.21)
where F (u) := y, −f (x) for u = (x, y) ∈ D.
The function E := T + U : D → R, called the total energy, is a first integral
of (8.21). This means that the motion of the particle conserves energy3 , that is,
every solution x ∈ C 2 (J, V ) of (8.21) satisfies E x(t), x(t) = E x(t0 ), x(t0 ) for
every t ∈ J and t0 ∈ J.
3 See
Exercise 6.12.
Obviously E belongs to C 1 (D, R). Every solution u : Ju → D of (8.20) has
E u(t)
= y 2 (t)/2 + U x(t)
= y(t)y(t) + f x(t) x(t) = 0
Therefore E(u) is constant.
(c) From (b), every solution of (8.20) lies in a level
set E −1 (c) of the total energy.
Conversely, the existence statement (a) means
that for every point (x0 , y0 )
on a level set of E, there
is a solution of (8.20) with
u(0) = (x0 , y0 ). Therefore, the set of level sets
of E is the same as the
set of all (maximal) solution curves of (8.20); this
set is called the phase portrait of (8.19).
The phase portrait has these properties:
(i) The critical points of E are exactly the points (x, 0) ∈ D with f (x) = 0.
This means that the stationary points of (8.20) lie on the x-axis and are exactly
the critical points of the potential U .
(ii) Every level set is symmetric about the x-axis.
(iii) If c is a regular value of E, then E −1 (c) admits a local representation
as the graph of a C 2 function. More precisely, for every u0 ∈ E −1 (c), there are
positive numbers b and ε and a ϕ ∈ C 2 (−ε, ε), R with ϕ(0) = 0 such that
E −1 (c) ∩ B(u0 , b) = tr(g) ,
(8.22)
where g ∈ C 2 (−ε, ε), R2 is defined by
g(s) :=
s, ϕ(s) + u0 ,
ϕ(s), s + u0 ,
∂2 E(u0 ) = 0 ,
∂1 E(u0 ) = 0 .
(iv) Suppose (x0 , 0) is a regular point of E and c := U (x0 ). Then the level
set E −1 (c) slices the x-axis orthogonally.
(i) This follows from \nabla E(x, y) = y, f (x) .
(ii) This statement is a consequence of E(x, −y) = E(x, y) for (x, y) ∈ D.
(iii) This follows from Remarks 8.4(e) and (f).
240
(iv) We apply the notation of (iii). Because
\nabla E(x0 , 0) = f (x0 ), 0) = (0, 0) ,
we see (8.22) is satisfied with u0 := (x0 , 0) and g(s) := ϕ(s) + x0 , s for s ∈ (−ε, ε).
Further, it follows from Example 3.6(c) that
0 = \nabla E(x0 , 0) g(0) =
f (x0 ), 0
ϕ(0), 1
= f (x0 )ϕ(0) .
Therefore ϕ(0) = 0, that is, the tangent to E −1 (c) at (x0 , 0) is parallel to the y-axis (see
Remark IV.1.4(a)).
Show that the system of equations
x2 + y 2 − u2 − v = 0 ,
x + 2y 2 + 3u2 + 4v 2 = 1
can be solved near (1/2, 0, 1/2, 0) for (u, v). What are the first derivatives of u and v in
(x, y)?
Suppose E, F , G and H are Banach spaces and X is open in H. Further suppose
A ∈ C 1 X, L(E, F ) and B ∈ C 1 X, L(E, G)
A(x), B(x) ∈ Lis(E, F × G)
Finally, suppose (f, g) ∈ F × G and
ϕ: X →E ,
x → A(x), B(x)
(f, g) .
∂ϕ(x)h = −S(x)∂A(x) h, ϕ(x) − T (x)∂B(x) h, ϕ(x)
for (x, h) ∈ X × H ,
where, for x ∈ X,
S(x) := A(x), B(x)
F × {0}
T (x) := A(x), B(x)
{0} × G .
Determine the general solution of the scalar linear inhomogeneous differential equation
x = a(t)x + b(t)
with time-dependent coeﬃcients a, b ∈ C(J, R).
4 Suppose D is open in R and f ∈ C(D, R). Show that the similarity differential
equation
x = f (x/t)
is equivalent to the separable differential equation
y = f (y) − y
by using the transformation y := x/t.
What is the general solution of x = (x2 + tx + t2 )/t2 ? (Hint: Exercise 4.)
Determine the solution of the initial value problem x + tx2 = 0, x(0) = 2.
Suppose a, b ∈ C(J, R) and α = 1. Show that the Bernoulli differential equation
x = a(t)x + b(t)xα
becomes linear differential equation
y = (1 − α) a(t)y + b(t)
through the transformation y := x1−α .
Calculate the general solution u of the logistic differential equation
x = (α − βx)x
for α, β > 0 .
Also find limt→t+ u(t) and the turning points of u.
Using the substitution y = x/t, determine the general solution to x = (t + x)/(t − x).
110 Suppose f ∈ C (D, E), and let u : J(ξ) → R denote the noncontinuable solution of
Determine D(f ) :=
x(0) = ξ .
(t, ξ) ∈ R × D ; t ∈ J(ξ)
if f is given through
x → x1+α ,
E→E ,
x → Ax ,
x → 1 + x2 .
α≥0;
A ∈ L(E) ;
11 Prove the error estimate of Remark 8.14(b). (Hint: With the notation of the proof
Theorem 8.13, it follows by induction that
um+1 (t) − um (t) ≤ M Lm
and consequently um+1 − um
|t − t0 |m+1
(m + 1)!
< M α2−m /(m + 1)! for m ∈ N.)
Suppose A ∈ C J, L(E) and b ∈ C(J, E). Show that the initial value problem
x = A(t)x + b(t) ,
has a unique global solution for every (t0 , x0 ) ∈ J × E. (Hints: Iterate an integral
equation, and use Exercise IV.4.1 and t0 t0 dσ ds = (t − t0 )2 /2.)
242
9 Manifolds
From Remark 8.5(e), we know that the solution set of nonlinear equations near
regular points can be described by graphs. In this section, we will more precisely
study the subsets of Rn that can be represented locally through graphs; namely,
we study submanifolds of Rn . We introduce the important concepts of regular
parametrization and local charts, which allow us to locally represent submanifolds
using functions.
• In this entire section, q belongs to N× ∪ {∞}.
Submanifolds of Rn
A subset M of Rn is said to be an m-dimensional C q submanifold of Rn if, for
every x0 ∈ M , there is in Rn an open neighborhood U of x0 , an open set V in Rn ,
and a ϕ ∈ Diff q (U, V ) such that ϕ(U ∩ M ) = V ∩ Rm × {0} .
One- and two-dimensional submanifolds of Rn are called (imbedded) curves in Rn
and (imbedded) surfaces in Rn , respectively. Submanifolds of Rn of dimension
n − 1 (or codimension 1) are called (imbedded) hypersurfaces (in Rn ). Instead
of C q submanifold of Rn we will often — especially when the “surrounding space”
Rn is unimportant — simply say C q manifold.1
An m-dimensional submanifold of Rn is defined locally in Rn , that is, in terms
of open neighborhoods. Up to small deformations, each of these neighborhood lies
in Rn in the same way that Rm is contained within Rn as a vector subspace.
9.1 Examples (a) A subset X of Rn is an n-dimensional C ∞ submanifold of Rn
if and only if X is open in Rn .
Proof If X is an n-dimensional C ∞ submanifold of Rn and x0 ∈ X, then there is an
open neighborhood U of x0 , an open set V in Rn , and a ϕ ∈ Diff ∞ (U, V ) such that
ϕ(U ∩ X) = V . Therefore U ∩ X = ϕ−1 (V ) = U . Thus U belongs to X, which shows
that X is open.
Suppose now X is open in Rn . We set U := X, V := X, and ϕ := idX . We then
see X is an n-dimensional C ∞ submanifold of Rn .
1 Note that the empty set is a manifold of every dimension ≤ n. The dimension of a nonempty
manifold is uniquely determined, as Remark 9.16(a) will show.
VII.9 Manifolds
(b) Let M := {x0 , . . . , xk } ⊂ Rn . Then M is a 0-dimensional C ∞ submanifold
Proof We set α := min{ |xi − xj | ; 0 ≤ i, j ≤ k, i = j } and choose y ∈ M . Then
B(y, α) is an open neighborhood of y in Rn , and for ϕ(x) := x − y such that x ∈ B(y, α),
we have ϕ ∈ Diff ∞ B(y, α), αB and ϕ B(y, α) ∩ M = {0}.
(c) Suppose ψ ∈ Diff q (Rn , Rn ) and M is an m-dimensional C q submanifold of Rn .
Then ψ(M ) is an m-dimensional C q submanifold of Rn .
We leave this to you as an exercise.
(d) Every C q submanifold of Rn is also a C r submanifold of Rn for 1 ≤ r ≤ q.
(e) The diffeomorphism ϕ and the open set V of Rn in the above definition can
be chosen so that ϕ(x0 ) = 0.
It suﬃces to combine the given ϕ with the C ∞ diffeomorphism y → y − ϕ(x0 ).
Graphs
The next theorem shows that graphs are manifolds and thus furnish a large class
of examples.
9.2 Proposition Suppose X is open in Rm and f ∈ C q (X, Rn ). Then graph(f ) is
an m-dimensional C q submanifold of Rm+n .
We set U := X × Rn and consider
(x, y) → x, y − f (x) .
Then we have ϕ ∈ C q (U, Rm × Rn ) with im(ϕ) = U . In addition, ϕ : U → U is
bijective with ϕ−1 (x, z) = x, z + f (x) . Therefore ϕ is a C q diffeomorphism of U
onto itself and ϕ U ∩ graph(f ) = X × {0} = U ∩ Rm × {0} .
The regular value theorem
The next theorem gives a new interpretation of Remark 8.5(f). It provides one of
the most natural ways to understand submanifolds.
9.3 Theorem (regular value) Suppose X is open in Rm and c is a regular value of
f ∈ C q (X, Rn ). Then f −1 (c) is an (m − n)-dimensional C q submanifold of Rm .
This follows immediately from Remark 8.5(f) and Proposition 9.2.
9.4 Corollary Suppose X is open in Rn and f ∈ C q (X, R). If \nabla f (x) = 0 for
x ∈ f −1 (c), the level set f −1 (c) of f is a C q hyperplane of Rn .
244
See Remark 8.5(c).
9.5 Examples
(a) The function
(x, y) → x2 − y 2
has (0, 0) as its only critical point. Its level
sets are hyperbolas.
(b) The (Euclidean) n-sphere S n := { x ∈ Rn+1 ; |x| = 1 } is a C ∞ hypersurface
in Rn+1 of dimension n.
Proof The function f : Rn+1 → R, x → |x|2 is smooth, and S n = f −1 (1). Because
\nabla f (x) = 2x, we know 1 is a regular value of f . The claim then follows from Corollary 9.4
(c) The orthogonal group2 O(n) := { A ∈ Rn×n ; A A = 1n } is a C ∞ submanifold of Rn×n of dimension n(n − 1)/2.
Proof (i) From Exercise 1.5, we know (because A = A∗ ) that A A is symmetric for
every A ∈ Rn×n . It is easy to verify that Rn×n has dimension n(n + 1)/2 (which is the
number of entries on and above the diagonal of an (n × n) matrix). For the map
we have O(n) = f
A→A A,
(1n ). First, we observe that
(A, B) → A B
is bilinear and consequently smooth. Then because f (A) = g(A, A), the map f is also
smooth. Further, we have (see Proposition 4.6)
∂f (A)B = A B + B A
for A, B ∈ Rn×n .
(ii) Suppose A ∈ f −1 (1n ) = O(n) and S ∈ Rn×n . For B := AS/2, we then have
∂f (A)B =
A AS + SA A = S
because A A = 1n . Therefore ∂f (A) is surjective for every A ∈ f −1 (1n ). Thus 1n is
a regular value of f . Because dim(Rn×n ) = n2 and n2 − n(n + 1)/2 = n(n − 1)/2, the
claim follows from Theorem 9.3.
The immersion theorem
Suppose X is open in Rm . The map f ∈ C 1 (X, Rn ) is called an immersion
(of X in Rn ) if ∂f (x) ∈ L(Rm , Rn ) is injective for every x ∈ X. Then f is a
regular parametrization of F := f (X). Finally, F is an m-dimensional (regular)
parametrized hypersurface, and X is its parameter domain. A 1-dimensional
or 2-dimensional parametrized hypersurface is a (regular) parametrized curve or
(regular) parametrized surface, respectively.
2 Here
1n denotes the identity matrix in Rn×n . For more on O(n), see also Exercises 1 and 2.
9.6 Remarks and Examples
(a) If f ∈ C 1 (X, Rn ) is an immersion, then m ≤ n.
Proof For n < m, that there is no injection A ∈ L(Rm , Rn ) follows immediately from
the rank formula (2.4).
(b) For every ∈ N× , the restriction of R → R2 , t → cos( t), sin( t) to (0, 2π)
is a C ∞ immersion. The image of [0, 2π) is the unit circle S 1 , which is traversed
times.
(c) The map (−π, π) → R2 , t → (1 + 2 cos t)(cos t, sin t) is a smooth immersion.
The closure of its image is called the lima¸on of Pascal.
(d) It is easy to see that (−π/4, π/2) → R2 , t → sin 2t(− sin t, cos t) is an injective
C ∞ immersion.
for (b)
for (c)
for (d)
The next theorem shows that m-dimensional parametrized hypersurfaces are
represented locally by manifolds.
9.7 Theorem (immersion) Suppose X is open in Rm and f ∈ C q (X, Rn ) is an
immersion. Then there is for every x0 ∈ X an open neighborhood X0 in X such
that f (X0 ) is an m-dimensional C q submanifold of Rn .
Proof (i) By permuting the coordinates, we can assume without loss of generality
that the first m rows of the Jacobi matrix ∂f (x0 ) are linearly independent.
(x0 ) = 0 .
(ii) We consider the open set X × Rn−m in Rn and the map
(x, y) → f (x) + (0, y) .
Obviously, ψ belongs to the class C q , and ∂ψ(x0 , 0) has the matrix
∂ψ(x0 , 0) =
A := ⎣
∂1 f m
1n−m
∂1 f m+1
⎦ (x0 ) and B := ⎣
· · · ∂m f m+1
⎦ (x0 ) .
246
Therefore ∂ψ(x0 , 0) belongs to Laut(Rn ), and thus
det ∂ψ(x0 , 0) = det A =
Consequently the inverse function theorem (Theorem 7.3) says there are open
neighborhoods V ∈ URn (x0 , 0) and U ∈ URn ψ(x0 , 0) with ψ |V ∈ Diff q (V, U ).
We now set Φ := (ψ |V )−1 ∈ Diff q (U, V ) and X0 :=
X0 is an open neighborhood of x0 in Rm with
Φ U ∩ f (X0 ) = Φ ψ X0 × {0}
x ∈ Rm ; (x, 0) ∈ V . Then
= X0 × {0} = V ∩ Rm × {0} .
9.8 Corollary Suppose I is an open interval and γ ∈ C q (I, Rn ). Further let t0 ∈ I
and γ(t0 ) = 0. Then there is an open subinterval I0 of I with t0 ∈ I0 such that
the trace of γ |I0 is an imbedded C q curve in Rn .
This follows immediately Theorem 9.7.
9.9 Remarks (a) The smooth path
γ : R → R2 ,
t → (t3 , t2 )
satisfies γ(t) = 0 for t = 0. At t = 0, the
derivative of γ vanishes. The trace of γ,
the Neil parabola, is “not smooth” there
but rather comes to a point.
(b) Let f ∈ C q (X, Rn ) be an immersion. Then f (X) is not generally a C q submanifold of Rn because f (X) can have “self intersections”; see Example 9.6(c).
(c) The immersion given in Example 9.6(c) is not injective. Also, images of injective immersions are generally not submanifolds; see Example 9.6(d) and Exercise 16. On the other hand, Example 9.6(b) shows that images of noninjective
immersions can indeed be submanifolds.
(d) Suppose X is open in Rm = Rm × {0} ⊂ Rn , and f ∈ C q (X, Rn ) is an
immersion. Then for every x0 ∈ X, there is an open neighborhood V of (x0 , 0)
in Rn , an open set U in Rn , and a ψ ∈ Diff q (V, U ) such that ψ(x, 0) = f (x) for
x ∈ X with (x, 0) ∈ V .
This follows immediately from the proof of Theorem 9.7.
Embeddings
Suppose g : I → R2 is the injective C ∞ immersion of Example 9.6(d). We have
already determined that S = im(g) represents an embedded curve in R2 . This
raises the question, What other properties must an injective immersion have so
that its image is a submanifold? If one analyzes the example above, it is easy to see
that g −1 : S → I is not continuous. Therefore the map g : I → S is not topological.
Indeed, the next theorem shows that if an injective immersion has a continuous
inverse, its image is a submanifold. We say a (C q ) immersion f : X → Rn is a
(C q ) embedding of X in Rn if f : X → f (X) is topological.3
9.10 Proposition Suppose X is open in Rm and f ∈ C q (X, Rn ) is an embedding.
Then f (X) is an m-dimensional C q submanifold of Rn .
Proof We set M := f (X) and choose y0 ∈ M . According to Theorem 9.7,
x0 := f −1 (y0 ) has an open neighborhood X0 in X such that M0 := f (X0 ) is an
m-dimensional submanifold of Rn .
Hence there are open neighborhoods U1 of y0 and V1 of 0 in Rn as well as a C q
diffeomorphism Φ from U1 to V1 such that Φ(M0 ∩U1 ) = V1 ∩ Rm × {0} . Because
f is topological, M0 is open in M . Hence there is an open set U2 in Rn such that
M0 = M ∩ U2 (see Proposition III.2.26). Therefore U := U1 ∩ U2 is an open
neighborhood of y0 in Rn , and V := Φ(U ) is an open neighborhood of 0 in Rn
with Φ(M ∩ U ) = V ∩ Rm × {0} . The claim follows because this holds for every
y0 ∈ M .
3 Naturally, f (X) carries with it the induced topology from Rn . If the context makes the
meaning of X and Rn unambiguous, we speak in short of an embedding.
248
9.11 Examples
(a) (spherical coordinates)
f 3 : R3 → R3 ,
(r, ϕ, ϑ) → (x, y, z)
be defined through
x = r cos ϕ sin ϑ ,
y = r sin ϕ sin ϑ ,
and let V3 := (0, ∞) × (0, 2π) × (0, π). Then g3 := f3 |V3 is a C ∞ embedding
of V3 in R3 , and F3 = g3 (V3 ) = R3 \H3 , where H3 denotes the closed half plane
R+ × {0} × R.
If one restricts (r, ϕ, ϑ) to a subset of V3
of the form
(r0 , r1 ) × (ϕ0 , ϕ1 ) × (ϑ0 , ϑ1 ) ,
one gets a regular parametrization of a “spherical shell”.
Consider f3 V 3 = R3 . We also have that f3 (W3 ) = R3 {0} × {0} × R for
W3 := (0, ∞) × [0, 2π) × (0, π) and that f3 maps W3 bijectively onto R3 {0} ×
{0} × R . Thus, by virtue of (9.1), every point (x, y, z) ∈ R3 {0} × {0} × R can
be described uniquely in the spherical coordinates (r, ϕ, ϑ) ∈ W3 . In other words,
f3 |W3 is a parametrization of R3 except for the z-axis. However, W3 is not open
in R3 .
Proof Obviously f3 ∈ C ∞ (R3 , R3 ), and g3 : V3 → F3 is topological (see Exercise 11).
Also, we find
cos ϕ sin ϑ
∂f3 (r, ϕ, ϑ) = ⎣ sin ϕ sin ϑ
−r sin ϕ sin ϑ
r cos ϕ sin ϑ
r sin ϕ cos ϑ ⎦ .
−r sin ϑ
The determinant of ∂f3 can be calculated by expanding along the last row, giving the
value −r 2 sin ϑ = 0 for (r, ϕ, ϑ) ∈ V3 . Hence g3 is an immersion and therefore an embedding. The remaining statements are clear.
(b) (spherical coordinates4 )
f 2 : R2 → R3 ,
(ϕ, ϑ) → (x, y, z)
x = cos ϕ sin ϑ ,
y = sin ϕ sin ϑ ,
and set V2 := (0, 2π) × (0, π). Then the restriction g2 := f2 |V2 is a C ∞ embedding
of V2 in R3 and F2 := g2 (V2 ) = S 2 \ H3 . In other words, F2 is obtained from
S 2 by removing the half circle where the half plane H3 intersects with S 2 . Restricting (ϕ, ϑ) to a subset V2 of the form (ϕ0 , ϕ1 ) × (ϑ0 , ϑ1 ), one gets a regular
parametrization of a section of S 2 , which means that the rectangle V 2 is “curved
into (part of) a sphere” by f2 . Note also that for W2 := [0, 2π) × (0, π), we have
f2 (W2 ) = S 2\{±e3 }, where e3 is the north pole and −e3 is the south pole. Also f2
maps the parameter domain W2 bijectively onto S 2\{±e3}. Using (9.2), S 2\{±e3}
can therefore be described through spherical coordinates (ϕ, ϑ) ∈ W2 , although
f2 |W2 is not a regular parametrization of S 2 \ {±e3} because W2 is not open in
R2 .
Proof It is clear that f2 = f3 (1, ·, ·) ∈ C ∞ (R2 , R3 ) and F2 = S 2 ∩ F3 . Since F3 is open
in R3 , it follows that F2 is open in S 2 . Further, g2 = g3 (1, ·, ·) maps V2 bijectively onto
F2 , and g2 = g3 | F2 . Therefore g2 : F2 → V2 is continuous. Because
− sin ϕ sin ϑ
∂f2 (ϕ, ϑ) = ⎣
sin ϕ cos ϑ ⎦
− sin ϑ
consists of the last two columns of the regular matrix ∂f3 (1, ϕ, ϑ) , we see that ∂g2 (ϕ, ϑ)
is injective for (ϕ, ϑ) ∈ V2 . Thus g2 is a regular C ∞ parametrization.
(c) (cylindrical coordinates) Define
f : R3 → R3 ,
(r, ϕ, z) → (x, y, z)
y = r sin ϕ ,
and let V := (0, ∞) × (0, 2π) × R. Then g := f |V is a C ∞ embedding of V in R3
with g(V ) = F3 = R3 \H3 . By further restricting g to a subset of V of the form
4 Note
the reuse of this terminology.
250
R := (r0 , r1 )× (ϕ0 , ϕ1 )× (z0 , z1 ), one gets a regular parametrization of “cylindrical
shell segments”. In other words, f “bends” the rectangular volume R into this
shape.
We also have f V = R3 and
f (W ) = R3 {0}×{0}×R =: Z for
W := (0, ∞) × [0, 2π) × R, and f |W
maps W bijectively onto Z, that is,
R3 without the z-axis. Therefore
(9.4) describes Z through cylindrical
coordinates, although f |W is not a
regular parametrization of Z because
W is not open.
Proof It is obvious that g maps bijectively onto F3 , and it is easy to see that g −1 is
smooth (see Exercise 12). Also, we have
∂f (r, ϕ, z) = ⎣ sin ϕ
−r sin ϕ
0 ⎦ ,
which follows from det ∂f (r, ϕ, z) = r. Therefore f | (0, ∞) × R × R is a C ∞ immersion
of the open half space (0, ∞) × R × R in R3 , and g is a C ∞ embedding of V in R3 . The
remaining statements are also straightforward.
(d) (cylindrical coordinates)
We set r = 1 in (9.4). Then
g(1, · , ·) is a C ∞ embedding of
(0, 2π)×R in R3 , for which the
obvious analogue of (c) holds.
(e) (surfaces of rotation) Suppose J is an open interval in R and ρ, σ ∈ C q (J, R)
with ρ(t) > 0 and ρ(t), σ(t) = (0, 0) for t ∈ J. Then
r : J × R → R3 ,
(t, ϕ) → ρ(t) cos ϕ, ρ(t) sin ϕ, σ(t)
is a C q immersion of J × R in R3 .
γ : J → R3 ,
t → ρ(t), 0, σ(t)
is a C q immersion of J in R3 . The image Γ of
γ is a regularly parametrized curve that lies in the
x-z plane, and R := r(J ×R) is generated by rotating Γ about the z-axis. Thus R is called a surface
of revolution, and Γ is a meridian curve of R. If
γ is not injective, then R contains circles in planes
parallel to the x-y-axis, at which R intersects itself.
Suppose I is an open subinterval of J such that γ |I is an embedding, and K is an
open subinterval of [0, 2π]. Then r |I × K is also an embedding.
The Jacobi matrix of r is
ρ(t) cos ϕ
⎣ ρ(t) sin ϕ
σ(t)
−ρ(t) sin ϕ
ρ(t) cos ϕ ⎦ .
The determinant of the (2 × 2)-matrix obtained by discarding the last row has the value
ρ(t)ρ(t). Therefore, the matrix (9.5) has rank 2 if ρ(t) = 0. If ρ(t) = 0, then σ(t) = 0, and
at one least of the two determinants obtained by striking either the first or second row
from (9.5) is different from 0. This proves that r is an immersion. We leave remaining
statements to you.
(f ) (tori) Suppose 0 < r < a. The equation (x − a)2 + z 2 = r2 defines a circle in
the x-z plane, and, by rotating it about the z-axis,
we get a (2-)torus, T2 . For τ2 ∈ C ∞ (R2 , R3 ) with
τ2 (t, ϕ) := (a + r cos t) cos ϕ, (a + r cos t) sin ϕ, r sin t ,
252
we find that τ2 [0, 2π]2 = T2 and that τ2 |(0, 2π)2 is an embedding. The image
of the open square (0, 2π)2 under τ2 is the surface obtained by “cutting” two
circles from the 2-torus: first, x2 + y 2 = (r + a)2 in the x-y plane, and, second,
(x − a)2 + z 2 = r2 in the x-z plane.
The image of τ2 could also be described
as follows: The square [0, 2π]2 is first made
into a tube by “identifying” two of its opposite sides. Then, after twisting this tube into
a ring, its circular ends are also identified.
Proof The map t → (a + r cos t, 0, r sin t) is a C ∞ immersion of R in R3 , whose image
is the circle described by (x − a)2 + z 2 = r 2 . Therefore the claim follows easily from (e)
with ρ(t) := a + r cos t and σ(t) := r sin t.
The following converse of Proposition 9.10 shows every submanifold is locally
the image of an embedding.
9.12 Theorem Suppose M is an m-dimensional C q submanifold of Rn . Then
every point p of M has a neighborhood U in M such that U is the image of an
open set in Rm under a C q embedding.
Proof For every p ∈ M , there are open neighborhoods U of p and V of 0 in Rn ,
as well as a C q diffeomorphism Φ : U → V , such that Φ(M ∩ U ) = V ∩ Rm × {0} .
We set U := M ∩ U , V := x ∈ Rm ; (x, 0) ∈ V , and
x → Φ−1 (x, 0) .
Then U is open in M , and V is open in Rm ; also g belongs to C q (V, Rn ). In
addition, g maps the set V bijectively onto U , and rank ∂g(x) = m for x ∈ V
because ∂g(x) consists of the first m column of the regular matrix ∂Φ−1 (x, 0) .
Because g −1 = Φ|U , we clearly see that g, interpreted as a map in the topological
subspace U of Rn , is a topological map from V to U .
We now use what we have developed to describe locally an m-dimensional submanifold M of Rn using maps between open subsets of M and Rm . In Volume III,
we will learn — the help of local charts — how to describe “abstract” manifolds,
which are not embedded (a priori) in a Euclidean space. This description is largely
independent of the “surrounding space”.
Suppose M is a subset of Rn and p ∈ M . We denote by
i M : M → Rn ,
the canonical injection of M into Rn . The map ϕ is called an m-dimensional
(local) C q chart of M around p if
• U := dom(ϕ) is an open neighborhood of p in M ;
• ϕ is a homeomorphism of U onto the open set V := ϕ(U ) of Rm ;
• g := iM ◦ ϕ−1 is a C q immersion.
The set U is the charted territory of ϕ, V is the parameter range, and g is the
parametrization of U in ϕ. Occasionally, we write (ϕ, U ) for ϕ and (g, V ) for g.
An m-dimensional C q atlas for M is a family { ϕα ; α ∈ A } of m-dimensional C q
charts of M whose charted territories Uα := dom(ϕα ) cover the set M , that is,
M = α Uα . Finally, the (x1 , . . . , xm ) := ϕ(p) are the local coordinates of p ∈ U
in the chart ϕ.
The next theorem shows that a manifold can be described through charts.
This means in particular that an m-dimensional submanifold of Rn locally “looks
like Rm ”, that is, it is locally homeomorphic to an open subset of Rm .
9.13 Theorem Suppose M is an m-dimensional C q submanifold of Rn . Then, for
every p ∈ M , there is an m-dimensional C q chart of M around p. Therefore M
has an m-dimensional C q atlas. If M is compact, then M has a finite atlas.
Proof Theorem 9.12 guarantees that every p ∈ M has an m-dimensional C q
chart ϕp around it. Therefore { ϕp ; p ∈ M } is an atlas. Because the charted
territories of this atlas form an open cover of M , the last statement follows from
the definition of compactness.
9.14 Examples (a) Suppose X is open in Rn . Then X has a C ∞ atlas with only
one chart, namely, the trivial chart idX .
(b) Suppose X is open in Rm and f ∈ C q (X, Rn ). Then, according to Proposition 9.2, graph(f ) is m-dimensional C q submanifold of Rm+n , and it has an atlas
consisting only of the chart ϕ with ϕ x, f (x) = x for x ∈ X.
(c) The sphere S 2 has a C ∞ atlas with exactly two charts.
Proof In the notation of Example 9.11(b), let U2 := g2 (V2 ) and ϕ2 : U2 → V2 , and let
(x, y, z) → (ϕ, ϑ) be the map that inverts g2 . Then ϕ2 is a C ∞ chart of S 2 .
254
Now we define g2 : V2 → S 2 through V2 := (π, 3π) × (0, π) and
g2 (ϕ, ϑ) := (cos ϕ sin ϑ, cos ϑ, sin ϕ sin ϑ) .
Then g2 V2 is obtained from S 2 by removing the half circle where it intersects with the
coordinate half plane (−R+ ) × R × {0}. Obviously U2 ∪ U2 = S 2 with U2 := g2 V2 , and
the proof of Example 9.11(b) shows that ϕ2 := g2 is a C ∞ chart of S 2 .
(d) For every n ∈ N, the n-sphere S n has an atlas with exactly two smooth
charts. For n ≥ 1, these charts are the stereographic projections ϕ± , where ϕ+
[ϕ− ] assigns to every p ∈ S n\{en+1 } [ p ∈ S n\{−en+1 }] the “puncture point”, that
is, the point where the line connecting the north pole en+1 [south pole −en+1 ] to
p intersects the equatorial hyperplane Rn × {0}.
Proof We know from Example 9.5(b) that S n is an n-dimensional C ∞ submanifold of
Rn . When n = 0, the S n consists only of the two points ±1 in R, and the statement is
trivial.
Suppose therefore n ∈ N× . The line t → tp±(1−t)en+1 through p and ±en+1 ∈ S n
intersects the hyperplane xn+1 = 0 when t = 1/(1 ∓ pn+1 ). Therefore the puncture point
has the coordinates x = p /(1 ∓ pn+1 ) ∈ Rn , where p = (p , pn+1 ) ∈ Rn × R. These define
the maps ϕ± : S n \{±en+1 } → Rn , p → x, and they are obviously continuous.
To calculate their inverses, we consider the lines
t → t(x, 0) ± (1 − t)en+1
through (x, 0) ∈ Rn × R and ±en+1 ∈ S n . They intersect S n \{±en+1 } when t > 0 and
t2 |x|2 + (1 − t)2 = 1 and, therefore, when t = 2/(1 + |x|2 ). With this we get
ϕ−1 (x) =
2x, ±(|x|2 − 1)
1 + |x|2
Thus g± := i S n ◦ ϕ−1 belongs to C ∞ (Rn , Rn+1 ). That rank ∂g± (x) = n for x ∈ Rn is
checked easily.
(e) The torus T2 is a C ∞ hypersurface in R3 and has an atlas with three charts.
Suppose X := R3 \{0} × {0} × R and
(x, y, z) →
x2 + y 2 − a
+ z 2 − r2 .
Then f ∈ C ∞ (X, R), and 0 is a regular value of f . One verifies easily that f −1 (0) = T2 .
Therefore, according to Theorem 9.3, T2 is a smooth surface in R3 . The map ϕ that
inverts τ2 | (0, 2π)2 is a two-dimensional C ∞ chart of T2 . A second such chart ϕ serves
as the map that inverts τ2 | (π, 3π)2 . Finally, we define a third chart ϕ as the inverse of
τ2 | (π/2, 5π/2)2 , and then {ϕ, ϕ, ϕ} is an atlas for T2 .
Change of charts
The local geometric meaning of a curve or a surface (generally, a submanifold of
Rn ) is independent of its description via local charts. For concrete calculations,
it is necessary to work using a specific chart. As we move around the manifold,
our calculation may take us to the boundary of the chart we are using, forcing a
“change of charts”. Thus, we should understand how that goes. In addition to this
practical justification, understanding changes of charts will help us understand how
the local description of a manifold is “put together” to form the global description.
Suppose (ϕα , Uα ) ; α ∈ A is an m-dimensional C q atlas for M ⊂ Rn . We
call the maps5
ϕβ ◦ ϕ−1 : ϕα (Uα ∩ Uβ ) → ϕβ (Uα ∩ Uβ ) for α, β ∈ A ,
transition functions. They specify how the charts in the atlas {ϕα ; α ∈ A} are
“stitched together”.
9.15 Proposition If (ϕα , Uα ) and (ϕβ , Uβ ) are m-dimensional C q charts of a C q
manifold of dimension m, then
ϕβ ◦ ϕ−1 ∈ Diff q ϕα (Uα ∩ Uβ ), ϕβ (Uα ∩ Uβ ) ,
where (ϕβ ◦ ϕ−1 )−1 = ϕα ◦ ϕ−1 .
Proof (i) It is clear that ϕβ ◦ ϕ−1 is bijective and its inverse is ϕα ◦ ϕ−1 . Thus
it suﬃces to verify that ϕβ ◦ ϕ−1 belongs to C q ϕα (Uα ∩ Uβ ), Rm .
(ii) We set Vγ := ϕγ (Uγ ), and gγ is the parametrization belonging to (ϕγ , Uγ )
for γ ∈ {α, β}. Further suppose xγ ∈ ϕγ (Uα ∩ Uβ ) with gα (xα ) = gβ (xβ ) =: p.
Because gγ is an injective C q immersion, Remark 9.9(d) gives open neighborhoods
5 Here and nearby, we always assume that U ∩ U = ∅ when working with the transition
function ϕβ ◦ ϕ−1 .
256
Uγ of p and Vγ of (xγ , 0) in Rn , as well as a ψγ ∈ Diff q (Vγ , Uγ ) such that ψγ (y, 0) =
gγ (y) for all y ∈ Vγ with (y, 0) ∈ Vγ . We now set
x ∈ Vα ; (x, 0) ∈ Vα
∩ ϕα (Uα ∩ Uβ ) .
Clearly V is an open neighborhood of xα in Rm . Using the continuity of gα , we
can (possibly by shrinking V ) assume that gα (V ) is contained in Uβ . Therefore
ϕβ ◦ ϕ−1 (x) = ψβ ◦ gα (x)
Because gα ∈ C q (Vα , Rn ), the chain rule shows that ψβ ◦gα belongs to C q (V, Rn ).6
It therefore follows that ϕβ ◦ ϕ−1 ∈ C q (V, Rm ), because the image of ψβ ◦ gα
coincides with that of ϕβ ◦ ϕα and therefore lies in R . Because this holds for
every xα ∈ ϕα (Uα ∩ Uβ ) and because belonging to the class C q is a local property,
the claim is proved.
9.16 Remarks (a) The dimension of a submanifold of Rn is unique. Thus for
a given m-dimensional manifold, it makes sense to speak simply of its “charts”
instead of its “m-dimensional charts”.
Proof Suppose M is an m-dimensional C q submanifold of Rn and p ∈ M . Then,
according to Theorem 9.13, there is an m-dimensional C q chart (ϕ, U ) around p. Let
(ψ, V ) be an m -dimensional C q chart around p. Then the proof of Proposition 9.15
shows that
ψ ◦ ϕ−1 ∈ Diff q ϕ(U ∩ V ), ψ(U ∩ V ) ,
where ϕ(U ∩ V ) is open in Rm and ψ(U ∩ V ) is open in Rm . This implies m = m (see
(7.2)), as desired.
(b) Through the charts (ϕ1 , U1 ), the charted territories U1 can described using
the local coordinates (x1 , . . . , xm ) = ϕ1 (q) ∈ Rm for q ∈ U . If (ϕ2 , U2 ) is a second
chart, then U2 has its own local coordinates (y 1 , . . . , y m ) = ϕ2 (q) ∈ Rm . Thus U1 ∩
U2 has a description in two coordinate systems (x1 , . . . , xm ) and (y 1 , . . . , y m ). The
transition function ϕ2 ◦ ϕ−1 is nothing other than the coordinate transformation
(x1 , . . . , xm ) → (y 1 , . . . , y m ) that converts the coordinates x into the coordinates
1 Let H be a finite-dimensional real Hilbert space, and let ϕ : H → H be an isometry
with ϕ(0) = 0. Verify that
(a) ϕ is linear;
(b) ϕ∗ ϕ = idH ;
6 Here and elsewhere, we often apply the same symbol for a map and its restriction to a subset
of its domain of definition.
7 Coordinate transformations will be discussed in detail in Volume III.
(c) ϕ ∈ Laut(H), and ϕ−1 is an isometry.
(a) For A ∈ Rn×n , show these statements are equivalent:
(i) A is an isometry;
(ii) A ∈ O(n);
(iii) | det A| = 1;
(iv) the column vectors a• = (a1 , . . . , an ) for k = 1, . . . , n form an ONB of Rn ;
(v) the row vectors aj = (aj , . . . , aj ) for k = 1, . . . , n form an ONB of Rn .
(b) Show that O(n) is a matrix group.
Prove the statement of Example 9.1(c).
4 Suppose M and N are respectively m and n-dimensional C q submanifolds of Rk and
R . Prove that M × N is an (m + n)-dimensional C q submanifold of Rk+ .
Decide whether Laut(Rn ) is a submanifold of L(Rn ).
Define f : R3 → R2 through
f (x, y, z) := (x2 + xy − y − z, 2x2 + 3xy − 2y − 3z) .
Show that f −1 (0) is an embedded curve in R3 .
Suppose f, g : R4 → R3 is given by
f (x, y, z, u) := (xz − y 2 , yu − z 2 , xu − yz) ,
g(x, y, z, u) := 2(xz + yu), 2(xu − yz), z 2 + u2 − x2 − y 2 .
(a) f −1 (0)\{0} is an embedded surface in R4 ;
(b) for every a ∈ R3 \{0}, g −1 (a) is an embedded curve in R4 .
Which of the sets
(a) K :=
(x, y) ∈ Rn × R ; |x|2 = y 2 ,
(x, y) ∈ K ; y > 0 ,
(c) K
(0, 0)
are submanifolds of Rn+1 ?
9 For A ∈ Rn×n , show x ∈ Rn ; (x | Ax) = 1 is a smooth hypersurface of Rn . Sketch
the curve for n = 2 and the surface for n = 3. (Hint: Exercise 4.5.)
Show for the special orthogonal group SO(n) := { A ∈ O(n) ; det A = 1 } that
(a) SO(n) is a subgroup of O(n);
(b) SO(n) is a smooth submanifold of Rn×n . What is its dimension?
Recursively define fn ∈ C ∞ (Rn , Rn ) through
f2 (y) := (y 1 cos y 2 , y 1 sin y 2 )
for y = (y 1 , y 2 ) ∈ R2 ,
258
fn+1 (y) := fn (y ) sin y n+1 , y 1 cos y n+1
for y = (y , y n+1 ) ∈ Rn × R .
Further let V2 := (0, ∞) × (0, 2π) and Vn := V2 × (0, π)n−2 for n ≥ 3.
(a) Find fn explicitly.
(b) Show that
(i) |fn (1, y 2 , . . . , y n )| = 1 and |fn (y)| = |y 1 |;
(ii) fn (Vn ) = Rn \Hn with Hn := R+ × {0} × Rn−2 ;
(iii) fn V n = Rn ;
(iv) gn := fn | Vn : Vn → Rn \Hn is topological;
(v) det ∂fn (y) = (−1)n (y 1 )n−1 sinn−2 (y n ) · · · · · sin(y 3 ) for y ∈ Vn and n ≥ 3.
Thus gn is a C ∞ embedding of Vn in Rn . The coordinates induced by fn are called
n-dimensional polar (or spherical) coordinates (see Example 9.11(a)).
12 Denote by f : R3 → R3 the cylindrical coordinates in R3 (see Example 9.11(c)).
Further let V := (0, ∞) × (0, 2π) × R and g := f | V . Prove that
(a) g is an C ∞ embedding of V in R3 ;
(b) g(1, ·, ·) is a C ∞ embedding of (0, 2π) × R in R3 ;
(c) f (V ) = R3 \H3 (see Exercise 11);
(d) f ( V = R3 .
(a) Show that the elliptical cylinder8
Ma,b :=
(x, y, z) ∈ R3 ; x2 /a2 + y 2 /b2 = 1
for a, b ∈ (0, ∞) ,
is a C ∞ hypersurface in R3 .
(b) Suppose W := (0, 2π) × R and
f1 : W → R3
f2 : W → R
for (ϕ, z) → (a cos ϕ, b sin ϕ, z) ,
for (ϕ, z) → (−a sin ϕ, b cos ϕ, z) .
Further let Uj := fj | W and ϕj := (fj | Uj )−1 for j = 1, 2. Show that
atlas of M , and calculate the transition function ϕ1 ◦ ϕ−1 .
(ϕ1 , ϕ2 )
is an
14 Suppose M is a nonempty compact m-dimensional C 1 submanifold of Rn with m ≥ 1.
Prove that M does not have an atlas with only one chart.
15 Show that the surface given by the last map of Example 9.11(e) is not a submanifold
of R3 .
For g : (−π/4, π/2) → R2 , t → sin(2t)(− sin t, cos t) verify that
(a) g is an injective C ∞ immersion;
(b) im(g) is not an embedded curve in R2 .
17 Calculate the transition function ϕ− ◦ ϕ−1 for the atlas {ϕ− , ϕ+ }, where ϕ± are the
stereographic projections of S n .
is the usual cylinder.
18 Suppose M and N are respectively submanifolds of Rm and Rn , and suppose that
(ϕα , Uα ) ; α ∈ A and (ψβ , Vβ ) ; β ∈ B are atlases of M and N . Verify that
(ϕα × ψβ , Uα × Vβ ) ; (α, β) ∈ A × B
where ϕα × ψβ (p, q) := ϕα (p), ψβ (q) , is an atlas of M × N .
260
We now introduce linear structures, which make it possible to assign derivatives
to maps between submanifolds. These structures will be described using local
coordinates, which are of course very helpful in concrete calculations.
To illustrate why we need such a structure, we consider a real function
ıve
f : S 2 → R on the unit sphere S 2 in R3 . The na¨ attempt to define the derivative of f through a limit of difference quotients is immediately doomed to fail:
For p ∈ S 2 and h ∈ R3 with h = 0, p + h does not generally lie in S 2 , and thus
the “increment” f (p + h) − f (p) of f is not even defined at the point p.
The tangential in Rn
We begin with the simple situation of an n-dimensional submanifold of Rn (see
Example 9.1(a)). Suppose X is open in Rn and p ∈ X. The tangential space Tp X
of X at point p is the set {p} × Rn equipped with the induced Euclidean vector
space structure of Rn = Rn , (·|·) , that is,
(p, v) + λ(p, w) := (p, v + λw) and (p, v) (p, w)
:= (v |w)
for (p, v), (p, w) ∈ Tp X and λ ∈ R. The element (p, v) ∈ Tp X is called a tangential
vector of X at p and will also be denoted by (v)p . We call v the tangent part of
(v)p .1
10.1 Remark The tangential space Tp X and Rn are isometric isomorphic Hilbert
spaces. One can clearly express the isometric isomorphism by “attaching” Rn at
the point p ∈ X.
Suppose Y is open in R and f ∈ C 1 (X, Y ). Then the linear map
Tp f : Tp X → Tf (p) Y ,
(p, v) → f (p), ∂f (p)v
is called the tangential of f at the point p.
10.2 Remarks
(a) Obviously
Tp f ∈ L(Tp X, Tf (p) Y ) .
Because v → f (p)+∂f (p)v approximates (up to first order) the map f at the point
p when v is in a null neighborhood of Rn , we know im(Tp f ) is a vector subspace
of Tf (p) Y , which approximates (to first order) f (X) at the point f (p).
1 We distinguish “tangential” from “tangent”: a tangential vector contains the “base point”
p, whereas its tangent part does not.
VII.10 Tangents and normals
(b) If Z is open in Rs and g ∈ C 1 (Y, Z), the chain rule reads
Tp (g ◦ f ) = Tf (p) g ◦ Tp f .
In other words, the following are commutative diagrams.
E Tf (p) Y
Tp (g ◦ f ) d
  Tf (p) g
Tg(f (p))
This follows from the chain rule for C 1 maps (see Theorem 3.3).
(c) For f ∈ Diff 1 (X, Y ), we have
Tp f ∈ Lis(Tp X, Tf (p) Y ) and (Tp f )−1 = Tf (p) f −1
This is a consequence of (b).
The tangential space
• M is an m-dimensional C q submanifold of Rn ;
p ∈ M , and (ϕ, U ) is a chart of M around p;
(g, V ) is the parametrization belonging to (ϕ, U ).
The tangential space Tp M of M at the point p is the image of Tϕ(p) V under Tϕ(p) g,
and therefore Tp M = im(Tϕ(p) g). The elements of Tp M are called tangential
vectors of M at p, and T M := p∈M Tp M is the tangential bundle2 of M .
2 For p ∈ M , we have the inclusion T M ⊂ M × Rn . Therefore T M is a subset of M × Rn . We
use the simpler term “tangent” here because it is more standard and because there is no need
for the “bundle” of tangent parts.
262
10.3 Remarks
chart (ϕ, U ).
(a) Tp M is well defined, that is, it is independent of the chosen
Proof Suppose (ϕ, U ) is another chart of M around p with associated parametrization
(g, V ). We can assume without loss of generality that the charted territories U and U
coincide. Otherwise consider U ∩ U . Due to the chain rule (Remark 10.2(b)), the diagram
r Tϕ(p) g
Tϕ(p) g ¨¨
Tϕ(p) (ϕ ◦ ϕ−1 )
E Tϕ(p) V
Tϕ(p) V
commutes. Because of Proposition 9.15 and Remark 10.2(c), Tϕ(p) (ϕ ◦ ϕ−1 ) is an isomorphism, which then easily implies the claim.
(b) If M is an subset of Rn , then the above definition of Tp M agrees with that of
Remark 10.1. In particular, we have T M = M × Rn .
(c) The tangential space Tp M is an m-dimensional vector subspace of Tp Rn and
therefore is an m-dimensional Hilbert space with the scalar product (·|·)p induced
Proof Let x0 := ϕ(p). For (v)x0 ∈ Tx0 V , we have (Tx0 g)(v)x0 = p, ∂g(x0 )v . It
therefore follows that
rank Tx0 g = rank ∂g(x0 ) = m ,
(d) Because Tϕ(p) g : Tϕ(p) V → Tp Rn is injective and Tp M = im(Tϕ(p) g), there
is exactly one A ∈ Lis(Tp M, Tϕ(p) V ) such that (Tϕ(p) g)(A) = iTp M , where iTp M
is the canonical injection of Tp M into Tp Rn . In other words, A is the inverse of
Tϕ(p) g, when Tϕ(p) g is understood as a map of Tϕ(p) V onto its image Tp M . We
call Tp ϕ := A the tangential of the chart ϕ at point p. Further, (Tp ϕ)v ∈ Tϕ(p) V
is the representation of the tangential vector v ∈ Tp M in the local coordinates
induced by ϕ. If (ϕ, U ) is another chart of M around p, then the diagram
Tϕ(p) ϕ(U )
commutes, where ∼ means “isomorphic”.
Proof Without loss of generality, assume U = U . For g := iM ◦ ϕ−1 , it follows from
g = (iM ◦ ϕ−1 ) ◦ (ϕ ◦ ϕ−1 ) = g ◦ (ϕ ◦ ϕ−1 ) and the chain rule of Remark 10.2(b) that
Tϕ(p) g = Tϕ(p) g Tϕ(p) (ϕ ◦ ϕ−1 ) .
From this, the definition of Tp ϕ and Tp ϕ, Remark 10.2(c), and Proposition 9.15, we get
the relation
Tp ϕ = Tϕ(p) (ϕ ◦ ϕ−1 ) Tp ϕ ,
Tp ϕ = Tϕ(p) (ϕ ◦ ϕ−1 )
(e) (the scalar product in local coordinates) Suppose x0 := ϕ(p) and
for 1 ≤ j, k ≤ m .
gjk (x0 ) := ∂j g(x0 ) ∂k g(x0 )
Then [gjk ] ∈ Rm×m is called the (first) fundamental matrix of M with respect to
the chart ϕ at p (or the parametrization g at x0 ). It is positive definite.
For v, w ∈ Tp M , we have3
(v |w)p =
gjk (x0 )v j wk ,
(10.1)
where v j and wk are respectively the components of the local representation of
the tangent part of (Tp ϕ)v and (Tp ϕ)w with respect to the standard basis.
For ξ, η ∈ Rm , we have
gjk (x0 )ξ j η k = ∂g(x0 )ξ ∂g(x0 )η .
(10.2)
In particular, because ∂g(x0 ) is injective, we have
[gjk ](x0 )ξ ξ = |∂g(x0 )ξ|2 > 0
for ξ ∈ Rm \{0} .
Thus the fundamental matrix is positive definite.
Because v = Tx0 g (Tp ϕ)v and (Tp ϕ)v =
(· | ·)p that
v j ej it follows from the definition of
(v | w)p = (Tx0 g)(Tp ϕ)v (Tx0 g)(Tp ϕ)w
Consequently (10.1) implies (10.2).
also Remark 2.17(c).
v j ej ∂g(x0 )
= ∂g(x0 )
w k ek
264
10.4 Example (the tangential space of a graph) Suppose X is open in Rm , and let
f ∈ C q (X, R ). According to Proposition 9.2, M := graph(f ) is an m-dimensional
C q submanifold of Rm × R = Rm+ . Then g(x) := x, f (x) for x ∈ X is a C q
parametrization of M , and with p = x0 , f (x0 ) ∈ M , we have
for (v)x0 ∈ Tx0 X .
(Tx0 g)(v)x0 = p, (v, ∂f (x0 )v)
p, (v, ∂f (x0 )v) ; v ∈ Rm
that is, Tp M is the graph of ∂f (x0 ) “attached to the point p = x0 , f (x0 ) ”.
The representation of Tp M in Rn for n = m + is obtained by identifying (η)p =
(p, η) ∈ Tp Rn with p + η ∈ Rn . Then it follows that
x0 , f (x0 ) , v, ∂f (x0 )v
= x0 + v, f (x0 ) + ∂f (x0 )v ∈ Rm × R = Rn ,
x, f (x0 ) + ∂f (x0 )(x − x0 ) ; x ∈ Rm
= graph x → f (x0 ) + ∂f (x0 )(x − x0 ) .
This representation shows once more that Tp M is the higher-dimensional generalization of the concept of the tangent to a curve (see Remark IV.1.4).
Suppose ε > 0 with ϕ(p) + tej ∈ V for t ∈ (−ε, ε) and j ∈ {1, . . . , m}. Then
γj (t) := g ϕ(p) + tej
is called the j-th coordinate path through p.
for t ∈ (−ε, ε) ,
10.5 Remark
For x0 := ϕ(p), we have
Tp M = span
∂1 g(x0 ) p , . . . , ∂m g(x0 )
that is, the tangential vectors at p on the coordinate
paths γj form a basis of Tp M .
For the j-th column of ∂g(x0 ) , we have
∂j g(x0 ) = ∂g(x0 )ej = γj (0) .
The remark follows because g is an immersion, and consequently the column vectors of
∂g(x0 ) are linearly independent, and dim Tp M = m.
Tangential vectors of M at p can be described as tangential vectors of regular paths
in M . The next theorem provides a geometric interpretation of the tangential
space.
10.6 Theorem For every p ∈ M , we have
(v)p ∈ Tp Rn ;
∃ ε > 0, ∃ γ ∈ C 1 (−ε, ε), Rn such that im(γ) ⊂ M, γ(0) = p, γ(0) = v .
In other words, for every (v)p ∈ Tp M ⊂ Tp Rn , there is a C 1 path in Rn passing
through p that is entirely contained in M and has (v)p as its tangential vector at
p. Every tangential vector of such a path belongs to Tp M .
Proof (i) Suppose (v)p ∈ Tp M and x0 := ϕ(p). Then there exists a ξ ∈ Rm such
that v = ∂g(x0 )ξ. Because V = ϕ(U ) at Rm is open, there is an ε > 0 such that
x0 + tξ ∈ V for t ∈ (−ε, ε). We now set γ(t) := g(x0 + tξ) for t ∈ (−ε, ε), so that
γ is a C 1 path in M with γ(0) = p and γ(0) = ∂g(x0 )ξ = v.
(ii) Suppose γ ∈ C 1 (−ε, ε), Rn for im(γ) ⊂ M and γ(0) = p. According
to Remark 9.9(d), there is an open neighborhood V of (x0 , 0) in Rn , an open
neighborhood U in Rn , and a ψ ∈ Diff q (V , U) such that ψ(x, 0) = g(x) for x ∈ V .
By shrinking ε, we can assume that im(γ) ⊂ U ∩ U . From this, it follows that
γ(t) = (g ◦ ϕ ◦ γ)(t) = (g ◦ prRm ◦ ψ −1 ◦ γ)(t) ,
and we get from the chain rule that
γ(0) = ∂g(x0 )(prRm ◦ ψ −1 ◦ γ) (0) .
266
For ξ := (prRm ◦ ψ −1 ◦ γ). (0) ∈ Rm and v := ∂g(x0 )ξ ∈ Rn , we have (v)p ∈ Tp M ,
and we are done.
If X is open in Rn and the point c ∈ R is a regular value of f ∈ C q (X, R ), we
know from Theorem 9.3 that M = f −1 (c) is a C q submanifold of Rn of dimension
(n − ). In the next theorem, we show that an analogous statement holds for the
“linearization”, that is,
Tp M = (Tp f )−1 (c) = ker(Tp f ) .
10.7 Theorem (regular value) Suppose X is open in Rn , and c ∈ R is a regular
value of f ∈ C q (X, R ). For the (n − )-dimensional C q submanifold M := f −1 (c)
of Rn , we then have Tp M = ker(Tp f ) for p ∈ M .
Proof Suppose (v)p ∈ Tp M ⊂ Tp Rn . According to Theorem 10.6, there is an
ε > 0 and a path γ ∈ C 1 (−ε, ε), Rn such that im(γ) ⊂ M , with γ(0) = p and
γ(0) = v. In particular, we have f γ(t) = c for every t ∈ (−ε, ε), and we find by
differentiating this relation that
∂f γ(0) γ(0) = ∂f (p)v = 0 .
It follows that Tp M ⊂ ker(Tp f ).
Because p is a regular point of f , we have
dim im(Tp f ) = dim im(∂f (p)) =
Consequently, the rank formula (2.4) gives
dim ker(Tp f ) = n − = dim(Tp M ) .
Therefore Tp M is not a proper vector subspace of ker(Tp f ).
Differentiable maps
Suppose N is a C r submanifold of R and 1 ≤ s ≤ min{q, r}. Also suppose
f ∈ C(M, N ) and (ψ, W ) is a chart of N around f (p). Then U ∩ f −1 (W ) is
an open neighborhood of p in M . Therefore by shrinking U , we can assume
without loss of generality that f (U ) ⊂ W . The function f is said to be (s-times)
[continuously] differentiable at p if the map
fϕ,ψ := ψ ◦ f ◦ ϕ−1 : ϕ(U ) → ψ(W )
at point ϕ(p) is (s-times) [continuously] differentiable.
We say the map f ∈ C(M, N ) is (s-times) [continuously] differentiable if f
is (s-times) [continuously] differentiable at every point M . We denote the set of
all s-times continuously differentiable functions from M to N by C s (M, N ), and
Diff s (M, N ) :=
f ∈ C s (M, N ) ; f is bijective, f −1 ∈ C s (N, M )
is the set of all C s diffeomorphisms from M to N . Finally, we say that M and N
are C s -diffeomorphic if Diff s (M, N ) is nonempty.
10.8 Remarks (a) The previous definitions are independent of the choice of charts.
Proof If (ϕ, U ) and (ψ, W ) are other charts of M around p and of N around f (p),
respectively, such that f U ⊂ W , then
fϕ,ψ = ψ ◦ f ◦ ϕ−1 = (ψ ◦ ψ −1 ) ◦ fϕ,ψ ◦ (ϕ ◦ ϕ−1 ) .
(10.3)
This, with Proposition 9.15 and the chain rule, then gives the theorem.
(b) If M and N respectively have dimension n and , that is, M is open in Rn
and N is open in R , then the above definition agrees with the one in Section 5.
(c) The function fϕ,ψ is the local representation of f in the charts ϕ and ψ, or
the local coordinate representation. In contrast with the function f , which maps
between the “curved sets” M and N , fϕ,ψ is a map between open subsets of
Euclidean spaces.
(d) It is generally not possible to define sensibly the concept of a C s map between
M and N that is coordinate independent if s > min{q, r}.
Proof This follows from (10.3), because the transition functions only belong to C q or
C r , respectively.
Suppose f : M → N is differentiable at p, and (ψ, W ) is a chart of N around
f (p) such that f (U ) ⊂ W . Then the diagram
Rm ⊃ ϕ(U )
(10.4)
E ψ(W ) ⊂ Rn
commutes, where ∼ means C 1 -diffeomorphic4 and n is the dimension of N . We
the local representation ϕϕ,id = idϕ(U ) .
268
now define the tangential Tp f of f at p by requiring the diagram
Tf (p) N
Tϕ(p) fϕ,ψ
(10.5)
Tf (p) ψ
E Tψ(f (p)) ψ(W )
commutes, where now ∼ means “isomorphic”.
10.9 Remarks (a) The tangential Tp f is coordinate independent, and Tp f ∈
L(Tp M, Tf (p) N ).
Proof Suppose ϕ, U is a chart of M around p and ψ, W is a chart around f (p) such
that f (U ) ⊂ W . Then (10.3) and the chain rule of Remark 10.2(b) gives
Tϕ(p) fϕ,ψ = Tϕ(p)
ψ ◦ ψ −1 ◦ fϕ,ψ ◦ ϕ ◦ ϕ−1
= Tψ(f (p)) ψ ◦ ψ −1 ◦ Tϕ(p) fϕ,ψ ◦ Tϕ(p) (ϕ ◦ ϕ−1 ) .
The remark then follows from Remark 10.3(d).
(b) Suppose O is another manifold, and g : N → O is differentiable at f (p). Then
we have the chain rule
(10.6)
Tp (g ◦ f ) = Tf (p) g Tp f
Tp idM = idTp M .
(10.7)
If f belongs to Diff (M, N ), then
Tp f ∈ Lis(Tp M, Tf (p) N ) and (Tp f )−1 = Tf (p) f −1
Proof The statements (10.6) and (10.7) follow easily from the commutativity of diagram (10.5) and the chain rule of Remark 10.2(b). The remaining claims are immediate
consequences of (10.6) and (10.7).
10.10 Examples (a) The canonical injection iM of M into Rn is in C q (M, Rn ),
and for ψ := idRn , we have
Tϕ(p) (iM )ϕ,ψ = Tϕ(p) g .
Thus Tp iM is the canonical injection of Tp M into Tp Rn .
This follows obviously from (iM )ϕ,ψ = iM ◦ ϕ−1 = g.
(b) Suppose X is an open neighborhood of M and f ∈ C s (X, R ). Further suppose
f (M ) ⊂ N . Then f := f M belongs to C s (M, N ) and
Tf (p) iN Tp f = Tp f Tp iM
that is, the diagram
Tp iM
Tf (p) iN
Tf (p) R
commutes.
Proof Because N is a C r submanifold of R , we can assume, possibly by shrinking W ,
that there exists an open neighborhood W of W in R and a C r diffeomorphism Ψ of W
on an open subset of R such that Ψ ⊃ ψ. Thus we find
fϕ,ψ = ψ ◦ f ◦ ϕ−1 = Ψ ◦ f ◦ g ∈ C s (V, R ) .
Because this holds for every pair of charts (ϕ, U ) of M and (ψ, W ) of N such that
f (U ) ⊂ W , we can conclude that f belongs to C s (M, N ). Because iN ◦ f = f ◦ iM , the
last part of the claim is a consequence of the chain rule.
(c) Suppose X is open in Rn , Y is open in R , and f ∈ C q (X, R ) with f (X) ⊂ Y .
Then f belongs to C q (X, Y ), where X and Y are respectively understood as nand -dimensional submanifolds of Rn and R . In addition, we have
Tp f = f (p), ∂f (p)
This is a special case of (b).
The differential and the gradient
If f : M → R is differentiable at p, then
Tp f : Tp M → Tf (p) R = f (p) × R ⊂ R × R .
With the canonical projection pr2 : R × R → R onto the second factor, we set
dp f := pr2 ◦ Tp f ∈ L(Tp M, R) = (Tp M ) ,
and call dp f differential of f at the point p. Therefore dp f is a continuous linear
form on Tp M . Because the tangential space is an m-dimensional Hilbert space,
there is, using the Riesz representation theorem, a unique vector \nabla p f := \nabla M f ∈
Tp M such that
(dp f )v = (\nabla p f |v)p for v ∈ Tp M ,
which we call the gradient of f at the point p.
270
10.11 Remarks (a) Suppose X is open in Rn and f ∈ C 1 (X, R). Then
\nabla p f = p, \nabla f (p)
where \nabla f (p) is the gradient of f at the point p, as defined in Section 2. Thus, the
two definitions of \nabla p f and \nabla f (p) are consistent.
Proof We describe the manifolds X using the trivial chart (idX , X). Then the local
representation of dp f agrees with ∂f (p). Now the claim follows from the definition of
(· | ·)p and that of the gradient.
(b) (representation in local coordinates) Suppose f ∈ C 1 (M, R), and fϕ :=
f ◦ ϕ−1 is the local representation of f in the charts ϕ of M and idR of R, that is,
fϕ = fϕ,idR . Further suppose [g jk ] is the inverse of the fundamental matrix [gjk ]
with respect to ϕ at p. Then, in the local coordinates induced by ϕ, the local
representation (Tp ϕ)\nabla p f of the gradient \nabla p f ∈ Tp M 5 has tangent part
g 1k (x0 )∂k fϕ (x0 ), . . . ,
g mk (x0 )∂k fϕ (x0 )
where x0 := ϕ(p).
Using the definitions of \nabla p f and dp f , it follows from Proposition 2.8(i) that
(\nabla p f | v)p = (dp f )v = ∂(f ◦ ϕ−1 )(x0 )(Tp ϕ)v =
∂j fϕ (x0 )v j
(10.8)
for v ∈ Tp M and (Tp ϕ)v =
10.3(e) that
v j ej . For (Tp ϕ)\nabla p f =
wj ej , we get using Remark
gjk (x0 )wj v k
(\nabla p f | v)p =
(10.9)
Now (10.8), (10.9), and the symmetry of [gjk ] imply
gjk (x0 )wk = ∂j fϕ (x0 )
for 1 ≤ j ≤ m ,
and therefore, after multiplication by [gjk ]−1 = [g jk ], we are done.
The next theorem gives a necessary condition for p ∈ M to be an extremal
point of a differentiable real-valued function on M . This generalizes Theorem 3.13.
10.12 Theorem If p ∈ M is a local extremal point of f ∈ C 1 (M, R), then \nabla p f = 0.
Proof Because fϕ ∈ C 1 (V, R) has a local extremum at x0 = ϕ(p), we have
∂fϕ (x0 ) = 0 (see Proposition 2.5 and Theorem 3.13). Thus, for v ∈ Tp M and the
tangent part ξ of (Tp ϕ)v, we have
(\nabla p f |v)p = (dp f )v = ∂fϕ (x0 )ξ = 0 .
this to formula (2.6).
Normals
The orthogonal complement of Tp M at Tp Rn is called the normal space of M at p
and will be denoted by Tp M . The vectors in Tp M are the normals of M at p,
and T M := p∈M Tp M is the normal bundle of M .
10.13 Proposition Suppose X is open in Rn , and c is a regular value of f ∈
C q (X, R ). If M := f −1 (c) is not empty, {\nabla p f 1 , . . . , \nabla p f } is a basis of Tp M .
Proof (i) According to Remark 10.11(a), \nabla p f j has the tangent part \nabla f j (p).
From the surjectivity of ∂f (p), it follows that the vectors \nabla f 1 (p), . . . , \nabla f (p) in
Rn — and thus \nabla p f 1 , . . . , \nabla p f in Tp Rn — are linearly independent.
(ii) Suppose v ∈ Tp M . From Theorem 10.6, we know that there is an ε > 0
and a γ ∈ C 1 (−ε, ε), Rn such that im(γ) ⊂ M , γ(0) = p, and γ(0) = v. Because
f j γ(t) = cj for t ∈ (−ε, ε), it follows that
0 = (f j ◦ γ) (0) = \nabla f j γ(0)) γ(0) = (\nabla p f j |v)p
Because this is true for every v ∈ Tp M , we see \nabla p f 1 , . . . , \nabla p f belongs to Tp M .
Then dim(Tp M ) = n − dim(Tp M ) = .
10.14 Examples
(a) For the sphere S n−1 in Rn , we have
S n−1 = f −1 (1) when
x → |x|2 .
Because \nabla p f = (p, 2p), we have
Tp S n−1 = (p, Rp) .
(b) For X := R3
{0} × {0} × R and
x2 + x2 − 2
f (x1 , x2 , x3 ) :=
+ x2 ,
we have f ∈ C ∞ (X, R), and6 f −1 (1) =
T2 . Then normal vector at the point p =
(p1 , p2 , p3 ) is given by \nabla p f = p, 2(p − k) ,
p2 + p2
This follows from Example 9.14(e) and recalculation.
Example 9.11(f).
272
(c) Suppose X is open in Rn and f ∈ C q (X, R). Then a unit normal, that is,
a normal vector of length 1, is given on M := graph(f ) at p := x, f (x) by
νp := p, ν(x) ∈ Tp Rn+1 , where
ν(x) :=
−\nabla f (x), 1
|\nabla f (x)|2
∈ Rn+1 .
For the parametrization g defined by g(x) := x, f (x) for x ∈ X, we have
∂j g(x) := ej , ∂j f (x)
1≤j≤n.
Clearly ν(x) is a vector of length 1 in Rn+1 and is orthogonal to every vector ∂j g(x).
Therefore the claim follows from Remark 10.5.
Constrained extrema
In many applications, we seek to find extremal points of a function F : Rn → R
subject to some constraint. In other words, the constraints mean that not every
point of Rn is allowed to extremize F , and instead the sought-for point must belong
to a subset M . Very often these constraints are described by equations of the form
h1 (p) = 0, . . . , h (p) = 0, and the solution set of these equations is a submanifold,
namely, the set M . If F |M has a local extremum at p ∈ M , then p is called an
extremal point of F under the constraints h1 (p) = 0, . . . , h (p) = 0.
The next theorem, which is important in practice, specifies a necessary condition for a point to be a constrained extremal point.
10.15 Theorem (Lagrange multipliers) Suppose X is open in Rn and we are given
functions F, h1 , . . . , h ∈ C 1 (X, R) for < n. Also suppose 0 is a regular value of
the map h := (h1 , . . . , h ), and suppose M := h−1 (0) is not empty. If p ∈ M is
an extremal point of F under the constraints h1 (p) = 0, . . . , h (p) = 0, there are
unique real numbers λ1 , . . . , λ , the Lagrange multipliers, for which p is a critical
point of
λj hj ∈ C 1 (X, R) .
Proof From the regular value theorem, we know that M is an (n− )-dimensional
C 1 submanifold of Rn . According to Example 10.10(b), f := F |M belongs to
C 1 (M, R), and because iR = idR , we have Tp f = Tp F Tp iM . From this, it follows
that dp f = dp F Tp iM , and therefore
\nabla p f (v)p
= dp F Tp iM (v)p = \nabla p F Tp iM (v)p
for (v)p ∈ Tp M ⊂ Tp Rn .
= \nabla F (p) v
(10.10)
If p is a critical point of f , Theorem 10.12 says \nabla p f = 0. Now \nabla F (p) ∈ Tp M
follows from (10.10). Therefore Proposition 10.13 shows there are unique real
numbers λ1 , . . . , λ such that
λj \nabla hj (p) ,
\nabla F (p) =
which, because of Remark 3.14(a), finishes the proof.
10.16 Remark The Lagrange multiplier method turns the problem of finding the
extrema of F under the constraints h1 (p) = 0, . . . , h (p) = 0 into the problem of
finding the critical points of the function
λj hj ∈ C 1 (X, R)
(without constraints). We determine the critical points and the Lagrange multipliers by solving the + n equations
hj (p) = 0 for 1 ≤ j ≤
λj hj (p) = 0 for 1 ≤ k ≤ n ,
for the n+ unknowns p1 , . . . , pn , λ1 , . . . , λ , where p = (p1 , . . . , pn ). Subsequently,
we must find which of these critical points are actually extrema.
Applications of Lagrange multipliers
In the following examples, which are of independent interest, we demonstrate
nontrivial applications of the Lagrange multipliers. In particular, we give a short
proof of the principal axis transformation theorem, which is shown by other means
in linear algebra.
10.17 Examples (a) Given arbitrary vectors aj ∈ Rn for 1 ≤ j ≤ n, Hadamard’s
inequality says
det[a1 , . . . , an ] ≤
|aj | .
Proof (i) From linear algebra, it is known that the determinant is an n-linear function
of column vectors. It therefore suﬃces to verify
−1 ≤ det[a1 , . . . , an ] ≤ 1
for aj ∈ S n−1 and 1 ≤ j ≤ n .
274
for hj (x) := |xj |2 − 1 ,
F (x) := det[x1 , . . . , xn ]
h = (h1 , . . . , hn ) ,
and x = (x1 , . . . , xn ) ∈ Rn × · · · × Rn = Rn . Then F belongs to C ∞ Rn , R , h belongs
to C ∞ Rn , Rn , and7
∂h(x) = 2 ⎢
n×n2
xn−1
Clearly the rank of ∂h(x) is maximal for every x ∈ h−1 (0). Therefore 0 is a regular value
of h, and M := h−1 (0) is an n(n − 1)-dimensional C ∞ submanifold of Rn . Further, M
is compact because M = S n−1 × · · · × S n−1 . Therefore f := F | M ∈ C ∞ (M, R) assumes
a minimum and a maximum.
(iii) Suppose p = (p1 , . . . , pn ) ∈ M is a extremal point of f . Using Lagrange
multipliers, there are λ1 , . . . , λn ∈ R such that
λj \nabla hj (p) = 2
λj (0, . . . , 0, pj , 0, . . . , 0)
(10.11)
= 2(λ1 p1 , . . . , λn pn ) ∈ R
In addition, Example 4.8(a) implies
(p) = det[p1 , . . . , pj−1 , ek , pj+1 , . . . , pn ]
for 1 ≤ j, k ≤ n .
(10.12)
We set B := [p1 , . . . , pn ] and denote by B := [bjk ]1≤j,k≤n the cofactor matrix to B whose
entries are bjk := (−1)j+k det Bjk , where Bjk is obtained from B by removing the k-th
row and the j-th column (see [Gab96, § A.3.7]). Then (10.11) and (10.12) result in
(p) = bjk = 2λj pk .
Because B B = (det B)1n , we have
bik pk = 2λi (pi | pj )
δij det B =
for 1 ≤ i, j ≤ n .
(10.13)
By choosing i = j in (10.13), we find
2λ1 = · · · = 2λn = det B .
means that xj is understood as a row vector.
(10.14)
In the case det B = 0, the claim is obviously true. If det B = 0, then (10.13) and (10.14)
show that pi and pj are orthogonal for i = j. Therefore B belongs to O(n), and we get
| det B| = 1 (see Exercise 9.2). Now the claim follows because F (p) = det B.
(b) (principal axis transformation) Suppose A ∈ Lsym (Rn ). Then there are real
numbers λ1 ≥ λ2 ≥ · · · ≥ λn and x1 , . . . , xn ∈ S n−1 such that Axk = λk xk for
1 ≤ k ≤ n, that is, xk is an eigenvector with eigenvalue λk . The x1 , . . . , xn form
an ONB of Rn . In this basis, A has the matrix [A] = diag(λ1 , . . . , λn ). We also
for k = 1, . . . , n ,
λk = max (Ax|x) ; x ∈ S n−1 ∩ Ek
where E1 := Rn and Ek := span{x1 , . . . , xk−1 }
for k = 2, . . . , n.
Proof (i) We set h (x) := |x| − 1 and F (x) := (Ax | x) for x ∈ Rn . Then 0 is a regular
value of h0 ∈ C ∞ (Rn , R), and F has a maximum on S n−1 = h−1 (0). Suppose x1 ∈ S n−1
is a maximal point of f := F | S n−1 . Using Lagrange multipliers, there is a λ1 ∈ R such
that \nabla F (x1 ) = 2Ax1 = 2λ1 x1 (see Exercise 4.5). Therefore x1 is an eigenvector of A
with eigenvalue λ1 . We also have
λ1 = λ1 (x1 | x1 ) = (Ax1 | x1 ) = f (x1 ) ,
because x1 ∈ S n−1 .
(ii) We now construct x2 , . . . , xn recursively. Supposing x1 , . . . , xk−1 are already
known for k ≥ 2, we set h := (h0 , h1 , . . . , hk−1 ) with hj (x) := 2(xj | x) for 1 ≤ j ≤ k − 1.
Then h−1 (0) = S n−1 ∩ Ek is compact, and there exists an xk ∈ S n−1 ∩ Ek such that
f (x) ≤ f (xk ) for x ∈ S n−1 ∩ Ek . In addition, one verifies (because rank B = rank B for
B ∈ Rk×n ) that
rank ∂h(x) = rank[x, x1 , . . . , xk−1 ] = k
for x ∈ S n−1 ∩ Ek .
Therefore 0 is a regular value of h, and we use Theorem 10.15 to find real numbers
μ0 , . . . , μk−1 such that
μj \nabla hj (xk ) = 2μ0 xk + 2
2Axk = \nabla F (xk ) =
(10.15)
Because (xk | xj ) = 0 for 1 ≤ j ≤ k − 1, we have
(Axk | xj ) = (xk | Axj ) = λj (xk | xj ) = 0
for 1 ≤ j ≤ k − 1 .
It therefore follows from (10.15) that
0 = (Axk | xj ) = μj
for j = 1, . . . , k − 1 .
We also see from (10.15) that xk is a eigenvector of A with eigenvalue μ0 . Finally we get
μ0 = μ0 (xk | xk ) = (Axk | xk ) = f (xk ) .
Thus we are finished (see Remark 5.13(b)).
276
10.18 Remark Suppose A ∈ Lsym (Rn ) with n ≥ 2, and let λ1 ≥ λ2 ≥ · · · ≥ λn
be the eigenvalues of A. Further let x1 , . . . , xn be an ONB of Rn , where xk is an
eigenvector of A with eigenvalue λk . For x = j=1 ξ j xj ∈ Rn , we then have
λj (ξ j )2 .
(Ax|x) =
(10.16)
We now assume that
λ1 ≥ · · · ≥ λk > 0 > λk+1 ≥ · · · ≥ λm
for some k ∈ {0, . . . , m}. Then γ ∈ {−1, 1} is a regular value of the C ∞ map
a : Rn → R ,
x → (Ax|x)
(see Exercise 4.5). Therefore, due to the regular value theorem,
a−1 (γ) =
x ∈ Rn ; (Ax|x) = γ
is a C ∞ hypersurface in Rn . Letting αj := 1
j=k+1
|λj |, it follows from (10.16) that
(10.17)
ξ j xj ∈ a−1 (γ).
If A is positive definite, then λ1 ≥ · · · ≥ λn > 0 follows from Remark 5.13(b).
In this case, we read off from (10.17) that a−1 (1) is an (n−1)-dimensional ellipsoid
with principal axes α1 x1 , . . . , αn xn . If A is indefinite, then (10.17) shows that
a−1 (±1) is (in general) a hyperboloid with principal axes α1 x1 , . . . , αn xn .
This extends the known even-dimensional case to higher dimensions.
These considerations clarify the name “principal axis transformation”. If one
or more eigenvalues of A vanishes, a−1 (γ) is a cylinder with an ellipsoidal or
hyperboloidal profile.
1 Denote by (gn , Vn ) the regular parametrization of Fn := Rn \ Hn by n-dimensional
polar coordinates (see Exercise 9.11).
(a) Show that the first fundamental matrix (gn )jk of Fn with respect to gn is given by
for n = 2 ,
or, in the case n ≥ 3, by
diag 1, r 2 sin2 (y 3 ) · · · · · sin2 (y n ), r 2 sin2 (y 3 ) · · · · · r 2 sin2 (y n−1 ), . . . , r 2 sin2 (y 3 ), r 2
for (r, y 2 , . . . , y n ) ∈ Vn .
(b) Suppose f ∈ C 1 (Fn , R), and let ϕn denote the chart belonging to gn . Calculate the
representation of \nabla p f in n-dimensional polar coordinates, that is, in the local coordinates
induced by ϕn .
2 Let (g, V ) be a parametrization of an m-dimensional C 1 submanifold M of Rn . Also
denote by
g(y) := det gjk (y) for y ∈ V
the Gram determinant of M with respect to g. Verify these statements:
(a) If m = 1, then g = |g|.
(b) If m = 2 and n = 3, then
∂(g 2 , g 3 )
∂(x, y)
∂(g 3 , g 1 )
∂(g 1 , g 2 )
= |∂1 g × ∂2 g| ,
where × denotes the cross (or vector) product (see also Section VIII.2).
(c) Suppose V is open in Rn and f ∈ C 1 (V, R). For the parametrization g : V → Rn+1 ,
x → x, f (x) of the graph of f , show that g = 1 + |\nabla f |2 .
Determine Tp S 2 at p = (0, 1, 0) in
(a) spherical coordinates (see Example 9.11(b));
(b) the coordinates coming from the stereographic projection.
4 Determine Tp T2 at p =
2, 2, 1 .
5 Suppose M is an m-dimensional C q submanifold of Rn and (ϕ, U ) is a chart around
(a) every open subset of M is an m-dimensional C q submanifold of Rn ;
(b) if U is understood as a manifold, then ϕ belongs to Diff q U, ϕ(U ) , and the tangential
of the chart Tp ϕ agrees with that of ϕ ∈ C q U, ϕ(U ) at the point p.
278
Find TA GL(n) for A ∈ GL(n) := Laut(Rn ).
7 Show the tangential space of the orthogonal group O(n) at 1n is a vector space of
skew-symmetric (n × n)-matrices, that is,
T1n O(n) = 1n , { A ∈ Rn×n ; A + A = 0 } .
(Hint: Use Example 9.5(c) and Theorem 10.7.)
Show that the tangential space of the special orthogonal group SO(n) at 1n is
T1n SO(n) = 1n , { A ∈ Rn×n ; tr(A) = 0 } .
(Hint: For A ∈ Rn×n with tr(A) = 0, note γ(t) = etA for t ∈ R, and see Theorem 10.6.)
(a) for ψ ∈ Diff q (M, N ) and p ∈ M , we have Tp ψ ∈ Lis(Tp M, Tψ(p) N );
(b) if M and N are diffeomorphic C q submanifolds of Rn , their dimensions coincide.
(a) S 1 × R (see Exercise 9.4) and
(b) S × S and
(x, y, z) ∈ R3 ; x2 + y 2 = 1
are diffeomorphic;
(c) S n and rS n for r > 0 are diffeomorphic.
Suppose M :=
ν : M → S2 ,
(x, y, z) → (x, y, 0) .
Then ν ∈ C ∞ (M, S 2 ). Also Tp ν is symmetric for p ∈ M and has the eigenvalues 0 and 1.
12 Suppose X is open in Rn and f ∈ C 1 (X, R). Also let ν : M → S n be a unit normal
on M := graph(f ). Show that ν belongs to C ∞ (M, S n ) and that Tp ν is symmetric for
Suppose M and ν are as in Exercise 12. Also suppose
ϕα : M → Rn+1 ,
p → p + αν(p)
for α ∈ R. Show that there is an α0 > 0 such that ϕα (M ) is a smooth hypersurface
diffeomorphic to M for every α ∈ (−α0 , α0 ).
14 Find the volume of the largest rectangular box that is contained within the ellipsoid
(x, y, z) ∈ R3 ; (x/a)2 + (y/b)2 + (z/c)2 = 1 with a, b, c > 0.
Chapter VIII
Line integrals
In this chapter, we return to the theory of integrating functions of a real variable.
We will now consider integrals which are not only over intervals but also over
continuously differentiable maps of intervals, namely, curves. We will see that this
generalization of the integral has important and profound consequences.
Of course, we must first make precise the notion of a curve, which we do in
Section 1. In addition, we will introduce the concept of arc length and derive an
integral formula for calculating it.
In Section 2, we discuss the differential geometry of curves. In particular,
we prove the existence of an associated n-frame, a certain set of n vectors defined
along the curve. For plane curves, we study curvature, and for space curves we
will also study the torsion. The material in this section contributes mostly to your
general mathematical knowledge, and it will not be needed for the remainder of
this chapter.
Section 3 treats differential forms of first order. Here we make rigorous the
ad hoc definition of differentials introduced in Chapter VI. We will also derive
several simple rules for dealing with such differential forms. These rules represent
the foundation of the theory of line integrals, as we will learn in Section 4 when
differential forms emerge as the integrands of line integrals. We will prove the
fundamental theorem of line integrals, which characterizes the vector fields that
can be obtained as gradients of potentials.
Sections 5 and 6 find a particularly important application of line integrals
in complex analysis, also known as the theory of functions of a complex variable.
In Section 5, we derive the fundamental properties of holomorphic functions, in
particular, Cauchy’s integral theorem and formula. With these as aids, we prove
the fundamental result, which says that a function is holomorphic if and only if it
is analytic. We apply to general theory to the Fresnel integral, thus showing how
the Cauchy integral theorem can be used to calculate real integrals.
In Section 6, we study meromorphic functions and prove the important
280
VIII Line integrals
residue theorem and its version in homology theory. To this end, we introduce
the concept of winding number and derive its most important properties. To conclude this volume, we show the scope of the residue theorem by calculating several
Fourier integrals.
VIII.1 Curves and their lengths
1 Curves and their lengths
This section mainly proves that a continuously differentiable compact curve Γ has
a finite length L(Γ) given by
L(Γ) =
|γ(t)| dt .
Here γ is an arbitrary parametrization of Γ.
• E = (E, |·|) is a Banach space over the field K and
I = [a, b] is a compact interval.
The total variation
Suppose f : I → E and Z := (t0 , . . . , tn ) is a
partition of I. Then
|f (tj ) − f (tj−1 )|
LZ (f ) :=
is the length of the piecewise straight path
f (t0 ), . . . , f (tn ) in E, and
Var(f, I) := sup LZ (f ) ; Z = (t0 , . . . , tn ) is a partition of I
is called the total variation (or simply the variation) of f over I. We say f is of
bounded variation if Var(f, I) < ∞.
1.1 Lemma For f : [a, b] → E and c ∈ [a, b], we have
Var f, [a, b] = Var f, [a, c] + Var f, [c, b] .
Proof (i) Suppose c ∈ [a, b]. Without loss of generality, we may assume that
Var f, [a, c] and Var f, [c, b] are finite, for otherwise the function f : [a, b] → E
would not be of bounded variation, and the claim would be already obvious.
(ii) Suppose Z is a partition of [a, b] and Z is a refinement of Z that contains
the point c. In addition, set Z1 := Z ∩ [a, c] and Z2 := Z ∩ [c, b]. Then
LZ (f ) ≤ LZ (f ) = LZ1 (f ) + LZ2 (f ) ≤ Var f, [a, c] + Var f, [c, b] .
By forming the supremum with respect to Z, we find
Var f, [a, b] ≤ Var f, [a, c] + Var f, [c, b] .
282
(iii) For ε > 0, there are partitions Z1 of [a, c] and Z2 of [c, b] such that
LZ1 (f ) ≥ Var f, [a, c] − ε/2 and LZ2 (f ) ≥ Var f, [c, b] − ε/2 .
For Z := Z1 ∨ Z2 , we have
LZ1 (f ) + LZ2 (f ) = LZ (f ) ≤ Var f, [a, b] ,
Var f, [a, c] + Var f, [c, b] ≤ LZ1 (f ) + LZ2 (f ) + ε ≤ Var f, [a, b] + ε .
The claim is now implied by (ii).
Rectifiable paths
Interpreting γ ∈ C(I, E) as a continuous path in E, we also call Var(γ, I) the
length (or arc length) of γ and write it as L(γ). If L(γ) < ∞, that is, if γ has a
finite length, we say γ is rectifiable.
1.2 Remarks (a) There are continuous paths that are not rectifiable.1
Proof We consider γ : [0, 1] → R with γ(0) := 0 and γ(t) := t cos2 (π/2t) for t ∈ (0, 1].
Then γ is continuous (see Proposition III.2.24). For n ∈ N× , suppose Zn = (t0 , . . . , t2n )
is the partition of [0, 1] with t0 = 0 and tj = 2/(2n + 1 − j) for 1 ≤ j ≤ 2n. Because
γ(tj ) =
j = 2k,
0≤k≤n,
j = 2k + 1, 0 ≤ k < n ,
|γ(tj ) − γ(tj−1 )| =
LZ n (γ) =
t2k+1 =
as n → ∞. Therefore γ is not rectifiable.
(b) Suppose γ : [a, b] → E is Lipschitz continuous with Lipschitz constant λ. Then
γ is rectifiable, and L(γ) ≤ λ(b − a).
For every partition Z = (t0 , . . . , tn ) of [a, b], we have
|γ(tj ) − γ(tj−1 )| ≤ λ
LZ (γ) =
|tj − tj−1 | = λ(b − a) .
(c) Of course, the length of a path γ depends on the norm on E. However, that
it is rectifiable does not change when going to a different but equivalent norm.
The path we considered in Remark 1.2(a) is indeed continuous but not differentiable at 0. The next result shows that continuously differentiable paths are
always rectifiable.
1 One can also show that there are continuous paths in R2 whose image fills out the entire
unit disc B (see Exercise 8). Such “space filling” paths are called Peano curves.
1.3 Theorem Suppose γ ∈ C 1 (I, E). Then γ is rectifiable, and we have
L(γ) =
Proof (i) It suﬃces to consider the case a < b, in which I is not just a single
point.
(ii) The rectifiability of γ follows immediately from the fundamental theorem
of calculus. That is, if Z = (t0 , . . . , tn ) is partition of [a, b], then
tj−1
γ(t) dt
L(γ) = Var γ, [a, b] ≤
|γ(t)| dt =
(iii) Suppose now s0 ∈ [a, b). From Lemma 1.1, we know for every s ∈ (s0 , b)
Var γ, [a, s] − Var γ, [a, s0 ] = Var γ, [s0 , s] .
We also find
|γ(s) − γ(s0 )| ≤ Var γ, [s0 , s] ≤
Here the first inequality follows because (s0 , s) is a partition of [s0 , s], and the
second comes from (1.2).
Thus because s0 < s, we have
Var γ, [a, s] − Var γ, [a, s0 ]
γ(s) − γ(s0 )
s − s0
Because γ has continuous derivatives, it follows from the mean value theorem in
integral form and from Theorem VI.4.12 that
|γ(s0 )| = lim
s→s0
≤ lim
s→s0 s − s0
|γ(t)| dt = |γ(s0 )| .
Thus (1.3) and like considerations for s < s0 show that s → Var γ, [a, s] is
differentiable with
Var γ, [a, s] = |γ(s)| for s ∈ [a, b] .
284
Therefore s → Var γ, [a, s] belongs to C 1 (I, R). In addition, the fundamental
theorem of calculus gives
Var γ, [a, b] =
|γ(t)| dt
because Var γ, [a, a] = 0.
1.4 Corollary For γ = (γ1 , . . . , γn ) ∈ C 1 (I, Rn ), we have
γ1 (t)
+ · · · + γn (t)
The image of a path is a set of points in E which does not depend on the particular function used to describe it. In other words, it has a geometric meaning
independent of any special parametrization. To understand this precisely, we must
specify which changes of parametrization leave this image invariant.
Suppose J1 and J2 are intervals and q ∈ N ∪ {∞}. The map ϕ : J1 → J2 is
said to be an (orientation-preserving) C q change of parameters if ϕ belongs2 to
Diff q (J1 , J2 ) and is strictly increasing. If γj ∈ C q (Jj , E) for j = 1, 2, then γ1 is
said to be an (orientation-preserving) C q reparametrization of γ2 if there is a C q
change of parameters ϕ such that γ1 = γ2 ◦ ϕ.
1.5 Remarks (a) When ϕ ∈ Diff q (J1 , J2 ) is strictly decreasing, we then say ϕ is
an orientation-reversing C q change of parameters. That said, in what follows, we
will always take changes of parameters to be orientation-preserving.
(b) A map ϕ : J1 → J2 is a C q change of parameters if and only if ϕ belongs to
C q (J1 , J2 ), is surjective, and satisfies ϕ(t) > 0 for t ∈ J1 .
This follows from Theorems III.5.7 and IV.2.8.
(c) Let I1 and I2 be compact intervals, and suppose γ1 ∈ C(I1 , E) is a continuous
reparametrization of γ2 ∈ C(I2 , E). Then
Var(γ1 , I1 ) = Var(γ2 , I2 ) .
Proof Suppose ϕ ∈ Diff 0 (I1 , I2 ) is a change of parameters satisfying γ1 = γ2 ◦ ϕ. If
Z = (t0 , . . . , tn ) is a partition of I1 , then ϕ(Z) := ϕ(t0 ), . . . , ϕ(tn ) is a partition of I2 ,
and we have
LZ (γ1 , I1 ) = LZ (γ2 ◦ ϕ, I1 ) = Lϕ(Z) (γ2 , I2 ) ≤ Var(γ2 , I2 ) .
2 If in addition J and J are not open, then this means ϕ : J → J is bijective, and ϕ
and ϕ−1 belong to the class C q . In particular, Diff 0 (J1 , J2 ) is the set of all topological maps
(homeomorphisms) of J1 to J2 .
Thus we have the relation
Var(γ1 , I1 ) = Var(γ2 ◦ ϕ, I1 ) ≤ Var(γ2 , I2 ) .
Noting that γ2 = (γ2 ◦ ϕ) ◦ ϕ−1 , we see from (1.4) (if we replace γ2 by γ2 ◦ ϕ and ϕ by
ϕ−1 ) that
Var(γ2 , I2 ) = Var (γ2 ◦ ϕ) ◦ ϕ−1 , I2 ≤ Var(γ2 ◦ ϕ, I1 ) = Var(γ1 , I1 ) .
On the set of all C q paths in E, we define the relation ∼ by
γ1 ∼ γ2 :⇐ γ1 is a C q reparametrization of γ2 .
It is not hard to see that ∼ is an equivalence relation (see Exercise 5). The
associated equivalence classes are called C q curves in E. Every representative of
a C q curve Γ is a C q parametrization of Γ. We may say a C 0 curve is continuous,
a C 1 curve is continuously differentiable, and a C ∞ curve is smooth. If the curve Γ
has a parametrization on a compact domain (or a compact parameter interval), we
say the curve is compact, and then, due to Theorem III.3.6, the image of Γ is also
compact. We say a parametrization γ of Γ is regular if γ(t) = 0 for t ∈ dom(γ).
When Γ has a regular parametrization, we call Γ a regular curve. Sometimes we
write Γ = [γ] to emphasize that Γ is an equivalence class of parametrizations that
contains γ.
1.6 Remarks (a) If Γ is a compact curve, every parametrization of it has a
compact domain of definition. If Γ is a regular C 1 curve, every parametrization of
it is regular.
Proof Suppose γ ∈ C q (J, E) is a parametrization of Γ, and let γ1 ∈ C q (J1 , E) be a
reparametrization of γ. Then there is a ϕ ∈ Diff q (J1 , J) such that γ1 = γ ◦ ϕ. When J
is compact, this holds also for J1 = ϕ−1 (J) because continuous images of compact sets
are also compact (Theorem III.3.6). From the chain rule, we find for q ≥ 1 that
γ1 (t) = γ ϕ(t) ϕ(t)
for t ∈ J1 .
Therefore, because γ(s) = 0 for s ∈ J and ϕ(t) = 0 for t ∈ J1 , we also have γ1 (t) = 0 for
t ∈ J1 .
(b) A regular curve can have a nonregular parametrization, which is, however, not
equivalent to its regular parametrization.
We consider the regular smooth curve Γ that is parametrized by the function
[−1, 1] → R2 ,
t → γ(t) := (t, t) .
We also consider the smooth path γ : [−1, 1] → R2 with γ(t) := (t3 , t3 ). Then im(γ) =
im(γ) but γ(0) = (0, 0). Therefore γ is not a C 1 reparametrization of γ.
286
(c) Suppose I → E, t → γ(t) is a regular C 1 parametrization of a curve Γ. Then
γ(t) = lim
γ(s) − γ(t)
can be interpreted the “instantaneous velocity” of the curve Γ at the point γ(t).
Suppose Γ is a continuous curve in E, and suppose γ and γ1 are (equivalent)
parametrizations of Γ. Then clearly, im(γ) = im(γ1 ). Therefore the image of Γ is
well defined through
im(Γ) := im(γ) .
If Γ is compact, dom(γ) = [a, b], and dom(γ1 ) = [a1 , b1 ], we have the relations
γ(a) = γ1 (a1 ) and γ(b) = γ1 (b1 ) .
Thus, for a compact curve, the initial point AΓ := γ(a) and the end point EΓ :=
γ(b) are well defined.
If AΓ and EΓ coincide, Γ is closed or a loop. Finally, we often simply write
p ∈ Γ for p ∈ im(Γ).
Rectifiable curves
Suppose Γ is a continuous curve in E and γ ∈ C(J, E) is a parametrization of Γ.
Also let α := inf J and β := sup J. Then Var γ, [a, b] is defined for α < a < b < β.
Then Lemma 1.1 implies for every c ∈ (α, β) that the function
b → Var γ, [c, b]
is increasing and that the map
(α, c] → R ,
a → Var γ, [a, c]
is decreasing. Thus from (1.1) the limit
Var(γ, J) := lim Var γ, [a, b]
a↓α
exists in R, and we call it (total) variation of γ over J. If γ1 ∈ C(J1 , E) is a
reparametrization of γ, it follows from Remark 1.5(c) that Var(γ, J) = Var(γ1 , J1 ).
Therefore the total variation or length (or the arc length) of Γ is well defined
L(Γ) := Var(Γ) := Var(γ, J) .
The curve Γ is said to be rectifiable if it has a finite length, that is, if L(Γ) < ∞.
1.7 Theorem Suppose γ ∈ C 1 (J, E) is a parametrization of a C 1 curve Γ. Then
If Γ is compact, Γ is rectifiable.
Proof This follows immediately from (1.5), Theorem 1.3, and the definition of
an improper integral.
1.8 Remarks (a) Theorem 1.7 says in particular that L(Γ), and thus also the
integral in (1.6), is independent of the special parametrization γ.
(b) Suppose Γ is a C q curve imbedded in Rn as set out in Section VII.8. Also
suppose (ϕ, U ) is a chart of Γ and (g, V ) is its associated parametrization. Then3
Γ := Γ ∩ U is a regular C q curve, and g is a regular C q parametrization of Γ.
(c) Regular C q curves are generally not imbedded C q curves.
This follows from Example VII.9.6(c).
(d) Because we admit only orientation-preserving changes of parameters, our
curves are all oriented, that is, they “pass through” in a fixed direction.
1.9 Examples (a) (graphs of real-valued functions) Suppose f ∈ C q (J, R) and
Γ := graph(f ). Then Γ is a regular C q curve in R2 , and the map J → R2 ,
t → t, f (t) is a regular C q parametrization of Γ. Also
1 + f (t)
(b) (plane curves in polar coordinates) Suppose r, ϕ ∈ C q (J, R) and r(t) ≥ 0 for
t ∈ J. Also let
γ(t) := r(t) cos(ϕ(t)), sin(ϕ(t))
Identifying R2 with C gives γ the representation
γ(t) = r(t)ei ϕ(t)
If r(t)
+ r(t)ϕ(t)
> 0, then γ is a regular C q parametrization of a curve Γ
r(t)
We have
γ(t) := r(t) + i r(t)ϕ(t) ei ϕ(t) ,
3 Here
and in other cases where there is little risk of confusion, we simply write Γ for im(Γ).
288
and thus |γ(t)|2 = r(t)
+ r(t)ϕ(t) , from which the claim follows.
(c) Suppose 0 < b ≤ 2π. For the circular arc
parametrized by
γ : [0, b] → R2 ,
t → R(cos t, sin t)
or the equivalent one obtained through
γ : [0, b] → C ,
t → Rei t ,
identifying R2 with C gives4 L(Γ) = bR.
(d) (the logarithmic spiral) For λ < 0 and a ∈ R,
γa,∞ : [a, ∞) → R2 ,
t → eλt (cos t, sin t)
is a smooth regular parametrization of the curve Γa,∞ := [γa,∞ ]. It has finite
length eλa 1 + λ2 |λ|.
When λ > 0, we have analogously that
γ−∞,a : (−∞, a] → R2 ,
is a smooth regular parametrization of the curve Γ−∞,a := [γ−∞,a ] with finite
length eλa 1 + λ2 λ. For λ = 0, the map
R → R2 ,
is a smooth regular parametrization of a logarithmic spiral. Its length is infinite.
When λ > 0 [or λ < 0], it spirals outward [or inward]. Identifying R2 with C gives
the logarithmic spiral the simple parametrization t → e(λ+i )t .
Proof Suppose λ < 0. We set r(t) = eλt and ϕ(t) = t for t ∈ [a, ∞). According to (b),
we then have
L(Γa,∞ ) = lim
1 + λ2 eλt dt =
The case λ > 0 works out analogously.
Exercise III.6.12.
1 + λ2
lim (eλb − eλa ) =
1 + λ2 λa
(e) For R > 0 and h > 0, the regular smooth
curve Γ parametrized by
γ : R → R3 ,
t → (R cos t, R sin t, ht) (1.7)
is the helix with radius R and pitch 2πh. By
identifying5 R2 with C and therefore R3 with
C × R, we give (1.7) the form t → (Rei t , ht).
L γ [a, b] = (b − a) R2 + h2
for −∞ < a < b < ∞. The curve Γ lies on the cylinder with radius R whose axis
is the z-axis. In one revolution, it rises coaxially by 2πh.
Because |γ(t)|2 = R2 + h2 , the claim follows from Theorem 1.7.
1 A disc rolls on a line without slipping. Find a parametrization for the path traced
by an arbitrary point fixed on the disc (or one “outside” the disc). Determine the arc
length of this path as the disc turns once.
Suppose −∞ < α < β < ∞. Calculate the length of these paths:
[0, π] → R2 ,
[1, ∞) → R ,
t → (cos t + t sin t, sin t − t cos t) ;
t → t−2 (cos t, sin t) ;
[α, β] → R2 ,
t → (t3 , 3t2 /2) ;
t → (t, t2 /2) .
3 Approximate (for example, with the trapezoid rule) the length of the lima¸on of Pascal
(see Example VII.9.1(c)).
5 as
a metric space
290
Sketch the space curve Γ = [γ] with
γ : [−3π, 3π] → R3 ,
t → t(cos t, sin t, 1) ,
and calculate its arc length.
5 Suppose Z = (α0 , . . . , αm ) for m ∈ N× is a partition of a compact interval I and
q ∈ N× ∪ {∞}. A continuous path γ ∈ C(I, E) is a piecewise C q path in E (on Z) if
γj := γ | [αj−1 , αj ] ∈ C q [αj−1 , αj ], E for j = 1, . . . , m. A path η ∈ C(J, E) that is
piecewise C q on the partition Z = (β0 , . . . , βm ) of J is called a C q reparametrization
of γ if there is a C q change of parameters ϕj ∈ Diff q [αj−1 , αj ], [βj−1 , βj ] such that
γj := ηj ◦ ϕj for j = 1, . . . , m. On the set of all piecewise C q paths in E, we define ∼
γ ∼ η :⇐ η is a reparametrization of γ .
(a) Show that ∼ is an equivalence relation. The corresponding equivalence classes are
called piecewise C q curves in E. Every representative of a piecewise C q curve Γ is a
piecewise C q parametrization of Γ. When γ ∈ C(I, E) is a piecewise C q parametrization
of Γ, we write symbolically Γ = m Γj , where Γj := [γj ] for j = 1, . . . , m.
(b) Suppose Γ is a curve in E with parametrization γ ∈ C(I, E) that is piecewise C q on
the partition Z = (α0 , . . . , αm ). Define the length (or the arc length) of Γ through
L(Γ) := Var(γ, I) .
Show L(Γ) is well defined and
αj−1
L(Γj ) =
|γj (t)| dt .
(Hint: Remark 1.5(c) and Lemma 1.1.)
If Γ = [γ] is a plane closed piecewise C q curve and (α0 , . . . , αm ) is a partition for γ,
A(Γ) :=
det γj (t), γj (t) dt
is the oriented area contained in Γ.
(a) Show that A(Γ) is well defined.
(b) Let −∞ < α < β < ∞, and suppose f ∈ C 1 [α, β], R satisfies f (α) = f (β) = 0. Let
a := α and b := 2β − α, and define γ : [a, b] → R2 by
γ(t) :=
α + β − t, f (α + β − t) ,
(α − β + t, 0) ,
t ∈ [a, β] ,
(α) Γ := [γ] is a closed piecewise C q curve (sketch it).
(β) A(Γ) = α f (t) dt, that is, the oriented area A(Γ) equals the oriented area between
the graph of f and [α, β] (see Remark VI.3.3(a)).
(c) Find the oriented area of the ellipse with semiaxes a, b > 0. Use the parametrization
[0, 2π] → R2 , t → (a cos t, b sin t).
(d) The path γ : [−π/4, 3π/4] → R2 with
t ∈ [−π/4, π/4] ,
cos(2t)(cos t, sin t) ,
| cos(2t)|(− sin t, − cos t) ,
t ∈ [π/4, 3π/4]
parametrizes the lemniscate. Verify that Γ =
[γ] is a plane piecewise C ∞ curve, and find
A(Γ).
Find A(Γ) if Γ is the boundary of a square.
Suppose f : R → [0, 1] is continuous and 2-periodic with
f (t) =
t ∈ [0, 1/3] ,
t ∈ [2/3, 1] .
2−k f (32k t)
r(t) :=
α(t) := 2π
2−k f (32k−1 t)
(a) γ : R → R2 , t → r(t) cos α(t), sin α(t) is continuous;
(b) γ [0, 1] = B.
(Hint: Suppose (r0 , α0 ) ∈ [0, 1]×[0, 2π] are polar coordinates for (x0 , y0 ) ∈ B\{0, 0}. Also
define ∞ gk 2−k and ∞ hk 2−k to be the respective expansions of r0 and α0 /2π as
binary fractions. For
n = 2k ,
an :=
n = 2k − 1 ,
show t0 := 2
an 3−n−1 ∈ [0, 1], f (3n t0 ) = an for n ∈ N, and γ(t0 ) = (x0 , y0 ).)
292
2 Curves in Rn
In Section VII.6, we allowed curves to pass through infinite-dimensional vector
spaces. However, the finite-dimensional case is certainly more familiar from classical (differential geometry) and gains additional structure from the Euclidean
structure of Rn . We now examine local properties of curves in Rn .
• n ≥ 2 and γ ∈ C 1 (J, Rn ) is a regular parametrization of a curve Γ.
Unit tangent vectors
t(t) := tγ (t) := γ(t), γ(t)/|γ(t)| ∈ Tγ(t) Rn
is a tangential vector in Rn of length 1 and is called the unit tangent vector of Γ
at the point γ(t). Any element of
Rt(t) := γ(t), Rγ(t) ⊂ Tγ(t) Rn
is a tangent of Γ at γ(t).
2.1 Remarks (a) The unit tangent vector t is invariant under change of parameters. Precisely, if ζ = γ ◦ ϕ is a reparametrization of γ, then tγ (t) = tζ (s) for
t = ϕ(s). Therefore is it meaningful to speak of the unit tangent vectors of Γ.
This follows immediately from the chain rule and the positivity of ϕ(s).
(b) According to Corollary VII.9.8, every t0 ∈ J has an open subinterval J0 of
J around it such that Γ0 := im(γ0 ) with γ0 := γ |J0 is a one-dimensional C 1
submanifold of Rn . By shrinking J0 , we can assume that Γ0 is described by a
single chart (U0 , ϕ0 ), where γ0 = iΓ0 ◦ ϕ−1 . Then clearly the tangent to the
curve Γ0 at p := γ(t0 ) clearly agrees with the tangential space of the manifold Γ0
at the point p, that is, Tp Γ0 = Rt(t0 ).
Now it can happen that there is a t1 = t0 in J such that γ(t1 ) = p = γ(t0 ).
Then there is an open interval J1 around t1 such that Γ1 := im(γ1 ) with γ1 := γ |J1
is a C 1 submanifold of Rn for which Tp Γ1 = Rt(t1 ). Thus it is generally not true
that Tp Γ0 = Tp Γ1 , that is, in general, t(t0 ) = ±t(t1 ). This shows that Γ can have
several such “double point” tangents. In this case, Γ is not a submanifold of Rn ,
although suﬃciently small “pieces” of it are, where “suﬃciently small” is measured
in the parameter interval.
VIII.2 Curves in Rn
The lima¸on of Pascal from Example VII.9.6(c) gives
a concrete example. Suppose Γ is a compact regular curve
in R2 parametrized by
γ : [−π, π] → (1 + 2 cos t)(cos t, sin t) .
Letting t0 := arccos(−1/2) and t1 := −t0 , we have γ(t0 ) =
γ(t1 ) = (0, 0) ∈ Γ. For the corresponding unit tangent
vectors, we find
t(tj ) = (0, 0), (−1)j , − 3
for j = 0, 1 .
Therefore t(t0 ) = t(t1 ).
Parametrization by arc length
If γ : J → Rn is a parametrization of Γ such that |γ(t)| = 1 for every t ∈ J, then
clearly L(Γ) = J dt, and the length of the interval J equals the length of Γ. We
say that γ parametrizes Γ by arc length.
2.2 Proposition Every regular C 1 curve in Rn can be parametrized by arc length.
This parametrization is unique up to a reparametrization of the form s → s+const.
If η : I → Rn is a parametrization by arc length, then tη (s) = η(s), η(s) for s ∈ I.
Proof Suppose γ ∈ C 1 (J, Rn ) is a regular parametrization of Γ. We fix an a ∈ J
ϕ(t) :=
|γ(τ )| dτ
and I := ϕ(J) .
The regularity of γ and |·| ∈ C Rn \{0}, R show that ϕ belongs to C 1 (J, Rn ). In
addition, ϕ(t) = |γ(t)| > 0. Due to Remark 1.5(b), ϕ is a C 1 change of parameters
from J onto I := ϕ(J). Letting η := γ ◦ ϕ−1 , this implies
γ ϕ−1 (s)
|η(s)| = γ ϕ−1 (s) (ϕ−1 ) (s) =
ϕ ϕ−1 (s)
for s ∈ I. Thus η parametrizes Γ by arc length.
Suppose η ∈ C 1 I, Rn also parametrizes Γ by arc length and ψ ∈ Diff 1 I, I
satisfies η = η ◦ ψ. Then
1 = |η(s)| = η ψ(s) ψ(s) = ψ(s)
and we find ψ(s) = s + const.
for s ∈ I ,
294
2.3 Remarks (a) Suppose Γ is a regular C 2 curve and γ : I → Rn parametrizes
it by arc length. Then γ(s) γ (s) = 0 for s ∈ I. Thus the vector γ(s), γ (s) ⊂
Tγ(s) Rn is orthogonal to the tangent Rt(s) ⊂ Tγ(s) Rn .
From |γ(s)|2 = 1, we find (|γ|2 ) (s) = 2 γ(s) γ (s) = 0 for s ∈ I.
(b) Parametrizing curves by arc length is a convenient tactic for much theoretical
work; see for example the proof of Proposition 2.12. However, for any concrete
example, performing such a parametrization comes at the cost of doing the arc
length integral, and the resulting new parametrization may not be an elementary
We consider the regular parametrization
γ : [0, 2π] → R2 ,
t → (a cos t, b sin t).
Its image is the ellipse (x/a) + (y/b) = 1 with a, b > 0. The proof of Proposition 2.2
shows that every change of parametrization of γ to one by arc length has the form
a2 sin2 s + b2 cos2 s ds + const
for 0 ≤ t ≤ 2π .
One can show that ϕ is not an elementary function.1
Oriented bases
Suppose B = (b1 , . . . , bn ) and C = (c1 , . . . , cn ) are ordered bases of a real vector
space E of dimension n. Also denote by TB,C = [tjk ] the transformation matrix
from B to C, that is, cj = k=1 tjk bk for 1 ≤ j ≤ n. We say B and C are identically
[or oppositely] oriented if det TB,C is positive [or negative].
2.4 Remarks (a) On the set of ordered bases of E, the assignment
B ∼ C :⇐ B and C are identically oriented
defines an equivalence relation. There are exactly two equivalent classes, the two
orientations of E. By selecting the first and calling it Or, we say (E, Or) is
oriented. Every basis in this orientation is positively oriented, and every basis in
the opposite orientation, which we call −Or, is negatively oriented.
(b) Two ordered orthonormal bases B and C of an n-dimensional inner product
space are identically oriented if and only if TB,C belongs to SO(n).2
(c) The elements of SO(2) are also called rotation matrices, because, for every
T ∈ SO(2), there is a unique rotation angle α ∈ [0, 2π) such that T has the
cos α − sin α
sin α
1 This
ϕ is in fact given by an elliptic integral. For the related theory, see for example [FB95].
Exercises VII.9.2 and VII.9.10.
(see Exercise 10). Therefore any two identically oriented bases of a two-dimensional
inner product space are related by a rotation.
(d) If E is an inner product space and B is an ONB, then TB,C = (cj |bk ) .
This follows immediately from the definition of TB,C .
(e) An oriented basis of Rn is positively oriented if its orientation is the same as
that of the canonical basis.
The Frenet n-frame
In the following, we will set somewhat stronger differentiability assumptions on
Γ and always assume that Γ is a regular C n curve in Rn with n ≥ 2. As usual,
let γ ∈ C n (J, Rn ) be a parametrization of Γ. We say Γ is complete if the vectors
γ(t), . . . , γ (n−1) (t) are linearly independent for every t ∈ J.
Suppose Γ is a complete curve in Rn . An n-tuple e = (e1 , . . . , en ) of functions
J → Rn is the moving n-frame or Frenet n-frame for Γ if
(FB1 ) ej ∈ C 1 (J, Rn ) for 1 ≤ j ≤ n;
(FB2 ) e(t) is a positive orthonormal basis of Rn for t ∈ J;
(FB3 ) γ (k) (t) ∈ span e1 (t), . . . , ek (t) for 1 ≤ k ≤ n − 1 and t ∈ J;
(FB4 ) γ(t), . . . , γ (k) (t) and e1 (t), . . . , ek (t) for t ∈ J and k ∈ {1, . . . , n − 1}
have the same orientation (as bases of span e1 (t), . . . , ek (t) ).
2.5 Remarks (a) From the chain rule, it follows easily that this concept is well
defined, that is, independent of the special parametrization of Γ, in the following
sense: If γ := γ ◦ ϕ is a reparametrization of γ and e is a moving n-frame for Γ that
satisfies (FB3 ) and (FB4 ) when γ is replaced by γ, then e(s) = e(t) for t = ϕ(s).
(b) A C 2 curve in R2 is complete if and only if it is regular.
(c) Suppose Γ = [γ] is a regular C 2 curve in R2 . Also let e1 := γ/|γ| =: (e1 , e2 )
and e2 := (−e2 , e1 ). Then (e1 , e2 ) is a Frenet two-frame of Γ.
(In these and similar maps we identify ej (t) ∈ R2 with the tangent part of
γ(t), ej (t) ∈ Tγ(t) Γ.)
Obviously e1 (t), e2 (t) is an ONB of R2 and is positive because
det[e1 , e2 ] = (e1 )2 + (e2 )2 = |e1 |2 = 1 .
296
2.6 Theorem Every complete C n curve in Rn has a unique moving n-frame.
Proof (i) Suppose Γ = [γ] is a complete C n curve in Rn . One can establish
the existence of a Frenet n-frame using the Gram–Schmidt orthonormalization
process (see for example [Art93, VII.1.22]). Indeed, as a complete curve, Γ is
regular. Therefore e1 (t) := γ(t)/|γ(t)| is defined for t ∈ I. For k ∈ {2, . . . , n − 1},
we define ek recursively. Using the already constructed e1 , . . . , ek−1 , we set
ek := γ (k) −
(γ (k) |ej )ej
and ek := ek /|ek | .
span γ(t), . . . , γ (k−1) (t) = span e1 (t), . . . , ek−1 (t)
and γ (k) (t) ∈ span γ(t), . . . , γ (k−1) (t) , we have ek (t) = 0 for t ∈ J, and thus ek is
well defined. In addition, we have
ej (t) ek (t) = δjk
and γ (k) (t) ∈ span e1 (t), . . . , ek (t)
for 1 ≤ j, k ≤ n − 1 and t ∈ J.
(ii) Now suppose Tk (t) is the transformation matrix from γ(t), . . . , γ (k) (t)
to e1 (t), . . . , ek (t) . From (2.1), we have for k ∈ {2, . . . , n − 1} the recursion
formula
T1 (t) = |γ(t)|−1
and Tk (t) =
Tk−1 (t)
ek (t)
Thus det Tk (t) > 0, which shows that γ(t), . . . , γ (k) (t) and e1 (t), . . . , ek (t) have
the same orientation.
(iii) Finally, we define en (t) for t ∈ J by solving the linear system
ej (t) x = 0
for j = 1, . . . , n − 1 and
det e1 (t), . . . en−1 (t), x = 1
for x ∈ Rn . Because e1 (t), . . . en−1 (t)
is 1-dimensional, (2.4) has a unique
solution. From (2.3) and (2.4), it now follows easily that e = (e1 , . . . , en ) satisfies
the postulates (FB2 )–(FB4 ).
(iv) It remains to verify (FB1 ). From (2.1), it follows easily that ek ∈
C 1 (J, Rn ) for k = 1, . . . , n − 1. Thus (2.4) and Cramer’s rule (see [Art93, § I.5])
shows that en also belongs to C 1 (J, Rn ). Altogether we have shown that e is a
Frenet n-frame of Γ.
(v) To check the uniqueness of e, suppose (δ1 , . . . , δn ) is another Frenet nframe of Γ. Then clearly we have e1 = δ1 = γ/|γ|. Also, from (FB2 ) and (FB3 ),
it follows that γ (k) has the expansion
(γ (k) |δj )δj
γ (k) =
for every k ∈ {1, . . . , n − 1}. Thus ej = δj for 1 ≤ j ≤ k − 1 and k ∈ {2, . . . , n − 1},
so (2.1) implies γ (k) |δk )δk = ek |ek |. Hence, there is for every t ∈ J a real
number α(t) such that |α(t)| = 1 and δk (t) = α(t)ek (t). Since δ1 (t), . . . , δk (t)
and e1 (t), . . . , ek (t) are identically oriented, it follows that α(t) = 1. Thus
(e1 , . . . , en−1 ) and therefore also en are uniquely fixed.
2.7 Corollary Suppose Γ = [γ] is a complete C n curve in Rn and e = (e1 , . . . , en )
is a Frenet n-frame of Γ. Then we have the Frenet derivative formula
(˙ j |ek )ek
ej =
We also have the relations
(˙ j |ek ) = −(ej | ek )
for 1 ≤ j, k ≤ n
(˙ j |ek ) = 0
for |k − j| > 1 .
Proof Because e1 (t), . . . , en (t) is an ONB of Rn for every t ∈ J, we get (2.5).
Differentiation of (ej |ek ) = δjk implies (˙ j |ek ) = −(ej | ek ). Finally (2.1) and (2.2)
show that ej (t) belongs to span γ(t), . . . , γ (t) for 1 ≤ j < n. Consequently,
ej (t) ∈ span γ(t), . . . , γ (j+1) (t) = span e1 (t), . . . , ej+1 (t)
and 1 ≤ j < n − 1, and we find (˙ j |ek ) = 0 for k > j + 1 and therefore for
|k − j| > 1.
2.8 Remarks Suppose Γ = [γ] is a complete C n curve in Rn .
(a) The curvature
κj := (˙ j |ej+1 ) |γ| ∈ C(J)
with 1 ≤ j ≤ n − 1 ,
is well defined, that is, it is independent of the parametrization γ.
This is a simple consequence of Remark 2.5(a) and the chain rule.
(b) Letting ωjk := (˙ j |ek ), the Frenet derivative formula reads
ωjk ek
for 1 ≤ j ≤ n ,
298
where the matrix [ωjk ] is
−κ2
−κn−2
−κn−1
κn−1
This follows from Corollary 2.7 and (a).
In the remaining part of this section, we will discuss in more detail the implications of Theorem 2.6 and Corollary 2.7 for plane and space curves.
Curvature of plane curves
In the following, Γ = [γ] is a plane regular C 2 curve and (e1 , e2 ) is the associated
moving two-frame. Then e1 coincides with the tangent part of the unit tangential
vector t = (γ, γ/|γ|). For every t ∈ J,
n(t) := γ(t), e2 (t) ∈ Tγ(t) R2
is the unit normal vector of Γ at the point γ(t). In the plane case, only κ1 is
defined, and it is called the curvature κ of the curve Γ. Therefore
κ = (˙ 1 |e2 ) |γ| .
2.9 Remarks (a) The Frenet derivative formula reads3
and n = −|γ| κt .
If η ∈ C 2 (I, R2 ) parametrizes Γ by arc length, equation (2.7) assumes the simple
t = κn and n = −κt .
In addition, we then have
κ(s) = e1 (s) e2 (s) = t(s) n(s)
η(s)
From (2.8) follows η (s) = κ(s)e2 (s), and thus |¨(s)| = |κ(s)| for s ∈ I. On these
grounds, κ(s)n(s) ∈ Tη(s) R2 is called the curvature vector of Γ at the point η(s).
3 Naturally,
t(t) is understood to be the vector γ(t), e1 (t) ∈ Tγ(t) R2 , etc.
In the case κ(s) > 0 [or κ(s) < 0], the unit tangent vector t(s) spins in the
positive [or negative] direction (because t(s)|n(s) η(s) is the projection of the
instantaneous change of t(s) onto the normal vector). One says that the unit
normal vector n(s) points to the convex [or concave] side of Γ.
(b) When η : I → R2 , s → x(s), y(s) parametrizes Γ by arc length, e2 = (−y, x)
κ = x¨ − y x = det[η, η ] .
This follows from Remark 2.5(c) and from (2.6).
For concrete calculations, it is useful to know how to find the curvature when
given an arbitrary regular parametrization.
2.10 Proposition When γ : J → R2 , t → x(t), y(t) is a regular parametrization
of a plane C 2 curve,
(x)2 +
(y)2
det[γ, γ ]
|γ|3
From e1 = γ/|γ|, it follows that
e1 =
e1 .
|γ|2 |γ|
|γ|2
According to Remark 2.5(c), we have e2 = (−y, x)/|γ|, and the theorem then
follows from (2.6).
2.11 Example (curvature of a graph) Suppose f ∈ C 2 (J, R). Then for the curve
parametrized as J → R2 , x → x, f (x) , the curvature is
1 + (f )2
This follows immediately from Proposition 2.10.
From the characterization of convex functions of Corollary IV.2.13 and the
formula above, we learn that graph(f ) at the point x, f (x) is positively curved
300
if and only if f is convex near x. Because, according to Remark 2.9(b), the unit
normal vector on graph(f ) has a positive second component, this explains the
convention adopted in Remark 2.9(a), which defined the notions of convex and
concave.
Identifying lines and circles
Next, we apply the Frenet formula to show that line segments and circular arcs
are characterized by their curvature.
2.12 Proposition Suppose Γ = [γ] is a plane regular C 2 curve. Then
(i) Γ is a line segment if and only if κ(t) = 0 for t ∈ J;
(ii) Γ is a circular arc with radius r if and only if |κ(t)| = 1/r for t ∈ J.
Proof (i) It is clear that the curvature vanishes on every line segment (see Proposition 2.10).
Conversely, suppose κ = 0. We can assume without losing generality that Γ
has an arc length parametrization η. It follows from Remark 2.9(a) that η (s) = 0
for s ∈ J. The Taylor formula of Theorem IV.3.2 then implies that there are
a, b ∈ R2 such that η(s) = a + bs.
(ii) Suppose Γ is a circular arc with radius r. Then there is an m ∈ R2 = C
and an interval J such that Γ is parametrized by J → C, t → rei t + m or
t → re−i t + m. The claim now follows from Proposition 2.10.
Conversely, suppose κ = δ/r with δ = 1 or δ = −1. We can again assume
that η parametrizes Γ by arc length. Denoting by (e1 , e2 ) the moving two-frame
of Γ, we find from the Frenet formula (2.8) that
η = e1 ,
e1 = (δ/r)e2 ,
e2 = −(δ/r)e1 .
From this follows
η(s) + (r/δ)e2
= η(s) + (r/δ)˙ 2 (s) = 0 for s ∈ I .
Hence there is an m ∈ R2 such that η(s) + (r/δ)e2 (s) = m for s ∈ I. Thus we get
|η(s) − m| = |re2 (s)| = r
which shows that η parametrizes a circular arc with radius r.
Instantaneous circles along curves
Suppose Γ = [γ] is a plane regular C 2 curve. If κ(t0 ) = 0 at t0 ∈ J, we call
r(t0 ) := 1 |κ(t0 )|
the instantaneous radius and
m(t0 ) := γ(t0 ) + r(t0 )e2 (t0 ) ∈ R2
the instantaneous center of Γ at the point γ(t0 ).
The circle in R2 with center m(t0 ) and radius
r(t0 ) is the instantaneously osculating circle of
Γ at γ(t0 ).
2.13 Remarks
(a) The osculating circle has at γ(t0 ) the parametrization
[0, 2π] → R2 ,
t → m(t0 ) + |κ(t0 )|−1 (cos t, sin t) .
(b) The osculating circle is the unique circle that touches Γ at γ(t0 ) to at least
second order.
Proof We can assume that η is a parametrization of Γ by arc length with η(s0 ) = γ(t0 ).
Also assume (a, b) is a positive ONB of R2 . Finally, let
γ : [s0 , s0 + 2πr] → R2 ,
s → γ(s)
γ(s) := m + r cos (s − s0 )/r a + r sin (s − s0 )/r b
parametrize by arc length the circle K with center m and radius r. Then Γ and K touch
at η(s0 ) to at least second order if η (j) (s0 ) = γ (j) (s0 ) for 0 ≤ j ≤ 2 or, equivalently, if
the equations
η(s0 ) = m + ra , η(s0 ) = b , η (s0 ) = −a/r
hold. Because e1 = η, the first Frenet formula implies κ(s0 )e2 (s0 ) = −a/r. Because the
orthonormal basis −e2 (s0 ), e1 (s0 ) is positively oriented, we find that |κ(s0 )| = 1/r and
e2 (t0 ).
(a, b) = −e2 (s0 ), e1 (s0 ) and m = η(s0 ) + re2 (s0 ) = γ(t0 ) + κ(t0 )
(c) When κ(t) = 0 for t ∈ J,
t → m(t) := γ(t) + e2 (t)/κ(t)
is a continuous parametrization of a plane curve, the evolute of Γ.
302
The vector product
From linear algebra, there is “another product” in R3 besides the inner product. It
assigns to two vectors a and b another vector a × b, the vector or cross product of a
and b. For completeness, we recall how it is defined and collect its most important
properties.
For a, b ∈ R3 ,
c → det[a, b, c]
is a (continuous) linear form on R . Therefore, by the Riesz representation theorem, there is exactly one vector, called a × b, such that
(a × b |c) = det[a, b, c] for c ∈ R3 .
(2.9)
Through this, we define the vector or cross product.
× : R3 × R3 → R3 ,
(a, b) → a × b .
2.14 Remarks (a) The cross product is an alternating bilinear map, that is, × is
bilinear and skew-symmetric:
a × b = −b × a
for a, b ∈ R3 .
This follows easily from (2.9), because the determinant is an alternating4 trilinear
(b) For a, b ∈ R3 , we have a × b = 0 if and only if a and b are linearly independent.
If they are, (a, b, a × b) is a positively oriented basis of R3 .
Proof From (2.9), we have a × b = 0 if and only if det[a, b, c] vanishes for every choice
of c ∈ R3 . Were a and b linearly independent, we would have det[a, b, c] = 0 by choosing
c such that a, b and c are linearly independent, which, because of (2.9), contradicts the
assumption a × b = 0. This implies the first claim. The second claim follows because
(2.9) gives
det[a, b, a × b] = |a × b|2 ≥ 0 .
(c) For a, b ∈ R3 , the vector a × b is perpendicular to a and b.
Proof This follows from (2.9), because, for c ∈ {a, b}, the determinant has two identical
(d) For a, b ∈ R3 , the cross product has the explicit form
a × b = (a2 b3 − a3 b2 , a3 b1 − a1 b3 , a1 b2 − a2 b1 ) .
4 An
one.
m-linear map is alternating if exchanging two of its arguments multiplies it by negative
This follows again from (2.9) by expanding the determinant in its last column.
(e) For a, b ∈ R3 \{0}, the (unoriented) angle ϕ := (a, b) ∈ [0, π] between a and
b is defined by cos ϕ := (a|b)/|a| |b|.5 Then we have
|a × b| =
|a|2 |b|2 − (a|b)2 = |a| |b| sin ϕ .
This means that |a × b| is precisely the (unoriented) area of the parallelogram spanned by a
and b.
Proof Because of the Cauchy–Schwarz inequality,
(a, b) ∈ [0, π] is well defined. The first equality can
be verified directly from (d), and the second follows
from cos2 ϕ + sin2 ϕ = 1 and sin ϕ ≥ 0.
(f ) When a and b are orthonormal vectors in R3 , the (ordered) set (a, b, a × b) is
a positive ONB on R3 .
This follows from (b) and (e).
Working now in three dimensions, suppose Γ = [γ] is a complete C 3 curve in R3 .
Then Γ has two curvatures κ1 and κ2 . We call κ := κ1 the curvature and τ := κ2
the torsion. Therefore
κ = (˙ 1 |e2 )/|γ| and τ = (˙ 2 |e3 )/|γ| .
(2.10)
Here we also call
n(t) := γ(t), e2 (t) ∈ Tγ(t) R3
the unit normal vector, while
b(t) := γ(t), e3 (t) ∈ Tγ(t) R3
is the unit binormal vector of Γ at the point γ(t). The vectors t(t) and n(t) span
the osculating plane of Γ at the point γ(t). The plane spanned by n(t) and b(t) is
the normal plane of Γ at γ(t).
(a) We have e3 = e1 × e2 , that is, b = t × n.
(b) When Γ is parametrized by arc length, the Frenet formula gives
e1 = κe2 ,
e2 = −κe1 + τ e3 ,
e3 = −τ e2 ,
5 This
angle can be defined in any inner product space, not just R3 . Also
(a, b) =
(b, a).
304
(c) The curvature of a complete curve is always positive. When η is an arc length
parametrization,
Because e1 = η and because (2.1) and Remark 2.3(a) imply that e2 = η /|¨|, we
κ = (˙ 1 | e2 ) = η η /|¨| = |¨| > 0 .
The claimed second equality now follows from Remark 2.14(e).
(d) When η is an arc length parametrization of Γ, we have τ = det[η, η , η ]/κ2 .
Proof Because η = e2 |¨| = κe2 and from the Frenet derivative formula for e2 , we
get η = κe2 − κ2 e1 + κτ e3 . Thus det[η, η , η ] = det[e1 , κe2 , κτ e3 ] = κ2 τ . The proof is
finished by observing that (c) together with the completeness of Γ implies that κ vanishes
nowhere.
The torsion τ (t) is a measure of how, in the vicinity
of γ(t), the curve Γ = [γ]
“winds” out of the osculating
plane, shown in lighter gray at
right. Indeed, a plane curve is
characterized by vanishing torsion, as the following theorem
2.16 Proposition A complete curve lies in a plane if and only if its torsion vanishes
everywhere.
Proof We can assume that η parametrizes Γ by arc length. If η(s) lies in a plane
for every s ∈ I, so does η(s), η (s), and η (s). With Remark 2.15(d), this implies
... 2
τ = det[η, η , η ]/κ = 0. Conversely, if τ = 0, the third Frenet derivative formula
gives that e3 (s) = e3 (α) for every s ∈ I and an α ∈ I. Because e1 (s) ⊥ e3 (s),
η(s) e3 (α) = η(s) e3 (α) = e1 (s) e3 (s) = 0 for s ∈ I .
Therefore η(·) e3 (α) is constant on I, which means that the orthogonal projection of η(s) onto the line Re3 (α) is independent of s ∈ I. Therefore Γ lies in the
plane orthogonal to e3 (α).
1 Suppose r ∈ C 2 (J, R+ ) has r(t) + r(t) > 0 for t ∈ J and γ(t) := r(t)(cos t, sin t).
Show the curve parametrized by γ has the curvature
2[r]2 − r¨ + r 2
r 2 + [r]2
Calculate the curvature of
the lima¸on of Pascal
[−π, π] → R2 ,
the logarithmic spiral
the cycloid
t → (1 + 2 cos t)(cos t, sin t) ;
t → eλt (cos t, sin t) ;
t → R(t − sin t, 1 − cos t) .
Suppose γ : J → R3 , t → x(t), y(t) is a regular C 2 parametrization of a plane curve
Γ, and κ(t) = 0 for t ∈ J. Show that the evolute of Γ is parametrized by t → m(t), where
(a) the evolute of the parabola y = x2 /2 is the Neil, or semicubical, parabola R → R2 ,
t → (−t3 , 1 + 3t2 /2);
(b) the evolute of the logarithmic spiral R → R2 , t → eλt (cos t, sin t) is the logarithmic
spiral R → R2 , t → λeλ(t−π/2) (cos t, sin t);
(c) the evolute of the cycloid R → R2 , t → (t − sin t, 1 − cos t) is the cycloid R → R2 ,
t → (t + sin t, cos t − 1).
5 Let γ ∈ C 3 (J, R2 ) be a regular parametrization of a plane curve Γ, and suppose
κ(t)κ(t) = 0 for t ∈ J. Also denote by m the parametrization given in Remark 2.13(c)
for the evolute M of Γ. Show
(a) M is a regular C 1 curve;
(b) the tangent to Γ at γ(t) is perpendicular to the tangent to M at m(t);
(c) the arc length of M is given by
|κγ | κ−2 dt .
L(M ) =
(Hint for (b) and (c): the Frenet derivative formulas.)
306
6 Show that the logarithmic spiral Γ slices through straight lines from the origin at a
constant angle, that is, at any point along the spiral, the angle between the tangent to
Γ and the line from the origin is the same.
Calculate the curvature and the torsion
(a) for the elliptic helix R → R3 , t → (a cos t, b sin t, ct) with ab = 0 for a, b, c ∈ R;
(b) for the curve R → R3 , t → t(cos t, sin t, 1).
Suppose Γ is a regular closed C 1 curve in the plane. Prove the isoperimetric inequality
4πA(Γ) ≤ L(Γ)
(see Exercise 1.6).
(Hints: (i) Identify R2 with C and consider first the case L(Γ) = 2π. Then, without
loss of generality, let γ ∈ C 1 [0, 2π], C parametrize Γ by arc length. Using Parseval’s
equation (Corollary VI.7.16) and Remark VI.7.20(b), show
|γ|2 dt =
k2 |γk |2 .
Further show
A(Γ) =
Im γ γ dt .
Thus from Exercise VI.7.11 and Remark VI.7.20(b), see that
k |γk |2 ,
A(Γ) = π
k2 |γk |2 = π = L(Γ)
A(Γ) ≤ π
4π .
(ii) The case L(Γ) = 2π reduces to (i) by the transformation γ → 2πγ/L(Γ).)
9 Suppose I and J are compact intervals and U is an open neighborhood of I × J in
R2 . Further, suppose γ ∈ C 3 (U, R2 ) satisfies the conditions
(i) for every λ ∈ I, the function γλ := γ(λ, ·) | J : J → R2 is a regular parametrization
of a C 3 curve Γλ ;
(ii) ∂1 γ(λ, t) = κΓλ (t)nΓλ (t)
for (λ, t) ∈ I × J.
Show that the function
: I→R,
λ → L(Γλ )
has continuous derivatives and that
(λ)
(λ) = −
κ s(λ)
ds(λ)
for λ ∈ I ,
where s(λ) is the arc length paramater of Γλ for λ ∈ I.
(Hint: Define v : I × J → R, (λ, t) → |∂1 γ(λ, t)|. Using the Frenet formulas and (ii),
conclude ∂1 v = −κ2 v.)
Show that for every T ∈ SO(2), there is a unique α ∈ [0, 2π) such that
(Hint: Exercise VII.9.2.)
− sin α
308
3 Pfaff forms
In Section VI.5, we introduced differentials in an ad hoc way, so that we could represent integration rules more easily. We will now develop a calculus for using these
previously formal objects. This calculus will naturally admit the simple calculation
rules from before and will also add much clarity and understanding. Simultaneously, this development will expand the concept of an integral from something
done on intervals to something that can be done on curves.
In the following let
• X be open in Rn and not empty; also suppose q ∈ N ∪ {∞}.
Vector fields and Pfaff forms
A map v : X → T X in which v(p) ∈ Tp X for p ∈ X is a vector field on X. To
every vector field v, there corresponds a unique function v : X → Rn , the tangent
part of v, such that v(p) = p, v(p) . With the natural projection
pr2 : T X = X × Rn → Rn ,
(p, ξ) → ξ ,
we have v = pr2 ◦ v.
A vector field v belongs to the class C q if v ∈ C q (X, Rn ). We denote the set
of all vector field of class C q on X by V q (X).
Suppose E is a Banach space over K and E := L(E, K) is its dual space.
·, · : E × E → K , (e , e) → e , e := e (e) ,
assigns every pair (e , e) ∈ E × E the value of e at the location e and is called
the dual pairing between E and E .
We will now think about the spaces dual to the tangential spaces of X. For
p ∈ X, the space dual to Tp X is the cotangent space of X at the point p and is
denoted Tp X. Also
is the cotangential bundle of X. The elements of Tp X, the cotangential vectors at
p, are therefore linear forms on the tangential space of X at p. The corresponding
dual pairing is denoted by
Every map α : X → T ∗ X in which α(p) ∈ Tp X for p ∈ X is called a Pfaff form
on X. Sometimes we call α a differential form of degree 1 or a 1-form.
VIII.3 Pfaff forms
3.1 Remarks (a) If E is a Banach space over K, we have ·, · ∈ L(E , E; K).
Proof It is clear that the dual pairing is bilinear. Further, Conclusion VI.2.4(a) gives
the estimate
| e , e | = |e (e)| ≤ e E e E for (e , e) ∈ E × E .
Therefore Proposition VII.4.1 shows that ·, · belongs to L(E , E; K).
(b) For (p, e ) ∈ {p} × (Rn ) , define J(p, e ) ∈ Tp X through
J(p, e ), (p, e)
= e ,e
for e ∈ Rn ,
where ·, · denotes the dual pairing between Rn and (Rn ) . Then J is an isometric
isomorphism of {p} × (Rn ) onto Tp X. Using this isomorphism, we identify Tp X
with {p} × (R ) .
Proof Obviously J is linear. If J(p, e ) = 0 for some (p, e ) ∈ {p} × (Rn ) , then e = 0
follows from (3.1). Therefore J is injective. Because
dim {p} × (Rn )
= dim {p} × Rn = dim(Tp X) = dim(Tp X) ,
J is also surjective. The claim then follows from Theorem VII.1.6.
Because Tp X = {p} × (Rn ) , there is for every Pfaff form α a unique map
α : X → (R ) , the cotangent part of α, such that α(p) = p, α(p) . A 1-form α
belongs to the class C q if α ∈ C q X, (Rn ) . We denote the set of all 1-forms of
class C q on X by Ω(q) (X).
On V q (X), we define multiplication by functions in C q (X) by
C q (X) × V q (X) → V q (X) ,
(a, v) → av
using the pointwise multiplication (av)(p) := a(p)v(p) for p ∈ X. Likewise, on
Ω(q) (X), we define multiplication by functions in C q (X) by
C q (X) × Ω(q) (X) → Ω(q) (X) ,
(a, α) → aα
using the pointwise multiplication (aα)(p) := a(p)α(p) for p ∈ X.
Then one easily verifies that V q (X) and Ω(q) (X) are modules1 over the (commutative) ring C q (X). Because C q (X) naturally contains R as a subring (using
the identification of λ ∈ R and λ1 ∈ C q (X)), we have in particular that V q (X)
and Ω(q) (X) are real vector spaces.
3.2 Remarks (a) For α ∈ Ω(q) (X) and v ∈ V q (X),
p → α(p), v(p)
= α(p), v(p)
∈ C q (X) .
completeness, we have put the relevant facts about modules at the end of this section.
310
(b) When no confusion is expected, we identify α ∈ Ω(q) (X) with its cotangent
part α = pr2 ◦ α and identify v ∈ V q (X) with its tangent part v = pr2 ◦ v. Then
V q (X) = C q (X, Rn ) and Ω(q) (X) = C q X, (Rn )
(c) Suppose f ∈ C q+1 (X), and
dp f = pr2 ◦ Tp f
is the differential of f (see Section VII.10). Then the map df := (p → dp f ) belongs
to Ω(q) (X), and we have dp f = ∂f (p). From now on, we write df (p) for dp f .
The canonical basis
In the following, let (e1 , . . . , en ) be the standard basis of Rn , and let (ε1 , . . . , εn )
be the corresponding dual basis2 of (Rn ) , that is,3
ε j , e k = δk
for j, k ∈ {1, . . . , n} .
We also set
dxj := d(prj ) ∈ Ω(∞) (X) for j = 1, . . . , n ,
using the j-th projection prj : Rn → R, x = (x1 , . . . , xn ) → xj .
3.3 Remarks (a) According to Remark 3.1(b), (εj )p = (p, εj ) belongs to Tp X for
every j = 1, . . . , n, and (ε )p , . . . , (ε )p is the basis that is dual to the canonical
basis (e1 )p , . . . , (en )p of Tp X.
(b) We have
dxj (p) = (p, εj ) for p ∈ X and j = 1, . . . , n .
The claim follows from Proposition VII.2.8, which gives
dxj (p), (ek )p
= ∂(prj )(p), ek = ∂k prj (p) = δjk .
(c) We set ej (p) := (p, ej ) for 1 ≤ j ≤ n and p ∈ X. Then (e1 , . . . , en ) is a module
basis of V q (X), and (dx1 , . . . , dxn ) is a module basis of Ω(q) (X). Also, we have
dxj (p), ek (p)
for 1 ≤ j, k ≤ n and p ∈ X ,
and (e1 , . . . , en ) is the canonical basis of V q (X). Likewise, (dx1 , . . . , dxn ) is the
canonical basis of Ω(q) (X).4
existence of dual bases is a standard result of linear algebra.
3 δ j := δ jk := δ
jk is the Kronecker symbol.
4 These basis are dual to each other in the sense
of module duality (see [SS88, § 70]).
Finally, we define
α, v : Ω(q) (X) × V q (X) → C q (X)
α, v (p) := α(p), v(p)
Then (3.2) takes the form
dxj , ek = δk
Suppose α ∈ Ω(q) (X). We set
aj (p) := α(p), ej (p)
Then, due to Remark 3.2(a), aj belongs to C q (X), and therefore β :=
to Ω(q) (X). Also,
aj dxj belongs
β(p), ek (p)
aj (p) dxj (p), ek (p)
= ak (p) = α(p), ek (p)
and k = 1, . . . , n. Therefore α = β, which shows that (e1 , . . . en ) and (dx1 , . . . , dxn ) are
module bases and that (3.2) holds.
(d) Every α ∈ Ω(q) (X) has the canonical basis representation
α, ej dxj .
(e) For f ∈ C q+1 (X),
df = ∂1 f dx1 + · · · + ∂n f dxn .
The map d : C q+1 (X) → Ω(q) (X) is R-linear.
df (p), ej (p)
= ∂j f (p)
the first claim follows from (d). The second is obvious.
(f ) V q (X) and Ω(q) (X) are infinite-dimensional vectors spaces over R.
Proof We consider X = R and leave the general case to you. From the fundamental
theorem of algebra, it follows easily that the monomials { X m ; m ∈ N } in V q (R) = C q (R)
are linearly independent over R. Therefore V q (R) is an infinite-dimensional vector space.
We already know that Ω(q) (R) is an R-vector space. As in the case of V q (R), we see that
{ X m dx ; m ∈ N } ⊂ Ω(q) (R) is a linearly independent set over R. Therefore Ω(q) (R) is
also an infinite-dimensional R-vector space.
312
(g) The C q (X)-modules V q (X) and Ω(q) (X) are isomorphic. We define a module
isomorphism, the canonical isomorphism, through
Θ : V q (X) → Ω(q) (X) ,
a j ej →
aj dxj .
This follows from (c) and (d).
(h) For f ∈ C q+1 (X), we have Θ−1 df = grad f , that is, the diagram5
$ Ω(q) (X)
C q+1 (X)
z V q (X)
grad
is commutative.
Due to (e), we have
Θ−1 df = Θ−1
∂j f ej = grad f
for f ∈ C q+1 (X) .
We have seen that every function f ∈ C q+1 (X) induces the Pfaff form df . Now
we ask whether every Pfaff form is so induced.
Suppose α ∈ Ω(q) (X), where in the following we apply the identification rule
of Remark 3.2(b) If there is a f ∈ C q+1 (X) such that df = α, we say α is exact,
and f is an antiderivative of α.
3.4 Remarks (a) Suppose X is a domain and α ∈ Ω(0) (X) is exact. If f and g
are antiderivatives of α, then f − g is constant.
First, α = df = dg implies d(f − g) = 0. Then apply Remark VII.3.11(c).
(b) Suppose α =
aj dxj ∈ Ω(1) (X) is exact. Then it satisfies integrability
∂k aj = ∂j ak
Proof Suppose f ∈ C 2 (X) has df = α. From Remarks 3.3(d) and (e), we have aj = ∂j f .
Thus it follows from the Schwarz theorem (Corollary VII.5.5(ii)) that
∂k aj = ∂k ∂j f = ∂j ∂k f = ∂j ak
5 Analogously to the notation df , we write grad f , or \nabla f , for the map p → \nabla  f (see Section
VII.10). In every concrete case, it will be clear from context whether grad f (p) means the value
of this function at the point p or the tangent part of \nabla p f .
(c) If X ⊂ R, every form α ∈ Ω(q) (X) is exact.
Proof Because X is a disjoint union of open intervals (see Exercise III.4.6), it suﬃces to
consider the case X = (a, b) with a < b. According to Remark 3.3(c), there is for every
α ∈ Ω(q) (X) an a ∈ C q (X) such that α = a dx. Suppose p0 ∈ (a, b) and
a(x) dx
f (p) :=
for p ∈ (a, b) .
Then f belongs to C q+1 (X) with f = a. Therefore df = a dx = α follows from Remark 3.3(e).
With the help of the canonical isomorphism Θ : V q (X) → Ω(q) (X), the properties of Pfaff forms can be transferred to vector fields. We thus introduce the
following notation: a vector field v ∈ V q (X) is a gradient field if there is an
f ∈ C q+1 (X) such that v = \nabla f . In this context, one calls f a potential for v.
Remarks 3.3 and 3.4 imply immediately the facts below about gradient fields.
3.5 Remarks (a) If X is a domain, then two potentials of any gradient field on
it differ by at most a constant.
(b) If v = (v1 , . . . , vn ) ∈ V 1 (X) is a gradient field, the integrability conditions
∂k vj = ∂j vk hold for 1 ≤ j, k ≤ n.
(c) A Pfaff form α ∈ Ω(q) (X) is exact if and only if Θ−1 α ∈ V q (X) is a gradient
field.
3.6 Examples (a) (central fields) Suppose X := Rn \{0} and the components of
the Pfaff form
aj dxj ∈ Ω(q) (X)
have the representation aj (x) := xj ϕ(|x|) for x = (x1 , . . . , xn ) and 1 ≤ j ≤ n,
where ϕ ∈ C q (0, ∞), R . Then α is exact. An antiderivative f is given by
f (x) := Φ(|x|) with
tϕ(t) dt
Φ(r) :=
for r > 0 ,
where r0 is a strictly positive number. Therefore
the vector field v := j=1 aj ej = Θ−1 α is a gradient field in which the vector v(x) ∈ Tx (X) lies
on the line through x and 0 for every x ∈ X. The
equipotential surfaces, that is, the level sets of f ,
are the spheres rS n−1 for r > 0, these spheres
are perpendicular to the gradient fields (see Example VII.3.6(c)).
314
Proof It is clear that Φ belongs to C q+1 (0, ∞), R and that Φ (r) = rϕ(r). It then
follows from the chain rule that
∂j f (x) = Φ (|x|) xj /|x| = xj ϕ(|x|) = aj (x)
for j = 1, . . . , n and x ∈ X .
The remaining claims are obvious.
(b) The central field x → cx/|x|n with c ∈ R× and n ≥ 2 is a gradient field. A
potential U is given by
n=2,
2−n
n>2,
/(2 − n),
for x = 0. This potential plays an important role in physics.6 Depending on the
physical setting, it may be either the Newtonian or the Coulomb potential.
This is a special case of (a).
The Poincar´ lemma
The exact 1-forms are especially important class of differential forms. We already
know that every exact 1-form satisfies the integrability conditions. On the other
hand, there are 1-forms that are not exact but still satisfy the integrability conditions. It is therefore useful to seek conditions that, in addition to the already
necessary integrability conditions, will secure the existence of an antiderivative.
We will now see, perhaps surprisingly, that the existence depends on the topological properties of the underlying domain.
A continuously differentiable Pfaff form is a closed if it satisfies the integrability conditions. A subset of M of Rn is star shaped (with respect to x0 ∈ M ) if
there is an x0 ∈ M such that for every x ∈ M the straight line path [[x0 , x]] from
x0 to x lies in M .
star shaped
name “potential” comes from physics.
not star shaped
3.7 Remarks (a) Every exact, continuously differentiable 1-form is closed.
(b) If M is star shaped, M is connected.
This follows from Proposition III.4.8.
(c) Every convex set is star shaped with respect to any of its points.
Suppose X is star shaped with respect to 0 and f ∈ C 1 (X) is an antiderivative
of α = j=1 aj dxj . Then aj = ∂j f , and Remark 3.3(c) gives
α(tx), ej = aj (tx) = ∂j f (tx) = \nabla f (tx) ej
Therefore α(tx), x = \nabla f (tx) x for x ∈ X, and the mean value theorem in
integral form (Theorem VII.3.10) gives the representation
\nabla f (tx) x dt =
f (x) − f (0) =
α(tx), x dt ,
which is the key to the next theorem.
3.8 Theorem (the Poincar´ lemma) Suppose X is star shaped and q ≥ 1. When
α ∈ Ω(q) (X) is closed, α is exact.
Proof Suppose X is star shaped with respect to 0 and α =
for x ∈ X, the segment [[0, x]] lies in X,
ak (tx)xk dt
α(tx), x dt =
aj dxj . Because,
is well defined. Also
∈ C q [0, 1] × X ,
(t, x) → α(tx), x
and, from the integrability conditions ∂j ak = ∂k aj , we get
α(tx), x = aj (tx) +
txk ∂k aj (tx) .
Thus Proposition VII.6.7 implies that f belongs to C q (X) and that
∂j f (x) =
aj (tx) dt +
xk ∂k aj (tx) dt .
Integrating the the second integral by parts with u(t) := t and vj (t) := aj (tx)
aj (tx) dt + taj (tx)
aj (tx) dt = aj (x)
316
Finally, since ∂j f = aj belongs to C q (X) for j = 1, . . . , n, we find f ∈ C q+1 (X)
and df = α (see Remark 3.3(e)).
The case in which X is star shaped with respect to the point x0 ∈ X can
clearly be reduced to the situation above by the translation x → x − x0 .
3.9 Remarks (a) The central fields of Example 3.6(a) show that a domain need
not be star shaped for a potential (or an antiderivative) to exist.
(b) The proof of Theorem 3.8 is constructive, that is, the formula (3.4) gives an
Until now, we exclusively used in Ω(q) (X) the canonical module basis consisting of the 1-forms dx1 , . . . , dxn . This choice is natural if we are using in V q (X)
the canonical basis (e1 , . . . , en ) (see Remark 3.3(c)). In other words, if we understand X as a submanifold of Rn , the module basis (dx1 , . . . , dxn ) is adapted to the
trivial chart ϕ = idX . If we describe X using another chart ψ, we anticipate that
α ∈ Ω(q) (X) will be better represented better in a module basis adapted to ψ.7
To pass from one representation to another, we must study how the Pfaff forms
transform under change of charts.
Dual operators
We first introduce — in a generalization of the transpose of a matrix — the concept
of the “transpose” or “dual” of a linear operator.
Suppose E and F are Banach spaces. Then, to each A ∈ L(E, F ), we define
a dual or transposed operator through
A : F →E ,
f →f ◦A .
3.10 Remarks (a) The operator dual to A is linear and continuous, that is,
A ∈ L(F , E ). Also8
A f , e = f , Ae
for e ∈ E and f ∈ F ,
L(F ,E )
≤ A
Proof From A ∈ L(E, F ) it follows that A f belongs to E for f ∈ F . It is obvious
that A : F → E is linear and that (3.5) holds. The latter implies
| A f , e | = | f , Ae | ≤ f
7 We
Ae ≤ f
for e ∈ E and f ∈ F .
will gain a better understanding of this issue in Volume III when we study differential
forms on manifolds.
8 In functional analysis, it is shown that A is characterized by (3.5) and that (3.6) holds with
equality.
From this we read off (see Remark VII.2.13(a)) the inequality
Therefore we get (3.6) from the definition of the operator norm (see Section VI.2). Now
Theorem VI.2.5 guarantees that A is continuous.
(b) If E = (e1 , . . . , en ) and F = (f1 , . . . , fm ) are bases of the finite-dimensional
Banach spaces E and F , then [A ]E ,F = [A]E,F , where M ∈ Kn×m denotes
the matrix dual to M ∈ Km×n and E and F are the bases dual to E and F .
Proof This follows easily from the definition of the representation matrix (see Section
VII.1) and by expanding f in the basis dual to F.
(c) If G is another Banach space,
(BA) = A B
for A ∈ L(E, F ) and B ∈ L(F, G) ,
and (1E ) = 1E .
(d) For A ∈ Lis(E, F ), A
belongs to Lis(F , E ), and (A )−1 = (A−1 ) .
This follows immediately from (c).
Transformation rules
In the following, suppose X is open in Rn and Y is open in Rm . In the special
cases n = 1 or m = 1, we also allow X or Y (as the case may be) to be a perfect
interval. Also suppose ϕ ∈ C q (X, Y ) with q ∈ N× ∪ {∞}.
Using ϕ, we define the map
the pull back, through
More precisely, ϕ∗ is the pull back of functions.9
For Pfaff forms, we define the pull back by ϕ through
ϕ∗ : Ω(q−1) (Y ) → Ω(q−1) (X) ,
ϕ∗ α(p) := (Tp ϕ) α ϕ(p) .
From its operand, it will always be obvious whether ϕ∗ means the pull back of
functions or of 1-forms. The pull back take functions (or 1-forms) “living” on Y
to ones on X, whereas the original ϕ goes the other way.
definition is obviously meaningful for f ∈ Z Y and any sets X, Y , and Z.
318
3.11 Remarks (a) The pull backs of f ∈ C q (Y ) and α ∈ Ω(q−1) (Y ) are defined
through the commutativity of the diagrams
where (T ϕ)v ∈ T Y for v ∈ T X is determined through
(T ϕ)v ϕ(p) := (Tp ϕ)v(p)
(b) For α ∈ Ω(q−1) (Y ),
ϕ∗ α(p) = (Tp ϕ) α ϕ(p) = p, ∂ϕ(p)
α ϕ(p)
From the definition of ϕ∗ α, we have
ϕ∗ α(p), v(p)
= α ϕ(p) , (Tp ϕ)v(p)
ϕ(p)
= α ϕ(p) , ∂ϕ(p)v(p) =
∂ϕ(p)
α ϕ(p) , v(p)
for p ∈ X and v ∈ V q (X), and the claim follows.
(c) The maps
ϕ∗ : C q (Y ) → C q (X)
ϕ∗ : Ω(q−1) (Y ) → Ω(q−1) (X)
(3.8)
are well defined and R-linear. It should be pointed out specifically that in (3.8)
the “order of regularity” of the considered Pfaff forms is q − 1 and not q, whereas
q is the correct order of regularity in (3.7). Because the definition involves the
tangential map, the one derivative order is “lost”.
Proof The statements about the map (3.7) follow immediately from the chain rule. The
properties of (3.8) are likewise implied by the chain rule and (b).
In the next theorem, we gather the rules for pull backs.
3.12 Proposition
interval. Then
Suppose Z is open in R or (in the case
= 1) is a perfect
(i) (idX )∗ = idF (X) for F (X) := C q (X) or F (X) := Ω(q−1) (X),
(ψ ◦ ϕ)∗ = ϕ∗ ψ ∗ for ϕ ∈ C q (X, Y ) and ψ ∈ C q (Y, Z);
(ii) ϕ∗ (f g) = (ϕ∗ f )(ϕ∗ g) for f, g ∈ C q (Y ),
ϕ∗ (hα) = (ϕ∗ h)ϕ∗ α for h ∈ C (q−1) (Y ) and α ∈ Ω(q−1) (Y );
(iii) ϕ∗ (df ) = d(ϕ∗ f ) for f ∈ C q (Y ), that is, the diagram
C q (Y )
C q (X)
E Ω(q−1) (Y )
E Ω(q−1) (X)
(i) The first claim is clear. To prove the second formula, we must show
(ψ ◦ ϕ)∗ f = (ϕ∗ ◦ ψ ∗ )f
for f ∈ C q (Z) ,
(ψ ◦ ϕ)∗ α = (ϕ∗ ◦ ψ ∗ )α
for α ∈ Ω(q−1) (Z) .
For f ∈ C q (Z), we have
(ψ ◦ ϕ)∗ f = f ◦ (ψ ◦ ϕ) = (f ◦ ψ) ◦ ϕ = (ψ ∗ f ) ◦ ϕ = (ϕ∗ ◦ ψ ∗ )f .
For α ∈ Ω(q−1) (Y ), it follows from the chain rule of Remark VII.10.2(b) and from
Remark 3.10(c) that, for p ∈ X,
(ψ ◦ ϕ)∗ α(p) = Tp (ψ ◦ ϕ)
α ψ ◦ ϕ(p) = (Tϕ(p) ψ) ◦ Tp ϕ
= (Tp ϕ) (Tϕ(p) ψ) α ψ ϕ(p)
α ψ ϕ(p)
= (Tp ϕ) ψ α ϕ(p)
= ϕ ψ α(p) .
(ii) The first statement is clear. The second follows, for p ∈ X, from
ϕ∗ (hα)(p) = (Tp ϕ) (hα) ϕ(p) = (Tp ϕ) h ϕ(p) α ϕ(p)
= h ϕ(p) (Tp ϕ) α ϕ(p) = (ϕ∗ h)ϕ∗ α (p) .
(iii) From the chain rule for the tangential map and the definition of differentials, we get
ϕ∗ (df )(p) = (Tp ϕ) df ϕ(p) = df ϕ(p) ◦ Tp ϕ
= pr2 ◦ Tϕ(p) f ◦ Tp ϕ = pr2 ◦ Tp (f ◦ ϕ) = d(ϕ∗ f )(p) ,
and therefore the claim.
320
3.13 Corollary For ϕ ∈ Diff q (X, Y ), the maps
are bijective with (ϕ∗ )−1 = (ϕ−1 )∗ .
This follows from Proposition 3.12(i).
3.14 Examples (a) Denote points in X and Y by x = (x1 , . . . , xn ) ∈ X and
y = (y 1 , . . . , y m ) ∈ Y , respectively. Then
for 1 ≤ j ≤ m .
From Proposition 3.12(iii), we have
ϕ∗ dy j = ϕ∗ d(prj ) = d(ϕ∗ prj ) = dϕj =
where the last equality is implied by Remark 3.3(e).
(b) For α =
aj dy j ∈ Ω(0) (Y ), we have
(aj ◦ ϕ)∂k ϕj dxk .
Proposition 3.12(ii) and the linearity of ϕ∗ give
ϕ∗ (aj dy j ) =
aj dy j =
(ϕ∗ aj )(ϕ∗ dy j ) .
The claim then follows from (a).
(c) Suppose X is open in R and α ∈ Ω(0) (Y ). Then
ϕ∗ α(t) = α ϕ(t) , ϕ(t) dt
(d) Suppose Y ⊂ R is a compact interval, f ∈ C(Y ), and α := f dy ∈ Ω(0) (Y ).
Also define X := [a, b], and let ϕ ∈ C 1 (X, Y ). Then, according to (b),
ϕ∗ α = ϕ∗ (f dy) = (f ◦ ϕ) dϕ = (f ◦ ϕ)ϕ dx .
Thus, the substitution rule for integrals (Theorem VI.5.1),
(f ◦ ϕ)ϕ dx =
can be written formally with using the Pfaff forms α = f dy and ϕ∗ α as
(3.9)
ϕ(X)
Because Ω(0) (X) is a one-dimensional C(X)-module, every continuous 1form β on X can be represented uniquely in the form β = b dx for b ∈ C(X).
Motivated by (3.9), we define the integral of β = b dx ∈ Ω(0) (X) over X by
(3.10)
where the right side is interpreted as a Cauchy–Riemann integral. Thus we have
replaced the previously formal definition of the “differential”, which we gave in
Remarks VI.5.2, with a mathematically precise definition (at least in the case of
real-valued functions).
Modules
For completeness, we collect here the most relevant facts from the theory of modules.
In Volume III, we will further study the modules V q (X) and Ω(q) (X) through some
examples.
Suppose R = (R, +, ·) is a commutative ring with unity. A module over the ring R,
an R-module, is a triple (M, +, ·) consisting of a nonempty set M and two operations: an
“inner” operation + , addition, and an “outer” operation · , multiplication, with elements
of R. These must satisfy
(M1 ) (M, +) is an Abelian group;
(M2 ) the distribute property, that is,
λ · (v + w) = λ · v + λ · w ,
(M3 ) λ · (μ · v) = (λ · μ) · v ,
3.15 Remarks
(λ + μ) · v = λ · v + μ · v
1·v =v
for λ, μ ∈ R and v, w ∈ M ;
for λ, μ ∈ R and v ∈ M .
Suppose M = (M, +, ·) is an R-module.
(a) As in the case of vector spaces, we stipulate that multiplication binds stronger than
addition, and we usually write λv for λ · v.
(b) The axiom (M3 ) shows that the ring R operates on the set M from the left (see
Exercise I.7.6).
322
(c) The addition operations in R and in M will both be denoted by + . Likewise, we
denote the ring multiplication in R and the multiplication product of R on M by the
same · . Finally, we write 0 for the zero elements of (R, +) and of (M, +). Experience
shows that this simplified notation does not lead to misunderstanding.
(d) If R is a field, M is a vector space over R.
(a) Every commutative ring R with unity is an R-module.
3.16 Examples
(b) Suppose G = (G, +) is an Abelian group. For (z, g) ∈ Z × G, let10
z>0,
k=1 g ,
− k=1 g ,
z<0.
Then is G = (G, +, ·) is a Z -module.
(c) Suppose G is an Abelian group and R is a commutative subring of the ring of all
endomorphisms of G. Then G together with the operation
(T, g) → T g ,
is an R-module.
(d) Suppose M is an R-module. A nonempty subset U of M is a submodule of M if
(UM1 ) U is a subgroup of M ,
(UM2 ) R · U ⊆ U .
It is not hard to verify that a nonempty subset U of M is a submodule of M if and only
if U is a module with the operation induced by M .
If { Uα ; α ∈ A } is a family of submodules of M , then α∈A Uα is also a submodule
Suppose M and N are modules over R. The map T : M → N is a module homomorphism if
T (λv + μw) = λT (v) + μT (w)
for λ, μ ∈ R and v, w ∈ M .
A bijective module homomorphism is a module isomorphism. Then the inverse map
T −1 : N → M is also a module homomorphism. When there is a module isomorphism
from M to N , we say M and N are isomorphic, and we write M ∼ N .
Suppose M is an R-module and A ⊂ M . Then
span(A) :=
{ U ; U is a submodule of M with U ⊇ A }
is called the span of A in M . Clearly span(A) is the smallest submodule of M containing
A. When span(A) equals M , we call A a generating system of M .
The elements v0 , . . . , vk ∈ M are said to be linearly independent over R if λj ∈ R
and k λj vj = 0 imply λ0 = · · · = λk = 0. A subset B of M is free over R if every
finite subset of B is linearly independent. If span(B) = M for some subset B over R,
then B forms a basis of M . An R-module that has a basis is called a free R-module.
10 See
Example I.5.12.
3.17 Remarks
(a) These statements are equivalent:
(i) M is a free R-module with basis B.
(ii) Every v ∈ M is of the form v =
λj vj for unique λj ∈ R× and vj ∈ B.
Refer to texts on algebra (for example [SS88, §§ 19 and 22]).
(b) Suppose M and N are modules over R and M is free with basis B. Further suppose
S and T are module homomorphisms from M to N with S | B = T | B. Then S = T , that
is, a homomorphism over a free module is uniquely determined by how it maps its basis.
(c) Suppose B and B are bases of a free R-module M . Then one can show that
Num(B) = Num(B ) if R is not the null ring (for example [Art93, § XII.2]). Thus
the dimension of M is defined by dim(M ) := Num(B).
(d) Z2 is not a free Z -module.
Proof From Exercise I.9.2, we know that Z2 is a ring. Therefore (Z2 , +) is an Abelian
group, and Example 3.16(b) shows that Z2 is a Z -module. Suppose B is a basis of Z2
over Z. Then the coset [1] belongs to B. On the other hand, [0] = 2 · [1] shows that [1]
is not linearly independent over Z, that is, [1] ∈ B.
(e) Suppose M is an R-module and M = span(B) for B ⊂ M . Then11 B does not
(generally) contain a basis of M .
Proof We consider in the Z -module12 Z the set B = {2, 3}. From Z = Z(3 − 2), we
have span(B) = Z. Further, it follows from
3 · 2 − 2 · 3 = 0 and z · 2 = 5 = z · 3
that neither {2, 3} nor {2} nor {3} is a basis of Z.
(f ) Suppose M is a free R-module and U is a free submodule of M with basis B . Then
B does not (generally) generate a basis of M .
Proof Z is a free module over Z. The even integers 2Z form a free submodule of Z.
However, neither {2} nor {−2} generates a basis of Z. Also, dim(Z) = dim(2Z) = 1.
(g) If R is a field, the above notions of “span”, “linear independence”, “bases”, and
“dimension” agree with those of Section I.12.
1 Suppose E, F and G are Banach spaces; also let A ∈ L(E, F ) and B ∈ L(G, E).
(a) (AB) = B A ;
(b) For A ∈ Lis(E, F ), we have A ∈ Lis(F , E ) and [A ]−1 = [A−1 ] .
Which of these 1-forms are closed?
(a) 2xy 3 dx + 3x2 y 2 dy ∈ Ω(∞) (R2 ).
11 Suppose V is a K vector space and V = span(B). The it is proved in linear algebra that B
contains a basis of V .
12 Note that Z is a free Z -module.
324
2(x2 − y 2 − 1) dy − 4xy dx
∈ Ω(∞) (X), where X := R2
(x2 + y 2 − 1)2 + 4y 2
Suppose X := R2
(−1, 0), (1, 0) .
(−1, 0), (1, 0) ,
ϕ : X → R2
(0, 0) ,
(x, y) → (x2 + y 2 − 1, 2y)
∈ Ω R2
u2 + v 2
Show that ϕ∗ α coincides with the 1-form of Exercise 2(b).
4 Suppose X is open in Rn and α ∈ Ω(1) (X). Denote by a ∈ C 1 (X, Rn ) the cotangent
part of Θ−1 α. Prove that α is closed if and only if ∂a(x) is symmetric for every x ∈ X.
α := x dy − y dx ∈ Ω(∞) (R2 )
β := α (x2 + y 2 ) ∈ Ω(∞) R2
ϕ : (0, ∞) × (0, 2π) → R2
(r, θ) → (r cos θ, r sin θ) .
Calculate ϕ α and ϕ β.
Determine ϕ∗ α for α := x dy − y dx − (x + y) dz ∈ Ω(∞) (R3 ) and
ϕ : (0, 2π) × (0, π) → R3 ,
(θ1 , θ2 ) → (cos θ1 sin θ2 , sin θ1 sin θ2 , cos θ2 ) .
7 Suppose X and Y are open in Rn , ϕ ∈ Diff q (X, Y ), and α ∈ Ω(q−1) (Y ) for some
q ≥ 1. Prove
(a) α is exact if and only if ϕ∗ α is exact;
(b) for q ≥ 2, α is closed if and only if ϕ∗ α is closed.
8 Suppose X is open in R2 and α ∈ Ω(1) (X). A function h ∈ C 1 (X) is an Euler
multiplier (or integrating factor) for α if hα is closed and h(x, y) = 0 for (x, y) ∈ X.
(a) Show that if h ∈ C 1 (X) satisfies h(x, y) = 0 for (x, y) ∈ X, then h is an Euler
multiplier for α = a dx + b dy if and only if
ahy − bhx + (ay − bx )h = 0 .
(3.11)
(b) Suppose a, b, c, e > 0, and let
β := (c − ex)y dx + (a − by)x dy .
Show that β has an integrating factor of the form h(x, y) = m(xy) on X = (0, ∞)2 .
(Hint: Apply (3.11) to find a differential equation for m, and guess the solution.)
9 Let J and D be open intervals, f ∈ C(J × D, R) and α := dx − f dt ∈ Ω(0) (J × D).
Further suppose u ∈ C 1 (J, D), and define ϕu : J → R2 , t → t, u(t) . Prove that
ϕ∗ α = 0 if and only if u solves x = f (t, x).
10 Verify that V q (X) and Ω(q) (X) are C q (X)-modules and that V q+1 (X) and Ω(q+1) (X)
are respectively submodules of V q (X) and Ω(q) (X).
Suppose R is a commutative ring with unity and a ∈ R. Prove
(a) aR is a submodule of R;
(b) aR = 0 ⇐ a = 0;
(c) aR = R ⇐ a has no inverse element.
A submodule U of an R-module M is said to be nontrivial if U = {0} and U = M . What
are the nontrivial submodules of Z2 and of Z6 ?
Prove the statements of Example 3.16(d).
13 Suppose M and N are R-modules and T : M → N is a module homomorphism.
Verify that ker(T ) := {v ∈ M ; T v = 0} and im(T ) are submodules of M and N ,
respectively.
326
4 Line integrals
In Chapter VI, we developed theory for integrating functions of a real variable over
intervals. If we regard these intervals as especially simple curves, we may then
wonder whether the concept of integration can be extended to arbitrary curves.
This section will do just that. Of course, we can already integrate functions of
paths, so to develop a new way to integrate that depends only on the curve, we
will have to ensure that it is independent of parametrization. It will turn out that
we must integrate Pfaff forms, not functions.
In these sections, suppose
• X is open in Rn ;
I and I1 are compact perfect intervals.
In addition, we always identify vector fields and Pfaff forms with their tangent
and cotangent parts, respectively.
We now take up the substitution rule for integrals. Suppose I and J are compact
intervals and ϕ ∈ C 1 (I, J). Further suppose a ∈ C(J) and α = a dy is a continuous
1-form on J. Then according to 3.14(d) and Definition (3.10), the substitution
rule has the form
α = ϕ∗ α = (a ◦ ϕ)ϕ dt .
ϕ(I)
Because Ω(0) (J) is a one-dimensional C(J)-module, every α ∈ Ω(0) (J) has a unique
representation α = a dy such that a ∈ C(J). Therefore (4.1) is defined for every
α ∈ Ω(0) (J). The last integrand can also be expressed in the form
α ϕ(t) , ϕ(t) dt = α ϕ(t) , ϕ(t)1 dt .
This observation is the basic result needed for defining line integrals of 1-forms.
For α ∈ Ω(0) (X) and γ ∈ C 1 (I, X)
α γ(t) , γ(t) dt
is the integral of α along the path γ.
4.1 Remarks (a) In a basis representation α =
(aj ◦ γ)γ j dt .
This follows directly from Example 3.14(b).
aj dxj , we have
VIII.4 Line integrals
(b) Suppose ϕ ∈ C 1 (I1 , I) and γ1 := γ ◦ ϕ. Then
for α ∈ Ω(0) (X) .
Proposition 3.12(i) and the substitution rule (3.9) give
(γ ◦ ϕ)∗ α =
ϕ∗ (γ ∗ α) =
This result shows that the integral of α along γ does not change under a
change of parametrization. Thus, for every compact C 1 curve Γ in X and every
α ∈ Ω(0) (X), the line integral of α along Γ
is well defined, where γ is an arbitrary C 1 parametrization of Γ.
4.2 Examples (a) Let Γ be the circle parametrized by γ : [0, 2π] → R2 , t →
R(cos t, sin t). Then
X dy − Y dx = 2πR2 .
γ ∗ dx = dγ 1 = −(R sin) dt and γ ∗ dy = dγ 2 = (R cos) dt
it follows that γ ∗ (X dy − Y dx) = R2 dt, and the claim follows.
(b) Suppose α ∈ Ω(0) (X) is exact. If f ∈ C 1 (X) is an antiderivative,1
df = f (EΓ ) − f (AΓ ) ,
where AΓ and EΓ are the initial and final points of Γ.
Proof We fix a C 1 parametrization γ : [a, b] → X of Γ. Then it follows from Proposition 3.12(iii) that
γ ∗ df = d(γ ∗ f ) = d(f ◦ γ) = (f ◦ γ) dt .
(f ◦ γ) dt = f ◦ γ
because EΓ = γ(b) and AΓ = γ(a).
statement is a generalization of Corollary VI.4.14.
= f (EΓ ) − f (AΓ )
328
(c) The integral of an exact Pfaff form along a C 1 curve Γ depends on the initial
and final points, but not on the path between them. If Γ is closed, the integral
evaluates to 0.
This follows directly from (b).
(d) There are closed 1-forms that are not exact.
(0, 0) and α ∈ Ω(∞) (X) with
α(x, y) :=
for (x, y) ∈ X .
One easily checks that α is closed. The parametrization γ : [0, 2π] → X, t → (cos t, sin t)
of the circle Γ gives γ ∗ α = dt and therefore
dt = 2π = 0 .
Then (c) shows α is not exact.
(e) Suppose x0 ∈ X and γx0 : I → X, t → x0 . This means that γx0 parametrizes
the point curve Γ, whose image is simply {x0 }. Then
α = 0 for α ∈ Ω(0) (X) .
γx0
This is obvious because γx0 = 0.
Elementary properties
Suppose I = [a, b] and γ ∈ C(I, X). Then
γ− : I → X ,
t → γ(a + b − t)
is the path inverse to γ, and −Γ := [γ ] is the curve inverse to Γ := [γ]. (Note
that Γ and −Γ have the same image but the opposite orientation).
Now let q ∈ N× ∪{∞}. Further suppose γ ∈ C(I, X) and (t0 , . . . , tm ) is a partition
of I. If2
γj := γ |[tj−1 , tj ] ∈ C q [tj−1 , tj ], X
2 See Exercise 1.5 and the definition of a piecewise continuously differentiable function in
Section VI.7. Here we also assume that γ is continuous.
then we say γ is piecewise-C q path in X or a sum of the C q paths γj . The
curve Γ = [γ] parametrized by γ is said to be a piecewise C q curve in X,
and we write Γ := Γ1 + · · · + Γm , where
Γj := [γj ]. Given a piecewise C 1 curve
Γ = Γ1 + · · · + Γm in X and α ∈ Ω(0) (X)
we define the line integral of α along Γ by
Finally, we can also join piecewise C q curves. So let Γ :=
Γj and
Γj be piecewise C curves with EΓ = AΓ . Then Σ = Σ1 + · · · + Σm+m
1≤j≤m,
m+1≤j ≤m+m ,
is a piecewise C q curve, the sum3 of Γ and Γ. In this case, we write Γ + Γ := Σ
α for α ∈ Ω(0) (X) .
Clearly, these notations are consistent with the prior notation Γ = Γ1 + · · · + Γm
for a piecewise C q curve and the notation Γ α = Γ1 +···+Γm α for line integrals.
The next theorem lists some basic properties of line integrals.
4.3 Proposition
Ω(0) (X). Then
Suppose Γ, Γ1 and Γ2 are piecewise C 1 curves and α, α1 , α2 ∈
(λ α + λ2 α2 ) = λ1 Γ α1 + λ2 Γ α2 , λ1 , λ2 ∈ R,
Γ 1 1
that is, Γ : Ω(0) (X) → R is a vector space homomorphism;
that is, the line integral is oriented;
(iii) if Γ1 + Γ2 is defined, we have
Γ1 +Γ2
that is, the line integral is additive with respect to its integrated curves;
(iv) for α =
aj dxj and a := Θ−1 α =
aj ej = (a1 , . . . , an ) ∈ V 0 (X), we
α ≤ max |a(x)| L(Γ) .
3 Note that for two piecewise C q curves Γ and Γ, the sum Γ + Γ is only defined when the final
point of Γ is the initial point of Γ.
330
Proof The statements (i) and (iii) are obvious.
(ii) Due to (iii), it is suﬃcient to consider the case where Γ is a C q curve.
Suppose γ ∈ C 1 [a, b], X parametrizes Γ. Then we write γ − as γ − = γ ◦ ϕ,
where ϕ(t) := a + b − t for t ∈ [a, b]. Because ϕ(a) = b and ϕ(b) = a, it follows
from Propositions 3.12(i) and (3.9) that
(γ − )∗ α =
(iv) The length of the piecewise C 1 curve Γ = Γ1 + · · · + Γm is clearly equal
to the sum of the lengths of its pieces4 Γj . Thus it suﬃces to prove the statement
for one C 1 curve Γ := [γ]. With the help of the Cauchy–Schwarz inequality and
Proposition VI.4.3, it follows that
a γ(t) |γ(t)| dt
α γ(t) , γ(t) dt =
a γ(t) |γ(t)| dt ≤ max |a(x)|
= max |a(x)| L(Γ) ,
where we identify (Rn ) with Rn , and in the last step, we have applied Theorem 1.3.
4.4 Theorem Suppose X ⊂ Rn is a domain and α ∈ Ω(0) (X). Then these
statements are equivalent:
(i) α is exact;
(ii) Γ α = 0 for every closed piecewise C 1 curve in X.
Proof The case “(i)=
⇒(ii)” follows from Example 4.2(b) and Proposition 4.3(iii).
⇒(i)” Suppose x0 ∈ X. According to Theorem III.4.10, there is for
every x ∈ X a continuous, piecewise straight path in X that leads to x from x0 .
Thus there is for every x ∈ X a piecewise C 1 curve Γx in X with initial point x0
and final point x. We set
Exercise 1.5.
To verify that f is well defined, that
is, independent of the special curve
Γx , we choose a second piecewise C 1
curve Γx in X with the same end
points. Then Σ := Γx + −Γx is a
closed piecewise C 1 curve in X. By
assumption we have Σ α = 0, and
we deduce from Proposition 4.3 that
0 = Γx α − Γx α. Therefore f is well
defined.
Suppose now h ∈ R+ with B(x, h) ⊂ X and Πj := [πj ] with
πj : [0, 1] → X ,
t → x + t h ej
Then Γx + Πj and Πj = (−Γx ) + (Γx + Πj ) are curves X. Because Γx and Γx + Πj
have the same initial point x0 ,
f (x + hej ) − f (x) =
Letting aj := α, ej , we find
α(x + thej ), hej dt = h
aj (x + thej ) dt .
f (x + hej ) − f (x) − aj (x)h = h
aj (x + thej ) − aj (x) dt = o(h)
as h → 0. Thus f has continuous partial derivatives ∂j f = aj for j = 1, . . . , n.
Now Corollary VII.2.11 shows that f belongs to C 1 (X), and Remark 3.3(e) gives
4.5 Corollary Suppose X is open in Rn and star shaped, and let x0 ∈ X. Also
suppose q ∈ N× ∪ {∞} and α ∈ Ω(q) (X) is closed. Let
where Γx is a piecewise C 1 curve in X with initial point x0 and final point x. This
function satisfies f ∈ C q+1 (X) and df = α.
Proof The proof of the Poincar´ lemma (Theorem 3.8) and of Theorem 4.4 guare
antee that f ∈ C 1 (X) and df = α. Because ∂j f = aj ∈ C q (X) for j = 1, . . . , n, it
follows from Theorem VII.5.4 that f belongs to C q+1 (X).
This corollary gives a prescription for constructing the antiderivative of a
closed Pfaff form on a star shaped domain. In a concrete calculation, one chooses
the curve Γx that makes the resulting integration the easiest to do (see Exercise 7).
332
Simply connected sets
In the following, M ⊂ Rn denotes a nonempty
path-connected set. Every closed continuous
path in M is said to be a loop in M . Two
loops5 γ0 , γ1 ∈ C(I, M ) are homotopic if there
is an H ∈ C I × [0, 1], M , a (loop) homotopy,
(i) H(·, 0) = γ0 and H(·, 1) = γ1 ;
(ii) γs := H(·, s) is a loop in M for every
s ∈ (0, 1).
When two loops γ0 and γ1 are homotopic, we write γ0 ∼ γ1 . We denote by γx0
the point loop [t → x0 ] with x0 ∈ M . Every loop in M that is homotopic to a
point loop is null homotopic. Finally, M is said to be simply connected if every
loop in M is null homotopic.
4.6 Remarks (a) On the set of all loops in M , ∼ is an equivalence relation.
Proof (i) It is clear the every loop is homotopic to itself, that is, the relation ∼ is
reﬂexive.
(ii) Suppose H is a homotopy from γ0 to γ1 and
H − (t, s) := H(t, 1 − s)
for (t, s) ∈ I × [0, 1] .
Then H − is a homotopy from γ1 to γ0 . Therefore the relation ∼ is symmetric.
(iii) Finally, let H0 be a homotopy from γ0 to γ1 , and let H1 be a homotopy from
γ1 to γ2 . We set
H(t, s) :=
H0 (t, 2s) ,
H1 (t, 2s − 1) ,
(t, s) ∈ I × [0, 1/2] ,
(t, s) ∈ I × [1/2, 1] .
It is not hard to check that H belongs to C I × [0, 1], M ; see Exercise III.2.13. Also,
H(·, 0) = γ0 and H(·, 1) = γ2 , and every H(·, s) is a loop in M .
(b) Suppose γ is a loop in M . These statements are equivalent:
(i) γ is null homotopic;
(ii) γ ∼ γx0 for some x0 ∈ M ;
(iii) γ ∼ γx for every x ∈ M .
Proof It suﬃces to verify the implication “(ii)=
⇒(iii)”. For that, let γ ∈ C(I, M ) be a
loop with γ ∼ γx0 for some x0 ∈ M . Also suppose x ∈ M . Because M is path connected,
there is a continuous path w ∈ C(I, M ) that connects x0 to x. We set
H(t, s) := w(s)
for (t, s) ∈ I × [0, 1] ,
and we thus see that γx0 and γx are homotopic. The claim now follows using (a).
5 It is easy to see that, without loss of generality, we can define γ and γ over the same
parameter interval. In particular, we can choose I = [0, 1].
(c) Every star shaped set is simply connected.
Proof Suppose M is star shaped with respect to x0 and γ ∈ C(I, M ) is a loop in M .
H(t, s) := x0 + s γ(t) − x0
Then H is a homotopy from γx0 to γ.
(d) The set Q := B2 \T with T := (−1/2, 1/2) × {0} ∪ {0} × (−1, 0) is simply
connected, but not star shaped.
The homotopy invariance of line integrals
The line integral of closed Pfaff forms is invariant under loop homotopies:
4.7 Proposition Suppose α ∈ Ω(1) (X) is closed. Let γ0 and γ1 be homotopic
piecewise C 1 loops in X. Then γ0 α = γ1 α.
Proof (i) Suppose H ∈ C I × [0, 1], X is a homotopy from γ0 to γ1 . Because I ×
[0, 1] is compact, it follows from Theorem III.3.6 that K := H I×[0, 1] is compact.
Because X c is closed and X c ∩ K = ∅, there is, according to Example III.3.9(c),
an ε > 0 such that
|H(t, s) − y| ≥ ε
for (t, s) ∈ I × [0, 1] and y ∈ X c .
Theorem III.3.13 guarantees that H is uniformly continuous. Hence there is a
|H(t, s) − H(τ, σ)| < ε for |t − τ | < δ ,
(ii) Now we choose a partition (t0 , . . . , tm ) of I and a partition (s0 , . . . , s ) of
[0, 1], both with mesh < δ. Letting Aj,k := H(tj , sk ), we set
γk (t) := Aj−1,k +
1 ≤ j ≤ m, and 0 ≤ k ≤ .
t − tj−1
(Aj,k − Aj−1,k ) for tj−1 ≤ t ≤ tj ,
tj − tj−1
334
Clearly every γk is a piecewise C 1 loop in X. The choice of δ shows that we can
apply the Poincar´ lemma in the convex neighborhood B(Aj−1,k−1 , ε) of the points
Aj−1,k−1 . Thus we get from Theorem 4.4 that
α = 0 for 1 ≤ j ≤ m and 1 ≤ k ≤
where ∂Vj,k denotes the closed piecewise straight curve from Aj−1,k−1 to Aj,k−1
to Aj,k to Aj−1,k and back to Aj−1,k−1 . Therefore
α for 1 ≤ k ≤
γk−1
because the integral cancels itself over the “connecting pieces” between γk−1 and
γk . Likewise, using the Poincar´ lemma we conclude that γ0 α = γ0 α and
α = γ α, as the claim requires.
As an application of homotopy invariance theorem, we now get a widereaching generalization of the Poincar´ lemma.
4.8 Theorem Suppose X is open in Rn and simply connected. If α ∈ Ω(1) (X) is
closed, α is exact.
Proof Suppose γ is a piecewise C 1 loop in X and x0 ∈ X. According to Remark 4.6(b), γ and γx0 are homotopic. Then Proposition 4.7 and Example 4.2(e)
give γ α = γx α = 0, and the claim follows from Theorem 4.4.
4.9 Examples
(a) The “punctured plane” R2
(0, 0) is not simply connected.
This follows from Theorem 4.8 and Example 4.2(d).
(b) For n ≥ 3, the set Rn \{0} is simply connected.
See Exercise 12.
4.10 Remarks (a) Suppose X is open in Rn and simply connected. Also, suppose
the vector field v = (v1 , . . . , vn ) ∈ V 1 (X) satisfies the integrability conditions
∂j vk = ∂k vj for 1 ≤ j, k ≤ n. Then v has a potential U , that is, v is a gradient
field. If x0 is any point in X, then U can be calculated through
v γx (t) γx (t) dt
where γx : [0, 1] → X is a piecewise C 1 path in X for which γx (0) = x0 and
γx (1) = x.
Proof The canonical isomorphism Θ defined in Remark 3.3(g) assigns to v the Pfaff
form α := Θv ∈ Ω(1) (X). Because v satisfies the integrability conditions, α is closed and
therefore exact by Theorem 4.8. Because, in the proof of Corollary 4.5, we can replace
the Poincar´ lemma by Theorem 4.8, it follows that, with Γx := [γx ],
defines a potential for α.
(b) Suppose α = j=1 aj dxj ∈ Ω(0) (X), and let a := Θ−1 α = (α1 , . . . , αn ) ∈
V 0 (X) be the associated vector field. It is conventional to write α symbolically as
a scalar product
α = a · ds
using the vector line element ds := (dx1 , . . . , dxn ). Then, due to Remark 3.3(g),
the line integral
a · ds :=
is well defined for every vector field a ∈ V(X) and for every piecewise C 1 curve Γ
in X.
(c) Suppose F is a continuous force field (for example, the electrostatic or gravitational field) in an open subset X of (three-dimensional Euclidean) space. If a
(small) test particle is guided along the piecewise C 1 curve Γ = [γ], then the work
done by F along Γ is given by the line integral
In particular, if γ is a piecewise C 1 parametrization of Γ, then
F γ(t) γ(t) dt
is a Cauchy–Riemann integral. Therefore it is approximated by the Riemann sum
F γ(tj−1 ) · γ(tj−1 )(tj − tj−1 ) ,
336
where Z := (t0 , . . . , tn ) is a suitable partition of I. Because
γ(tj ) − γ(tj−1 ) = γ(tj−1 )(tj − tj−1 ) + o(
is the mesh of Z, the sum
F γ(tj−1 ) · γ(tj ) − γ(tj−1 )
represents, for a partition with suﬃciently small mesh, a good approximation for
A which, by passing the limit Z → 0, converges to A (see Theorem VI.3.4). The
scalar product
is the work of a (constant) force F γ(tj−1 ) done on a particle displaced on a
line segment from γ(tj−1 ) to γ(tj ) (“work = force × distance” or, more precisely,
“work = force × displacement in the direction of the force”).
A force field F is said to be conservative if it has a potential. In that case,
according to Example 4.2(c), the work is “path independent” and only depends
on the initial and final position. If X is simply connected (for example, X is
the whole space) and F has continuous derivatives, then F , according to (a) and
Theorem 4.8, is conservative if and only if the integrability conditions are satisfied.
If the latter is the case, one says that the force field is irrotational.6
Calculate
(x2 − y 2 ) dx + 3z dy + 4xy dz
along one winding of a helix.
Suppose ϕ ∈ C 1 (0, ∞), R and
α(x, y) := xϕ
What is the value of
radius R > 0?
x2 + y 2 dy − yϕ
x2 + y 2 dx
for (x, y) ∈ R2 .
α when Γ is the positively oriented7 circle centered at 0 with
2xy 3 dx + 3x2 y 2 dy ,
where Γ is parametrized by [α, β] → R2 , t → (t, t2 /2).
will clarify this language in Volume III.
circle is by convention positively oriented when it is traversed in the counterclockwise
direction (see Remark 5.8).
4 Suppose Γ+ are Γ− are positively oriented circles of radius 1 centered at (1, 0) and
(−1, 0), respectively. Also suppose
∈ Ω(∞) R2
(−1, 0), (1, 0)
α = ±1 .
(Hint: Exercise 3.3.)
Suppose X is open in Rn and Γ is a C 1 curve in X. For q ≥ 0, prove or disprove that
: Ω(q) (X) → R ,
is a C q (X) module homomorphism.
6 Suppose X and Y are open in Rn and Rm , respectively, and α ∈ Ω(q) (X ×Y ). Further
suppose Γ is a C 1 curve in X. Show that
α(·, y)
belongs to C q (Y, R). Calculate \nabla f for q ≥ 1.
7 Suppose −∞ ≤ αj < βj ≤ ∞ for j = 1, . . . , n and X := n (αj , βj ). Also suppose
α = n aj dxj ∈ Ω(1) (X) is closed. Determine (in a simple way) all antiderivatives
Suppose X := (0, ∞)2 and a, b, c, e > 0. According to Exercise 3.8(b),
α := (c − ex)y dx + (a − by)x dy
has an integrating factor h of the form h(x, y) = m(xy). Determine all antiderivatives of
hα. (Hint: Exercise 7.)
9 Suppose X is open in Rn and Γ is a compact piecewise C 1 curve in X with the
parametrization γ. Also suppose f, fk ∈ C(X, Rn ) for k ∈ N. Prove
(a) if (γ ∗ fk ) converges uniformly to γ ∗ f ,
(b) if
Γ j=1
γ ∗ fk converges uniformly to γ ∗ f ,
338
10 Suppose aj ∈ C 1 (Rn , R) is positively homogeneous of degree λ = −1 and the 1-form
α := n aj dxj is closed. Show that
λ+1
xj aj (x)
for x = (x1 , . . . , xn ) ∈ Rn
defines an antiderivative for α. (Hint: Apply the Euler homogeneity relation (see Exercise VII.3.2).)
11 Show that if γ ∈ C(I, X) is a loop, there is a change of parameter ϕ : [0, 1] → I such
that γ ◦ ϕ is a loop.
12 (a) Suppose X is open in Rn and γ is a loop in X. Show that γ is homotopic to a
polygonal loop in X.
(b) Show that Rn \R+ a is simply connected for a ∈ Rn \{0}.
(c) Suppose γ is a polygonal loop in Rn \{0} for n ≥ 3. Show that there is a half ray
R+ a with a ∈ Rn \{0} that does not intersect the image of γ.
(d) Prove that Rn \{0} is simply connected for n ≥ 3.
Prove or disprove that every closed 1-form in Rn \{0} is exact for n ≥ 3.
14 Suppose X ⊂ Rn is nonempty and path connected. For γ1 , γ2 ∈ C [0, 1], X with
γ1 (1) = γ2 (0),
γ2 ⊕ γ1 : [0, 1] → X ,
γ1 (2t) ,
γ2 (2t − 1) ,
0 ≤ t ≤ 1/2 ,
1/2 ≤ t ≤ 1
is the path joining γ2 to γ1 . For x0 ∈ X, define
Sx0 :=
γ ∈ C [0, 1], X ; γ(0) = γ(1) = x0
Also denote by ∼ the equivalence relation induced by the loop homotopies of Sx0 (see
Example I.4.2(d)).
(a) the map
(Sx0 /∼) × (Sx0 /∼) → Sx0 /∼ ,
[γ1 ], [γ2 ] → [γ2 ⊕ γ1 ]
is a well defined operation on Sx0 / ∼, and Π1 (X, x0 ) := (Sx0 / ∼, ⊕) is a group, the
fundamental group, or the first homotopy group, of X with respect to x0 ;
(b) for x0 , x1 ∈ X, the groups Π1 (X, x0 ) and Π1 (X, x1 ) are isomorphic.
Remark Part (b) justifies speaking of the “fundamental group” Π1 (X) of X; see the
construction in Example I.7.8(g). The set X is simply connected if and only if Π1 (X) is
trivial, that is, if it consists only of the identity element.
(Hint for (b): Let w ∈ C [0, 1], X be a path in X with w(0) = x0 and w(1) = x1 . Then
the map [γ] → [w ⊕ γ ⊕ w− ] is a group isomorphism from Π1 (X, x0 ) to Π1 (X, x1 ).)
VIII.5 Holomorphic functions
5 Holomorphic functions
Line integrals becomes particularly useful in complex analysis, the theory of complex functions. With the results from the previous section, we may deduce almost
effortlessly the (global) Cauchy integral theorem and the Cauchy integral formula.
These theorems form the core of complex analysis and have wide-reaching consequences, some of which we will see in this section and the next.
• U is in C and f : U → C is continuous.
Also in this section, we often decompose z ∈ U and f into real and imaginary
parts, that is, we write
z = x + i y ∈ R + i R and f (z) = u(x, y) + i v(x, y) ∈ R + i R .
Complex line integrals
Suppose I ⊂ R is a compact interval, and suppose Γ is a piecewise C 1 curve in U
I → U , t → z(t) = x(t) + i y(t) .
u dx − v dy + i
f (z) dz :=
is the complex line integral of f along Γ.1
5.1 Remarks (a) We denote by Ω(U, C) the space of continuous complex 1-forms
and define it as follows:
On the product group Ω(0) (U ) × Ω(0) (U ), + , we define an outer multiplication
C(U, C) × Ω(0) (U ) × Ω(0) (U ) → Ω(0) (U ) × Ω(0) (U ) ,
(f, α) → f α ,
f α := (ua1 − vb1 ) dx + (ua2 − vb2 ) dy, (ub1 + va1 ) dx + (va2 + ub2 ) dy
for α = (a1 dx + a2 dy, b1 dx + b2 dy). Then one immediately verifies that
Ω(U, C) := Ω(0) (U ) × Ω(0) (U ), +, ·
1 In
this context, the curve Γ may also be called the contour.
340
is a free module over C(U, C). In addition, we have
1(a1 dx + a2 dy, 0) = (a1 dx + a2 dy, 0) ,
i(a1 dx + a2 dy, 0) = (0, a1 dx + a2 dy)
for a1 dx + a2 dy ∈ Ω(0) (U ). Therefore we can identify Ω(0) (U ) with Ω(0) (U ) × {0}
in Ω(U, C) and represent (a1 dx + a2 dy, b1 dx + b2 dy) ∈ Ω(U, C) uniquely in the
a1 dx + a2 dy + i (b1 dx + b2 dy) ,
which we express through the notation
Ω(U, C) = Ω(0) (U ) + iΩ(0) (U ) .
Finally, we have
(a1 + i b1 )(dx, 0) = (a1 dx, b1 dx) ,
(a2 + i b2 )(dy, 0) = (a2 dy, b2 dy) ,
(a1 + i b1 )(dx, 0) + (a2 + i b2 )(dy, 0) = (a1 dx + a2 dy, b1 dx + b2 dy) .
This means we can also write (a1 dx + a2 dy, b1 dx + b2 dy) ∈ Ω(U, C) uniquely as
(a1 + ib1 ) dx + (a2 + ib2 ) dy ,
Ω(U, C) =
a dx + b dy ; a, b ∈ C(U, C)
(b) With ux := ∂1 u etc, we call
df := (ux + i vx ) dx + (uy + i vy ) dy ∈ Ω(U, C)
the complex differential of f . Clearly, dz = dx + i dy, and we get2
f dz = (u + i v)(dx + i dy) = u dx − v dy + i(u dy + v dx)
for f = u + i v ∈ C(U, C).
5.2 Proposition
t → z(t). Then
f (z) dz =
Suppose Γ is a piecewise C 1 curve parametrized by I → U ,
f z(t) z(t) dt;
f (z) dz ≤ maxz∈Γ |f (z)| L(Γ).
(5.1) with the definition of the complex line integral of f .
(i) For the piecewise C 1 path γ : I → R2 , t → x(t), y(t) , we have
γ ∗ (u dx − v dy) + i
γ ∗ (u dy + v dx)
(u ◦ γ)x − (v ◦ γ)y dt + i
(u ◦ γ)y + (v ◦ γ)x dt
(u ◦ γ + i v ◦ γ)(x + i y) dt
f z(t) z(t) dt .
(ii) This statement follows from (i) by estimating the last integral using
Theorem 1.3 and Proposition VI.4.3.
5.3 Examples (a) Suppose z0 ∈ C and r > 0, and let z(t) := z0 + rei t for
t ∈ [0, 2π]. Then, for Γ := [z], we have
(z − z0 )m dz =
m ∈ Z\{−1} ,
m = −1 .
2πi ,
r m ei tm i rei t dt = i r m+1
ei (m+1)t dt ,
the claim follows from the 2πi -periodicity of the exponential function.
(b) Suppose Γ is as in (a) with z0 = 0. Then for z ∈ C, we have
2πi
λ−k−1 eλz dλ =
From Exercise 4.9, it follows that
λn−k−1 dλ ,
and the claim is implied by (a).
(c) Suppose Γ is the curve in R parametrized by I → C, t → t. Then
f (t) dt ,
that is, the complex line integral and the Cauchy–Riemann integral from Chapter
VI agree in this case.
342
This follows from Proposition 5.2(i).
(d) For the curves Γ1 and Γ2 parametrized respectively by
[0, π] → C ,
t → ei (π−t)
[−1, 1] → C ,
|z| dz = −i
ei (π−t) dt = ei (π−t)
|t| dt = 1 .
Thus complex line integrals generally depend on the integration path and not only
on the initial and final points.
Holomorphism
A holomorphic function f is a function that has continuous complex derivatives,
that is, f ∈ C 1 (U, C). We call f continuously real differentiable if
U → R2 ,
(x, y) → u(x, y), v(x, y)
belongs to C 1 (U, R2 ) (see Section VII.2).
5.4 Remarks (a) At the end of this section we will show that the assumption of
continuous complex derivatives is unnecessary, that is, every complex differentiable
function has a continuous derivative and is therefore holomorphic. We apply here
the stronger assumption of continuous complex differentiability because it brings
us more quickly to the fundamental theorem of complex analysis, that is, of the
theory of complex-valued functions of a complex variable.
(b) A function f is holomorphic if and only if it is continuously real differentiable
and satisfies the Cauchy–Riemann equations
ux = vy and uy = −vx .
In this case, f = ux + i vx .
This a reformulation of Theorem VII.2.18.
(c) According to Remark 5.1(b), we have
f dz = u dx − v dy + i (u dy + v dx) .
If f is holomorphic, the Cauchy–Riemann equations say that the 1-forms u dx−v dy
and u dy + v dx are closed.
(d) Suppose U is a domain and f is holomorphic. Then f is constant if and only
if these conditions are satisfied: 3
(i) u = const;
(ii) v = const;
(iii) f is holomorphic;
(iv) |f | = const.
If f is constant, (i)–(iv) clearly hold.
Because f = ux + i vx , it follows from the Cauchy–Riemann equations and from
Remark VII.3.11(c) that either (i) or (ii) implies f is constant.
(iii) If f and f are holomorphic, so is u = f + f 2. Since Im(u) = 0, it follows
from (ii) that u is constant and therefore, by (i), so is f .
(iv) It suﬃces to consider the case f = 0. Because |f | is constant, f vanishes
nowhere. Therefore 1/f is defined and holomorphic on U . Thus f = |f |2 /f is also
holomorphic in U , and the claim follows from (iii).
We have seen in Example 5.3(d) that complex line integrals generally depend on
more than the initial and final points of the integration path. However, in the case
of holomorphic integrands, the theorems of the previous sections give the following
important results about path independence.
5.5 Theorem (the Cauchy integral theorem) Suppose U is simply connected and
f is holomorphic. Then, for every closed piecewise C 1 curve Γ in U ,
f dz = 0 .
Proof According to Remark 5.4(c) the 1-forms α1 := u dx−v dy and α2 := u dy +
v dx are both closed. Because U is simply connected, it follows from Theorem 4.8
that α1 and α2 are exact. Now Theorem 4.4 implies
α1 + i
for every closed, piecewise C 1 curve Γ in U .
Exercise V.3.5.
α2 = 0
344
5.6 Theorem Suppose U is simply connected and f is holomorphic. Then f has
a holomorphic antiderivative. Every antiderivative ϕ of f satisfies
f dz = ϕ(EΓ ) − ϕ(AΓ )
for every piecewise C 1 curve Γ in U .
Proof We use the notation of the previous proofs. Because α1 and α2 are exact,
there are h1 , h2 ∈ C 2 (U, R) such that dh1 = α1 and dh2 = α2 . From this, we read
(h1 )x = u , (h1 )y = −v , (h2 )x = v , (h2 )y = u .
Therefore ϕ := h1 + i h2 satisfies the Cauchy–Riemann equations. Thus ϕ is
holomorphic, and
ϕ = (h1 )x + i (h2 )x = u + i v = f .
This shows that ϕ is an antiderivative of f . The second statement follows from
Example 4.2(b).
5.7 Proposition Suppose f is holomorphic and γ1 and γ2 are homotopic piecewise
C 1 loops in U . Then
Proof This follows from Remark 5.4(c) and Proposition 4.7 (see also the proof
of Theorem 5.5).
The orientation of circles
5.8 Remark We recall the notation D(a, r) = a + rD for an open disc in C with
center a ∈ C and radius r > 0. In the following, we understand by ∂D(a, r) =
a + r∂D the positively oriented circle with center a and radius r. This means
that ∂D(a, r) is the curve Γ = [γ] with γ : [0, 2π] → C, t → a + rei t . With this
orientation, the circle is traversed so that the disc D(a, r) stays on the left side,
or it is traversed counterclockwise. This is equivalent to saying that the negative
unit normal vector −n always points outward.
Proof From Remark 2.5(c) and the canonical identification of C with R2 , it follows
that the Frenet two-frame is given by e1 = (− sin, cos) and e2 = (− cos, − sin). Letting
x(t) = a + r(cos t, sin t) ∈ ∂D(a, r), we have the relation
x(t) + re2 (t) = a
Thus e2 (t) points inward along ∂D(a, r), and its negative points outward.
The Cauchy integral formula
Holomorphic functions have a remarkable integral representation, which we derive
in the next theorem. The implications of this formula will be explored in later
applications.
5.9 Theorem (the Cauchy integral formula)
D(z0 , r) ⊂ U . Then
∂D(z0 ,r)
f (ζ)
Suppose f is holomorphic and
for z ∈ D(z0 , r) .
Proof Suppose z ∈ D(z0 , r) and ε > 0. Then there is an δ > 0 such that D(z, δ) ⊂
U and
|f (ζ) − f (z)| ≤ ε for ζ ∈ D(z, δ) .
We set Γδ := ∂D(z, δ) and Γ := ∂D(z0 , r). Exercise 6 shows that Γδ and Γ are
homotopic in U \{z}. In addition,
ζ → 1/(ζ − z)
is holomorphic. Therefore it follows from Proposition 5.7 and Example 5.3(a) that
= 2πi .
ζ → f (ζ) − f (z)
(ζ − z)
is also holomorphic, we know from Proposition 5.7 that
f (ζ) − f (z)
Combining (5.3) with (5.4), we get
f (z)
= f (z) +
Then Proposition 5.2(ii) and (5.2) imply the estimate
2πδ = ε ,
2πδ
dζ − f (z) ≤ ε .
2πi Γ ζ − z
Because ε > 0 was arbitrary, the claim is proved.
346
5.10 Remarks
(a) Under the assumptions of Theorem 5.9, we have
f (z0 + rei t ) i t
z0 + rei t − z
for z ∈ D(z0 , r)
and, in particular,
f (z0 ) =
f (z0 + rei t ) dt .
(b) A function f has the mean value property if, for every z0 ∈ U , there is an
r0 > 0 such that
f (z0 + rei t ) dt
for r ∈ [0, r0 ] .
It follows from (a) that holomorphic functions have the mean value property.
(c) If f has the mean value property, then so does Re f , Im f , f , and λf for λ ∈ C.
If g ∈ C(U, C) also has the mean value property, then so does f + g.
This follows from Theorems III.1.5 and III.1.10 and Corollary VI.4.10.
(d) The Cauchy integral formula remains true for essentially general curves, as we
will show in the next section.
Analytic functions
As another consequence of the Cauchy integral formula, we now prove the fundamental theorem, which says that holomorphic functions are analytic. We proceed
as follows: Suppose z0 ∈ U and r > 0 with D(z0 , r) ⊂ U . Then the function
D(z0 , r) → C ,
z → f (ζ)/(ζ − z)
is analytic for every ζ ∈ ∂D(z0 , r) and admits a power series expansion. With the
help of the Cauchy integral formula we can transfer this expansion to f .
5.11 Theorem A function f is holomorphic if and only if it is analytic. Therefore
C 1 (U, C) = C ω (U, C) .
Proof (i) Suppose f is holomorphic. Let z0 ∈ U and r > 0 with D(z0 , r) ⊂ U .
We choose z ∈ D(z0 , r) and set r0 := |z − z0 |. Then r0 < r, and
|z − z0 |
|ζ − z0 |
for ζ ∈ Γ := ∂D(z0 , r) .
Therefore the geometric series
ak with a := (z − z0 )/(ζ − z0 ) converges and has
the value
z − z0 k
ζ − z0
1 − (z − z0 )/(ζ − z0 )
z − z0
(z − z0 )k .
(ζ − z0 )k+1
Because Γ is compact, there is an M ≥ 0 such that |f (ζ)| ≤ M for ζ ∈ Γ. It
(z − z0 )k ≤ k+1 r0 =
(ζ − z0 )
Because r0 /r < 1, the Weierstrass majorant criterion (Theorem V.1.6) says that
the series in (5.5) converges uniformly with respect to ζ ∈ Γ. Setting
ak :=
it follows4 from Proposition VI.4.1 that
Γ k=0
(z − z0 )k dζ
dζ (z − z0 )k
ak (z − z0 )k .
Because this is true for every z0 ∈ U , we find f can be expanded in the neighborhood of every point in U in a convergent power series. Therefore f is analytic.
(ii) When f is analytic, it has continuous complex derivatives and is thus
holomorphic.
5.12 Corollary (Cauchy’s derivative formula)
and r > 0 with D(z, r) ⊂ U . Then
f (n) (z) =
also Exercise 4.9.
∂D(z,r)
Suppose f is holomorphic, z ∈ U ,
(ζ − z)n+1
348
Proof From Theorem 5.11, Remark V.3.4(b), and the identity theorem for analytic functions, it follows that f is represented in D(z, r) by its Taylor series, that
is, f = T (f, z). The claim now follows from (5.6) and the uniqueness theorem for
power series.
5.13 Remarks (a) Suppose f is holomorphic and z ∈ U . Then the above proof
shows that the Taylor series T (f, z) replicates the function f in at least the largest
disc that lies entirely in U .
(b) If f is holomorphic, u and v belong to C ∞ (U, R).
This follows from Theorem 5.11.
(c) If f is analytic, then so is 1/f (in U \f −1(0)).
Proof From the quotient rule, 1/f has continuous derivatives in U\f −1 (0) and is therefore holomorphic. Then the claim follows from Theorem 5.11.
(d) Suppose V is open in C. If f : U → C and g : V → C are analytic with
f (U ) ⊂ V , the composition g ◦ f : U → C is also analytic.
This follows from the chain rule and Theorem 5.11.
Liouville’s theorem
A function holomorphic on all of C said to be entire.
5.14 Theorem (Liouville) Every bounded entire function is constant.
According to Remark 5.13(a), we have
f (k) (0) k
By assumption, there is an M < ∞ such that |f (z)| ≤ M for z ∈ C. Thus it
follows from Proposition 5.2(ii) and Corollary 5.12 that
f (k) (0)
for r > 0 .
For k ≥ 1, the limit r → ∞ shows that f (k) (0) = 0. Therefore f (z) equals f (0)
for every z ∈ C.
5.15 Application Liouville’s theorem helps with an easy proof of the fundamental
theorem of algebra, that is, every nonconstant polynomial on C[X] has a zero.
Proof We write p ∈ C[X] in the form p(z) =
with n ≥ 1 and an = 0.
k=0 ak z
an−1
p(z) = z n an +
we have |p(z)| → ∞ for |z| → ∞. Thus there is an R > 0 such that |p(z)| ≥ 1 for
z ∈ RD. We assume that p has no zeros in RD. Because RD is compact, it follows
from the extreme value theorem (Corollary III.3.8) that there is a positive number ε such
that |p(z)| ≥ ε. Therefore 1/p is entire and satisfies |1/p(z)| ≤ max{1, 1/ε} for z ∈ C.
Liouville’s theorem now implies that 1/p is constant and thus so is p, a contradiction.
The Fresnel integral
The Cauchy integral theorem also can be used to calculate real integrals whose
integrands are related to holomorphic functions. We can regard integrals with real
integration limits as complex line integrals on a real path. Then the greater freedom to choose the integration curve, which is guaranteed by the Cauchy integral
theorem, will allow us to calculate many new integrals.
The next theorem demonstrates this method, and we will generalize the techniques in the subsequent sections. Still more can be learned from the exercises.
5.16 Proposition The following improper Fresnel integrals converge, and
cos(t ) dt =
sin(t ) dt =
The convergence of these integrals follows from Exercise VI.8.7.
Consider the entire function z → e−z
along the closed piecewise C 1 curve Γ =
Γ1 + Γ2 + Γ3 that follows straight line segments from 0 to α > 0 to α + i α and finally
back 0. From the Cauchy integral theorem,
e−z dz =
e−z dz +
e−z dz .
Application VI.9.7 shows
−z 2
−t2
(α → ∞) .
The integral over Γ2 can be estimated as
e−(α+i t) i dt ≤
Noting
et dt ≤
e− Re(α+i t) dt = e−α
eαt dt =
1 α2
(e − 1) ,
et dt .
350
e−z dz ≤
(1 − e−α ) → 0 (α → ∞) .
lim −
With the parametrization t → t + i t of −Γ3 , we have
e−(1+i )
(1 + i ) dt = (1 + i )
e−2i t dt
= (1 + i )
cos(2t ) dt − i
sin(2t2 ) dt .
2t = τ and the limit α → ∞, we find using (5.7) that
2 π
(1 − i) .
cos(τ ) dτ − i
sin(τ ) dτ =
2(1 + i )
By the substitution
The maximum principle
We have already seen in Remark 5.10(b) that holomorphic functions have the mean
value property. In particular, the absolute value of a holomorphic function f at
the center of a disc cannot be larger than the maximum absolute value of f on the
boundary.
5.17 Theorem (generalized maximum principle) Suppose the function f has the
mean value property. If |f | has a local maximum at z0 ∈ U , then f is constant in
a neighborhood of z0 .
Proof (i) The case f (z0 ) = 0 is clear. Because f (z0 ) = 0, there is a c such that
|c| = 1 and cf (z0 ) > 0. Because cf also has the mean value property, we can
assume without loss of generality that f (z0 ) is real and positive. By assumption,
there is an r0 > 0 such that D(z0 , r0 ) ⊂ U and |f (z)| ≤ f (z0 ) for z ∈ D(z0 , r0 ).
f (z0 + rei t ) dt for r ∈ [0, r0 ] .
(ii) The function h : U → R, z → Re f (z) − f (z0 ) satisfies h(z0 ) = 0 and
h(z) ≤ |f (z)| − f (z0 ) ≤ 0
for z ∈ D(z0 , r0 ) .
According to Remark 5.10(c), h also has the mean value property. Therefore
0 = h(z0 ) =
h(z0 + rei t ) dt
for 0 ≤ r ≤ r0 .
Because h(z0 + rei t ) ≤ 0 for r ∈ [0, r0 ] and t ∈ [0, 2π], Proposition VI.4.8 and
(5.8) imply that h vanishes identically on D(z0 , r0 ). Therefore Re f (z) = f (z0 ) for
z ∈ D(z0 , r0 ). Now it follows from |f (z)| ≤ |f (z0 )| = Re f (z0 ) that Im f (z) = 0,
and therefore f (z) equals f (z0 ) for every z ∈ D(z0 , r0 ).
5.18 Corollary (maximum principle) Suppose U is connected and f has the mean
value property.
(i) If |f | has a local maximum at z0 ∈ U , then f is constant.
(ii) If U is bounded and f ∈ C U , C , then |f | assumes its maximum on ∂U ,
that is, there is a z0 ∈ ∂U such that |f (z0 )| = maxz∈U |f (z)|.
Proof (i) Suppose f (z0 ) = w0 and M := f −1 (w0 ). The continuity of f shows
that M is closed in U (see Example III.2.22(a)). According to Theorem 5.17, every
z1 ∈ M has a neighborhood V such that f (z) = f (z0 ) = w0 for z ∈ V . Therefore
M is open in U . Thus it follows from Remark III.4.3 that M coincides with U .
Therefore f (z) = w0 for every z ∈ U .
(ii) Because f is continuous on the compact set U , we know |f | assumes its
maximum at some point z0 ∈ U. If z0 belongs to ∂U , there is nothing to prove. If
z0 lies inside U , the claim follows from (i).
Harmonic functions
Suppose X is open in Rn and not empty. The linear map
Δ : C (X, K) → C(X, K) ,
is called the Laplace operator or Laplacian (on X). A function g ∈ C 2 (X, K) is
harmonic on X if Δg = 0. We denote the set of all functions harmonic on X by
Harm(X, K).
5.19 Remarks
(a) We have Δ ∈ Hom C 2 (X, K), C(X, K) , and
Harm(X, K) = Δ−1 (0) .
Thus the harmonic functions form a vector subspace of C 2 (X, K).
(b) For f ∈ C 2 (X, C), we have
f ∈ Harm(X, C) ⇐ Re f, Im f ∈ Harm(X, R) .
(c) Every holomorphic function in U is harmonic, that is, C ω (U, C) ⊂ Harm(U, C).
If f is holomorphic, the Cauchy–Riemann equations imply
∂x f = ∂x ∂y v − i ∂x ∂y u ,
∂y f = −∂y ∂x v + i ∂y ∂x u .
352
Therefore we find Δf = 0 because of Corollary VII.5.5(ii).
(d) Harm(U, C) = C ω (U, C).
Proof The function U → C, x + i y → x is harmonic but not holomorphic (see Remark 5.4(b)).
The previous considerations imply that the real part of a holomorphic function is harmonic. The next theorem shows that on simply connected domains
every harmonic real-valued function is the real part of a holomorphic function.
5.20 Proposition Suppose u : U → R is harmonic. Then we have these:
(i) Suppose V := D(z0 , r) ⊂ U for some (z0 , r) ∈ U × (0, ∞). Then there is a
holomorphic function g in V such that u = Re g.
(ii) If U is simply connected, there is a g ∈ C ω (U, C) such that u = Re g.
Proof Because u is harmonic, the 1-form α := −uy dx + ux dy satisfies the integrability conditions. Therefore α is closed.
(i) Because V is simply connected, Theorem 4.8 says there is a v ∈ C 1 (V, R)
such that dv = α |V . Therefore vx = −uy |V and vy = ux |V . Setting g(z) :=
u(x, y) + i v(x, y), we find from Remark 5.4(b) and Theorem 5.11 that g belongs
to C ω (V, C).
(ii) In this case, we can replace the disc V in the proof of (i) by U .
5.21
Corollary Suppose u : U → R is harmonic. Then
u ∈ C ∞ (U, R);
u has the mean value property;
if U is a domain and there is a nonempty subset V of U such that u |V = 0,
then u = 0.
Proof (i) and (ii) Suppose V := D(z0 , r) ⊂ U for some r > 0. Because differentiability and the mean value property are local properties, it suﬃces to consider u
on V . Using Proposition 5.20, we find a g ∈ C ω (V, C) such that Re g = u |V , and
the claims follow from Remark 5.13(b) and Remarks 5.10(b) and (c).
(iii) Suppose M is the set of all z ∈ U for which there is a neighborhood V
such that u |V = 0. Then M is open and, by assumption, not empty. Suppose
z0 ∈ U is a cluster point of M . By Proposition 5.20, there is an r > 0 and
a g ∈ C ω D(z0 , r), C such that Re g = u |D(z0 , r). Further, M ∩ D(z0 , r) is
not empty because z0 is a cluster point of M . For z1 ∈ M ∩ D(z0 , r), there
is a neighborhood V of z1 in U such that u |V = 0. Therefore it follows from
Remark 5.4(d) that g is constant on V ∩D(z0 , r). The identity theorem for analytic
functions (Theorem V.3.13) then shows that g is constant on D(z0 , r). Therefore
u = 0 on D(z0 , r), that is, z0 belongs to M . Consequently M is closed in U , and
Remark III.4.3 implies M = U .
5.22 Corollary (maximum and minimum principle for harmonic functions)
Suppose U is a domain and u : U → R is harmonic. Then
(i) if u has a local extremum in U , then u is constant;
(ii) if U is bounded and u ∈ C U , R , then u assumes its maximum and its
minimum on ∂U .
Proof According to Corollary 5.21(ii), u has the mean value property.
(i) Suppose z0 ∈ U is a local extremal point of u. When u(z0 ) is a positive maximum of u, the claim follows from Theorem 5.17 and Corollaries 5.18
and 5.21(iii). If u(z0 ) is a positive minimum of u, then z → 2u(z0 ) − u(z) has a
positive maximum at z0 , and the claim follows as in the first case. The remaining
cases can be treated similarly.
(ii) This follows from (i).
5.23 Remarks (a) The set of zeros of a holomorphic function is discrete.5 However,
the set of zeros of a real-valued harmonic function is not generally discrete.
Proof The first statement follows from Theorem 5.11 and the identity theorem analytic
functions (Theorem V.3.13). The second statement follows by considering the harmonic
function C → R, x + i y → x.
(b) One can show that a function is harmonic if and only if has the mean value
property (see for example [Con78, Theorem X.2.11]).
Goursat’s theorem
We will now prove, as promised in Remark 5.4(a), that every complex differentiable
function has continuous derivatives and is therefore holomorphic. To prepare,
we first prove Morera’s theorem, which gives criterion for proving a function is
holomorphic. This criterion is useful elsewhere, as we shall see.
Suppose X ⊂ C. Every closed path Δ of line segments in X that has exactly
three sides is called a triangular path in X if the closure of the triangle bounded
by Δ lies in X.
5.24 Theorem (Morera) Suppose the function f satisfies
triangular path Δ in U . Then f is analytic.
f dz = 0 for every
Proof Suppose a ∈ U and r > 0 with D(a, r) ⊂ U . It suﬃces to verify that
f |D(a, r) is analytic. So suppose z0 ∈ D(a, r) and
F : D(a, r) → C ,
f (w) dw .
[[a,z]]
5 A subset D of a metric space X is said to be discrete if every d ∈ D has a neighborhood U
in X such that U ∩ D = {d}.
354
Our assumptions imply the identity
F (z) =
f (w) dw +
[[a,z0 ]]
f (w) dw = F (z0 ) +
[[z0 ,z]]
F (z) − F (z0 )
− f (z0 ) =
f (w) − f (z0 ) dw
for z = z0 .
Suppose now ε > 0. Then there is a δ ∈ (0, r −|z0 −a|) such that |f (w)−f (z0 )| < ε
for w ∈ D(z0 , δ). Thus it follows from (5.9) that
− f (z0 ) < ε
for 0 < |z − z0 | < δ .
Therefore F (z0 ) = f (z0 ), which shows that F has continuous derivatives. Then
Theorem 5.11 shows F belongs to C ω D(a, r), C , and we find that F = f |D(a, r)
belongs to C ω D(a, r), C .
5.25 Theorem (Goursat) Suppose f is differentiable. Then
triangular path Δ in U .
dz = 0 for every
Proof (i) Suppose Δ is a triangular path in U . Without loss of generality, we
can assume that Δ bounds a triangle with positive area. Then Δ has the vertices
z0 , z1 , and z2 , and z2 ∈ [[z0 , z1 ]]. Thus Δ f (z) dz = 0, as is easily verified. We
denote by K the closure of the triangle bounded by Δ.
By connecting the midpoints
of the three sides of Δ, we get
four congruent closed subtriangles
K1 , . . . , K4 of K. We orient the
(topological) boundaries of the Kj
so that shared sides are oppositely
oriented and denote the resulting
triangular paths by Δ1 , . . . , Δ4 .
f (z) dz ≤ 4 max
1≤j≤4
f (z) dz .
Of the four triangular paths Δ1 , . . . , Δ4 , there is one, which we call Δ1 , that
satisfies
f (z) dz = max
f (z) dz ≤ 4
(ii) To Δ1 , we apply the same decomposition and selection procedure as we
did for Δ. Thus we inductively obtain a sequence (Δn ) of triangular paths and
corresponding closed triangles K n such that
K 1 ⊃ K 2 ⊃ · · · ⊃ K n ⊃ · · · and
f (z) dz
Δn+1
for n ∈ N× . Clearly { K n ; n ∈ N× } has the finite intersection property. Therefore it follows from the compactness of K 1 that n K n is not empty (see Exercise III.3.5). We fix a z0 in K n .
(iii) The inequality in (5.10) implies
f (z) dz ≤ 4n
In addition, we have the elementary geometric relations
L(Δn+1 ) = L(Δn )/2 and diam(K n+1 ) = diam(K n )/2 for n ∈ N× .
L(Δn ) = /2n and diam(K n ) = d/2n
where := L(Δ) and d := diam(K).
(iv) Suppose ε > 0. The differentiability of f at z0 implies the existence of a
δ > 0 such that D(z0 , δ) ⊂ U and
|f (z) − f (z0 ) − f (z0 )(z − z0 )| ≤
for z ∈ D(z0 , δ) .
We now choose an n ∈ N× with diam(K n ) = d/2n < δ. Because z0 ∈ K n , we then
have Δn ⊂ D(z0 , δ). The Cauchy integral theorem implies
f (z) − f (z0 ) − f (z0 )(z − z0 ) dz
max |z − z0 | L(Δn ) ≤
diam(K n )L(Δn ) = n .
Now (5.11) finishes the proof.
5.26 Corollary If f is differentiable, f is holomorphic.
This follows from Theorems 5.24 and 5.25.
356
The Weierstrass convergence theorem
As a further application of Morera’s theorem, we prove a theorem due to Weierstrass concerning the limit of a locally uniformly convergent sequence of holomorphic functions. We have already applied this result — in combination with
Theorem 5.11 — in Application VI.7.23(b) to prove the product representation of
the sine.
5.27 Theorem (Weierstrass convergence theorem) Suppose (gn ) is a locally
uniformly convergent sequence of holomorphic functions in U . Then g := lim gn
is holomorphic in U .
Proof According to Theorem V.2.1, g is continuous. From Remark V.2.3(c) we
know that (gn ) is uniformly convergent on every compact subset of U . Therefore (gn ) to g converges uniformly on every triangular path Δ in U . Therefore
Proposition VI.4.1(i) implies
g dz = lim
gn dz = 0 ,
where the last equality follows from the Cauchy integral theorem applied to gn .
Because (5.12) holds for every triangular path in U , Morera’s theorem finishes the
If f : U → C is real differentiable,
(∂x f − i ∂y f )
(∂x f + i ∂y f )
are called the Wirtinger derivatives6 of f .
(a) ∂ W f = ∂W f , and ∂ W f = ∂W f ;
(b) f is holomorphic ⇐ ∂ W f = 0;
(c) 4∂W ∂ W f = 4∂ W ∂W f = Δf if f is twice real differentiable;
(d) det
= det
= |∂W f |2 − |∂ W f |2 .
Suppose d z := d(z → z). Then dz = dx − i dy and
for f ∈ C 1 (U, C) .
6 Usually, the Wirtinger derivative will be denoted by ∂ (or ∂). However, when this notation
might collide with our notation ∂ for the derivative operator, we will write ∂W (or respectively
3 The 1-form α ∈ Ω(1) (U, C) is said to be holomorphic if every z0 ∈ U has a neighborhood V such that there is an f ∈ C 1 (V, C) with df = α | V .
(a) Show these statements are equivalent:
(i) α is holomorphic.
(ii) There is a real differentiable function a ∈ C(U, C) such that α = a dz and α is
closed.
(iii) There is an a ∈ C 1 (U, C) such that α = a dz.
is a holomorphic 1-form in C× . Is α is globally exact, that is, is there a holomorphic
function f in C× such that α = f dz?
4 Suppose U is connected and u : U → R is harmonic. If v ∈ C 1 (U, R) satisfies the
relations vx = −uy and vy = ux , we say that v is conjugate to u. Prove
(a) if v is conjugate to u, then v is harmonic;
(b) if U is simply connected, then to every function harmonic in U there is a harmonic
conjugate. (Hint: Consider ux − i uy .)
5 Prove the Weierstrass convergence theorem using the Cauchy integral formula. Also
show that, under the assumptions of Theorem 5.27, for every k ∈ N the sequence (fn )n∈N
of k-th derivatives converges locally uniformly on U to f . (Hint: Proposition VII.6.7).
6 Suppose z0 ∈ U and r > 0 with D(z0 , r) ⊂ U . Further suppose z ∈ D(z0 , r) and δ > 0
with D(z, δ) ⊂ U . Verify that ∂D(z, δ) and ∂D(z0 , r) are homotopic in U \{z}.
With the help of the Cauchy integral theorem, show
8 Suppose p =
dx/(1 + x2 ) = π.
ak X k ∈ C[X] is such that an = 1. Let R := max 1, 2
|z|n /2 ≤ |p(z)| ≤ 2 |z|n
|ak | .
Suppose 0 ≤ r0 < r < r1 ≤ ∞ and f is holomorphic in D(z0 , r1 ). Prove that
|f (n) (z)| ≤
(r − r0 )n+1
w∈∂D(z0 ,r)
|f (w)|
for z ∈ D(z0 , r0 ) and n ∈ N .
10 Given an entire function f , suppose there are M, R > 0 and an n ∈ N such that
|f (z)| ≤ M |z|n for z ∈ RDc . Show f is a polynomial with deg(f ) ≤ n. (Hint: Exercise 9.)
11 Suppose Γ1 and Γ2 are compact piecewise C 1 curves in U and f ∈ C(U × U, C).
f (w, z) dw dz =
f (w, z) dz dw .
(Hint: Exercise VII.6.2).
12 Suppose Γ is a compact piecewise C 1 curve in U and f ∈ C(U × U, C). Also suppose
f (w, ·) is holomorphic for every w ∈ U . Show that
f (w, z) dw
358
is holomorphic and that F = Γ ∂2 f (w, ·) dw.
(Hint: Exercise 11, Morera’s theorem, and Proposition VII.6.7.)
13 Suppose L = a + Rb, a, b ∈ C is a straight line in C, and f ∈ C(U, C) is holomorphic
in U \L. Show f is holomorphic in all of U . (Hint: Morera’s theorem.)
Suppose U is connected and f is holomorphic in U . Prove the following.
(a) If |f | has a global minimum at z0 , then either f is constant or f (z0 ) = 0.
(b) Suppose U is bounded and f ∈ C U , C . Show either that f has a zero in U or that
|f | assumes its minimum on ∂U .
For R > 0,
(ζ, z) →
R2 − |z|2
|ζ − z|2
is called the Poisson kernel for RD.
(a) PR (ζ, z) = Re (ζ + z) (ζ − z) for (ζ, z) ∈ R∂D × RD;
(b) for every ζ ∈ R∂D, the function PR (ζ, ·) is harmonic in RD;
(c) for r ∈ [0, R] and t, θ ∈ [0, 2π),
PR (Rei θ , rei t ) =
(d) P1 (1, rei t ) =
R2 − r 2
− 2Rr cos(θ − t) + r 2
r |n| ei nt for r ∈ [0, 1), and t ∈ R;
PR (Rei θ , z) dθ = 2π for z ∈ RD.
(Hint for (d):
it k
k=0 (re )
= 1/(1 − rei t ).)
Suppose ρ > 1 and f is holomorphic in ρD. Show that
P1 (ei θ , z)f (ei θ ) dθ
(Hints: (i) For g ∈ C 1 (ρ0 D, C) with ρ0 > 1, show
g(z) =
g(ei θ )
1 − e−i θ z
(ii) For z ∈ D, ρ0 := min(ρ, 1/|z|), g : ρ0 D → C, show w → f (w)/(1 − wz) is holomorphic.)
Show these:
(a) For g ∈ C(∂D, R),
P1 (ei θ , z)g(ei θ ) dθ
(b) If f ∈ C D, R is harmonic in D, then
(Hints: (a) Exercise 15(b) and Proposition VII.6.7. (b) Suppose 0 < rk < 1 with
lim rk = 1 and fk (z) := f (rk z) for z ∈ rk D. Exercise 16 gives
fk (z) =
P1 (ei θ , z)fk (ei θ ) dz
Now consider the limit k → ∞.)
Suppose a ∈ C and α = Re a. Show that for γα : R → C, s → α + i s, we have
eta =
eλt (λ − a)−1 dλ
for t > 0 .
(Hint: The Cauchy integral formula gives
∂D(a,r)
Now apply the Cauchy integral theorem.)
for t ∈ R and r > 0 .
360
6 Meromorphic functions
In central sections of this chapter, we explored complex functions that were holomorphic except at isolated points. Typical examples are
z → e1/z ,
z → sin(z)/z ,
C\{±i} → C ,
z → 1/(1 + z 2 ) ,
It turns out that these “exceptional points” lead to an amazingly simple classification. This classification is aided by the Laurent series, which expands a function
as a power series in both positive and negative coeﬃcients. It thus generalizes the
Taylor series to one that can expand holomorphic functions.
In this context, we will also extend the Cauchy integral theorem and prove
the residue theorem, which has many important applications, of which we give
only a few.
The Laurent expansion
For c := (cn ) ∈ CZ , we consider the power series
cn X n and hc :=
n≥0
Suppose their convergence radii are ρ1 and 1/ρ0 , and 0 ≤ ρ0 < ρ1 ≤ ∞. Then the
function nc [or hc] represented by nc [or hc] in ρ1 D [or (1/ρ0 )D] is holomorphic
by Theorem V.3.1. Because z → 1/z is holomorphic in C× and |1/z| < 1/ρ0 for
|z| > ρ0 , Remark 5.13(d) guarantees that the function
z → hc(1/z) =
is holomorphic at |z| > ρ0 . Therefore
is a holomorphic function in the annulus around 0 given by
Ω(ρ0 , ρ1 ) := ρ1 D\ρ0 D = { z ∈ C ; ρ0 < |z| < ρ1 } .
Suppose now z0 ∈ C. Then
cn (z − z0 )n :=
c−n (z − z0 )−n
cn (z − z0 )n +
VIII.6 Meromorphic functions
is called the Laurent series about the expansion point z0 , and (cn ) is the se−n
quence of coeﬃcients. In (6.1),
is the principal part and
n≥1 c−n (z − z0 )
n≥0 cn (z − z0 ) is the auxiliary part. The Laurent series (6.1) is convergent [or
norm convergent] in M ⊂ C if both the principal part and the auxiliary part are
convergent [or norm convergent] in M . Its value at z ∈ M is by definition the sum
of the value of the principal and auxiliary parts in z, that is,
c−n (z − z0 )−n .
From these preliminaries and Theorem V.1.8, it follows that the Laurent series
n∈Z cn (z − z0 ) converges normally in every compact subset of the annulus
around z0 given by
z0 + Ω(ρ0 , ρ1 ) = { z ∈ C ; ρ0 < |z − z0 | < ρ1 }
and that the function
z0 + Ω(ρ0 , ρ1 ) → C ,
cn (z − z0 )n
is holomorphic. The considerations above show that, conversely, every function
holomorphic in an annulus can be represented by a Laurent series.
6.1 Lemma For ρ > 0 and a ∈ C,
z−a
|a| < ρ ,
|a| > ρ .
Proof (i) Suppose |a| < ρ and δ > 0 with D(a, δ) ⊂ ρD. Because ∂D(a, δ) and
ρ∂D are homotopic in C\{a} (see Exercise 5.6), the claim follows from Proposition 5.7 and Example 5.3(a).
(ii) If |a| > ρ, then ρ∂D is null homotopic in C \ {a}, and the claim again
follows from Proposition 5.7.
6.2 Lemma Suppose f : Ω(r0 , r1 ) → C is holomorphic.
(i) For r, s ∈ (r0 , r1 ), we have
(ii) Suppose a ∈ Ω(ρ0 , ρ1 ) with r0 < ρ0 < ρ1 < r1 . Then
f (a) =
ρ1 ∂D
ρ0 ∂D
362
Proof (i) Because r∂D and s∂D are homotopic in Ω := Ω(r0 , r1 ), the claim follows
from Proposition 5.7.
(ii) Suppose g : Ω → C is defined through
f (z) − f (a) (z − a) ,
f (a) ,
z ∈ Ω\{a} ,
z=a.
Obviously g is holomorphic in Ω\{a} with
g (z) =
f (z)(z − a) − f (z) + f (a)
(z − a)2
for z ∈ Ω\{a} .
With Taylor’s formula (Corollary IV.3.3)
f (z) = f (a) + f (a)(z − a) + f (a)(z − a)2 + o |z − a|2
as z → a, we find
o |z − a|2
f (z) − f (a)
g(z) − g(a)
− f (a) = f (a) +
as z → a in Ω\{a}. Thus g is differentiable at a with g (a) = f (a)/2. Therefore
g is holomorphic in Ω. The claim follows by applying Lemma 6.1 and (i) to g.
Now we can prove the aforementioned expansion theorem.
6.3 Theorem (Laurent expansion) Every function f holomorphic in Ω := Ω(r0 , r1 )
has a unique Laurent expansion
The Laurent series converges normally on every compact subset of Ω, and its
coeﬃcients are given by
ζ n+1
for n ∈ Z and r0 < r < r1 .
Proof (i) We first verify that f is represented by the Laurent series with the
coeﬃcients given in (6.5). From Lemma 6.2(i) it follows that cn is well defined for
n ∈ Z, that is, it is independent of r.
Suppose r0 < s0 < s1 < r1 and z ∈ Ω(s0 , s1 ). For ζ ∈ C with |ζ| = s1 , we
have |z/ζ| < 1, and thus
ζ 1 − z/ζ
with normal convergence on s1 ∂D. Therefore we have
s1 ∂D
(see Exercise 4.9). For ζ ∈ C with |ζ| = s0 , we have |ζ/z| < 1, and therefore
z 1 − ζ/z
z m+1
with normal convergence on s0 ∂D. Therefore
s0 ∂D
f (ζ)ζ m dζ z −m−1
Thus we get from Lemma 6.2(ii) the representation (6.4).
(ii) Now we prove the coeﬃcients in (6.4) are unique. Suppose f (z) =
n=−∞ an z converges normally on compact subsets of Ω. For r ∈ (r0 , r1 ) and
m ∈ Z, we have (see Example 5.3(a))
f (z)z −m−1 dz =
z n−m−1 dz = am .
This shows am = cm for m ∈ Z.
(iii) Because we already know that the Laurent series converges normally on
compact subsets of Ω, the theorem is proved.
As a simple consequence of this theorem, we obtain the Laurent expansion
of a function holomorphic in the open punctured disc
D• (z0 , r) := z0 + Ω(0, r) = { z ∈ C ; 0 < |z − z0 | < r } .
6.4 Corollary Suppose f is holomorphic in D• (z0 , r). Then f has a unique Laurent
expansion
for z ∈ D• (z0 , r) ,
∂D(z0 ,ρ)
(z − z0 )n+1
for n ∈ Z and ρ ∈ (0, r) .
364
The series converges normally on every compact subset of D• (z0 , r), and
z∈∂D(z0 ,ρ)
|f (z)| for n ∈ Z and ρ ∈ (0, r) .
Proof With the exception of (6.6), all the statements follow from Theorem 6.3
applied to z → f (z + z0 ). The estimate (6.6) is implied by (6.5) and Proposition 5.2(ii).
Removable singularities
• U is an open subset of C and z0 is a point in U .
Given a holomorphic function f : U \{z0 } → C, the point z0 is a removable singularity if f has a holomorphic extension F : U → C. When there is no fear of
confusion, we reuse the symbol f for this extension.
6.5 Example Let f : U → C be holomorphic. Then z0 is removable singularity of
g : U \{z0} → C ,
z → f (z) − f (z0 )
(z − z0 ) .
In particular, 0 is a removable singularity of
z → (cos z − 1)/z ,
z → log(z + 1)/z .
This follows from the proof of Lemma 6.2(ii).
Removable singularities of a function f can be characterized by the local
boundedness of f :
6.6 Theorem (Riemann’s removability theorem) Suppose f : U \ {z0 } → C is
holomorphic. Then the point z0 is a removable singularity of f if and only if f is
bounded in a neighborhood of z0 .
Proof Suppose r > 0 with D(z0 , r) ⊂ U .
If z0 is a removable singularity of f , there is an F ∈ C ω (U ) such that F ⊃ f .
From the compactness of D(z0 , r), it follows that
z∈D• (z0 ,r)
|f (z)| =
|F (z)| ≤
max |F (z)| < ∞ .
z∈D(z0 ,r)
Therefore f is bounded in the neighborhood D• (z0 , r) of z0 in U \{z0}.
To prove the converse, we set
M (ρ) :=
|f (z)| for ρ ∈ (0, r) .
By assumption, there is an M ≥ 0 such that M (ρ) ≤ M for ρ ∈ (0, r) (because |f |
is continuous on D(z0 , r)\{z0 } and thus is bounded for every 0 < r0 < r on the
compact set D(z0 , r)\D(z0 , r0 )). Thus it follows from (6.6) that
|cn | ≤ M (ρ)ρ−n ≤ M ρ−n
Therefore the principal part of Laurent expansion of f vanishes, and from Corollary 6.4 it follows that
for z ∈ D• (z0 , r) .
The function defined through
for z ∈ D(z0 , r) ,
is holomorphic on D(z0 , r) and agrees with f on D• (z0 , r). Therefore z0 is a
removable singularity of f .
Isolated singularities
Suppose f : U \ {z0 } → C is holomorphic and r > 0 with D(z0 , r) ⊂ U . Further
for z ∈ D• (z0 , r)
is the Laurent expansion of f in D• (z0 , r). If z0 is a singularity of f , it is isolated
if it is not removable. Due to (the proof of) Riemann’s removability theorem, this
is the case if and only if the principal part of the Laurent expansion of f does not
vanish identically. If z0 is an isolated singularity of f , we say z0 is a pole of f if
there is an m ∈ N× such that c−m = 0 and c−n = 0 for n > m. In this case, m
is the order of the pole. If infinitely many coeﬃcients of the principal part of the
Laurent series are different from zero, z0 is an essential singularity of f . Finally,
we define the residue of f at z0 through
Res(f, z0 ) := c−1 .
A function g is said to be meromorphic in U if there is a closed subset P (g)
of U on which g is holomorphic in U \ P (g) and every z ∈ P (g) is a pole of g.1
Then P (g) is the set of poles of g.
1 P (g)
can also be empty. Therefore every holomorphic function on U is also meromorphic.
366
6.7 Remarks (a) Suppose f : U \{z0} → C is holomorphic. Then we have
Res(f, z0 ) =
for every r > 0 such that D(z0 , r) ⊂ U . The residue of f at z0 is therefore (up
to the factor 1/2πi) what is “left over” (or residual) after integrating f over the
path ∂D(z0 , r).
This follows from Corollary 6.4 and Example 5.3(a).
(b) The set of poles P (f ) of a function f meromorphic in U is discrete and
countable, and this set has no cluster point in U .2
Proof (i) Suppose z0 ∈ P (f ). Then there is an r > 0 such that D(z0 , r) ⊂ U . Hence f
is holomorphic in D• (z0 , r). Therefore P (f ) ∩ D(z0 , r) = {z0 }, which shows that P (f ) is
discrete.
(ii) Assume P (f ) has a cluster point z0 in U . Because P (f ) is discrete, z0 does not
belong to P (f ). Therefore z0 lies in an open set U \ P (f ), and we find an r > 0 with
D(z0 , r) ⊂ U \P (f ). Therefore z0 is not a cluster point of P (f ), which is a contradiction.
(iii) For every z ∈ P (f ), there is an rz > 0 with D• (z, rz ) ∩ P (f ) = ∅. If K is a
compact subset of U , then K ∩P (f ) is also compact. Therefore we find z0 , . . . , zm ∈ P (f )
D(zj , rzj ) .
K ∩ P (f ) ⊂
Consequently K ∩ P (f ) is a finite set.
(iv) To verify that P (f ) is countable, we set
x ∈ U ; d(x, ∂U ) ≥ 1/j, |x| ≤ j
Due to Examples III.1.3(l) and III.2.22(c) and because of the Heine–Borel theorem, every
Kj is compact. Also j Kj = U , and Kj ∩ P (f ) is finite for every j ∈ N× . It then follows
from Proposition I.6.8 that P (f ) = j Kj ∩ P (f ) is countable.
The next theorem shows that a function is meromorphic if and only if it is
locally the quotient of two holomorphic functions.3
6.8 Proposition The function f is meromorphic in U if and only if there is a
closed subset A of U and these conditions are satisfied:
(i) f is holomorphic in U \A.
(ii) To every a ∈ A, there is an r > 0 such that D(a, r) ⊂ U , g, h ∈ C ω D(a, r)
with h = 0, and f = g/h in D• (a, r).
2 However,
it is entirely possible that the f ’s poles may accumulate on the boundary of U .
explains the name “meromorphic”, which means “fractional form”, while “holomorphic”
can be translated as “whole form”.
3 This
Proof (a) Suppose f is meromorphic in U . Then, for a ∈ P (f ) =: A, there is an
r > 0 with D(a, r) ⊂ U such that f has the Laurent expansion
cn (z − a)n
for z ∈ D• (a, r)
in D• (a, r) for a suitable m ∈ N× . The holomorphic function
D• (a, r) → C ,
z → (z − a)m f (z) =
ck−m (z − a)k
has a removable singularity at a. Thus there is a g ∈ C ω D(a, r) such that
g(z) = (z − a)m f (z) for 0 < |z − a| < r .
Therefore f has the representation f = g/h in D• (a, r) with h := (X − a)m ∈
C ω (C).
(b) Suppose the given conditions are satisfied. If h(a) = 0, we can assume
(by shrinking r) that h(z) = 0 for z ∈ D(a, r). Then f = g/h is holomorphic
in D(a, r). In the case h(a) = 0, we can assume that there is an m ∈ N× such that
h has the power series expansion
ck (z − a)k = (z − a)m
h(z) =
cn+m (z − a)n
in D(a, r), where cm is distinct from zero because h = 0. Therefore the function
defined through
ϕ(z) :=
for z ∈ D(a, r)
is holomorphic on D(a, r) (see Proposition V.3.5) with ϕ(a) = cm = 0. Thus there
is a ρ ∈ (0, r) such that ϕ(z) = 0 for |z − a| ≤ ρ, which implies g/ϕ ∈ C ω D(a, ρ) .
Denoting by n≥0 bn (z − a)n the Taylor series for g/ϕ, we find
g(z)
bn (z − a)n−m
h(z)
(z − a)
for z ∈ D• (a, ρ) .
From this expression and from the uniqueness of the Laurent expansion, we learn
that a is a pole of f .
368
6.9 Examples
many poles.
(a) Every rational function is meromorphic in C and has finitely
(b) The tangent and the cotangent are meromorphic in C. Their sets of poles are
respectively π(Z + 1/2) and πZ. The Laurent expansion of the cotangent in πD•
ζ(2k) 2k−1
cot z = − 2
π 2k
Proof Because tan = sin / cos and cot = 1/ tan, the first two claims follow from Proposition 6.8. The given representation of the cotangent is implied by (VI.7.18), Theorem
VI.6.15, and the uniqueness of the Laurent expansion.
(c) The gamma function is meromorphic in C. Its set of poles is −N.
Proof The Weierstrass product representation of Proposition VI.9.5 and the Weierstrass
convergence theorem (Theorem 5.27) implies that Γ is the reciprocal of an entire function
whose set of zeros is −N. The claim then follows from Proposition 6.8.
(d) The Riemann ζ function is meromorphic in C.
This follows from Theorem VI.6.15.
(e) The function z → e1/z is not meromorphic in C.
Because e1/z = exp(1/z), we have
e1/z =
= 1 + + 2 + ···
Therefore 0 is an essential singularity of z → e1/z .
Simple poles
As we shall see, the residues of meromorphic functions play a particularly important role. Therefore, we should seek ways to determine residues without explicitly
carrying out the Laurent expansion. This is especially easy for first order poles,
the simple poles, as we show next.
6.10 Proposition The holomorphic function f : U \{z0} → C has a simple pole at
z0 if and only if z → g(z) := (z − z0 )f (z) has a removable singularity at z0 with
g(z0 ) = 0. Then
Res(f, z0 ) = lim (z − z0 )f (z) .
z→z0
Proof Suppose g is holomorphic in U with g(z0 ) = 0. Then there is an r > 0
with D(z0 , r) ⊂ U and a sequence (bn ) in C such that
bn (z − z0 )n for z ∈ D(z0 , r) and g(z0 ) = b0 = 0 .
n=−1
where cn := bn+1 and c−1 := b0 = 0, the point z0 is a simple pole, and
Res(f, z0 ) = c−1 = b0 = g(z0 ) = lim (z − z0 )f (z) .
Conversely, if z0 is a simple pole of f , there is an r > 0 such that D(z0 , r) ⊂ U and
for z ∈ D• (z0 , r) and c−1 = 0 .
(z − z0 )f (z) =
cn−1 (z − z0 )n
Now the claim is implied by the Riemann removability theorem and the identity
theorem for analytic functions.
6.11 Examples (a) Suppose g and h are holomorphic in U and h has a simple
zero at z0 , that is,4 h(z0 ) = 0 and h (z0 ) = 0. Then f := g/h is meromorphic in
U , and z0 is a simple pole of f with Res(f, z0 ) = g(z0 )/h (z0 ) if g(z0 ) = 0.
The Taylor formula gives
h(z) = (z − z0 )h (z0 ) + o(|z − z0 |) (z → z0 ) ,
and therefore h(z)(z − z0 )−1 → h (z0 ) for z → z0 . This implies
lim (z − z0 )f (z) = lim g(z)/ h(z)(z − z0 )−1 = g(z0 )/h (z0 ) ,
and the claim then follows from Theorems 6.8 and 6.10 and the Riemann removability
theorem.
(b) The tangent and the cotangent have only simple pole poles. Their residues
are given by
Res tan, π(k + 1/2) = − Res(cot, kπ) = −1 for k ∈ Z .
This follows immediately from (a).
simple zero is a zero of order 1 (see Exercise IV.3.10).
370
(c) The gamma function has only first order poles, and
Res(Γ, −n) = (−1)n /n! for n ∈ N .
From (VI.9.2) we obtain for z ∈ C\(−N) with Re z > −n − 1 the representation
(z + n)Γ(z) =
Therefore (z + n)Γ(z) converges for z → −n to (−1)n Γ(1)/n! . Because Γ(1) = 1, the
claim is implied by Example 6.9(c) and Proposition 6.10.
(d) The Riemann ζ function has a simple pole at 1 with residue 1.
(e) Suppose p ∈ R and a > 0. Then the function defined by f (z) := e−i pz /(z 2 +a2 )
is meromorphic in C and has simple poles at ±i a with
Res(f, ±i a) = ±e±pa /2ia .
Obviously (z ∓ i a)f (z) = e−i pz /(z ± i a) is holomorphic at ±i a, and
lim (z ∓ i a)f (z) = ±e±pa /2i a .
z→±i a
The claim therefore follows from Proposition 6.10.
(f ) For p ∈ R and f (z) := e−i pz /(z 4 + 1), the function f is meromorphic in C and
P (f ) = { zj := ei (π/4+jπ/2) ; j = 0, . . . , 3 } .
Every pole is simple, and every residue has
Res(f, zj ) = e−i pzj /4zj
for 0 ≤ j ≤ 3 .
An elementary calculation gives
(zj − zk ) = 4zj .
Therefore this claim also follows from Proposition 6.10.
The winding number
We have already seen in the last section that the homotopy invariance of line integrals of holomorphic functions (Proposition 5.7) can be used to effectively calculate
other integrals. We will now derive a like result for meromorphic functions. For
this we must know more about the position of the poles relative to the integration
curve.
In the following
• Γ is always a closed compact piecewise C 1 curve in C.
For a ∈ C\Γ,
w(Γ, a) :=
is called the winding number, the circulation number, or the index of Γ about a.
6.12 Examples (a) Suppose m ∈ Z× and r > 0. Then for z0 ∈ C, the parametrization γm : [0, 2π] → z0 + rei mt is that of a smooth curve Γm := Γm (z0 , r),
whose image is the oriented circle ∂D(z0 , r). If m is positive [or negative], then Γm
is oriented the same as [or opposite to] ∂D(z0 , r). Therefore ∂D(z0 , r) will circulate
|m| times in the positive [or negative] direction as t goes from 0 to 2π. On these
grounds, we call Γm the m-times traversed circle with center z0 and radius r. We
|a − z0 | < r ,
w(Γm , a) =
|a − z0 | > r .
As in the proof of Lemma 6.1, it follows in the case |a − z0 | < r that
i mrei mt
dt = 2πi m .
rei mt
When |a − z0 | > r, the curve Γm is null homotopic in C\{a}.
(b) For z0 ∈ C, suppose Γz0 is a point curve with im(Γz0 ) = {z0 }.
w(Γz0 , a) = 0 for a ∈ C\{z0 }.
(c) If γ1 and γ2 are homotopic piecewise C 1 loops in U and a ∈ U c , then
w(Γ1 , a) = w(Γ2 , a)
for Γ1 = [γ1 ] and Γ2 = [γ2 ].
Proof For a ∈ U c , the function z → 1/(z − a) is holomorphic in U . Therefore the claim
(d) If U is simply connected and Γ ⊂ U , then w(Γ, a) = 0 for a ∈ U c .
This follows from (b) and (c).
According to Example 6.12(a), the winding number w(Γm , a) of the m-times
traversed circle Γm := Γm (z0 , r) is the (signed) number of times Γm “winds
around” the point a. We next show that this geometric interpretation of the
winding is valid for every closed piecewise C 1 curve, but we must first prove a
technical result.
372
6.13 Lemma Suppose I is a perfect compact interval and γ : I → C× has piecewise
continuous derivatives. Then there is a continuous and piecewise continuously
differentiable function ϕ : I → C such that exp ◦ ϕ = γ. Here, γ and ϕ have
continuous derivatives on the same interval I.
Proof (i) We define logα := Log |(C \ R+ ei α ) for α ∈ R. Then from Proposition III.6.19 and Exercise III.6.9, logα is a topological map of C \ R+ ei α onto
R + i (α, α + 2π) satisfying
exp(logα z) = z
for z ∈ C\R+ ei α .
From this we find (see Example IV.1.13(e)) that logα is a C 1 diffeomorphism of
C\R+ ei α onto R + i (α, α + 2π) satisfying
(logα ) (z) = 1/z
(ii) Because γ(I) is compact, there exists an r > 0 with rD ∩ γ(I) = ∅.
From the uniform continuity of γ, there is a partition (t0 , . . . , tm ) of I such that
diam γ |[tj−1 , tj ] < r/2 for 1 ≤ j ≤ m. Because γ has piecewise continuous
derivatives, we can choose this partition so that γ |[tj−1 , tj ] is continuously differentiable for 1 ≤ j ≤ m. Because the disc Dj := D γ(tj ), r has no zeros, we
can fix an αj ∈ (−π, π) such that R+ ei αj ∩ Dj = ∅ for 1 ≤ j ≤ m. Then we
set logj := logαj and ϕj := logj ◦ γ |[tj−1 , tj ]. From (i), we know ϕj belongs to
C 1 [tj−1 , tj ] , and because γ(t) ∈ Dj ∩ Dj+1 for t ∈ [tj−1 , tj ], we find using (6.8)
exp ϕj (tj ) = γ(tj ) = exp ϕj+1 (tj ) for 1 ≤ j ≤ m − 1 .
Consequently, Proposition III.6.13 and the addition theorem for the exponential
function guarantee the existence of kj ∈ Z such that
ϕj (tj ) − ϕj+1 (tj ) = 2πikj
for 1 ≤ j ≤ m − 1 .
Now we define ϕ : I → C by
j−1
ϕ(t) := ϕj (t) + 2πi
for tj−1 ≤ t ≤ tj and 1 ≤ j ≤ m ,
where k0 := 0. Then it follows from (6.10) and the continuous differentiability
of logj and γ |[tj−1 , tj ] that ϕ has piecewise continuous derivatives. Finally, we
get exp ◦ ϕ = γ from (6.8), the definition of ϕj , and the 2πi -periodicity of the
exponential function.
6.14 Theorem For every a ∈ C\Γ, the index w(Γ, a) is a whole number.
Proof Suppose γ is a piecewise C 1 parametrization of Γ and (t0 , . . . , tm ) is a
partition of the parameter interval I such that γ |[tj−1 , tj ] is continuously differentiable for 1 ≤ j ≤ m. Then, according to Lemma 6.13, to a ∈ C \ Γ there is
a ϕ ∈ C(I) such that ϕ|[tj−1 , tj ] ∈ C 1 [tj−1 , tj ] for 1 ≤ j ≤ m, and eϕ = γ − a.
From this it follows that γ(t) = ϕ(t) γ(t) − a for tj−1 ≤ t ≤ tj and 1 ≤ j ≤ m.
Therefore we get
γ(t) − a
ϕ(t) dt = ϕ(tm ) − ϕ(t0 ) .
Because Γ is closed, we have
exp ϕ(tm ) = EΓ − a = AΓ − a = exp ϕ(t0 ) ,
and therefore ϕ(tm ) − ϕ(t0 ) ∈ 2πi Z, according to Proposition III.6.13(i). So
w(Γ, a) is whole number.
6.15 Remarks (a) Suppose the assumptions of Lemma 6.13 are satisfied. With
the notation of its proof, we denote by argj γ(t) for 1 ≤ j ≤ m and t ∈ [tj−1 , tj ]
the unique number η ∈ (αj , αj + 2π) such that Im ϕj (t) = η. Then it follows
elog |γ(t)| = |γ(t)| = |eϕ(t) | = eRe ϕ(t)
ϕj (t) = logj γ(t) = log |γ(t)| + i argj γ(t)
for tj−1 ≤ t ≤ tj and 1 ≤ j ≤ m. Now we set
argc,γ (t) := argj γ(t) + 2π
for tj−1 ≤ t ≤ tj and 1 ≤ j ≤ m. Then (6.11) and (6.13) imply
ϕ = log ◦ |γ| + i argc,γ .
Because ϕ is piecewise continuously differentiable, (6.14) shows that this also holds
for argc,γ , where argc,γ and γ are continuously differentiable on the same subintervals of I. In other words, argc,γ is a piecewise continuously differentiable function
that “selects from” the set-valued function Arg ◦ γ, that is,
argc,γ ∈ Arg γ(t)
Likewise, ϕ is a piecewise continuously differentiable “selection from” the setvalued logarithm Log ◦ γ.
(b) Suppose γ is a piecewise C 1 parametrization of Γ and a ∈ C\Γ. Also suppose
ϕ = log ◦ |γ − a| + i argc,γ−a
374
is a piecewise C 1 selection from Log ◦ (γ − a), where ϕ belongs to C 1 [tj−1 , tj ] for
1 ≤ j ≤ m. Then we have
ϕ(t) dt = log ◦ |γ − a|
+ i argc,γ−a
= ϕ(tj ) − ϕ(tj−1 ) .
Because log |γ(tm ) − a| = log |γ(t0 ) − a|, it thus follows from (6.12) that
w(Γ, a) =
argc,γ−a (tm ) − argc,γ−a (t0 ) .
This shows that 2π times the winding number of Γ about a is the “accumulated change” of the argument argc,γ−a of γ − a as Γ passes from γ(t0 ) to γ(tm ).
Therefore w(Γ, a) specifies how many times Γ winds around the point a, where
w(Γ, a) > 0 means it winds counterclockwise and w(Γ, a) < 0 means it winds
clockwise.
A curve with w(Γ, 0) = 3
The continuity of the winding number
We first prove a simple lemma for maps in discrete spaces.
6.16 Lemma Suppose X and Y are metric spaces and Y is discrete. If f : X → Y
is continuous, then f is constant on every connected component of X.
Proof Suppose Z is a connected component of X. By Theorem III.4.5, f (Z)
is connected. Because Y is discrete, every connected component of Y consists of
exactly one point. Therefore f is constant on Z.
Because Γ is compact, there is an R > 0 such that Γ ⊂ RD. Therefore C\Γ
contains the set RDc , which shows that C\Γ has exactly one unbounded connected
component.
6.17 Corollary The map w(Γ, ·) : C\Γ → Z is constant on every connected component. If a belongs to the unbounded connected component of C\Γ, then w(Γ, a) = 0.
Proof Suppose a ∈ C\Γ, and define d := d(a, Γ). We then choose an ε > 0 and
define δ := min επd2 \L(Γ), d/2 . Then we have
|z − a| ≥ d > 0 and |z − b| ≥ d/2 for z ∈ Γ and b ∈ D(a, δ) .
|w(Γ, a) − w(Γ, b)| ≤
a−b
L(Γ) 2 δ ≤ ε
(z − a)(z − b)
for b ∈ D(a, δ). Because ε was arbitrary, w(Γ, ·) is continuous in a ∈ C\Γ. The
first statement now follows from Theorem 6.14 and Lemma 6.16.
Denote by Z the unbounded connected component of C \ Γ. Also suppose
R > 0 is chosen so that Z contains the set RDc . Finally, we choose a ∈ Z with
|a| > R and |a| > L(Γ)/π + max{ |z| ; z ∈ Γ }. Then we have
|w(Γ, a)| ≤
|z − a|
Because w(Γ, a) ∈ Z, we have w(Γ, a) = 0, and therefore w(Γ, b) = 0 for every
6.18 Corollary Suppose f is meromorphic in U and w(Γ, a) = 0 for a ∈ U c . Then
z ∈ P (f )\Γ ; w(Γ, z) = 0 is a finite set.
Proof By Corollary 6.17, B := z ∈ U \Γ ; w(Γ, z) = 0 is bounded. When B ∩
P (f ) is not finite, this set has a cluster point z0 ∈ B. According to Remark 6.7(b),
P (f ) has a cluster point in U , and therefore z0 belongs to U c . Thus, by assumption,
w(Γ, z0 ) = 0. On the other hand, it follows from the continuity of w(Γ, ·) and from
w(Γ, B) ⊂ Z× that w(Γ, z0 ) is distinct from zero. Therefore B ∩ P (f ) is finite.
376
The concept of winding number allows us to expand the domain of the Cauchy
integral theorem and the Cauchy integral formula. We first present a lemma.
6.19 Lemma
Suppose f : U → C is holomorphic and Γ ⊂ U .
(i) The function
f (w) − f (z)
(z, w) →
(w − z) ,
f (z) ,
is continuous.
g(z, w) dw
(i) Obviously g is continuous at every point (z, w) with z = w.
Suppose z0 ∈ U and ε > 0. Then there is an r > 0 such that D(z0 , r) ⊂ U and
|f (ζ) − f (z0 )| < ε for ζ ∈ D(z0 , r). For z, w ∈ D(z0 , r) and γ(t) := (1 − t)z + tw
with t ∈ [0, 1], it follows from im(γ) ⊂ D(z0 , r) and the mean value theorem that
f γ(t) − f (z0 ) dt .
g(z, w) − g(z0 , z0 ) =
Therefore |g(z, w) − g(z0 , z0 )| < ε, which shows that g is continuous at (z0 , z0 ).
(ii) Because of (i), h is well defined. We will verify that h is analytic with
help from Morera’s theorem (Theorem 5.24). So let Δ be a triangular path in U .
From Exercise 5.11 it follows that5
g(z, w) dw dz =
g(z, w) dz dw .
Also, the proof of Lemma 6.2 shows that g(·, w) belongs to C ω (U ) for every w ∈ U .
Therefore it follows from Theorem 5.25 that Δ g(z, w) dz = 0 for w ∈ Γ, and we
get Δ h(z) dz = 0 from (6.15). Therefore h is analytic.
A curve Γ in U is said to be null homologous in U if it is closed and piecewise
continuously differentiable and if w(Γ, a) = 0 for a ∈ U c .6
5 This is an elementary version of the Fubini’s theorem, which will be proved in full generality
in Volume III. See also Exercise 5.12.
6 When U = C, every closed piecewise C 1 curve is null homologous.
6.20 Theorem (homology version of the Cauchy integral theorem and formula)
Suppose U is open in C and f is holomorphic in U . Then, if the curve Γ is null
homologous in U ,
dζ = w(Γ, z)f (z) for z ∈ U \Γ ,
f (z) dz = 0 .
Proof We use the notations of Lemma 6.19.
(i) Clearly (6.16) is equivalent to the statement
h(z) = 0 for z ∈ U \Γ .
To verify this, suppose U0 :=
h0 (z) :=
z ∈ C\Γ ; w(Γ, z) = 0
for z ∈ U0 .
According to Theorem 6.14 and because w(Γ, ·) is continuous, U0 is open. We also
dζ − f (z)w(Γ, z) = h(z) for z ∈ U ∩ U0 .
h0 (z) =
Because h0 is holomorphic, the uniqueness theorem for analytic functions (Theorem V.3.13) says there is a function H holomorphic on U ∪ U0 such that H ⊃ h0
and H ⊃ h. By assumption, we have w(Γ, a) = 0 for a ∈ U c . Thus U c belongs to
U0 , and we learn that H is an entire function.
(ii) Suppose R > 0 with Γ ⊂ RD. Because RDc lies in the unbounded
connected component of C\ Γ, we have RDc ⊂ U0 . Suppose now ε > 0. We set
M := maxζ∈Γ |f (ζ)| and R := R + L(Γ)M/2πε. For z ∈ R Dc , we then have
|ζ − z| ≥ |z| − |ζ| > L(Γ)M/2πε for ζ ∈ Γ ,
2π Γ ζ − z
Because h0 ⊂ H and because H as an entire function is bounded on bounded sets,
it follows that H is bounded on all of C. Therefore we get from Liouville’s theorem
and (6.19) that H vanishes everywhere. Now h ⊂ H implies (6.18).
(iii) Suppose a ∈ C\Γ and
|h0 (z)| ≤
z → (z − a)f (z) .
Because F is holomorphic and F (a) = 0, (6.16) shows that
F (z)
dz = w(Γ, a)F (a) = 0 .
378
6.21 Remarks (a) If U is simply connected, every closed piecewise C 1 curve is null
homologous in U . So Theorem 6.20 is a generalization of Theorems 5.5 and 5.9.
This follows from Example 6.12(d).
(b) Under the assumptions of Theorem 6.20, we have the generalized Cauchy
derivative formula
w(Γ, z)f (k) (z) =
(ζ − z)k+1
This follows easily from Theorem 6.20.
The residue theorem
The next theorem further generalizes the homology version of the Cauchy integral
theorem to the case of meromorphic functions.
6.22 Theorem (residue theorem) Suppose U is open in C and f is meromorphic
in U . Further suppose Γ is a curve in U\P (f ) that is null homologous in U . Then
f (z) dz = 2πi
Res(f, p)w(Γ, p) ,
p∈P (f )
where only finitely many terms in the sum are distinct from zero.
Proof From Corollary 6.18 we know that A := a ∈ P (f ) ; w(Γ, a) = 0 is a
finite set. Thus the sum in (6.20) has only finitely many terms distinct from zero.
Suppose A = {a0 , . . . , am } and fj is the principal part of the Laurent expansion of f at aj for 0 ≤ j ≤ m. Then fj is holomorphic in C \ {aj }, and the
singularities of F := f − j=0 fj at a0 , . . . , am are removable (because F locally
at aj , where gj is the auxiliary part of f at aj ). Therefore, by the Riemann
removability theorem, F has a holomorphic continuation (also denoted by F ) on
P (f )\A = A ∪ U \P (f ) .
U0 := U
Because Γ lies in U \P (f ) and is null homologous in U , Γ lies in U0 and is null
homologous there. Thus it follows from the generalized Cauchy integral theorem
that Γ F dz = 0, which implies
Because aj is a pole of f , there are nj ∈ N× and cjk ∈ C for 1 ≤ k ≤ nj and
0 ≤ j ≤ m such that
cjk (z − aj )−k
fj (z) =
for 0 ≤ j ≤ m .
It therefore follows from Remark 6.21(b) (with f = 1) that
= 2πi cj1 w(Γ, aj ) .
(z − aj )k
Because cj1 = Res(f, aj ), the claim follows from (6.21) using the definition of A.
Fourier integrals
The residue theorem has many important applications, which we now exemplify by
calculating some improper integrals. We concentrate on a particularly significant
class, namely, Fourier integrals.
Suppose f : R → C is absolutely integrable. For p ∈ R, |e−i px | = 1 for
x ∈ R, and therefore the function x → e−i px f (x) is absolutely integrable. Thus
the Fourier integral of f at p, given by
e−i px f (x) dx ∈ C ,
is defined for every p ∈ R. The function f : R → C defined through (6.22) is called
the Fourier transform of f .
The next theorem and the subsequent examples show that the residue theorem helps calculate many Fourier transforms.
6.23 Proposition Suppose the function f meromorphic on C has the properties
(i) P (f ) is finite,
(ii) P (f ) ∩ R = ∅,
(iii) lim|z|→∞ zf (z) = 0.
⎨ −2πi
f (p) =
z∈P (f )
Im z<0
Im z>0
Res(f e−i p · , z) ,
Res(f e
−i p ·
p≥0,
p≤0.
Proof Suppose p ≤ 0. By assumption (i), there is an r > 0 such that P (f ) ⊂ rD.
We choose Γ as a positively oriented boundary of V := rD ∩ { z ∈ C ; Im z > 0 }.
380
w(Γ, z) =
(see Exercise 13). Therefore it follows
from the residue theorem that
f (x)e−i px dx + i
f (rei t )e−i pre rei t dt = 2πi
Res(f e−i p · , z) .
Because p ≤ 0, the integral over the semicircle can be estimated as
f (rei t )e−i pre rei t dt ≤ π max rf (rei t )epr sin t ≤ π max |zf (z)| .
0≤t≤π
The assumption (iii) now implies the claim for p ≤ 0.
The case p ≥ 0 is similar, but the integration path is in { z ∈ C ; Im z ≤ 0 }.
6.24 Examples
(a) For f (x) := 1/(x2 + a2 ) with a > 0, we have
f (p) = πe−|p| a /a for p ∈ R .
Proof The function f (z) := 1/(z 2 + a2 ) is meromorphic in C. Further f | R is absolutely
integrable (see Example VI.8.4(e)), and lim|z|→∞ zf (z) = 0. For the residues of z →
e−i pz f (z) at the simple poles ±i a, we found in Example 6.11(e) that
Res e−i pz f (z), ±i a = ±e±pa /2i a .
Therefore the claim follows from Proposition 6.23.
Proof We consider the function f meromorphic in C√
defined by f (z) := 1/(z 4 + 1). The
poles of f in { z ∈ C ; Im z > 0 } are z0 := (1 + i ) 2 and z1 := i z0 = (−1 + i ) 2.
From Example 6.11(f), we know
Res(f e−i p· , z0 ) + Res(f e−i p· , z1 ) =
(e−i pz0 + i e−i pz1 ) .
Now it follows from Proposition 6.23 that
(1 + i ) =
= f (0) = 2πi
(−z0 )(1 + i ) = √ .
We can use the residue theorem to evaluate the convergent improper integral
sin(x) dx/x, even though 0 is a removable singularity of x → sin(x)/x and the
integral is not absolutely convergent (see Exercise VI.8.1).
6.25 Proposition For p ∈ R, we have
|p| < 1 ,
|p| = 1 ,
|p| > 1 .
sin x −i px
π/2 ,
Proof (i) Suppose R > 1. We integrate the
entire function
sin z −i pz
over the path γR that goes from −R along
the real axis to −1, along the upper half of
the unit circle to +1, and finally along the
real axis to R. The Cauchy integral theorem
Because sin z = (ei z − e−i z )/2i , it follows that
(e−i z(p−1) − e−i z(p+1) )
(ii) Now we wish to calculate the integral
hR (q) :=
e−i zq
So consider the curve Γ± parametrized by the loop γR + kR , where
kR : [0, π] → C ,
t → Re±i t .
Then w(Γ+ , 0) = 0 and w(Γ− , 0) = −1. Because Res(e−i zq /z, 0) = 1, it follows
hR (q) = −
hR (q) = −1 −
e−i qRe dt
dz = −1 +
e−i qRe dt .
382
(iii) Next we want to show
e−i qRe dt → 0 (R → ∞) for q < 0 .
So suppose ε ∈ (0, π/2). Because q sin t ≤ 0 for t ∈ [0, π] it follows that
|e−i qRe | = eqR sin t ≤ 1
for q < 0 ,
R>1,
t ∈ [0, π] ,
e−i qRe dt ≤ 2ε
for q < 0 and R > 1 .
Because q sin ε < 0, there is an R0 > 1 such that
eqR sin t ≤ eqR sin ε ≤ ε
for R ≥ R0 and ε ≤ t ≤ π − ε .
e−i qRe dt ≤ επ
for R ≥ R0 ,
which, together with (6.24), proves (6.23). Similarly, one verifies for q > 0 that
e−i qRe dt → 0 (R → ∞) .
(iv) One easily checks that hR (0) = −1/2 for R > 1.
(ii) and (iii), we have
q<0,
−1/2 ,
q=0,
lim hR (q) =
⎩ −1 ,
q>0.
Because of (i), we know
dx = π lim hR (p − 1) − hR (p + 1) ,
from which the claim follows.
6.26 Corollary
Proof The convergence of the improper integral −∞ (sin x/x) dx follows from
Exercises VI.4.11(ii) and VI.8.1(ii). We get its value from Proposition 6.25.
6.27 Remarks (a) It does not follow from Proposition 6.25 that improper integral
sin x i px
e dx
converges for p ∈ R . For that to hold we must confirm that limR→∞
and limR→∞ −R · · · dx exist independently. When the limit
f := lim
exists for a (continuous) function f : R → C, it is called the Cauchy principal
value7 of −∞ f . If f is improperly integrable, the Cauchy principal value exists
and agrees with −∞ f . However, the converse of this statement does not hold, as
shown by the example f (t) := t for t ∈ R.
(b) Because the function g : R → R, x → sin(x)/x is not absolutely integrable,
we cannot assign to g a Fourier transform g. Instead, one can (and must!) use
a more general definition of the Fourier transformation — from the framework of
the theory of distributions8 — as we have done here. In the general theory, the
Fourier transform of g is defined, and g is the same piecewise constant function
defined through the Cauchy principal value given in Proposition 6.25.
(c) The Fourier transformation f → f is very important in many areas of math and
physics. However, a more precise study of this map awaits the Lebesgue integration
theory. Accordingly, we will return to the Fourier integral in Volume III.
We refer you to the exercises and to the literature of complex analysis (for
example, [Car66], [Con78], [FB95], [Rem92]) for a great number of further applications of the Cauchy integral theorem and the residue theorem, as well as for a
more in-depth treatment of the theory of holomorphic and meromorphic functions.
1 Suppose f is holomorphic in U and z0 ∈ U with f (z0 ) = 0. In addition, suppose
g : C → C is meromorphic and has a simple pole at w0 := f (z0 ). Show that g ◦ f has a
simple pole at z0 with Res(g ◦ f, z0 ) = Res(g, w0 )/f (z0 ).
2 Suppose the function f is meromorphic in C and, in Ω(r0 , r1 ) for r0 > 0, has the
Laurent expansion ∞
n=−∞ cn z for n ∈ N with c−n = 0. Prove or disprove that f has
an essential singularity. (Hint: Consider z → 1/(z − 1) in Ω(1, 2).)
Suppose a, b ∈ C with 0 < |a| < |b| and
f : C\{a, b} → C ,
Exercise VI.8.9.
an introduction of the theory of distributions and many important applications, see for
example [Sch65].
384
(a) Determine the Laurent expansion of f around 0 in Ω(0, |a|), Ω(|a|, |b|), and Ω(|b|, ∞).
(b) What is the Laurent expansion of f at a? (Hint: geometric series.)
4 dz
1 + 4z 2
ez sin z
(1 − ez )2
5 The holomorphic function f : U \ {z0 } → C has an isolated singularity at z0 ∈ U .
Prove the equivalence of these statements:
(i) z0 is a pole of order n.
(ii) g : U\{z0 } → C, z → (z−z0 )n f (z) has a removable singularity at z0 with g(z0 ) = 0.
(iii) There are ε, M1 , M2 > 0 such that D(z0 , ε) ⊂ U and
M1 |z − z0 |−n ≤ |f (z)| ≤ M2 |z − z0 |−n
for z ∈ D• (z0 , ε) .
6 Show that z0 ∈ U is a pole of a function f holomorphic in U \ {z0 } if and only if
|f (z)| → ∞ as z → z0 .
7 Suppose z0 is an isolated singularity of f , which is holomorphic in U\{z0 }. Show that
(i) z0 is an essential singularity.
(ii) For every w0 ∈ C, there exists a sequence (zn ) in U \ {z0 } such that lim zn = z0
and lim f (zn ) = w0 .
(Hint: “(i)=
⇒(ii)” If the statement is false, there are a w0 ∈ C and r, s > 0 such that
f D(z0 , r) ∩ D(w0 , s) = ∅. Then g : D• (z0 , r) → C, z → 1 f (z) − w0 is holomorphic
and bounded. With help from Theorem 6.6 and Exercise 6, rule out the cases g(z0 ) = 0
and g(z0 ) = 0.)
Determine the singular points of
z → e1/(z−1) /(ez − 1) ,
z → (z + 1) sin 1/(z − 1) .
Are they poles or essential singularities?
9 Verify that i is an essential singularity of z → sin π (z 2 + 1) .
10 Suppose U is simply connected and a ∈ U is an isolated singularity of f , which is a
holomorphic function in U \{a}. Prove that f has an antiderivative in U \{a} if and only
if Res(f, a) = 0.
Prove Remark 6.21(a).
For p ∈ (0, 1), prove that
1 + ex
sin(pπ)
(Hint: Integrate z → epz (1 + ez )−1 counterclockwise along the rectangular path with
corners ±R and ±R + 2πi .)
13 Suppose Γj is a piecewise C 1 curve parametrized by γj ∈ C(I, C) for j = 0, 1. Also
suppose z ∈ C with |γ0 (t) − γ1 (t)| < |γ0 (t) − z| for t ∈ I. Show w(Γ0 , z) = w(Γ1 , z).
(Hint: For γ := (γ1 − z)/(γ0 − z), show
w [γ], 0 = w(Γ1 , z) − w(Γ0 , z)
and |1 − γ(t)| < 1 for t ∈ I.)
14 Let N (f ) := z ∈ U \P (f ) ; f (z) = 0
function f . Prove these:
define the set of zeros of a meromorphic
(i) If f = 0, then N (f ) is a discrete subset of U .
(ii) The function
1/f : U \N (f ) → C ,
z → 1/f (z)
is meromorphic in U and P (1/f ) = N (f ) and N (1/f ) = P (f ).
(Hints: (i) The identity theorem for analytic functions.
(ii) Exercise 6.)
15 Show that the set of functions meromorphic in U is a field with respect to pointwise
addition and multiplication.
1 + x4
(Hint: Proposition 6.23.)
πe−a
for a > 0 .
(Hint: Example 6.24(a).)
Suppose f is meromorphic in U , f = 0, and g := f /f . Prove
(i) g is meromorphic in U and has only simple poles;
(ii) if z0 is a zero of f of order m, then Res(g, z0 ) = m;
(iii) if z0 is a pole of f of order m, then Res(g, z0 ) = −m.
19 Suppose f is meromorphic in U and P is a curve in U
N (f ) ∪ P (f ) that is null
homologous in U . For z ∈ N (f ) ∪ P (f ), denote by ν(z) the multiplicity of z. Then
w(Γ, p)ν(p) .
For 0 < a < b < 1, calculate
w(Γ, n)ν(n) −
n∈N(f )
Determine
2z − (a + b)
z 2 − (a + b)z + ab
dx/(x4 + 4x2 + 3).
References
[Ama95] H. Amann. Gew¨hnliche Differentialgleichungen. W. de Gruyter, Berlin, 1983,
2. Auﬂ. 1995.
[Ape96]
A. Apelblat. Tables of Integrals and Series. Verlag Harri Deutsch, Frankfurt,
1996.
[Art31]
E. Artin. Einf¨hrung in die Theorie der Gammafunktion. Teubner, Leipzig,
1931.
[Art93]
M. Artin. Algebra. Birkh¨user, Basel, 1993.
[BBM86] A.P. Brudnikov, Yu.A. Brychkov, O.M. Marichev. Integrals and Series, I.
Gordon & Breach, New York, 1986.
[BF87]
M. Barner, F. Flohr. Analysis I, II. W. de Gruyter, Berlin, 1987.
[Bla91]
Ch. Blatter. Analysis I–III. Springer Verlag, Berlin, 1991, 1992.
[Br¨95]
J. Br¨dern. Einf¨hrung in die analytische Zahlentheorie. Springer Verlag,
Berlin, 1995.
[Car66]
H. Cartan. Elementare Theorie der analytischen Funktionen einer oder
mehrerer komplexen Ver¨nderlichen. BI Hochschultaschenb¨cher, 112/112a.
Bibliographisches Institut, Mannheim, 1966.
[Con78]
J.B. Conway. Functions of One Complex Variable. Springer Verlag, Berlin,
1978.
[FB95]
E. Freitag, R. Busam. Funktionentheorie. Springer Verlag, Berlin, 1995.
[Gab96]
P. Gabriel. Matrizen, Geometrie, Lineare Algebra. Birkh¨user, Basel, 1996.
[GR81]
I.S. Gradstein, I.M. Ryshik. Tables of Series, Products, and Integrals. Verlag
Harri Deutsch, Frankfurt, 1981.
[Koe83]
M. Koecher. Lineare Algebra und analytische Geometrie. Springer Verlag,
Berlin, 1983.
[K¨n92]
K. K¨nigsberger. Analysis 1, 2. Springer Verlag, Berlin, 1992, 1993.
[Pra78]
K. Prachar. Primzahlverteilung. Springer Verlag, Berlin, 1978.
[Rem92] R. Remmert. Funktionentheorie 1, 2. Springer Verlag, Berlin, 1992.
[Sch65]
L. Schwartz. M´thodes Math´matiques pour les Sciences Physiques. Hermann,
Paris, 1965.
388
References
[Sch69]
W. Schwarz. Einf¨hrung in die Methoden und Ergebnisse der Primzahltheorie.
BI Hochschultaschenb¨cher, 278/278a. Bibliographisches Institut, Mannheim,
1969.
G. Scheja, U. Storch. Lehrbuch der Algebra. Teubner, Stuttgart, 1988.
[Wal92]
W. Walter. Analysis 1, 2. Springer Verlag, Berlin, 1992.
Index
acceleration, 208
action, least, 207
adjoint
Hermitian matrix, 146
operator, 146
algebra, normed, 14
algebraic multiplicity, 134
alternating m-linear map, 302
angle
– between two vectors, 303
rotation –, 294
anti self adjoint, 147
antiderivative, 312
arc length, 282
– of a curve, 286
parametrization by –, 293
area of a curve, 290
asymptotically equivalent, 57
atlas, 253
automorphism
topological –, 119, 120
basis
canonical –, 310
Jordan –, 135
of a module, 322
Bernoulli
– equation, 241
– numbers, 51
– polynomial, 53
Bessel’s inequality, 79
bilinear map, 173
binormal, 303
boundary conditions, natural, 204
bundle
contangential –, 308
normal –, 271
tangential –, 261
canonical basis, 310
canonical isomorphism, 312
Cauchy
– Riemann equations, 162
– Riemann integral, 20
– integral formula and theorem
(homology version), 377
– principal value, 97, 383
center, instantaneous, 301
central field, 313
chain rule, 166, 185, 261, 268
change of parameters, 284
characteristic polynomial, 141
chart
– transition function, 255
–ed territory, 253
local –, 253
tangential of –, 262
trivial, 253
circle
– instantaneous, 301
– osculating, 301
circulation number, 371
closed
– 1-form, 314
– curve, 286
codimension, 242
cofactor matrix, 274
complete
– curve, 295
– orthonormal system, 79
completeness relation, 79
complexification, 133
cone, 28
conjugate
harmonic, 357
conservative force, 208, 336
390
continuous
– complex 1-form, 339
– curve, 285
– differentiable, 150
– linear form, 158
– partially differentiable, 153, 183
– real differentiable, 342
–ly differentiable curve, 285
–ly differentiable map, 267
infinitely –ly differentiable, 180
locally Lipschitz –, 233
m-times – differentiable, 180
piecewise – function, 4
uniformly Lipschitz –, 233
contour, 339
convergence
– of integrals, 91
– quadratic mean, 68
Weierstrass – theorem, 356
convolution, 89
coordinate
– path, 264
– transformation, 256
cylindrical –s, 250
generalized –, 207
local –s, 253, 267
n-dimensional polar –s, 258
n-dimensional spherical –s, 258
spherical –s, 248, 249
velocity –s, 207
cosine series, 75
cotangent
– function, partial fraction decomposition of, 84
– part of a 1-form, 309
cotangential
– bundle, 308
– space, 308
– vector, 308
Coulomb potential, 314
covering operator, 195
critical point, 160, 171
cross product, 302
curvature
– of a plane curve, 298
– of a space curve, 303
– vector, 298
Index
curve
C q , 285
closed, 286
compact, 285
complete, 295
curvature of, 298
imbedded, 242
inverse, 328
length of, 286
parametrized, 244
Peano –, 282
point –, 328
rectifiable, 286
regular, 285
smooth, 285
cycloid, 305
cylinder, elliptical, 258
d’Alembertian, 192
damped
– driven oscillator, 148
oscillator, 143
Darboux
Riemann – integral, 24
definite
negative [semi]–, 188
positive, 146, 161, 188
positive [semi]–, 188
derivative, 150
directional –, 152
logarithmic, 33
m-th, 180
normalized, 83
partial, 153
partial – m-th order, 183
Wirtinger –, 356
determinant
– function, 133, 178
Gram –, 277
Hadamard’s – inequality, 273
Jacobian –, 219
diagonalizable matrix, 135
diffeomorphic, 267
diffeomorphism, 267
C q –, 217
local C q –, 217
differentiable, 149
– map, 150, 266
continuously – curve, 285
continuously partially, 153, 183
continuously real, 342
m-times, 180
piecewise continuous, 82, 83
totally, 154
differential, 38, 159, 269, 310
– form, 308
– operator, 192
complex –, 340
equation, 206, 227
– of order m, 237
Bernoulli, 241
logistic, 241
similarity –, 240
direction, first variation in, 206
directional derivative, 152
Dirichlet kernel, 88
doubly periodic, 43
dual
exponent, 35
norm, 158
operator, 316
pairing, 308
space, 158
eigenvalue, 133
– equation, 133
semisimple, 134
simple, 134
eigenvector, 134
ellipsoid, 276
elliptic
– cylinder, 258
– helix, 306
embedding, 247
energy
– conservation, 238
kinetic –, 207
potential –, 207
total –, 211
entire function, 348
Bernoulli –, 241
Cauchy–Riemann –s, 162
differential –, 206, 227
differential – of order m, 237
eigenvalue –, 133
Euler–Lagrange, 204
integral –, 129
logistic –, 241
of motion, Newton’s, 208
similarity differential –, 240
equipotential surfaces, 313
essential singularity, 365
Euclidean sphere, 244
Euler
– Lagrange equation, 204
– Maclaurin sum formula, 55
– Mascheroni constant, 59
beta function, 111
first integral, 110
formula for ζ(2k), 84
gamma integral, 98
homogeneity relation, 172
multiplier, 324
second integral, 98
exact 1-form, 312
exponential map, 125
extremal, 203
point under constraints, 272
field
central –, 313
gradient –, 313
vector –, 308
1-, 308
closed 1-, 314
continuous complex 1- –, 339
cotangent part of a 1- –, 309
differential –, 308
exact 1- –, 312
linear –, 28
Pfaff, 308, 357
Euler – for ζ(2k), 84
Euler–Maclaurin sum –, 55
Frenet derivative, 297
Legendre duplication –, 113
of de Moivre and Stirling, 58
quadrature –, 65
rank –, 159
reﬂection – for the Γ function, 105
signature –, 133
Stirling’s, 108
392
Taylor’s, 185
variation of constants, 132
Fourier
– coeﬃcient, 78
– integral, 379
– polynomial, 75
– series, 78
– transformation, 383
n-frame, moving, 295
free
– R-module, 322
– subset, 322
Frenet
– derivative formula, 297
– n-frame, 295
frequency, 143
Fresnel integral, 349
function
– theory, 342
admissible –, 90
determinant –, 133, 178
differentiable –, 150
elementary –, 43
entire –, 348
Euler beta –, 111
gamma –, 98, 100
generating – for the Bernoulli numbers, 51
harmonic –, 191, 351
holomorphic –, 342
improperly integrable –, 90
jump continuous –, 4
meromorphic –, 365
piecewise continuous –, 4
Riemann ζ –, 60
product representation, 61
smooth –, 180
staircase –, 4, 17, 18
functional
– equation of gamma function, 99
linear, 28
fundamental
group, 338
matrix, 137, 263
system, 136, 142
gamma function, 98, 100
functional equation of, 99
gamma integral, Euler’s, 98
Gaussian error integral, 105
generating system, 322
geometric
– multiplicity, 133
– series, 212
gradient, 160, 161, 269
– field, 313
Gram determinant, 277
Gronwall’s lemma, 129
fundamental –, 338
homotopy –, 338
orthogonal, 244
special orthogonal, 257
topological automorphism –, 120
Hadamard’s inequality, 273
Hamilton’s principle of least action, 207
harmonic
– conjugate, 357
– function, 191, 351
– oscillator, 143
heat operator, 192
helix, 289
elliptic, 306
Hessian matrix, 188
Hilbert–Schmidt norm, 123
H¨lder inequality, 35
holomorphic
– Pfaff form, 357
– function, 342
homeomorphism, 217
local, 217
homogeneity relation, Euler, 172
homogeneous, positively, 172
homologous, null –, 376
homomorphism of module, 322
homotopic, 332
null –, 332
– group, 338
loop –, 332
hyperboloid, 276
hyperplane, equatorial –, 254
hypersurfaces, 242
image of a curve, 286
imbedded curves and surfaces, 242
immersion, 244
– theorem, 245
C q –, 246
improper
– integral, 91
–ly integrable function, 90
indefinite
form, 188
integral, 32
independent, linearly, 322
index, 371
inequality
Bessel’s, 79
Hadamard’s, 273
H¨lder, 35
isoperimetric, 306
Minkowski, 35
Wirtinger’s, 89
initial value problem, 227, 238
instantaneous velocity, 286
integrability conditions, 312
integrable
absolutely – function, 94
improperly, 90
Riemann, 22
integral, 26
– equation, 129
Cauchy–Riemann –, 20
complex line –, 339
elliptic –, 294
first –, 229
first Euler –, 110
Fourier –, 379
Fresnel –, 349
Gaussian error –, 105
improper –, 91
indefinite –, 32
line – of α along Γ, 327
Riemann –, 22
Riemann–Darboux –, 24
second Euler –, 98
integrating factor, 324
integration, partial, 40
isolated singularity, 365
isomorphic, topologically, 119
isomorphism
canonical, 159, 312
of modules, 322
topological, 119
isoperimetric inequality, 306
Jacobi
– identity, 193
– matrix, 155
Jacobian determinant, 219
Jordan basis, normal form, 135
jump continuous function, 4
kernel
Dirichlet –, 88
Poisson –, 358
kinetic energy, 207
L2 scalar product and norm, 68
Lagrangian, 207
Laplace operator, 351
Laplacian, 191
Laurent
– expansion, 362
– series, 361
Legendre
– duplication formula, 113
– polynomials, 48
Leibniz rule, 194
lemma
Gronwall’s, 129
Poincar´ –, 315
Riemann’s, 81
lemniscate, 291
length, 282
arc –, 282, 286
of a piecewise straight path, 281
level set, 168, 243
Lie bracket, 193
lima¸on, 245, 289, 305
Lindel¨f, Picard – theorem, 235
line
– integral, 327
vector – element, 335
linear
– functional, 28
– ordering, 28
linear form, 28
394
continuous, 158
monotone, 28
positive, 28
linearly independent, 322
Lipschitz continuous, 233
logarithmic
– derivative, 33
– spiral, 288, 305
logistic equation, 241
loop, 332
– homotopy, 332
point –, 332
m-linear map, 173
– alternating, 302
Maclaurin, Euler – sum formula, 55
manifold, sub–, 242
bilinear –, 173
differentiable –, 150, 266
exponential –, 125
local topological –, 217
m-linear –, 173
m-linear alternating –, 302
multilinear –, 173
open –, 218
regular –, 226
topological –, 217
trilinear –, 173
Markov matrix, 147
Mascheroni, Euler – constant, 59
matrix
cofactor –, 274
diagonalizable –, 135
fundamental –, 137, 263
Hermitian conjugate (adjoint), 146
Hessian –, 188
Jacobi –, 155
Markov –, 147
representation –, 124
similar –, 145
trace of –, 145
transformation –, 294
mean value
property, 346
theorem, 169
for integrals, 170
for integrals, second, 36
theorem
for integrals, 33
meromorphic function, 365
methods of the calculus of variations,
Minkowski inequality, 35
module
– homomorphism, 322
– isomorphism, 322
free R–, 322
R–, 321
sub–, 322
Moivre, formula of de – and Stirling, 58
monotone linear form, 28
multilinear map, 173
multiplication operator, 198
multiplicity
algebraic, 134
geometric, 133
multiplier, Euler, 324
Neil parabola, 246, 305
Nemytskii operator, 195
Newton’s equation of motion, 208
Newtonian potential, 314
nilpotent, 126
–ed algebra, 14
dual –, 158
Hilbert –, 122
Hilbert–Schmidt –, 123
operator –, 12
stronger –, 35
weaker –, 35
normal
– bundle, 271
– plane, 303
– space, 271
– unit vector, 298, 303
bi–, 303
unit –, 272
normal form, Jordan, 135
normalized
– derivative, 83
– function, 67
– polynomial, 45
normals, 271
null homologous, 376
null homotopic, 332
damped, 143
damped driven, 148
undamped, 143
oscillator, harmonic, 143
osculating, 303
ONB, 79
ONS, 71
complete, 79
open maps, 218
– norm, 12
adjoint, 146
bounded linear, 12
covering –, 195
d’Alembertian –, 192
differential –, 192
dual, 316
heat –, 192
Laplace –, 191, 351
multiplication –, 198
Nemytskii –, 195
self adjoint, 146
symmetric, 146
transposed, 316
wave –, 192
order
– of a pole, 365
–ed Banach space, 28
–ed vector space, 28
ordering
induced, 28
natural, 29
orientation-preserving
– change of parameters, 284
– reparametrization, 284
orientations, 294
oriented
– vector space, 294
positively – circle, 344
orthogonal, 71, 191
– group, 244
orthogonal projection, 79
orthogonal system, 71
orthonormal basis, 79
orthonormal system, 71
oscillator
parabola, Neil, 246, 305
parabola, semicubical, 305
parameter
– domain, 244
– interval, 285
– range, 253
orientation-preserving change of –,
parametrization, 253, 285
by arc length, 293
piecewise C q –, 290
regular, 244, 285
Parseval’s theorem, 79, 89
partial derivative, 153
partial fraction decomposition, 84
particular solution, 131
partition, 4
mesh of, 20
refinement of, 4
Pascal lima¸on, 245, 289, 305
path
coordinate –, 264
integral along a –, 326
inverse –, 328
piecewise C q –, 290, 329
piecewise straight –, 281
rectifiable –, 282
sums of –s, 329
triangular –, 353
Peano curves, 282
periodic, doubly, 43
Pfaff
holomorphic – form, 357
phase
– plane, 140
– portrait, 239
Picard–Lindel¨f, – theorem, 235
piecewise
– C q curve, 290, 329
– C q parametrization, 290
– C q path, 290, 329
396
plane
equatorial hyper–, 254
normal –, 303
osculating –, 303
phase –, 140
Poincar´ lemma, 315
point
– curve, 328
– loop, 332
critical, 160, 171
extremal – under constraints, 272
saddle –, 172, 190
Poisson kernel, 358
polar coordinates, n-dimensional, 258
pole, 365
order of –, 365
set of –s, 365
simple –, 368
polynomial
Bernoulli –, 53
characteristic –, 133, 141
Fourier –, 75
Legendre –s, 48
normalized –, 45
splitting –, 134
positive
– cone, 28
– definite, 146, 161
– linear form, 28
– oriented circle, 344
–ly homogeneous, 172
–ly oriented, 294
potential, 313
Coulomb, 314
Newtonian, 314
potential energy, 207
prime number theorem, 63
principal
– axis transformation, 275
– part of a Laurent series, 361
Cauchy – value, 97, 383
principle of least action, 207
– representation of ζ function, 61
– representation of sine, 85
– rule, 169
generalized, 179
cross –, 302
infinite –, 42
vector –, 302
Wallis –, 42
Weierstrass – form of 1/Γ, 104
projection, 79
orthogonal, 79
stereographic, 254
pull back, 317
quadratic mean, 68
quadrature formula, 65
R-module, 321
free, 322
radius, instantaneous, 301
rank, 226
– formula, 159
rectifiable
– path, 282
recursion, 41
reﬂection formula, 105
regular
– map, 226
– parametrization, 244, 285
– point, 226
– value, 226
reparametrization
C q –, 290
orientation-preserving –, 284
– local coordinates, 267
– matrix, 124
Riesz – theorem, 159
spectral –, 188
residue, 365
Riemann
– ζ function, 60
– Darboux integral, 24
– hypothesis, 63
– integrable, 22
– lemma, 81
– removability theorem, 364
– sum, 23
Cauchy – equations, 162
Cauchy integral, 20
integral, 22
Riesz representation theorem, 159
rotation matrix, 294
rule
chain –, 166, 185, 261, 268
derivative – for Fourier series, 83
generalized product –, 179
Leibniz, 194
product –, 169
Simpson’s, 66
substitution –, 38
saddle point, 172, 190
Schmidt, Hilbert – norm, 123
Schwarz, – theorem, 184
self adjoint operator, 146
semisimple eigenvalue, 134
separation of variables, 230
classical Fourier –, 75
cosine –, 75
geometric, 212
Laurent –, 361
sine –, 75
– of poles, 365
– of zeros, 385
level –, 243
star shaped –, 314
signature formula, 133
similar matrices, 145
similarity differential equation, 240
simple
– pole, 368
– zero, 369
eigenvalue –, 134
simply connected, 332
Simpson’s rule, 66
sine
– series, 75
product representation of, 85
singularity, 365
skew symmetric, 147
– function, 180
solution
– of differential equation, 140
global, 227
maximal, 227
noncontinuable, 227
particular, 131
space
cotangential –, 308
ordered Banach –, 28
tangential –, 260, 261
span, 322
spectral representation, 188
spectrum, 133
sphere, Euclidean, 244
spherical coordinates
n-dimensional –, 258
spiral, logarithmic, 288, 305
staircase function, 4, 17, 18
star shaped, 314
stationary value, 206
stereographic projections, 254
Stirling
– formula, 108
formula of de Moivre and –, 58
submanifold, 242
submersion, 226
submodule, 322
subset, free, 322
substitution rule, 38
– of paths, 329
Euler–Maclaurin – formula, 55
Riemann, 23
surface
equipotential –, 313
hyper–, 242
imbedded –, 242
parametrized –, 244
torus –, 251
symmetric, 176
– operator, 146
tangent, 292
– part, 308
– unit vector, 292
part, 260
tangential, 260, 268
398
– bundle, 261
– of a chart, 262
– space, 260, 261
– vector, 260, 261
Taylor
– formula, 185
– theorem, 187
– implicit function, 223
– inverse function, 215
– of the partial fraction expansion,
– regular value, 243, 266
bounded linear operators, continuous extension of, 15
energy conservation –, 238
extension –, 10
homology version of the Cauchy integral –, 377
immersion –, 245
mean value –, 169
mean value – for integrals, 33
mean value – of integrals, second,
Parseval’s, 79, 89
Picard–Lindel¨f –, 235
prime number –, 63
Riemann’s removability –, 364
Riesz representation –, 159
Schwarz –, 184
second fundamental – of calculus,
Taylor’s –, 187
Weierstrass convergence –, 356
topological
– automorphisms, 119
– isomorphic, 119
– isomorphism, 119
– map, 217
local – map, 217
torsion of a curve, 303
torus, 251
totally differentiable, 154
trace, 145
transformation
coordinate –, 256
Fourier –, 383
matrix, 294
principal axis –, 275
transpose of an operator, 316
trilinear map, 173
trivial chart, 253
undamped oscillator, 143
unit binormal vector, 303
unit normal, 272
variation, 281, 286
– of constants formula, 132
bounded, 281
first, 207
total, 286
fixed boundary conditions, 203
free boundary conditions, 202
vector
– field, 308
– product, 302
angle between two –s, 303
curvature –, 298
eigen–, 134
line element –, 335
ordered – space, 28
tangent part of – field, 308
unit binormal –, 303
unit normal –, 298, 303
unit tangent –, 292
velocity
– coordinates, 207
instantaneous –, 286
Wallis product, 42
wave operator, 192
Weierstrass
– convergence theorem, 356
– product form of 1/Γ, 104
winding number, 371
Wirtinger derivative, 356
Wirtinger’s inequality, 89
zero
set of –s, 385
simple –, 369
δ jk , 310
δk , 310
δjk , 310
β ≤ α, 194
, 194
σ(·), 133
p , 35
[Re z > 0], 98
D• (z0 , r), 363
∂D(a, r), 344
w(Γ, a), 371
Res, 365
S n , 244
Ta,r , 251
ϕ , 195
δF , 207
1n , 244
A∗ , 146
A , 316
B , 274
Hf , 188
[A]E , 124
[a1 , . . . , am ], 178
det, 133, 178
diag, 127, 189
Km×n , 123
Rm×m , 188
GL(n), 278
O(n), 244
SO(n), 257
Laut, 119
Lis, 119
inv, 212
tr, 145
eA , 125
BC k , 192
B + (X), 29
C[α, β], 67
C 1 , 150
C m , 180, 267
C ∞ , 180
C 1- , 233
C 0,1- , 233
C 0,p , 197
C0 , 69
C0 , 203
Diff q , 217, 267
Diff q (J1 , J2 ), 284
Diff q , 217
Harm, 351
L(E), 14
L(E, F ), 12, 15
L(E1 , . . . , Em ; F ), 174
Lm (E, F ), 174
Lm , 176
S(I, E), 5
SC(I, E), 5
S + (I), 29
SC, 67
SC2π , 74
T (I, E), 5
, 20
, 326
, 327
F |β , 32
P V a , 97, 383
Sf , 74
Sn f , 75
fk , 74
f , 379
D, 150
Dm , 180
D1 , 222
Dv , 152
∂, 150
∂ m , 180
∂k , ∂xk , 153, 197
f , 150, 180
f (m) , 180
fx , 162
∂(f 1 ,...,f m )
, 219
∂(x1 ,...,xm )
∂(f 1 ,...,f n )
∂(xm+1 ,...,xm+n )
grad, 160, 312
\nabla , 160, 312
\nabla g , 161
\nabla p f , 269
400
dϕ, 38
df , 159, 310
dxj , 310
dp f , 269
ds, 335
2, 192
Δ, 191
Tp f , 260
Tp M , 261
T M , 261
Tp M , 271
Tp X, 308
V (X), 308
(v)p , 260
g, 277
fϕ,ψ , 266
νp , 272
Ω(q) (X), 309
Θ, 312
ϕ∗ , 317
Var(f, I), 281
Var(Γ), 286
L(γ), 282
L(Γ), 286
im(Γ), 286
t, 292
n, 298
ej , 310
κ, 297, 298, 303
τ , 303
|| ·| , 123
· L(E,F ) , 12
· k,∞ , 192
·, · , 308
·, · p , 309
(· | ·)2 , 68
⊥, 71
(a, b), 303
×, 302
Or, 294
