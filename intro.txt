Convex Optimization — Boyd & Vandenberghe
1. Introduction
• mathematical optimization
• least-squares and linear programming
• convex optimization
• example
• course goals and topics
• nonlinear optimization
• brief history of convex optimization
1–1
Mathematical optimization
(mathematical) optimization problem
minimize f0(x)
subject to fi(x) ≤ bi,
i = 1, . . . , m
• x = (x1, . . . , xn): optimization variables
• f0 : Rn → R: objective function
• fi : Rn → R, i = 1, . . . , m: constraint functions
optimal solution x⋆ has smallest value of f0 among all vectors that
satisfy the constraints
Introduction
1–2
Examples
portfolio optimization
• variables: amounts invested in diﬀerent assets
• constraints: budget, max./min. investment per asset, minimum return
• objective: overall risk or return variance
device sizing in electronic circuits
• variables: device widths and lengths
• constraints: manufacturing limits, timing requirements, maximum area
• objective: power consumption
data ﬁtting
• variables: model parameters
• constraints: prior information, parameter limits
• objective: measure of misﬁt or prediction error
Introduction
1–3
Solving optimization problems
general optimization problem
• very diﬃcult to solve
• methods involve some compromise, e.g., very long computation time, or
not always ﬁnding the solution
exceptions: certain problem classes can be solved eﬃciently and reliably
• least-squares problems
• linear programming problems
• convex optimization problems
Introduction
1–4
Least-squares
minimize
Ax − b
solving least-squares problems
• analytical solution: x⋆ = (AT A)−1AT b
• reliable and eﬃcient algorithms and software
• computation time proportional to n2k (A ∈ Rk×n); less if structured
• a mature technology
using least-squares
• least-squares problems are easy to recognize
• a few standard techniques increase ﬂexibility (e.g., including weights,
adding regularization terms)
Introduction
1–5
Linear programming
minimize cT x
subject to ai x ≤ bi,
i = 1, . . . , m
solving linear programs
• no analytical formula for solution
• reliable and eﬃcient algorithms and software
• computation time proportional to n2m if m ≥ n; less with structure
• a mature technology
using linear programming
• not as easy to recognize as least-squares problems
• a few standard tricks used to convert problems into linear programs
(e.g., problems involving ℓ1- or ℓ∞-norms, piecewise-linear functions)
Introduction
1–6
Convex optimization problem
minimize f0(x)
subject to fi(x) ≤ bi,
i = 1, . . . , m
• objective and constraint functions are convex:
fi(αx + βy) ≤ αfi(x) + βfi(y)
if α + β = 1, α ≥ 0, β ≥ 0
• includes least-squares problems and linear programs as special cases
Introduction
1–7
solving convex optimization problems
• no analytical solution
• reliable and eﬃcient algorithms
• computation time (roughly) proportional to max{n3, n2m, F }, where F
is cost of evaluating fi’s and their ﬁrst and second derivatives
• almost a technology
using convex optimization
• often diﬃcult to recognize
• many tricks for transforming problems into convex form
• surprisingly many problems can be solved via convex optimization
Introduction
1–8
Example
m lamps illuminating n (small, ﬂat) patches
lamp power pj
illumination Ik
intensity Ik at patch k depends linearly on lamp powers pj :
akj pj ,
Ik =
−2
akj = rkj max{cos θkj , 0}
problem: achieve desired illumination Ides with bounded lamp powers
minimize maxk=1,...,n | log Ik − log Ides|
subject to 0 ≤ pj ≤ pmax, j = 1, . . . , m
Introduction
1–9
how to solve?
1. use uniform power: pj = p, vary p
2. use least-squares:
minimize
k=1 (Ik
− Ides)2
round pj if pj > pmax or pj < 0
3. use weighted least-squares:
minimize
k=1 (Ik
− Ides)2 +
j=1 wj (pj
− pmax/2)2
iteratively adjust weights wj until 0 ≤ pj ≤ pmax
4. use linear programming:
minimize maxk=1,...,n |Ik − Ides|
subject to 0 ≤ pj ≤ pmax, j = 1, . . . , m
which can be solved via linear programming
of course these are approximate (suboptimal) ‘solutions’
Introduction
1–10
5. use convex optimization: problem is equivalent to
minimize f0(p) = maxk=1,...,n h(Ik /Ides)
subject to 0 ≤ pj ≤ pmax, j = 1, . . . , m
with h(u) = max{u, 1/u}
h(u)
f0 is convex because maximum of convex functions is convex
exact solution obtained with eﬀort ≈ modest factor × least-squares eﬀort
Introduction
1–11
additional constraints: does adding 1 or 2 below complicate the problem?
1. no more than half of total power is in any 10 lamps
2. no more than half of the lamps are on (pj > 0)
• answer: with (1), still easy to solve; with (2), extremely diﬃcult
• moral: (untrained) intuition doesn’t always work; without the proper
background very easy problems can appear quite similar to very diﬃcult
problems
Introduction
1–12
Course goals and topics
goals
1. recognize/formulate problems (such as the illumination problem) as
convex optimization problems
2. develop code for problems of moderate size (1000 lamps, 5000 patches)
3. characterize optimal solution (optimal power distribution), give limits of
performance, etc.
topics
1. convex sets, functions, optimization problems
2. examples and applications
3. algorithms
Introduction
1–13
Nonlinear optimization
traditional techniques for general nonconvex problems involve compromises
local optimization methods (nonlinear programming)
• ﬁnd a point that minimizes f0 among feasible points near it
• fast, can handle large problems
• require initial guess
• provide no information about distance to (global) optimum
global optimization methods
• ﬁnd the (global) solution
• worst-case complexity grows exponentially with problem size
these algorithms are often based on solving convex subproblems
Introduction
1–14
Brief history of convex optimization
theory (convex analysis): ca1900–1970
algorithms
1947: simplex algorithm for linear programming (Dantzig)
1960s: early interior-point methods (Fiacco & McCormick, Dikin, . . . )
1970s: ellipsoid method and other subgradient methods
1980s: polynomial-time interior-point methods for linear programming
(Karmarkar 1984)
• late 1980s–now: polynomial-time interior-point methods for nonlinear
convex optimization (Nesterov & Nemirovski 1994)
applications
• before 1990: mostly in operations research; few in engineering
• since 1990: many new applications in engineering (control, signal
processing, communications, circuit design, . . . ); new problem classes
(semideﬁnite and second-order cone programming, robust optimization)
Introduction
1–15
