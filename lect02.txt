Lehrstuhl fur
Datenverarbeitung
Technische Universitat Munchen
Data Analysis for Computer Engineering
Bandits and Markov Decision Processes
Johannes Gunther
Institute for Data Processing
Slide 1/25
Part I
N-armed Bandits
Slide 2/25
Bandits? Never heard, don’t care!?!
Paper from the 2013 NIPS about Bandits:
N. Cesa-Bianchi, C. Gentile, and G. Zappella, “A gang of bandits.”
E. Hillel, Z. S. Karnin, T. Koren, R. Lempel, and O. Somekh, “Distributed exploration in multi-armed bandits.”
J. Djolonga, A. Krause, and V. Cevher, “High-dimensional gaussian process bandits.”
N. Korda, E. Kaufmann, and R. Munos, “Thompson sampling for 1-dimensional exponential family bandits.”
N. Alon, N. Cesa-Bianchi, C. Gentile, and Y. Mansour, “From bandits to experts.”
T. Bonald and A. Proutiere, “Two-target algorithms for inﬁnite-armed bandits with bernoulli rewards.”
M. Gheshlaghi azar, A. Lazaric, and E. Brunskill, “Sequential transfer in multi-armed bandit with ﬁnite set of
models.”
M. Xu, T. Qin, and T.-Y. Liu, “Estimation bias in multi-armed bandit algorithms for search advertising.”
S. Zhang and A. J. Yu, “Forgetful bayes and myopic planning: Human learning and decision-making in a
bandit setting.”
V. Gabillon, B. Kveton, Z. Wen, B. Eriksson, and S. Muthukrishnan, “Adaptive submodular maximization in
Slide 3/25
Multi Armed Bandit, http://www.research.microsoft.com/
Slide 4/25
N-Armed Bandits
Choose repeatedly from a set of N actions, each choice is
denoted a play
Each play At ends in a reward Rt , where 
E{Rt } =: q^* (At )
[ $q^*$ True value of an action ]
These action values are not known
Non-associative and evaluative feedback problem
Objective: maximize reward in the long term
Way: explore the actions and exploit the best
Slide 5/25
Bandit Game
Slide 6/25
Action-Value Methods
Method for estimating the values of actions
Estimation: Qt (a) ≈ q∗ (a)
Playing k times
Simple average: Qt (a) =
lim Qt (a) = q∗ (a)
Slide 7/25
Incremental Implementation
Disadvantage of simple averaging: all data needs to be
stored
Suggestion: calculating value incremental
Qk +1 (a) = Qk (a) +
(k +1)
− Qk (a)]
A general update rule can be formulated as:
NewEstimate = OldEstimate + StepSize[Target - OldEstimate]
[ StepSize \alpha ]
Slide 8/25
Exploration/Exploitation Dilemma
Best action: At∗ = argmax Qt (a)
Exploration: At
Exploitation: At = At∗
You can’t exploit all the time, you can’t explore all the time
But maybe you want to reduce exploring over the time
Slide 9/25
-Greedy Action Selection
Greedy action selection:
At = At∗ = argmax Qt (a)

[ _to('140603081235') ]
***
###$\epsilon$- Greedy:
[ Greedy: $\epsilon$ = 0 ]
![()](140603081507.png)
simplest way to balance exploration and exploitation

Slide 10/25
So -greedy it is?
Is it reasonable to try each arm with equal probability?
Suppose you see the following samples:
Slide 11/25
Softmax Action Selection
Converts values into action probabilities
A way to order the actions you want to explore
The most common softmax uses Gibbs or Boltzmann
distribution:
π(a) =
e Qt (a)/τ
Qt (b)/τ
b=1 e
where τ is the “computational temperature”
If τ is high, all actions are (nearly) equiprobable
If τ is low, the policy is shifted to greedy action-selection
Slide 12/25
10-Armed Testbed
n = 10 possible actions
each q∗ (a) is chosen randomly from a normal distr.: N(0, 1)
each Rt is also normal: N(q∗ (At ), 1)
one run is 1000 plays
average over 2000 runs
Slide 13/25
-Greedy Methods on the 10-Armed Testbed
Slide 14/25
Which will result in the best
long term performance?
= 0(greedy)
D: will all perform equally
E: not possible to tell
Optimistic Initial Values
So far, all methods depend on Q0 (a), i.e. they are biased
Suppose we initialize the values optimistically,
e.g. Q0 (a) = 5 for all a and \alpha = 0.1
[ step size parameter ]
Slide 15/25
What you have learned so far
-greedy Policy
The difference between a sample, an estimate and a true
expected values
the difference between the greedy action and the optimal
action
a learning rule (averaging in an incremental way)
a complete example of a goal-seeking problem, including
solutions
your ﬁrst AI problem and solution
Slide 16/25
Part II
Markov Decision Processes
Slide 17/25
The Agent-Environment Interface
Agent and environment interact at discrete time steps:
Agent observes state at step t: St ∈ S
produces action at step t: At ∈ A(St )
gets resulting reward: Rt+1 ∈ R
and resulting in next state: St+1 ∈ S+
Slide 18/25
Learning a Policy
('140513162146')
Policy π_t at step t =
a mapping from states to action probabilities
$\pi(a|s) = $ probability that $A_t = a$ when $S_t = s$
Reinforcement learning methods specify, how the agent
changes its policy as a result of experience
Rougly, the agent’s goal is to get as much reward, as it can
over the long run
Slide 19/25
If a reinforcement learning task has the Markov Property, it
is basically a Markov Decision Process (MDP)
If state and action sets are ﬁnite, it is a ﬁnite MDP
To deﬁne a ﬁnite MDP, you need to give:
state and action sets
one-step “dynamics”
p(s , r|s, a) = Pr{St+1 = s , Rt+1 = r|St = s, At = a}
Slide 20/25
The Markov Property I
The state at time t denotes whatever information is
available to the agent at step t about its environment
This can include “raw” immediate information, processed
information or information, built up over a time
Ideally, a state should summarize all past events and
inherent all essential information, i.e. it should have the
Markov Property
Slide 21/25
The Markov Property II
p(s , r|s, a) = Pr{Rt+1 = r, St+1 = s |St , At }
for all s ∈ S+ , r ∈ R, and all histories
Slide 22/25
An Example Finite MDP
Recycling Robot
At each step, robot has to decide whether it should (1)
actively search for a can, (2) wait for someone to bring it a
can, or (3) go to home base and recharge.
Searching is better but runs down the battery; if it runs out
of power while searching, it has to be rescued (which is
bad)
Decisions are made on basis of current energy level: high,
Reward = number of cans collected
Slide 23/25
Recycling Robot MDP
S = {high, low}
rsearch = expected no. of cans
while searching
A(high) = {search, wait}
rwait = expected no. of cans
while waiting
A(low) = {search, wait, recharge}
Slide 24/25
rsearch > rwait
What you have learned
(stochastic) Policy: (stochastic) rule for selecting actions
Markov Properties
Markov Decision Process
Transition probabilities
Expected rewards
Slide 25/25
