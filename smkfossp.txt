Fundamentals of
Statistical Signal Processing:
Estimation Theory
Steven M. Kay
University of Rhode Island
For book and bookstoreinformarion
httpJ/wmnprenhrllcom
gopher to gophenprenhallcom
Prentice all PTR
Upper Saddle River, NJ 07458
Contents
Preface
1 Introduction
1.1 Estimation in Signal Processing . . . . . . . . . . . . . . . . . . . . . . .
1.2 The Mathematical Estimation Problem
1.3 Assessing Estimator Performance . . . . . . . . . . . . . . . . . . . . . .
1.4 Some Notes to the Reader . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Minimum Variance Unbiased Estimation
2.1 Ir1troduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Unbiased Estimators
2.4 Minimum Variance Criterion
2.5 Existence of the Minimum Variance Unbiased Estimator . . . . . . . . .
2.6 Finding the Minimum Variance Unbiased Estimator
2.7 Extension to a Vector Parameter
3 Cramer-Rao Lower Bound
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Estimator Accuracy Considerations . . . . . . . . . . . . . . . . . . . . .
3.4 Cramer-Rao Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 General CRLB for Signals in White Gaussian Noise . . . . . . . . . . . .
3.6 Transformation of Parameters . . . . . . . . . . . . . . . . . . . . . . . .
3.7 Extension to a Vector Parameter
3.8 Vector Parameter CRLB for Transformations
3.9 CRLB for the General Gaussian Case
3.10 Asymptotic CRLB for WSS Gaussian Random Processes . . . . . . . . .
3.11 Signal Processing Examples . . . . . . . . . . . . . . . . . . . . . . . . .
3A Derivation of Scalar Parameter CRLB
3B Derivation of Vector Parameter CRLB . . . . . . . . . . . . . . . . . . .
3C Derivation of General Gaussian CRLB . . . . . . . . . . . . . . . . . . .
3D Derivation of Asymptotic CRIB
4 Linear Models s3 3.3 The Least Squares Approach -----~---"-"""""f1__ 223
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 8.4 Linear Least Squares - _- - ~ " ' ' ' ' ' ' ' ' ' ' ' i i i i i i i i i _ _ _ 226
4.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 8.5 Geometrical Interpretations . . . . . . . . . . . . . . . . . . . . . . . I I 232
4.3 Deﬁnition and Properties . . . . . . . . . . . . . . . . . . . . . . . . . . 83 8.6 Order-Recursive Least Squares ~ ~ ' ' ' ' ' ' ' ' ' ' ' ' i i i i i i _ _ _ 242
4.4 Linear Model Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 8.7 Sequential Least Squares- - - " ' ' ' ' ' ‘ ' ' ' i i i i i i i i i i  _ _ _ 251
4.5 Extension to the Linear Model . . . . . . . . . . . . . . . . . . . . . . . 94 8.8 Constrained Least Squares ~ - - - ' ' ' ' ' ' ' ' ' ' ' i ' i i i i i _ _ 2 254
g_9 Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . . . 2 260
5 General Minimum Variance Unbiased Estimation 101 310 Signal Processing Examples . . . . . . . . . . . . . . . . . . . . . . . . 2 282
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 3A Derivation of Order-Recursive Least Squares . . . . . . . . . . . . . . . 285
5.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 3B Derivation of Recursive PIOJGCIZIOII Matrix . . . . . . . . . . . . . . . . . 286
5.3 Sufficient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 3C Derivation of Sequential Least Squares . . . . . . . . . . . . - - - - - - -
5.4 Finding Sufficient Statistics . . . . . . . . . . . . . . . . . . . . . . . . . 104 239
5.5 Using Sufficiency to Find the MVU Estimator . . . . . . . . . . . . . . . 107 9 Method 0f Moments U I I _ _ _ 289
5.6 Extension to a Vector Parameter . . . . . . . . . . . . . . . . . . . . . . 116 9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . ‘ ~ ~ . ' 289
5A Proof of Neyman-Fisher Factorization Theorem (Scalar Parameter) . . . 127 9.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ~ I I . I 289
5B Proof of Rao-Blackwell-Lehmann-Scheﬂie Theorem (Scalar Parameter) . 130 9.3 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . . I . 292
9.4 Extension to a Vector Parameter . . . . . . . . . . . . . . . . . . . 2 294
6 Best Linear Unbiased Estimators 133 9,5 Statistical Evaluation of Estimators . . . . . . . . . . . . . . . . . . . I 299
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 9_6 Signal Processing EXarrlple - - ~ - " ' ' ' ' ' ' ' ' i ' i i i i i i i i i
6.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 309
6.3 Deﬁnition of the BLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 10 The Bayesian Philosophy U _ _ _ _ _ 309
6.4 Finding the BLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . ~ ~ . I 2 i 309
6.5 Extension to a Vector Parameter . . . . . . . . . . . . . . . . . . . . . . 139 10.2 Summary - - - - " ' ' ' _' ‘ ', ' ' ' ' i i i i i i i i i i i i i i _ _ _ _ 310
6.6 Signal Processing Example . . . . . . . . . . . . . . 1 . . . . . . . . . . 141 10.3 Prior Knowledge and Estimation . . . . . . . . . . . . . . . . . . I I 2 316
6A Derivation of Scalar BLUE . . . . . . . . . . . . . . . . . . . . . . . . . 151 10.4 Choosing a Prior PDF _ . . . . . . . . . . . . . . . . . . . . . . . . . ~ 2 I 321
6B Derivation of Vector BLUE . . . . . . . . . . . . . . . . . . . . . . . . . 153 10.5 Properties of the Gaussian PDF. . . . . . . . . . . . . . . . .  .  ‘ ' 2 325
10.6BayesianLinearModel......................~.UU328
7 Maximum Likelihood Estimation 157 10.7 Nuisance Parameters . . . . . . _. I. ._ . . . . . . . . . . . . . . I 330
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 10.8 Bayesian Estimation for Deterministic Parameters . . . . . . . . . . . 337
7.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 10A Derivation of Conditional Gaussian PDF . . . . . . . . . . . . - - - - - ~
7.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 341
7.4 Finding the MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 11 General Bayesian Estimators I I _ I I I _ 341
7.5 Properties of the MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I . ~ . ' 341
7.6 MLE for Transformed Parameters . . . . . . . . . . . . . . . . . . . . . 173 11.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . I . . 2 ‘ 342
7.7 Numerical Determination of the MLE . . . . . . . . . . . . . . . . . . . 177 11.3 Risk Functions . . . . . . . . . ._ . . . . . . . . . . . . . . . . . I I ' 344
7.8 Extension to aVector Parameter . . . . . . . . . . . . . . . . . . . . . . 182 11.4 Minimum Mean Square Erfor Estimators ' ' ' i i i i i i i i i i  _ _ _ 350
7.9 Asymptotic MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 11.5 MaximumA Posteriori Estimators . . . . . . . . . . . . . . .   2 2 i I 359
7.10 Signal Processing Examples . . . . . . . . . . . . . . . . . . . . . . . . . 191 11.6 Performance Description . . . . . . . . . . . . . . . . . . . . . . I U I 2 2 365
7A Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 11.7 Signal Processing Example ' ' ' ' ' ' ' ' ' i ' i i i i i i i i
7B Asymptotic PDF of MLE for a Scalar Parameter . . . . . . . . . . . . . 211
11A Conversion of Continuous-Time System to Discrete Time Ys em
7C Derivation of Conditional Log-Likelihood for EM Algorithm Example . 214
12 Linear Bayesian Estimators _ U I . I I _ _ _ 379
8 Least Squares 219 12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . ' . ~ I ‘ ' ' . I 379
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 12.2 Summary . . . . . . . . . . . . . . . . . . . . . . - - - -
8.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 12.3 Linear MMSE Estimation . . . . . . . . . . . . . . -
12.4 Geometrical Interpretations . . . . . . . . . . . . . . . . . . . . . . . . . 384
12.5 The Vector LMMSE Estimator . . . . . . . . . . . . . . . . . . . . . . . 389
12.6 Sequential LMMSE Estimation . . . . . . . . . . . . . . . . . . . . . . . 392
12.7 Signal Processing Examples - Wiener Filtering . . . . . . . . . . . . . . 400
12A Derivation of Sequential LMMSE Estimator . . . . . . . . . . . . . . . . 415
13 Kalman Filters 419
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
13.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
13.3 Dynamical Signal Models . . . . . . . . . . . . . . . . . . . . . . . . . . 420
13.4 Scalar Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
13.5 Kalman Versus Wiener Filters . . . . . . . . . . . . . . . . . . . . . . . . 442
13.6 Vector Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
13.7 Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
13.8 Signal Processing Examples . . . . . . . . . . . . . . . . . . . . . . . . . 452
13A Vector Kalman Filter Derivation . . . . . . . . . . . . . . . . . . . . . . 471
13B Extended Kalman Filter Derivation . . . . . . . . . . . . . . . . . . . . . 476
14 Summary of Estimators 479
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
14.2 Estimation Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
14.3 Linear Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
14.4 Choosing an Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
15 Extensions for Complex Data and Parameters _ 493
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
15.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
15.3 Complex Data and Parameters . . . . . . . . . . . . . . . . . . . . . . . 494
15.4 Complex Random Variables and PDFs . . . . . . . . . . . . . . . . . . . 500
15.5 Complex WSS Random Processes . . . . . . . . . . . . . . . . . . . . . . 513
15.6 Derivatives. Gradients, and Optimization . . . . . . . . . . . . . . . . . 517
15.7 Classical Estimation with Complex Data . . . . . . . . . . . . . . . . . . 524
15.8 Bayesian Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
15.9 Asymptotic Complex Gaussian PDF . . . . . . . . . . . . . . . . . . . . 535
15.10Signal Processing Examples . . . . . . . . . . . . . . . . . . . . . . . . . 539
15A Derivation of Properties of Complex Covariance Matrices . . . . . . . . 555
15B Derivation of Properties of Complex Gaussian PDF . . . . . . . . . . . . 558
15C Derivation of CRLB and MLE Formulas . . . . . . . . . . . . . . . . . . 563
A1 Review of Important Concepts 567
A1.1 Linear and Matrix Algebra . . . . . . . . . . . . . . . . . . . . . . . . . 567
A1.2 Probability, Random Processes. and Time Series Models . . . . . . . . . 574
A2 Glossary of Symbols and Abbreviations 583
Preface
Parameter estimation is a subject that is standard fare in the many books available
on statistics. These books range from the highly theoretical expositions written by
statisticians to the more practical treatments contributed by the many users of applied
statistics. This text is an attempt to strike a balance between these two extremes.
The particular audience we have in mind is the community involved in the design
and implementation of signal processing algorithms. As such, the primary focus is
on obtaining optimal estimation algorithms that may be implemented on a digital
computer. The data sets are therefore assumed to be samples of a continuous-time
waveform or a sequence of data points. The choice of topics reflects what we believe
to be the important approaches to obtaining an optimal estimator and analyzing its
performance. As a consequence, some of the deeper theoretical issues have been omitted
with references given instead.
It is the author’s opinion that the best way to assimilate the material on parameter
estimation is by exposure to and working with good examples. Consequently, there are
numerous examples that illustrate the theory and others that apply the theory to actual
signal processing problems of current interest. Additionally, an abundance of homework
problems have been included. They range from simple applications of the theory to
extensions of the basic concepts. A solutions manual is available from the publisher.
To aid the reader, summary sections have been provided at the beginning of each
chapter. Also, an overview of all the principal estimation approaches and the rationale
for choosing a particular estimator can be found in Chapter 14. Classical estimation
is ﬁrst discussed in Chapters 2-9, followed by Bayesian estimation in Chapters 10-13.
This delineation will, hopefully, help to clarify the basic differences between these two
principal approaches. Finally, again in the interest of clarity, we present the estimation
principles for scalar parameters ﬁrst, followed by their vector extensions. This is because
the matrix algebra required for the vector estimators can sometimes obscure the main
concepts.
This book is an outgrowth of a one-semester graduate level course on estimation
theory given at the University of Rhode Island. It includes somewhat more material
than can actually be covered in one semester. We typically cover most of Chapters
1-12, leaving the subjects of Kalman ﬁltering and complex data/ parameter extensions
to the student. The necessary background that has been assumed is an exposure to the
basic theory of digital signal processing, probability and random processes, and linear
and matrix algebra. This book can also be used for self-study and so should be useful
to the practicing engineer as well as the student.
The author would like to acknowledge the contributions of the many people who
over the years have provided stimulating discussions of research problems, opportuni-
ties to apply the results of that research, and support for conducting research. Thanks
are due to my colleagues L. Jackson, R. Kumaresan, L. Pakula, and D. Tufts of the
University of Rhode Island, and L. Scharf of the University of Colorado. Exposure to
practical problems, leading to new research directions, has been provided by H. Wood-
sum of Sonetech, Bedford, New Hampshire, and by D. Mook, S. Lang, C. Myers, and
D. Morgan of Lockheed-Sanders, Nashua, New Hampshire. The opportunity to apply
estimation theory to sonar and the research support of J. Kelly of the Naval Under-
sea Warfare Center, Newport, Rhode Island, J. Salisbury of Analysis and Technology,
Middletown, Rhode Island (formerly of the Naval Undersea Warfare Center), and D.
Sheldon of the Naval Undersea Warfare Center, New London, Connecticut, are also
greatly appreciated. Thanks are due to J. Sjogren of the Air Force Ofﬁce of Scientiﬁc
Research, whose continued support has allowed the author to investigate the ﬁeld of
statistical estimation. A debt of gratitude is owed to all my current and former grad-
uate students. They have contributed to the ﬁnal manuscript through many hours of
pedagogical and research discussions as well as by their speciﬁc comments and ques-
tions. In particular, P. Djurié of the State University of New York proofread much
of the manuscript, and V. Nagesha of the University of Rhode Island proofread the
manuscript and helped with the problem solutions. ,
Chapter 1
Introduction
1.1 Estimation in Signal Processing
systems designed to extract information. These systems include
1. Radar
. Sonar
. Speech
Steven M. Kay
University of Rhode Island
Kingston» R1 02881 . Biomedicine
4. Image analysis
6. Communications
8. Seismology,
and all share the common problem of needing to estimate the values of a group of pa-
rameters. We brieﬁy describe the ﬁrst three of these systems. In radar we are interested
in determining the position of an aircraft, as for example, in airport surveillance radar
[Skolnik 1980]. To determine the range R we transmit an electromagnetic pulse that is
reﬂected by the aircraft, causin an echo to be received b the antenna 1- seconds later,
as shown in Figure 1.1a. The range is determined by the equation 1'0 = 2R/c, where
c is the speed of electromagnetic propagation. Clearly, if the round trip delay T0 can
be measured, then so can the range. A typical transmit pulse and received waveform
are shown in Figure 1.1b. The received echo is decreased in amplitude due to propaga-
ion losses and hence may be obscured by environmental noise. Its onset may also be
perturbed by time delays introduced by the electronics of the receiver. etermination
of the round trip delay can therefore require more than Just a means of detecting a
Jump in the power level at the receiver. It is important to note that a typical modern
Modern estimation theor can be found at the heart of man electronic si nal rocessing
receive
antenna
Radar processing
system
(a) Radar
Transmit pulse
Time
Time
(b) Transmit and received waveforms
Figure 1.1 Radar system
radar system will input the received continuous-time waveform into a digital computer
by taking samples via an analog-to-digital converter. Once the waveform has been
sampled, the data compose a time series. (See also Examples 3.13 and 7.15 for a more
detailed description of this problem and optimal estimation procedures.)
Another common application is in sonar, in which we are also interested in the
position of a target, such as a submarine [Knight et al. 1981, Burdic 1984] . A typical
passive sonar is shown in Figure 1.2a. The tar et radiates noise due to machiner
on board, propellor action, etc. This noise, which is actually the signal of interest,
propagates through the water and is received by an array of sensors. The sensor outputs
, , w- __._. ...s/ws-..,t...-v-vemimwqv~»us<~w~
Sea surface
TOWBd aTYaY Noisy target
Sea bottom
(a) Passive sonar
Sensor 1 output
Time
Sensor 2‘ output
Time
Sensor 3 output
Time
(b) Received signals at array sensors
Figure 1.2 Passive sonar system
are then transmitted to a tow sh1 for in Iut to a dlglfliﬂlhﬂ)?! ﬂiers-i Ealtilcavliierzceiv:
positions of the sensors relative to the arrival angle o dtle Si? ngsenisors We can
the signals shown in Figure 12b.  
determine the bearing [3 from the BXEYESSiOII
ﬂ = arccos  (L1)
where c is the speed of sound in water and d is the distance between sensors (see
Examples 3.15 and 7.17 for a more detailed description). Again,  
Vowel / a /
$
Time (ms)
Figure 1.3 Examples of speech sounds
waveforms are not “clean” as shown in Figure 12b but are ‘embedded in noise makin
the determination of To more difficult. The value of obtained from (1.1) 1s then onl
an estimate. _
Another application is in speech processing systems [Rabmer and Schafer 1978].
A particularly important problem is speech recognition, which is the recognition of
speech by a machine (digital computer). The sim lest example of this is in  
individual speech sounds or honemes. Phonemes are the vowels, consonants, etc., or
the fundamental sounds of speech. As an example, the vowels la/ and 7e? are shown
in Figure 1.3. Note that they are eriodic waveforms whose eriod is called the pitch.
To recognize whether a sound is an /a or an /e/ the following simple strategy might
be employed. Have the person whose s eech is to be reco nized sa each vowel three
times and store the waveforms. To reco nize the s oken vowel com are it to the
d T / Periodogram
"E -5°-i v 1 a r— “r
a Frequency (Hz)
a
a 1
Frequency (Hz)
Figure 1.4 LPC spectral modeling
minimizes some distance measure. Difficulties arise if the itch of the speaker’s voice
changes from the time he or she records the sounds (the training session) to the time
when the speech recognizer is used. This is a natural variability due to the nature of
human speech. In practice, attributes, other than the waveforms themselves, are used
to measure distance. Attributes are chosen that a 'ble to variation. For
example, the spectral envelope will not change with pitch since the Fourier transform
of a periodic signal is a sampled version of the Fourier transform of one period of the
signal. The period affects only the spacing between frequency samples, not the values.
coding LPC). The parameters of the model determine the s ectral envelope. For the
Speech sounds 1n Figure 1.3 the power spectrum (magnitude-squared Fourier transform
divided by the number of time samples) or periodogram and the estimated LPC spectral
envelope are shown in Figure 1.4. (See Examples 3.16 and 7.18 for a description of how
the parameters of the model are estimated and used to ﬁnd the spectral envelope.) It
is interesting that in this example a human interpreter can easily discern the spoken
vowel. The real problem then is to design a machine that is able to do the same. In
the radar/ sonar problem a human interpreter would be unable to determine the target
position from the received waveforms, so that the machine acts as an indispensable
In all these systems we are faced with the problem of extracting values of parameters
based on continuous-time waveforms. Due to the use of digital computers to sample
and store t e continuous-time wave orm, we have the equivalent problem of extracting
arameter values from a discrete-time form or a data set. Iat ematically, we have
the N -point data set {a:[0], a:[1], . . . ,a:[N — 1]} which depends on an unknown parameter
0. We wish to determine 0 based on the data or to deﬁne an estimator
é= g(:v[0],a:[1], . . . ,:v[N - 1]) (1.2)
where g is some function. This is the problem of parameter estimation, which is the
subject of this book. Although electrical engineers at one time designed systems based
on analog signals and analog circuits, the current and future trend is based on discrete-
time signals or sequences and digital circuitry. With this transition the estimation
problem has evolved into one of estimating a parameter based on a time series, which
is just a discrete-time process. Furthermore, because the amount of data is necessarily
ﬁnite, we are faced with the determination of g as in (1.2). Therefore, our problem has
now evolved into one which has a long and glorious history, dating back to Gauss who
in 1795 used least squares data analysis to predict planetary moyements [Gauss 1963
(English translation)]. All the theory and techniques of statistical estimation are at
our disposal [Cox and Hinkley 1974, Kendall and Stuart 1976-1979, Rao 1973, Zacks
Before concluding our discussion of application areas we complete the previous list.
4. Image analysis - estimate the osition and orientation of an object from a camera
ima e, necessary when using a robot to pick up an object [Jain 1989]
5. Biomedicine - estimate the heart rate of a fetus [Widrow and Stearns 1985]
6. Communications - estimate the carrier freguency of a signal so that the signal can
be demodulated to baseband [Proakis 1983]
7. Control - estimate the position of a owerboat so that corrective navigational
action can be tﬁen, as in a LORAN system [Dabbous 1988]
8. Seismology - estimate the underggound distance of an oil deposit based on sound
reflections due to the different densities of oil and rock layers [Justice 1985].
Finally, the multitude of applications stemming from analysis of data from physical
experiments, economics, etc., should also be mentioned [Box and Jenkins 1970, Holm
and Hovem 1979, Schuster 1898, Taylor 1986].
Figure 1.5 Dependence of PDF on unknown parameter
1.2 The Mathematical Estimation Problem
In determining good estimators the ﬁrst step is to mathematicall model the data.
Because the data are inherentl random we describe it b it probability density func-
ti°11 (PDF) or Pfitlol» ‘Fill, - - - 7 llN - 1]; 9). The PDF is Qarameterized by the unknown
arameter 9 i.e., we have a class of PDFs where each one is different due to a different
value of 0. We will use a semicolon to denote this dependence. As an example, if N = 1
and 0 denotes the mean, then the PDF of the data might be
ﬁ EXP [—%(w[0] - 0f]
which is shown in Figure 1.5 for various values of 0. It should be intuitively clear that
because the value of 0 affects the probability of a: 0 we should be able to in er the value
3f Bbfrforln llalhe observed value of a: 0 . For example, if the value of a:[0] is negative, it is
0“ t u t a] 9 ?_92~ The value 0 = 01 might be more reasonable. This specification
of the PDF is critical in determining a good estimator. In an actual problem we are
not given a DF but must choose one that is not only consistent with the problem
clrlinstraintshand any prior knowledge, but one that is also mathematically tractable. To
i ulstrate t e approach consider the hypothetical Dow-Jones industrial average shown
in igure 1.6. It mi ht be con ectured that this data, althou h a earin to fluctuate
wildly actuall is “on the avera e” increasin . To determine if this is true we could
assume that the data actually consist of a straight line embedded in random noise or
$l"l=A+Bn+w[n] n=0,1,...,N—1.
A reasonable model for the noise is that w n is white Gaussian noise WGN or each
sample of w[n] has the PDF N (0,a ) (denotes a Gaussian distribution with a mean
of 0 and a variance of a2) and is uncorrelated with all the other samples. Then, the
unknown parametersT are  and B, which arranged as a vector become the vector
Parameter 9 = [A Bl - Letting x = [a:[0] a:[1] . . .@[1v - 111T, the PDF is
P(X,9) —   exp —5; Z(a:[n] — A — Bn)2] . (1.3)
51:2 3110i; 0f a straight linefor the ‘signal component is consistent with the knowledge
6 ow- ones average is hovering around 3000 (A models this) and the conjecture
Dow-Jones average
0 10 20 30 40 50 so 70 so 90 100
Day number
Figure 1.6 Hypothetical Dow-Jones average
that it is increasing (B > 0 models this). The assumption of WGN is justified by the
need to formulate a mathematically tractable model so that closed form estimators can
be found. Also, it is reasonable unless there is strong evidence to the contrary, such as
highly correlated noise. Of course, the performance of any estimator obtained will be
critically dependent on the PDF assumptions. We can only hope the estimator obtained
is robust, in that slight changes in the PDF do not severely affect t performance of the
estimator. More conservative approaches utilize robust statistical procedures [Huber
iesiy
Estimation based on PDFs such as (1.3) is termed classical estimation in that the
parameters of interest are assumed to be deterministic but unknown. In the Dow-Jones
average example we know a priori that the mean is somewhere around 3000. It seems
inconsistent with reality, then, to choose an estimator of A that can result in values as
low as 2000 or as high as 4000. We might be more willing to constrain the estimator
to produce values of A in the range [2800, 3200]. To incorporate this prior knowledge
__llF_‘, possibly uni orm over the [2800, 3200] interval. Then, any subsequent estimator
will yield values in this range. Such an approach is termed Bayesian estimation. The
p e attem tin to estimate is then viewed as a realization of the random
va.ria e . s suc , the data are described by the joint PDF
110ml?) = P(1\9)P(9)
where p 0 is the prior PDF, summarizin our knowled e about 0 before an data
are observed, and p xIO is a conditional PDF, summarizing our knowledge provided
e reader should compare t e notationa
b the data x conditioned on knowin 9. lli h l
differences between pix; 0) a family of PDFs) and p(x|9) (a conditional PDF), as well
as the imphH interpretations lsee also Problem 1.3). ' *"""
-i.o
0 10 20 30 40 50 so 70 sh 9i) 100
Figure 1.7 Realization of DC level in noise
Once the PDF has been speciﬁed, the roblem becomes one of deter
optimal estimator or unction of the data, as in (1.2). Note that an estimafld: m2;
depena on other Parameters. but only if they are known. An estimator may be thought
  The estimate of 9 '
the va ue o 0 obtained for a given realization of x. This distinction is analo o t 1S
random variable (which is a function deﬁned on the sample space) and the valuge ifstaie:
on. Although some authors distinguish betwee th t b ' '
letters, we will not do so. The meaning will, hgpefiiliyrobeycifjarfififigntlileadbitiziic?rcase
1.3 Assessing Estimator Performance
Consider the data set shown in Fi " - -
_ gure 1.7. From a cursory inspection it th
a:[n] consists of a DC level A in noise. (The use of the term DC is in refereiiigiltafirii at
current, which is equivalent to the constant function ) We could model the d t mac
- a a as
where w n denotes some zero mean noise rocess. Based on the data set {a;[O], d1], _ _ _ ,
fllN —_ 1]}, we would like to estimate A. Intuitively, since A is the average level of a:[n]
(u/[n] is zero mean), it would be reasonable to estimate A as
0f y the sample mean of the data. Several questions come to mmd;
1. How close will A be to A?
2- Are there better estimators than the sample mean?
10 CHAPTER 1" INTRODUCTION 1 a ASSESSING ESTIMATOR PERFORMANCE
For the data set in Figure 1.7 it turns out that A = 0.9, which is close to the true value 307
of A = 1. Another estimator might be _ r
Intuitively, we would not expect this estimator to perform as well since it does not g 20
make use of all the data. There is no averaging to reduce the noise effects. However, 5
for the data set in Figure 1.7, A = 0.95, which is closer to the true value of A than "6 15
the sample mean estimate. Can we conclude that A is a better estimator than A? E
The answer is of course no. Because an estimator is a function of the data, which g m
are random variables, it too is a random variable, subject to many possible outcomes. ‘ Z 5
The fact that A is closer to the true value only means that for the given realization of
data, as shown in Figure 1.7, the estimate A = 0.95 (or realization of A) is closer to 0 F r_
the true value than the estimate A = 0.9 (or realization of A). To assess performance -3 _2 _1 (l, T l
we must do so statistically. One possibility would be to repeat the experiment that Sample mean value A 3
Number of Outcomes
generated the data and apply each estimator to every data set. Then, we could ask
which estimator produces a better estimate in the majority of the cases. Suppose we
repeat the experiment by ﬁxing A = 1 and adding different noise realizations of w[n] to
generate an ensemble of realizations of  Then, we determine the values of the two
estimators for each data set and ﬁnally plot the histograms. (A histogram describes the
number of times the estimator produces a given range of values and is an approximation
to the PDF.) For 100 realizations the histograms are shown in Figure 1.8. It should
now be evident that A is a better estimator than A because the values obtained are
more concentrated about the true value of A = 1. Hence, A will usually produce a value
closer to the true one than A. The skeptic, however, might argue"that if we repeat the 5 _ l ﬁ j_
experiment 1000 times instead, then the histogram of A will be more concentrated. To  Tl l a
dispel this notion, we cannot repeat the experiment 1000 times, for surely the skeptic 0 , \ a1.  l l 1 l Ell;
would then rea.ssert his or her conjecture for 10,000 experiments. To prove that A is -3 —2 -1 Q 1 2 3
better we could establish that the variance is less. The modeling assumptions that we p First sample value A
must employ are that the w[n]’s, in addition to being zero mean, are uncorrelated and _ '
have equal variance a2. Then, we ﬁrst show that the mean of each estimator is the true Flgure 1-8 Hismgr ‘"115 for Sample mean and ﬁrst Sample estimator
value or
= — a: n
E(A) E 1% [1 1 N“
N “:0 = W 2 var(a:[n])
= l Z E(z[n]) - = LN 2
N N2 a
A since the w[n]’s are uncorrelated and thus
so that on the average the estimators produce the true value. Second, the variances are
 = var  NZ-l 
var(A)
a»
Furthermore, if we could assume that w[n] is Gaussian, we could also conclude that the
obabilit of a given magnitude error is less for A than for A (see Problem 2.7).
Several important pointslare illustrated by the previous example, which should
always be ept in mind.
1. An estimator is a random variable. As such its erformance can onl be com-
pletely described statistically or by its PDF.
2. The use of computer simulations for assessing estimation performance, although
quite valuable for gaining insight and motivating conjectures, is never conclusive.
At best, the true performance may be obtained to the desired degree of accuracy.
At worst, for an insufficient number of experiments andlor errors in the simulation
techniques employed, erroneous results may be obtained (see Appendix 7A for a
further discussion of Monte Carlo computer techniques).
Another theme that we will repeatedly encounter is the tradeoff between perfor-
mance and computational complexity. As in the previous example, even though A
has better performance, it also requires more computation. We will see that optimal
estimators can sometimes be difﬁcult to implement, re uirin a multidimensional o ti-
mization or integration. In these situations, alternative estimators that are suboptimal,
but which can be implemented on a digital computer, may be preferred. For any par-
ticular application, the user must determine whether the loss in performance is offset
by the reduced computational complexity of a suboptimal estimator.
1.4 Some Notes to the Reader
main ideas necessary or etermining optima estimators. We have included results
that we deem to be most useful in practice, omitting some important theoretical issues.
The latter can be found in many books on statistical estimation theory which have
been written from a more theoretical viewpoint [Cox and Hinkley 1974, Kendall and
Stuart 1976—1979, Rao 1973, Zacks 1981]. As mentioned previously, our goal is to
obtain an o timal estimator, and we resort to a subo timal one if the former cannot
be found or is not implementable. The sequence of chapters in this book follows this
approach, so that optimal estimators are discussed ﬁrst, followed by approximately
optimal estimators, and ﬁnally suboptimal estimators. In Chapter 14 a “road map” for
ﬁnding a good estimator is presented along with a summary of the various estimators
and their properties. The reader may wish to read this chapter ﬁrst to obtain an
overview.
We have tried to maximize insight by including many examples and minimizing
long mathematical expositions, although much of the tedious algebra and proofs have
been included as appendices. The DC level in noise described earlier will serve as a
standard example in introducing almost all the estimation approaches. It is hoped
that in doing so the reader will be able to develop his or her own intuition by building
upon previously assimilated concepts. Also, where possible, the scalar estimator is
presented ﬁrst followed by the vector estimator. This approach reduces the tendency
of vector/matrix algebra to obscure the main ideas. Finally, classical estimation is
described ﬁrst, followed by Bayesian estimation, again in the interest of not obscuring
the main issues. The estimators obtained using the two approaches, although similar
in appearance, are fundamentally different.
The mathematical notation for all common symbols is summarized in Appendix 2.
The distinction between a continuous-time waveform and a discrete-time waveform or
sequence is made through the symbolism :r(t) for continuous-time and a:[n] for discrete-
time. Plots of a:[n], however, appear continuous in time, the points having been con-
nected by straight lines for easier viewing. All vectors and matrices are boldface with
all vectors being column vectors. All other symbolism is deﬁned within the context of
the discussion.
References
Box, G.E.P., G.M. Jenkins. Time Series Analysis: Forecasting and Control Holden-Dav San
Francisco, 1970. l " l
Burdic, W.S., Underwater Acoustic System Analysis, Prentice-Hall, Englewogd Cliffs N J 19g4
Cox, D.R., D.V. Hinkley, Theoretical Statistics, Chapman and Hall, New York 1974 l l ll l
Dabbous, T.E., N.U. Ahmed. J.C. McMillan, D.F. Liang, “Filtering of Discdntinuous Processes
Arising in Marine Integrated Navigation.” IEEE Trans. Aerosp. Electron Syst Vol 24
Gauss, K.G., Theory of Motion of Heavenly Bodies, Dover, New York 1963
Holm S. J.M. Hovem “Estimation of Scalar Ocean Wave Spectral b I '
’ i ’ i y the Maximum Entropy
Method,’ IEEE J. Ocean Eng., Vol. 4, pp. 76-83, 1979.
Huber, P.J., Robust Statistics, J. Wiley, New York, 1981.
Jain, A.K., Fundamentals of Digital Image Processing, Prentice-Hall Englewood Cliffs N J 1989
Justice, J “Array Processing in Exploration Seismology,” in Array Signal Processin l S. I-Ila ki l
ed., Prentice-Hall, Englewood Cliffs, N.J., 1985. g7 l y n’
en allé7$llgigA Stuart, The Advanced Theory of Statistics, Vols. 1-3, Macmillan, New York,
night, W.S., R.G. Pridham, S.M. Kay, Digital Signal Processing for Sonar, Proc. IEEE, Vol.
69, pp. 1451-1506, Nov. 1981.
$219195,  gtﬁlll Communications, McGraw-Hill, New York, 1983.
m?) ~ lgn- - sChafer, Digital Processing of Speech Signals, Prentice-Hall, Englewood Cliffs,
RAO, C.R., Linear Statistical Inference and Its Applications J Wiley New Yqi-k 1973
c us £252‘; 0?; @1118 Iiivestigation of Hidden Periodicities with Application to a Supposed 26 Day
skolnik MI Il ejro ogical Phenomena, Terrestrial Magnetism, Vol. 3, pp. 13-41, March 1898,
, . . ., ntro uction to Radar Systems, McGraw-Hill, New York, 1980.
zlsylor, S., Modeling Financial Time Series J. Wiley New York 1986
‘d B. a - ’ - ’ - ’ ' .
Zapkpovig P, Stearns, S.D.,'Adaptive Signal Processing, Prentice-Hall, Englewood Cliffs, N.J., 1985.
» -, ammetmc Statistical Inference, Pergamon, New York, 1981,
Problems
1. In a radar system an estimator of round trip delay To has the PDF T}, ~ N (To, 01o),
where 1'0 is the true value. If the range is to be estimated, propose an estimator R
and ﬁnd its PDF. Next determine the standard deviation 0+0 so that 99% of the
time the range estimate will be within 100 m of the true value. Use c = 3 >< 108 _
m/ s for the speed of electromagnetic propagation. .V  a p t e r 2
2. An unknown parameter 0 inﬂuences the outcome of an experiment which is mod-
eled by the random variable 2:. The PDF of a: is _g
1 i 1,, _ 9y] _ _ Minimum Variance Unbiased
paw) = -exp ~-
l Estimation
A series of experiments is performed, and a: is found to always be in the interval
[97, 103]. As a result, the investigator concludes that 0 must have been 100. Is
this assertion correct?
3. Let a: = 0 + w, where w is a random variable with PDF p,,,(w). If 0 is a determin-
istic parameter, ﬁnd the PDF of a: in terms of p“, and denote it by p(r; 0). Next
assume that 0 is a random variable independent of w and ﬁnd the conditional
PDF p(:r\0). Finally, do not assume that 0 and w are independent and determine
p(:c\0). What can you say about p(a:; 0) versus p(a:\0)?
4° ll ls deslled to estllllalle llle value of a DC level A lll WGN or t In this chapter we will be in our search for good estimators of unknown deterministic
Mn] = A i. wln] n = 0,17 _ __ V N _ 1i 13181115513. e will restrict our attention to estimators which on the avera e ield
‘ the true parameter value. Then within this class of estimators the goal will be to ﬁnd
is Zero mean and uncorrelated, and each Sample has Variance <72 = 1- l t e one that exhibits the least variability Hopefully the estimator thus obt ' d '11
- 7 aine wi
2.1 Introduction
where w[n]
Consider the two estimators produce values close to the true va ue most of the time The notion of ' '
A 1 N-l variance unbiased estimator is examined within this chapter but the meaillls 
A = N Z a:[n] Will reouire some more theory. Succeeding chapters will provide that theory as well as
“=0 . apply it to many of the typical problems encountered in signal processing.
A = N+2 @210] + Z :v[n] +2a:[N —1]>.
"l 2.2 Summary
Which one is better? Does it depend on the value of A?
n unbiased estimator 1S deﬁned by 2.1 with the im ortant roviso that this holds for
5. For the same data set as in Problem 1.4 the following estimator is proposed: an ossible l f h
A: 2 with the _va ues o _t _e unknown arameter. Within this class of estimators the one
A a:[0] 7 = A > 1000 t b d fnlnlnnlm varlance is Sought. The unbiased constraint is shown by example
A = 1 N“ A, _ A2 < 1000 i i o_ _e eslrable from a practical viewpoint since the more natural error criterion, the
N Z a:[n] F - _ . alilrlllimllm mean square error, deﬁned in (2.5), generally leads to unrealizable estimators.
1i method? variance unbiased estimators do not, in eneral, exist. When they do, Several
The rationale for this estimator is that for a high enough signal-to-noise ratio and th can e used to ﬁnd tnefn- The methods rel on the CIamer-Rag ower mm
conce t of a sufﬁcient statistic. If a minimum variance unbiase estimator
(SNR) or A2/a2, we do not need to reduce the effect of noise by averaging and d _ _
hence can avoid the added computation Comment on this approach got GXtIStbOI‘ if both of the previous two approaches fail, a further constraint on the
estimate? 0 ein linear in the data, leads to an easily implemented, but subo timal
2.3 Unbiased Estimators
For an estimator to be unbiased we mean that on the avera e the estimator will yield
the true value of the unknown parameter. Since the parameter value may in general be
anywhere in the interval a < 0 < b, unbiasedness asserts that no matter what the true
value of 0, our estimator will yield it on the avera e. Mathematically, an estimator is
1m zase 1
E(é)=0 a<0<b (2.1)
where (a,b) denotes the range of possible values of 0.
Example 2.1 - Unbiased Estimator for DC Level in White Gaussian Noise
Consider the observations
where A is the parameter to be estimated and w n is WGN. The parameter A can
take on an value in the interval -oo < A < oo. Then, a reasonable estimator for the
average value of a:[n] is
A = w g m] (2.2)
or the sample mean. Due to the linearity properties of the expectation operator
a.
for all A. The sample mean estimator is therefore unbiased.
In this example A can take on any value, although in general the values of an unknown
parameter may be restricted by physical considerations. Estimating the resistance R
of an unknown resistor, for example, would necessitate an interval 0 < R < oo.
Unbiased estimators tend to have symmetric PDFs centered about the true value of
9, although this is not necessary (see Problem 2.5). For Example 2.1 the PDF is shown
in Figure 2.1 and is easily shown to be N(A,az/N) (see Problem 2.3).
The restriction that E (d) = 9 for all 0 is an important one. Lettin 0 =
x , where
E@y=/g@»awmx=0 brﬂ0. ma
Figure 2.1 Probability density function for sample mean estimator
It is possible» h0W6ver, that (2.3) may hold for some values of 0 and not others, as the
next example illustrates.
Example 2.2 - Biased Estimator for DC Level in White Noise
Consider again Example 2.1 but with the modiﬁed sample mean estimator
Then,
E(A) = 5A
¢ AuA¢o
It is seen that (2.3) holds for the modiﬁed estimator only for A = 0. Clearly, A is a
biased estimator. o
That an estimator is unbiased does not necessaril mean that it is a ood estimator.
It onl uarantees that on the avera e it will attain the true value. On the other hand
biased estimators are ones that are characterized by a systematic error, which presum-
ably should not be present. A persistent bias will always result in a poor estimator.
As an example, the unbiased property has an important implication when several es-
timators are combined (see Problem 2.4). t sometimes occurs that multiple estimates
of the same parameter are available, i.e., {P1, 92, . . . , 0n}. A reasonable rocedure ' o
combine these estimates into, hopefully, a better one by averaging them to form
6=—Z6.~ (2.4)
Assuming the estimators are unbiased, with the same variance and uncorrelated with
each other,
w A ‘$1
¢l4 fgo>
(DA 
11(9)
11(9)
n increases
(a) Unbiased estimator
11(9)
11(9)
n increases
5(6) e
(b) Biased estimator
5(6)
Figure 2.2 Effect of combining estimators
and
so that as more estimates are avera ed the variance will decrease. Ultimately, as
n -> oo, 0 -> 0. However, if the estimators are biased or E(0,») = 0 + b(0), then
E(9)
and no matter how many estimators are avera ed 9 will not conver e to the true value.
This is depicted in igure 2.2. Note that, in general,
hl-z  V3l‘(éi)
9 + 5(9)
m) = 12(6) - 0
is deﬁned as the bias of the estimator.
2.4 Minimum Variance Criterion
In searching for optimal estimators we need to ado t some o timalit criterion. A
natura one is the mean square ervior (MSE), deﬁned as
mseui) = E [(6 - of] . (2.5)
This measures the avera e mean s uared deviation of the estimator from the t e value.
Unfortunate y, adoption of this natural criterion leads to unrealizable estimators ones
that cannot be written solely as a function of the data. To understand the problem
which arises we ﬁrst rewrite the MSE as
E { W ~ En + (En » ti}
= var(9) +  -— 9r
= W03) + 52(9) (2.6)
rnsew)
which shows that the MSE is composed of errors due to the variance of the estimator as
well as the bias. As an example, for the problem in Example 2.1 consider t e mo i e
estimator
A = (11-V- Tg .7:[n]
for some constant a. We will attempt to ﬁnd the a which results in the minimum MSE
Since E(A) = aA and var(A) = azaz/N, we have, from (2.6), i
mse(A) = + (a —1)2A2.
Differentiating the MSE with respect to a yields
dmse(A) 2002
T = N +2(a—1)A2
which upon setting to zero and solving yields the optimum value
Li; T2611 lfklllat, unfortunately, the optimal value of a depends upon the unknown param-
. _ e estimator 1S therefore not realizable. In retrospect the estimator depends
upon A since the bias term in (2.6) is a function of A. It would seem that an criterion
which depends on the bias will lead to an unrealizable estimator. Although this is
generally true, on occasion realizable minimum MSE estimators can be foun Bibby
an outen urg 1977, Rao 1973, Stoica and Moses 1990].
val-(é) var(9i)
02 g2 No MVU
é MVU t_ t estimator
3 = es ima or
(a) (b)
Figure 2.3 Possible dependence of estimator variance with 0
From a practical view oint the minimum MSE estimator needs to be abandoned.
An alternative approach is to constrain the bias to be zero and ﬁnd the estimator which
minimizes the variance. Such an estimator is termed the minimum variance unbiased
(MVU) estimator. Note that from (2.6) that the MSE of an unbiased estimator is just
the variance.
Minimizin the variance of an unbiased estimator also has the effect of concentrating
the PDF of the estimation error, 0 — 0, about zero (see Problem 2.7). The estimation
error w1 t erefore be less likely to be large.
2.5 Existence of the Minimum Variance Unbiased
Estimator
The uestion arises as to whether a MVU estimator exists i.e., an unbiased estimator
wit minimum variance for all 9. Two possible situations are describe in Figure . .
If there are three unbiased estimators that exist and whose variances are shown in
Figure 2.3a, then clearly 0A3 is the MVU estimator. If the situation in Figure 2.3b
exists, however, then there is no MVU estimator since for 0 < 00, (i; is better, while
for 0 > 90, 03 is better. In the former case 93 is sometimes referred to as the uniformly
minimum variance unbiased estimator to emphasize that the variance is smallest for
all 9. In general, the MVU estimator does not always exist, as the following example
illustrates.
Example 2.3 - Counterexample to Existence of MVU Estimator
If the form of the PDF changes with 0, then it would be expected that the best estimator
would also change with 0. Assume that we have two independent observations a:[0] and
a:[1] with PDF
a:[O] ~ A/(B, 1)
var(d)
27/as
Figure 2.4 Illustration of nonex-
9 istence of minimum variance unbi-
ased estimator
The two estimators
é. = gap] + 1.111)
a. = §a01+§411
can easily be shown to be unbiased. To compute the variances we have that
Vafwh) = i(var(wl0l)+var(wlll))
var(§=_,) =  + 
so that A 2T2 if 0 Z 0
varw‘) 2i g if0<0
and A g-g if 0 3 0
varw’) zl g3 1w < 0.
The variances are shown in Figure 2.4. Clearly, between these two estimators no MVU
estimator exists. It is shown in Problem 3.6 that for 9 Z 0 the minimum possible
variance of an unbiased estimator is 18/ 36, while that for 0 < 0 is 24/36. Hence, no
single estimator can have a variance uniformly less than or equal to the minima shown
in Figure 2.4. O
To conclude our discussion of existence we should note that it is also ossible that t
may not exist even a single unbiased estimator (see Problem 2.11).  
searc or a MV estimator is fruitless.
2.6 Finding the Minimum Variance
Unbiased Estimator
Even if a MVU estimator exists, we may not be able to ﬁnd. it. is I10 KIIOWII
“turn-t e-crank” procedure which will always roduce the estimator. In the next few
chapters we shall discuss several ossible a roaches. They are:
var(§) Q3
Figure 2.5 Cramer-Rao
9 lower bound on variance of unbiased
estimator
1. Determine the Cramer-Rao lower bound CRLB and check to see if some estimator
satisﬁes it lChapters 3 and 4).
2. Apply the Rao-Blackwell-Lehmann-Scheife lRBLS) theorem (Chapter 5).
3. Further restrict the class of estimators to be not only unbiased but also linear. Then,
ﬁnd t e minimum variance estimator wit in this restricted class (Chapter 6).
Approaches 1 and 2 may produce the MVU estimator, while 3 will yield it only if the
MV estimator 1S inear 1n t e ata.
The CRLB allows us to determine that for any unbiased estimator the variance
must be greater than or e ual to a given value, as shown in Figure 2.5. If an estimator
estimator. n t is case, the t eory o the CRLB immediately yields the estimator. l;
estimator may still exist, as for instance in the case of 0, in Figure 2.5. Then, we
must resort to the Rao-Blackwell-Lehmann-Scheffe theorem. This procedure ﬁrst ﬁnds
a su cien s atistic, one whic uses a the data efﬁcientl and then nds a unction
of the su cient statistic which is an unbiased estimator of 0. With a slight restriction
of the PDF of the data this procedure will then be guaranteed to produce the MVU
estimator 
restriction, and chooses the best linear estimator. Of course, only for particular data
sets can this approach produce the MVU estimator.
2.7 "Extension to a Vector Parameter
If 0 = [01 02 . . . 9plT is a vector of unknown parameters, then we say that an estimator
9 = [9192 . . .0p]T is unbiased if
 = 0,‘ a,- < 0," < b," 
for i = 1,2, . . . ,p. By deﬁning
we can equivalently deﬁne an unbiased estimator to have the property
E(§) = a
for every 0 contained ' ' A ace deﬁned in (2.7). A MVU estimator has the
additional property that var(0,-) for i = 1,2, . . . ,p is minimum among all unbiased
estimators.
References
Bibby, J., H. Toutenburg, Prediction and Improved Estimation in Linear Models, J . Wiley, New
York. 1977.
Rm, C.R., Linear Statistical Inference and Its Applications, J. Wiley, New York, 1973.
Stoica, P., R. Moses, “On Biased Estimators and the Unbiased Cramer-Rao Lower Bound,” Signal
Process, Vol. 21, pp. 349—350, 1990.
Problems
2.1 The data {a:[0],ac[1], . . . ,:c[N — 1]} are observed where the r[n]’s are independent
and identically distributed (IID) as N (0, a2). We wish to estimate the variance
azas v
a Nrgarln].
Is this an unbiased estimator? Find the variance of d? and examine what happens
as N —> oo.
2.2 Consider the data {:v[0],a:[1], . . . ,:c[N — 1]}, where each sample is distributed as
Ll [0, 9] and the samples are IID. Can you ﬁnd an unbiased estimator for 9? The
rangeof6is0<9<oo.
2.3 Prove that the PDF of A given in Example 2.1 is A/(A, (IQ/N).
2.4 The heart rate h of a patient is automatically recorded by a computer every 100 ms.
In 1 s the measurements {h1,h2A,. . . ,h10} are averaged to obtain h. If E(h,») = ah
for some constant a and var(h,4) = 1 for each i, determine whether averaging
improves the estimator if a = 1 and a = 1/2. Assume each measurement is
uncorrelated.
2.5 ’l“wo samples {a:[0],  are independently observed from a N (0, a2) distribution.
The estimator
ai-l
($201+ ﬁll)
is unbiased. Find the PDF of d? to determine if it is symmetric about a2.
2.6 For the problem described in Example 2.1 the more general estimator
A = Z anzhi]
’s so that the estimator is unbiased and the variance is
grangian multipliers with unbiasedness as the constraint
is proposed. Find the an
minimized. Hint: Use La
equation.
2.7 Two unbiased estimators are proposed whose variances satisfy var(§) < var((I). If
both estimators are Gaussian, prove that
for any e > O. This says that the estimator with
less variance is to be preferred
since its PDF is more concentrated about the tru
e value.
2.8 For the problem described in Example 2.1 show that as N —> oo, A —> A by using
the results of Problem 2.3. To do so prove that
AlilgoPrﬂA-AI >6} =0
for any e > O. In this case the estimator A is said to be consistent. Investigate
what happens if the alternative estimator = $ 2:01  is used instead.
2.9 This problem illustrates what happens to an
a nonlinear transformation. In Example 2.1,
parameter 0 = A2 by
unbiased estllnator when it undergoes
if we choose to estimate the unknown
can we say that the estimator is unbiased? What happens as N —+ o0?
2.10 In Example 2.1 assume now that in addition to A, the value of a2 is also unknown.
We wish to estimate the vector parameter
Is the estimator
unbiased?
. w. ~v:Qﬁ(i¢*lilh‘»ravilu—-qi.vv—4t'i;  
- - ' ' ' desired to
_ . b t" 0] from the distribution Ll[0,1/9], ItAIS
2.11 Giventa zingtleisoaizﬁvnaeirghzg 0 > 0. Show that for an estimator 9 = g(q;[0]) to
' e .
gseinfllilliiased we must have
4 ' t be found to satisfy this condition for all
Next prove that a function g CaIIIIO
Chapter 3
Cramer-Rao Lower Bound
3.1 Introduction
Bein able to place a lower bound on the variance of an unbiased estimator proves
to be extremely useful in practice. At best, it allows us to assert that an estimator is
the MVU estimator. This will be the case if the estimator attains the bound for all
values of the unknown parameter. At worst, it provides a benchmark against which we
can compare the performance of an unbiased estimator. Furthermore, it alerts us to
the physical impossibility of ﬁnding an unbiased estimator whose variance is less than
the bound. The latter is often useful in signal processing feasibility studies. Although
many such variance bounds exist [McAulay and Hofstetter 1971, Kendall and Stuart
1979, Seidman 1970, Ziv and Zakai 1969], the Cramer-Rao lower bound (CRLB) is by
far the easiest to determine. Also, the theory allows us to immediately determine if
an estimator exists that attains the bound. If no such estimator exists, then all is not
lost since estimators can be found that attain the bound in an approximate sense, as
described in Chapter 7. For these reasons we restrict our discussion to the CRLB.
3.2 Summary
The CRLB for a scalar parameter is given by (3.6). If the condition (3.7) is satisﬁed,
then the bound will be attained and the estimator that attains it is readily found.
An alternative means of determining the CRLB is given by (3.12). For a signal with
an unknown parameter in WGN, (3.14) provides a convenient means to evaluate the
bound. When a function of a parameter is to be estimated, the CRLB is given by
(3.16). Even though an efﬁcient estimator may exist for 0, in general there will not be
one for a function of 0 (unless the function is linear). For a vector parameter the CRLB
is determined using (3.20) and (3.21). As in the scalar parameter case, if condition
(3.25) holds, then the bound is attained and the estimator that attains the bound is
easily found. For a function of a vector parameter (3.30) provides the bound. A general
formula for the Fisher information matrix (used to determine the vector CRLB) for a
multivariate Gaussian PDF is given by (3.31). Finally, if the data set comes from a
(a) U1I1/3  02:1
Figure 3.1 PDF dependence on unknown parameter
' RLB, that depends on the PSD,
WSS Gaussian random process, th
gth becomes large.
is given by (3.34). It is valid asymp
3.3 Estimator Accuracy Considerations
it is worthwhile to expose the hidden factors that
ince all our information is embodied
determine how well we can estimate a parameter.  
in the observed data and the underlying PDF for that data it is not sur risin t at the
estimation accuracy depen s irect y on the PDF. For instance, we should not expect
ameter with any degree of accuracy if the PDF depends
{the PDF does not depend
to be able to estimate a par
he unknown parameter, the
If a single sample is observed as
rlOl = A + wlol
where u/[O] ~ N (0, a2), and it is desired to estimate A, then we expect a better es
if a2 is small. Indeed, a good unbiased estima
2 so that the estimator accuracy improves
just a ,
of viewing this is shown in Figure 3.1, where the
shown. They are
as a2 decreases. Ari alternative way
PDFs for two different variances are
PiUPlORA) = épexp l-yieaioi - AV] an
versus the unknown parameter A for a given
value of  If of < 0;, then we should be able to estimate A more accurately based
on p1(:r[0]; A). We may interpret this result by referring to Figure 3.1. If a:[0] = 3 and
a1 = 1/ 3, then as shown in Figure 3.1a, the values of A > 4 are highly unlikely. To see
for i‘ = 1,2. The PDF has been plotted
timate i
tor is A =  The variance is, of course, _
this we determine the probability of observing a:[0] in the interval [.710] 6/2 [o] +6/2]
_ , (l; =
[3 — 6/2, 3 + 6/2] when A takes on a given value or
til?) 2 S rlo] S 3+ 5} “fps P¢(u;A)du
which for 6 small is p,~(q;[0] = 3; A)5_ But 0 = 3_ A _ _ _
3; A = 3)6 = 1.206. The probability of obsiiiiriiig] a:[0] iin aTsiiiasll-iiiligrlvsziilwhlle plum] z
:r[0] = 3 when A = 4 is small with respect to that when A = 3 H cenﬁered about
A > 4 can be eliminated from consideration. It might be ‘d ence’ t e values of
the interval 3 i 3a, = [27 4] are viable candidates For th 61%;? fhat values of A in
is a much weaker dependence on A. Here our viable candiceiates a mrltgllne 3'1}? there
I6 1n e muc wider
interval 3 i 302 = [0, 6].
when thrill.“ is  .
it is termed the likelihood function. Two exgintlhlogsuglfllilgvla Daéafmeter (With x ﬁxed)»
i" Figure 31- Intuitivelyi 
  T oo unction determines how
that t e S ar ness is e eetivel measured b the m; t9 quantify this notion observe
 311$ f; tilefiiild deivaﬁ“? °f
 - In Example 3-1 if We consider the natural lo ' a we 9LT
’ garithm of the PDF
111M310]; A) = — lnV21ra2 - gﬂmm] __ A)2
then the ﬁrst derivative is
Hln (a: ;
w) _ 1 (1-[0] _ A) (3.2)
and the negative of the second derivative becomes
8A2 = {;~ (3.3)
The curvature increases as a2
- decreases. Since 1 .
A = dilﬂ] has variance 02, then for this example we a ready know that the estlmator
var(A) = (3.4)
and the variance decreases as t
_ he c t ‘ . .
7 g a i W1 . Thus, a more appropriate
_ E lQIPFl4QBiLEQl (8-5)
"lea-Sure of curvature is
which measures the average curvature of the log-likelihood function. The expectation Thus, no unbiased estimator can exist whose variance is lower than a2 for even a Sin 1e
ut 1n act we know that if A = :c[0], then var(A) = a2 for all A. Since :z:[0]
is taken with respect to p(;z:[0]; A), resultin in a function of A onl . The expectation  i value o -
aclinowledges the fact that the liEelihood function, which depends on zlO], is itself a A l is unbiased and attains the CRLB, it must therefore be the MVU estimator, Had we
random variable. The larger the quantity in (3.5), the smaller the variance of the ~' been unable t0 %ues)s that :c[O] would be a good estimator, we could have used (3.7).
estimator. ‘A py-om (3,2) and 3.7 we make the identiﬁcation
3.4 Cramer-Rao Lower Bound f m) _ 1
We are now ready to state the CRLB theorem.   M1101) ___ gm]
Theorem 3.1 (Cramer-Rao Lower Bound - Scalar Parameter) It is assumed
that the  p(x; satisﬁes the “regularity” condition SO that (3?) iS S8tlSﬁEd- Hence; A =  = z[0] iS the MVU estimator. AlSO, HOtE
that var( = a2 = 1/I(0), so that according to (3.6) we must have
E [alnpbgm] = 0 for all 0 2
30 I(0)=_E [3 lnp(x;9)]‘
where the expectation is taken with respect to p(x; 0). Then, the variance of any unbiased 302
 U A 1 . We will return to ‘this after the next example. See also Problem 3.2 for a generalization
var(9) Z   (3.6) ~ to the non-Gaussian case. <>
where the derivative is evaluated at th e value 0 0 and the expectation is taken with ' Y Example 3_3 - DC Level in White Gaussian Noise
respect to p(x;0). Furthermore, an unbiased estimator may be found that attains the ‘
b 1 for a” 0 1 and ‘ml Z ' ' J Generalizing Example 3.1, consider the multiple observations
aln ;9 l
~31 =I<v><g<==>-v> (w) ~ ; I[’!l~]=A+w[1l] n=0,1,...,N—1
 _ That estimate,- whteh t; the M VU estimator is é = x), ‘ l‘ l where w[n] is WGN with variance a2. To determine the CRLB for A
and the minimum variance is 171(0). l.
The expectation 1n (3.6) 1s explicitly given by g  pg; A) = _o 27mg exp [_  _ [L02]
a =  w; Z<@l"1-A>
since the second derivative is a random variable de endent on x. Also, the bound will  "=°
de end on 0 in eneral so that it is displayed as in Figure 2.5 (dashed curve). An  Taking the ﬁrst derivative
example oi a PDF that does not satisfy the regularity condition is given in Problem l
3.1. For a proof of the theorem see Appendix 3A. I ». alnmr A) 8 1 N_1
Some examples are now given to illustrate the evaluation of the CRLB.  WL- = 5X i- lu[(21rg2)i!] _ i;  _ Af]
Example 3.2 - CRLB for Example 3.1
For Example 3.1 we see that from (3.3) and (3.6)
 Z a2 for all A. (3-3)
where i is the sample mean. Differentiating again
82 lnp(x; A) _ __£
8A2 — a2
and noting that the second derivative is a constant, we have from (3.6)
moi) z (w)
as the CRLB. Also, by comparing (3.7) and (3.8) we see that the sample mean estimator
attains the bound and must therefore be the MVU estimator. Also, once again the
minimum variance is given by the reciprocal of the constant N / a2 in (3.8). (See also
Problems 3.3—3.5 for variations on this example.) <>
We now prove that when the CRLB is attained
where 1(0) z _E [82 lnmmm]
From (3.6) and (3.7) ,
and   _ 1(g)(9“_ g)
a0 " '
Differentiating the latter produces
82 ln p(x; 0) 81(0)
802 =—?a0—(0—0)—1(0)
and taking the negative expected value yields
82 ln p(x; 0) _ 81(9) ~
—E[ 602 l - — 80 [E(0)—0]+1(0)
= 1(0)
and therefore 1
In the next example we will see that the CRLB is not always satisﬁed.
Example 3.4 - Phase Estimation
z[n]=Acos(21rfon+¢)+w[n] n=0,1,...,N-—1.
The amplitude A and fre uenc 0 are assumed known (see Example 3.14 for the case
when they are unknown). The PDF is
P(X; <15) = W QXP {-563 n=0 lilnl ' Ac°s(2"fon + ¢)l2}-
Differentiating the log-likelihood function produces
alngg!’ (p) = —(—713  — Acos(21rf0n + zD)]Asin(21rf0n + Q5)
= —;;  s1n(21rf0n + qb) — 5 s1n(41rfon + 245)]
  = —; Z  cos(21rfon + (p) — A cos(41rf0n + 2¢)].
Upon taking the negative expected value we have
—E  = F Z [A cos2(21rf0n + g5) — A cos(41rfon + 2q5)]
= 8-2  + 5 cos(41rf0n + 2d>) — cos(41rf0n + 2(1))
since
F Z cos(41rf0n + 245) z 0
for not near 0 or 1/2 (see Problem 3.7). Therefore,
In this example the condition for the bound to hold is not satisﬁed. Hence, a phase
estimator does not exist which 1s unbiased and attains the CRLB. It is still possible:
owever, that an MVU estimator may exist. At this point we do not know how to
was) z
var(0) g2
(b) é, MVU but not efficient
(a) 01 eﬂicient and MVU
Figure 3.2 Efficiency vs. minimum variance
determine whether an MVU estimator exists, and if it does, how to ﬁnd it. The theory
of sufficient statistics presented in Chapter 5 will allow us to answer these questions.
An estimator which is unbiased and attains the CRLB as the sample mean estimator
 ciently uses the data. An MVU
estimator ma or may not be eﬁicient. For instance, in Figure 3.2 the variances of all
 illustration there are three unbiased estimators)
are displayed. In Figure 3.2a, 01 is eﬁicient in that it attains the CRLB. Therefore, it
is also the MVU estimator. On the other hand, in Figure 3.2b, 01 does not attain the
CRLB, and hence it is not eﬁicient. However, since its variance is uniformly less than
that of all other unbiased estimators, it is the MVU estimator.’
The CRLB given by (3.6) may also be expressed in a slightly different form. Al-
though (3.6) is usually more convenient for evaluation, the alternative form is sometimes
useful for theoretical work. It follows from the identity (see Appendix 3A)
E[(21»_=;g=_@)“]  <3...)
so that
(3.12)
3 ln p(x; 0) > 2
E  a0
(see Problem 3.8).
The denominator in (3.6) is referred to as the Fisher information 1(0) for the data
1(0) = -E [ ] . (3.13)
As we saw previously, when the CRLB is attained the variance is the ci rocal of the
Fisher information. intuitively, the more information the lower the bound. It has the
essential properties of an information measure in that it is
1. nonnegative due to (3.11)
2, additive for independent observations.
The latter property leads to the result that the CRLB for N IID observations is 1 N
times t a, or one o servation. To verify this, note that for independent observations
lnp(x; 0) = 2 lnp(z[n];0).
This results in 821 ( 0) MA 821 ( [ 1 6)
El we l‘  W l
and ﬁnally for identically distributed observations
1(0) = N
here
"——— w z _E[6*1nz>(w[n];@)]
is the Fisher information for one sam le. For nonindependent samples we might expect
that the information will be less than N 110), as Problem 3.9 illustrates. For ggrnpletely
dependent samples, as for example, z[0] = z[l] = - - - = z[N—1], we will have 1(0) = 1(0)
(see also Problem 3.9). Therefore, additional observations carry no information, and
3.5 General CRLB for Signals
in White Gaussian Noise
Since it is common to assume white Gaussian noise, it is worthwhile to derive the
CRLB for this case. Later, we will extend this to nonwhite Gaussian noise and a vector
parameter as given by (3.31). Assume that a deterministic signal with an unknown
parameter 0 is observed in WGN as
The dependence of the signal on 0 is explicitly noted. The likelihood function is
(2132) z 2G2
p(x; 0) = ——}—F exp {——1—- N_1(:z:[n] — s[n;0])2}.
Differentiating once roduces
3lnp(x; 0) ___ 1 3s[n; 0]
a, ;;Z(w[n]—s{n;0]) a,
36 A CHAPTER s. CRAMER-RAO LOWER BOUND 3 a TRANSFORMATION OF PARAMETERS 37
and a second differentiation results in
5.o~.
32lnp(x;0) 1 NA 3zs[n;0] 3s[n;0] 2  g 45*
——a:,-a— = ;,-,-  (101 - Shaw;- —  -  g i?‘
Taking the expected value yields é 301
ad 2.5-1
32lnp(x;0) 1 NA 3s[n;9] 2 i? l
El 80* )='a‘;,( w)  i2“.
so that ﬁnally ' l-o-l- '1 F T T l l l I “l _f
val‘ 2 U’ (3.14) I  Frequency
Figure 3.3 Cramer-Rao lower bound for sinusoidal frequency estimation
1VE_1(3s[n;0]>2‘
The form of the bound demonstrates the importance of the si nal de endence on 0.
Signals that change rapidly as the unknown parameter changes result in accurate esti-
mators. A simple application of (3.14) to Example 3.3, in which s[n;0] = 0, produces
a CRLB of az/N. The reader should also verify the results of Example 3.4. As a ﬁnal
example we examine the problem of frequency estimation.
3.6 Transformation of Parameters
It fre uentl occurs in practice that the parameter we wish to estimate is a function
  may wish to estimate A2 or the power of the
signal. Knowing the CRLB for A, we can easily obtain it for A2 or in general for any
function of A. As shown in Appendix 3A, if it is desired to estimate a = 9(0), then the
'“' 3a
(w)
var(d) z -—-—-- (3.16)
8'lnp(x;9) '
‘El av" 
Flor the present example this becomes a = g(A) = A2 and
Example 3.5 - Sinusoidal Frequency Estimation
We assume that the signal is sinusoidal and is represented as
s[n;fo] = Acos(21rfon + qb) 0 < f0 < é
where»the amplitude and phase are known (see Example 3.14 for the case when they
are unknown). From (3.14) the CRLB becomes A (214)? 414252
var(A2)Z N/Uz = N
(3.17)
a2
W00) Z   - (3-15) Note that in using (3.16) the CRLB is expressed in terms of 9-
A2 Z mm sinwﬂfon + (m? We saw in Example 3.3 that the sample mean estimator was eﬁicient for A. It might
"=0 be sup osed that :22 is eﬁicient for A2. To uickl dispel this notion we ﬁrst show that
i’ is not even an unbiased estimator. Since :2 ~ JLV (A, a2 7N l
12w) = E2(5:)+var(5:)=A2+%
9e AT?’ (3.1s)
The CRLB is plotted in Figure 3.3 versus frequency for an SNR of A2/cr2 = 1, a data
record length of N = 10, and a phase of qﬁ = 0. It is interesting to note that there
appear to be preferred frequencies (see also Example 3.14) for an approximation to
(3.15) ). Also, as f0 --> 0, the CRLB goes to inﬁnity. This is because for f0 close to zero
a slight change in frequency will not alter the signal signiﬁcantly. <> _ _
Hence, we immediately conclude that the eﬁiciency of an CStQMGtOT w destroyed by a
Jwnlinear tmnsfonnatioﬂ? That it is maintained for linear actuall afﬁne transfor-
mations is easily veriﬁed. Assume that an eﬁicient estimator for 0 exists and is given
_ g(0) = g(0A) = aé + b. Then,
‘by 0A. It is desired to estimate 0 = a9 + b. As our estimator of 9(0), we choose
E(a9+b) = aE(l§) +b= ¢o+b r»
= 0(9)“
so that  is unbiased. The CRLB for 9(0), is from (3.16),
we?» a» 
( >2var(9l)
But var(y(l9)) = varlaé + b) = azvarwl), so that the CRLB is achieved.
Although eﬁiciency is preserved only over linear transformations, it is approximately
maintained over nonlinear transformations if the data record is large enoug . 1S as
great practical signiﬁcance in that we are frequently interested in estimating functions
of parameters. To see why this property holds, we return to the previous example of
estimating A2 by i2. Although i2 is biased, we note from (3.18) that i2 is asymptotically
unbiased or unbiased as N --> oo. Furthermore, since i ~ N (A, a2 /N ), we can evaluate .
the variance
var(i2) = E(i4) - EH5’);
E(£’) = u’ + <12‘!
 = p4 + (M202 + 3a“.»;
and therefore
VH0?) = 5(9) — E2052)
For our problem we have then
var(i2) -_- 4AA,” + % ' (3.19) »
Hence, as N —> oo the variance a roac 2 2 N , the last term in (3.19) converging
to zero faster than the ﬁrst. But this is just the CRLB as given by (3.17). Our assertion
that i2 is an asymptotically eﬁicient estimator of A2 is veriﬁed. Intuitively, this situation ~
occurs due to the statistical linearity of the transformation, as illustrated in Figure 3.4.
As N increases, the PDF of i becomes more concentrated about the mean A. Therefore,
(b) Large N
(a) Small N
Figure 3.4 Statistical linearity of nonlinear transformations
the values of i that are observed lie in a small interval about i = A (the i3 standard
deviation interval is displayed). Over this small interval the nonlinear transformation
is appIOXimatGly linear. Therefore, the transformation may be replaced by a linear one
since a value of i in the nonlinear region rarely occurs. In fact, if we linearize g about
g<r>~g<A>+ fjAla-Aia
It follows that, to within this approximation,
Elton = you = A2
or the estimator is unbiased (asymptoticall . Also,
var[g(i)] =  var(i)
(2A)2a2
4A2a2
so that the estimator achieves the CRLB (asymptotically). Therefore, it is asymp-
totically efﬁcient. This result also yields insight into the form of the CRLB given by
3.7 Extension to a Vector Parameter
We now extend the results of the previous sections to the case where we wish to estimate
a vector parameter 0 = [91 02 . . .0,,]T3. We will assume that the estimator 0 is unbiased
as deﬁned in Section 2.7. The vector parameter CRLB will allow us to place a bound
on the variance of each element. As derived in Appendix 3B, the CRLB is found as the
i. i] element of the inverse of a matrix or
ma?» 2 [I"(@>1.-.-  (3-20)
where I(9) is the p >< p Fisher information matriar. ‘The latter is deﬁned by
82 lnp(x;0) V
Walla; ’ “E  s (3-21)
In evaluating (3.21) the true value of 0 is used.
= 1(0) an we ave the scalar CRLB. Some
Note t at in t e scalar case (p = 1),
vxzunples follow.
Example 3.6 - DC Level in White Gaussian Noise (Revisited)
We now extend Example 3.3 to the case where in addition to A the noise variance a2
is also unknown. The parameter vector is 0 = [A 02F, and hence p = 2. The 2 x 2
Fisher information matrix is
_E [82lnp(x;0)] _E [82lnp(x;0)]
1(a) _ 8A2 8A8U2
_E [82lnp(x;9)] _E [82lnp(x;9)]
lt is clear from (3.21) that the matrix is symmetric since the order of partial differenti-
ation may be interchanged and can also be shown to be positive deﬁnite (see Problem
3.1m. The log-likelihood function is, from Example 3.3,
lnp(x; 0) = —€ln21r -— glue? — % nzzohﬂn] — A)2.
‘Che derivatives are easily found as
8lnp(x; a) _ 1 "*1
8lnp(x; 9) N 1 NA
w?" = ‘i; + 51 aft“ " Alz
82 lnp(x; 0) _ __1\£
82 lnp(x; 0) 1 NA
"EMT = ‘a? F0221- A)
82 lnp(x;9) N 1 N“
i972“ = a; - F (“"1 - Ali
Upon taking tne negative expectations, the Fisher information matrix becomes
1(0)= <3’ N
Although not true in general, for this example the Fisher information matrix is diagonal
and hence easily inverted to yield
var(A) Z %
var(a2) 2 2%.
Note that the CRLB for A is the same as for the case when 02 is known due to the
diagonal nature of the matrix. Again this is not true in general, as the next example
illustrates. O
Example 3.7 - Line Fitting
Consider the problem of line ﬁtting or given the observations
where u/[n] is WGN, determine the CRLB for the slope B and the intercept A. The
parameter vector in this case is 0 = [A BlT. We need to ﬁrst compute the 2 >< 2 Fisher
information matrix,
_E [82 lnp(x;0)] _E [82 lnp(x; 9)]
m) am 6A8B
_ E 82lnp(x;0) _E 82lnp(x;0)]
The likelihood function is
page) = —l-—exp{-i 1211M - A- Bar}
(21ra2)% "=0
from which the derivatives follow as
8lnp(x; 0) _ i NA _ _
8ln p(x; 0) _ i N‘ _ _
and
82 ln p(x;
82 ln p(x;
82 ln p(x;
1(6) = p N-l sf?
N N(N—1)
U’ N(N—1) N(N—1)(2N—1)
2 e
where we have used the identities
Inverting the matrix yields
2(2N - 1) _ 6
1"‘(0)=¢2 N(N?) N(i\;+1>
_N(N+ 1) N(N? - 1)
It follows from (3.20) that the CRLB
var(A)
var(B)
2(2N — 1)a2
N (N +1)
N(N? -1)‘
- ~ r-<"-xrvvqxqanr\sﬁ‘
(3.22)
a
(a) A=0,B=0toA—_-1,B=0 (b) A=0,B=0toA=0,B=1
Figure 3.5 Sensitivity of observations to parameter changes—no noise
Some interesting observations follow from examination of the CRLB. Note ﬁrst that
the CRLB for A has increased over that obtained when B is known, for in the latter
case we have
var(A) > —i——l-i — i
“ E[82lnp(x;A)] _ N
and for N Z 2, 2(2N — 1)/(N + 1) > l. This is a quite general result that asserts that
the CRLB always increases as we estimate more parameters isee Problems 3.11 and
3.12). A second point is that
CRLB(A) z (2N -1()(1v - 1) > 1
CRLB(B)
for N 2 3. Hence, B is easier to estimate, its CRLB decreasing as 1/N3 as opposed to
the 1/N dependence for the CRLB of A. These differing dependences indicate that a: n
is more sensitive to changes in B than to changes in A. A simple calculation reveals
a
Am] ~ aziniAB-RAB
Changes in B are magniﬁed by n, as illustrated in Figure 3.5. This effect is reminiscent
of (3.14), and indeed a similar type of relationship is obtained in the vector parameter
case (see (3.33)). See Problem 3.13 for a generalization of this example. <>
As an alternative means of computing the CRLB we can use the identity
a0,» as, aeiaa, (323)
as shown in Appendix 3B. The form given on the right-hand side is usually easier to
evaluate, however.
We now formally state the CRLB theorem for a vector parameter. Included in the
theorem are conditions for eguality. The bound is stated in terms of the covariancg
matrix of 0, denoted by C5, from which (3.20) follows.
Theorem 3.2 (Cramer-Rao Lower Bound - Vector Parameter) It is assumed
that the PDF p(x;0) satisﬁes the “re ularit ” conditions
= o for all 05V"
where the expectation is taken with res ect to x; 9). Then, the covariance matrix 0f
any unbiased estimator 0 satisﬁes
c, - 14(0) z 0g’ (3.24)
where Z 0 is interpreted as meaning that the matrix is positive semide nite. The Fisher v
where the derivatives are evaluated at the true value of 0 and the e ectation is taken
with respect to x;9). Furthermore, an unbiased estimator may be found that attain; '
the bound in that C9 = I“‘(9) if and only 
a-"igﬂl = lunar!) - v) 3
(3.25)
for some p-dimensional function g and some p X p matrix I. That estimator, which is _
the M VU estimator, is 0 = g(x), and its covaria matrix is 1'1 0 .
The proof is given in Appendix 3B. That (3.20) follows from (3.24) is shown by noting
lcé T I_l(0)lii 2 O
and therefore _
var(0,~) = [CéLi Z [I'1(0)]h. .  (3.26)
Additionally, when e ualit holds or C-
9 = (x) is efficient and hence is the MVU estimator. An example of equality occurs
1n xamp e . . There we found that
8lnp(x; 0)
<91I11>(X;9) _ 8A -
T _ 6lnp(x; 0) (327)
a "=° . (3.2s)
Althoughnot obvious, this may be rewritten as
g N(N - 1) A
mm z U’ 2”’ l f‘ ‘ A l (3-29)
80 N(N— 1) N(N—1)(2N—1) B-B
where 2(2N _ l) N4 6 v_1
A z 1v(1v+1) 7;“ l‘ N(N+1) 7507mm]
B = -N(N6+1) 2:30 x[n] + N(1V13_1)n=o nx[n]
H ce the conditions for equality are satisﬁed and [A BA’]T iS 8H Bfﬁcient and llhefefoTe
Meg/‘U lestimator. Furthermore, the matrix in (3.29) is the inverse of the covariance
matllfllzche equality conditions hold, the reader may ask whether we can be assured that
é is unbiased. Because the regularity conditions
8 ln p(x' 9)]
E l a0
are always assumed to hold, we can apply them to (3.25). This then yields E[g(x)] =
 ﬁnding MVU estimators for a vector arameter the CRLB theorem provides a
power u tool. In particular, it allows us to ﬁnd the MVU estimator for an im ortant
class of data models. This class is the linear modeland is described in detail in Chap-
 ng example just discussed is a special case. Suﬁice 1t to say that
if we can model our ata 1n t e inear model form, then the MVU estimator and its
~ performance are easily found.
3.8 Vector Parameter CRLB for Transformations
The discussion in Section 3.6 extends readil to the vector case. Assume that it is
desired to estimate a = 5(9) for g, an r-dimensional function. Then, as shown in
Appendix” 6140)-. agwr  330)
a-Wr (a) a9 20,, (-
where, as before, Z 0 is to be intergreted as gositive semideﬁnite. In (3.30),
is the 1' >< Q Jacobian matrixideﬁned as
591(9) 391(9)
891(0)
1992(9)
a0,
Example 3.8 - CRLB for Signal-to-Noise Ratio
which can be considered to be the SNR for a single sample. Here 0 = [A a2]T and
9(0) = 0f/0; = Az/az. Then, as shown in Example 3.6,
1(0) = v2 N
The Jacobian is
so that
6g(0) __, 6g(0
30 I (0 89
Finally, since a is a scalar
- - ' ' ' tained over linear transformations
' d in Section 3.6, eﬁiciency 1S ma"!
As discusse
a=;(a)=A0+€
. ~ _ A A ' fﬁcient
- ' dbisanr><1vector. 
where A is an r X p matrix an
or é: r ,then
EM) = A0+b = a“
so that d is unbiased d
of, =1= ACgAT = AI*‘(0)AT
aw) _, fizwVf,"
%I (a) a8 ,.
- - ‘ ‘ ' tained onl
' . For nonlinear transformations eﬁicienc is mam
the latter bein the CRL
” he tr
' h t th PDF of 0 becomes concentrated about t
as N T)  (rjrvhls assumestﬁa? 0 i: consistent) Again this is due to the statistical
value of as —+ oo or -
linearity o g 0) about the true value of 0.
3 9 CRLB for the General Gaussian Case ‘
_ . - - 1 ression for the CRLB.  
It 1S qulte cots/cum? at tlmii$odgiiti itlhiegiliBetﬁat generalizes 3.14 . Assume that
of Gaussian o serva ions we
x ~ N (aw), 0(0))
on 0. Then, as shown in AppendiX
so that both the mean and covariance m8 d6
3C the Fisher information matrix is give“ by
a- -... s. CRAMER-RAO LOWER BOUND
For the scalar parameter case in which
x ~N(;ii6 I ~
this reduces to
1(6) = [wil- -  q
a0 _ a0
+ 5n [g c -  J (3.32)
which generalizes (3.14). We now illustrate the TZICTBUOI! with some examples.
Example 3.9 - Parameters of a Signal in Tme Gaussian Noise
Assume that we wish to estimate a scalar signa tarazeter 0,’ for the data set
where w[n] is WGN. The covariance matrix is C = "I and does not depend on 0, The
second term in (3.32) is therefore zero. The ﬁr? '1 ﬁelds
1 aiuw-‘uai
m’) = Fl as 
which agrees with (3.14). O
Generalizing to a vector signal parameter estimate;
:_ tiie presence of WGN, we have
from (3.31)
h" h ‘elds _
w ic yi I 0 ’ in 185W 9] 35h; 9] (333)
l ( llij — U2 n20 391. 80]-
the elements of the Fisher information matrix.
as
EXamPIeISJO - Parameter of Noise
Assume that we observe
l l ' WGN ith unknown variance 0 = 02 Then according to (3-32), Sime
where w Tl 15 W ' l
C(02) = J21, we have
8C(cr2))2
l<<a>~>>l
lell
h' h grees with the results in Example 3-6- A Sllglltly more compllcated example
w ic a Q
C—1(0,2
loin-l IOIP‘
Example 3.11 - Random DC Level in WGN
Consider the data rm] z A + mm] n z 0,1,. ‘MN _1
. 1 ' G ‘ random variable with zero mean
white  1S vgGliflndjlistlilzﬁgelrfd/giitliiflw[jlllsillllle power of the signal or variance
variance a . so, ' . - -
i?‘ is the unknglwn parameinen Then, Ix = [110]  .  rﬂg/ntxilll" 15 Gaussian with zero
mean and an N >< N covariance matrix whose [2, J] e 6m
[owing = Elwli —11wU—1ll
= E[(A+w[i~1l)(-4 +wl1 — 11)]
= (Ti-PU 6U‘-
Therefore,
qo-j) = ajiiT + v21
where 1 = [11. . . 1]T. Using Woodbury’s identity (see Appendix 1), we have
_ 1 a?
C: 1e12,) _ F (I - U2 + 14%?‘ 111") f
Also, since  ~ ‘f1 f2 %
actafi) z HT ‘i
6G3 Ps$(.fi fa)
we have that a ( 2)
_ C a‘ 1
C ‘((75) 803,4 z a2 + N03, n?
Substituting this in (3.32) Droduces f: _ f1 f fc +1.2 1
I(crA) - 2tr [(U—————2+Nai) 11 11 ] i’ f _l_f2
I ;  truly‘)  ' ti ation
2 U + N U A f‘ Figure 3.6 Signal PSD for center frequency es m
_ §(a2+Na_24) _ 
As h n in Appendix 3D the elements of the Fisher information are approximately
(as N —> 0°)
so that the CRLB is
2 1 2 3 N i ai P,,(f;9)3lnP==(f;9) * _
mien.) z 2 (of, +  . [Kent] = 3 L _“_55‘__Tdf  (3 s4)
Note that even as N --> oo, the CRLB does not decrease below 20f‘. This is because
each additional data sample yields the same value of A (see Problem 3.14). <>
. - 'th the ex licit de endence on 0 shown. it
where PM“, 0) 1S the PSD f0; 316i: jeieossﬁis form, which is somewhat reminiscent of
‘gggsflmll a it: gillllililie the accurac‘ with which PSD or e uivalentl covariance
‘ _ , a ows us
.Wme 
3.10 Asymptotic CRLB for WSS Gaussian Random '
Proggssgg" ivllxample 3.12 - Center Frequency of Prmiess
At times it is diﬁicult to analytically compute the CRLB usin 3.31 due to the need to s‘ typical problem is to estimate the center frequency fc of a PSD which otherwise is
invert the covariance matrix. UT course we ca alwa s resort to a computer evaluation. 'k_nown_ Given
An alternative ‘form which can be applied to Gaussian rocesses t at are is very P11“; fc) = Q(f _ fc) ‘t Q(—f — fa) + a:
useful. lt 1S easily computed
and provides much insight due to its simpliﬁed form. 1
rinci al drawback is that strictl s eakin it is valid onl as N —> 0o or as m totically.
In practice it provides a good approximation to the true CRLB if the data record lengt ..
N is muc greater t an the correlation time of the process. The correlation time is
e ne as t e maximum lag k of the ACF rufk] = E :|: n]a:[n + lg] for which the ACF
is essentially nonzero. Hence, for processes with broad PSDs the approximation will be
goo or moderate length data records, while for narrowband processes longer length
data records are required.
we wish to determine the CRLB for fa assumln? that Qéfgdazd awﬁgkfifilvgncenteﬁ
may view the process as consisting of a random signal em e e in -
. . - ' Q(f) and the signal
frequency of the signal PSD is to be estimated. The real function _
PSD P_,,( f ; f6) are shown in Figure 3.6. Note that the possible ienter freecﬁsueﬁglzis $12:
untrained to be in the interval [f1,1/2 — f2l-_F°1' these centeli reqgeilcl _ a 5651M
PSD for f Z 0 will be contained in the [0,1 / 2] interval. Then, since — fc 15 i
we have from (3.34)
191111210312) ___ <91" lQU- fc) +Q(-f - fcl-l-ﬁl
afc 3ft
3Q(f ~11) _,_ 3Q(-f — f0)
31$ 31$
Q(f - f.) + Q(—f — f.) + a?"
This is an odd function of f, so that
% alnlamf.) 2 _ i alnPmoﬁr.) ’
/< 8f. )df‘2/0( ﬁr. ldf‘
Also, for f Z 0 we have that Q(- f — f6) = O, and thus its derivative is zero due to the
assumption illustrated in Figure 3.6. It follows that
var f‘ " é 812(6) gf-m 2
O (Q0 - f.) M) -
é scar-r.) _1 2
o Q(f — f0) + a
L}. Q(f')+‘72
where we have let f’ = f-fc. But 1/2—fc Z 1/2—fcm_x = f2 and ~fc ﬁ —fc,,,,,, = -f1,
so that we may change the limits of integration to the interval [-1 / 2, 1 / 2]. Thus,
Vaﬂfll Z  
Nlouwraz) ‘if
Q(f) = exp l-é 
where a; << 1/2, so that Q(f) is bandlimited as shown in Figure 3.6. Then, if Q(f) >>
g2, we have approximately
As an example, consider
var(fc)> g 2 = 13f.
Narrower bandwidth (smaller a2 spectra yield lower bounds for the center freguency
since the PSD changes more rapidly as f, changes. See also Problem 3.16 for another
example. <>
3.11 Signal Processing Examples
We now apply the theory of the CRLB to several signal processing problems of interest.
The problems to be considered and some of their areas of application are:
1. Range estimation - sonar, radar, robotics
2. Frequenc estimation - sonar, radar, econometrics, spectrometry
3. Bearing estimation - sonar, radar
4. Autore ressive parameter estimation - speech, econometrics.
These examples will be revisited in Chapter 7, in which actual estimators that asymp-
totically attain the CRLB will be studied.
Example 3.13 - Range Estimation
In radar or active sonar a si nal ulse is transmitted. The round tri dela r from
the transmitter to the target and back is related to the range R as 1'0 = 2R/c, where c
is the speed of propagation. Estimation of ran e is therefore e uivalent to estimation
of the time delay, assuming that c is known. If sit} is the transmitted signal, a simple
model for the received continuous waveform is
:z:(t) = s(t — 1'0) + w(t) O 3 t g T.
The transmitted signal pulse is assumed to be nonzero over the interval 0,T, . Addi-
tionally, the signal is assumed to be essentially bandlimited to B Hz. If the maximum
time delay is To , then the observation interval is chosen to include the entire signal
5y letting T = T, + 10m; The noise is modeled as Gaussian with PSD and ACF as
ACF of w(t)
rw,,,(r) = NOB 27MB
Figure 3.7 Properties of Gaussian observation noise“
shown in Figure 3.7. The bandlimited nature of the noise results from filterin the con-
tinuous waveform to the signa andwidth of B Hz. The continuous received waveform
is sampled at the Nyguist rate or samples are taken every A = 1/ (2B)seconds to form
a:(nA) = s(nA — 1'0) + w(nA) n = 0, 1, . . . , N ~11?
Letting z[n] and w[n] be the sampled sequences, we have our discrete data model
z[n] = s(nA — To) + w[n]. (3.35)
Note that w[n] is WGN since the sam les are se arated b kA = k 2B which
corresponds to the zeros of the KCF of wgt) as shown in Figure 3.7. Also, w n] has
variance a = rww(0) = NOB. Because the signal is nonzero onl o ' l
‘r0 g t g T0 + T,, (3.35) reduces to
w n 0 g n g no — 1 I.)
:c[n] = s(nA — 1'0) + w[n] no _<_ n 5 no + M — 15> (3.36)
where M is the length of the sampled signal and no = TMA is the delay in samples.
For simplicity we assume that A is so small that 10/ A can be approximated by an
integer.) With this formulation we can apply (3.14) in evaluatin the CRLB.
a H2
‘%1 (ash; T0152
V3.I'(7A'0) Z
"MfA ds(t) 2
a2
. T _ n A Assuming that A is small enough to approximate the Sum by an
integral, we have
i- a2
var(1°o) Z m‘
l! (ds(t)) dt
Finally,   We have
varhzo) >   (3.37)
An alternative form observes that the energy 5 iS
5=/ §(t)dt s,
which results in A
i- vﬂﬁo) Z i  (338)
where
s2(t)dt
1m» be Show“ that i; N0 .2 1s a s“ W2“ T???“ l-gfilmiilifl’... p.;i2.§§1il?““‘e °
the bandwidth of the signal since, using stan ar ourie i
[m <21rF)’1S(F>|’dF
1W :   (3-39)
/ |S(F)|’dF
Where F denotes continuous-time frequency, and S (F) is the Fourier transform of s(t).
—- ' ' al. Fr
In this form it becomes clear that F2 is the mean s garjiltbllllﬂltillilédliilvgrtilﬁeslclflmlg. F:
(338) and (3.39), the larger the mean square ban W1‘ b t _ x (__a (t _
instance, assume t at e signal is a Gaussian pulse given y S( ) —- e P 2 F
T /2)2) and that 5U) is essentially nonzero Over the interval loiTsl- The“ is“, :
(aF/\/21r) exp(—21r2F2/0§~) and F2 = aﬂ/Z. As the mean square bandwidth increases, 1(0)] : _L 2 Acosasin a = —F Z: sin 2a ~ 0
the signal pulse becomes narrower and it becomes easier to estimate the time delay l 13 a2 nzo a n=0
Finally, by noting that R = crO/2 and using (3.16), the CRLB for range is
~—1 .. (wi/HZN” 2 1i 2)
A 62/4 llwllzz = i2 Z Awmirsnfa = ,2  " 2 2 °°S a
var(R) Z 5 (3.40) a n=0 "-
NO/QF z (211102 Z n2
[I(0)l23 = F 20A 2mm“ a ~ a2 n20
Example 3 14 - Sinusoidal Parameter Estimation 3:1 NAZ
In many ﬁelds we are confronted with the roblem of estimatin the arameters of a lllall” — U2 g Sm a 20"’
sinusoidal signal. Economic data which are cyclical in nature may naturally ﬁt such a _
' ' formation matrix bBCOIIIQS
The Fisher in
z[n]=Acos(21rfon+¢)+w[n] n=0,1,...,N—-1
where A > O and O < 0 < 1 2 (otherwise the parameters are not identiﬁable, as is
veri e yconsi eringA =1,¢ = OversusA = —1,¢$= 1rorfo =OwithA =1/2,¢ =0
versus A = 1/\/§, ¢ = 1r/4). Since multiple parameters are unknown we use (3.33)
Using (3.22),  »
for 0 = A f0 Q T. In evaluating the CRLB it is assumed that f0 is not near 0 or 1Z2, ~ > ii;
which allows us to ma e certain simpi cations based on the approximations [Stoica vadfo) - (21r)21;N(N’ — 1)
1989] (see also Problem 3.7): 2(2N _ 1)
A (3.41)
var(¢) Z  
 - - f ‘ id is of considerable
where n = 142/ (M) l’: the SIhRi‘ nsilueincyuifcnynlitégrllasse:  SNR increases and
1 NA interest. Note that the CRL or e req
. ‘ (41rfon+ ZdJ) z O
N: +1 i n cos
- - . See
' ' ‘t ensitive to data record length
that the bound decreases  1_/ N 3» malfmg 1t q‘; e S
also Problem 3.17 for a variation of this examp -
for i = O, 1,2. Usin
g these approximations and letting a = 21rf0n + ¢$, we have
[I(6)]11 U2 7;) cos a U2 g (2 + 2 cos a) 2U; Example 3.15 - Bearing Estimation 8 T
_ - hown in Figure 3. . _<_>_
1 N—1 AN-l . . - tt t‘ ate the 1n toatar etass
lI(0)l12 : ‘F’ 2 A21rncosasind = ‘FT nsin2a z 0  
. all s aced sensors in a
- bserved b an arra of e 11
do so the acoustic Pressure ﬁeld 1S O
k. o Target
Planar
wavefronts
I Figure 3.8 Geometry of array for
bearing estimation
l_i_n_e Assuming that the target radiates a sinusoidal si nal Acos(21rFot + g5), then the
received si nal at the nth sensor IS Z cos 21rFo t— t") +¢), where tn is the pro a 811$;
time to the nth sensor. If the array is located far rom the target, then the circular
wavefronts can be considered to e p anar a e array. s s own in Figure 3.8, the
wavefront at the (n — 1)st sensor lags that at the nth sensor by dcos 5/ c due to the
extra propagation distance. Thus, the ropagation time to the nth sensor is
where to is ‘the propagation time to the zeroth sensor, and the observed si nal at the
nt sensor is
s,,(t) = Acos [21rFo(t - to + nij cos ﬁ) + g5},
If a single “snapshot” of data is taken or the array element outputs are sampled at a
given time t,, then
s,,(t,) = A cos[21r(F0-c- cos ﬂ)n + ¢'] (3,112)
are sinusoidal with frequency f, = Fo(d/c) cos 5. l To complete the description of the
data we assume that tlie sensor out uts are corru ted b Gaussian noise with zero mean
and variance a which is independent from sensor to sensor. The data are modeled as
where w[n] is WGN. SIIDCG typicall A, are unknown as well as ﬂ, we have the roblem
  based on (3.42) as in Example 3.14. Once the CRLB for these
rans ormation is for 9 = [A f, qb’ “'-
arccos < 
where 4:’ = ¢+21rFo(t_, —t0). In this form it becomes clear that the spatial observations
so that from (3.30)
(ca, _ 5LVmI'1(0)8g(6)Tl z 0.
Because of the diagonal J acobian this yields
varw‘) z  [I‘1(9)l2z~
But from (3.41) we have
ll (ollzz “ (27T)277M(M2 _1)
and therefore
m”) Z (21r)217M(M2 - 1) F341 sin’ a
or ﬁnally
(21r)2M1) (—-) sinz 
where /\ = c/Fo is the wavelength of the ro a atin
is t e ength of t e arra . Note that it is easiest to estimate bearing if Q = 90°,
an impossible if Q = 0°. Also, the bound depends critically on the array length in
wave engths or L[/\, as well as the SNR at the array output or M17. The use of the
CRLB for easibility studies is examined in Problem 3.19. O
lane wave and L =
Example 3.16 - Autoregressive Parameter Estimation
In speech processing an im ortant model for s eech production is the autore ressive
(AR) process. As shown in Figure 3.9, the data are modeled as the output of a causal
all-pole discrete ﬁlter excited at the input by WGN u n . The excitation noise u n] is
an in erent part of the model, necessary to ensure t at a: n is a WSS random process.
The all-pole ﬁlter acts to mode e vocal tract, while the excitation noise models the
.orcin of air throu h a constriction in the throat necessary to produce an unvoiced
sound such as an “s.” The effect of the ﬁlter is to color the white noise so as to model
  This model is also referred to as a linear predictive coding
(LPC) modeHMakhoul 1975]. Since the AR model is capable of producing a variety
of PSDs, depending on the choice of the AR ﬁlter parameters {a[1], a[2], . . . , a[p]} and
Pwdf) PZI(f)
Figure 3.9 Autoregressive model l
A(z) = 1 + t a[m]z""'  l
excitation white noise variance 0i, it has also been successfully used for high-resolution
spectral estimation. Based on observed data z O ,z 1 .. . z N — 1 . the aramete
are estimated (see Example 7.18), and hence the PSD is estimated as [Kay 1988]
1 + E &[m] exp(—j21rfm) 
The derivation of the CRLB roves to be a diﬁicult task. The interested reader may
consult [Box and Jenkins 1970, Porat and Friedlander 1987] for details. In practice the
as rnptotic CRLB given by (3.34) is quite accurate, even for short data records of about
N = 100 points i1 the poles are not too close to t e unit circle in the z lane. Therefore,
we now determine the asymptotic CRLB. The PSD implied by the AR model is
Pn(f;9)=
|A(r)|’*“
where 0 = a 1 a[2] . . . a[p]crZ]T and A(f) = 1 + Zfn=1a[m]exp(—j21rfm). The partial
derivatives are
6a[k] _ 8a[k]
6 ln P,,(f; 0) __
8a: 02'
F k_12 p.l:1,2,...,pwehavefrom(3-34)
[1(0)],c, = % é !A(1)c)|4[A(f)Q3CP(J2 fk)+A (f)e p< ,2 r
- [A( f) exp(j21r f1) + A*(f) 6x14211111] d)’
% _ 1 ex . Tr _l
=  1A*<1r>2e“’“2"f('“+m*11111112 PL” m“ )1
A2U) exp["j27l'f(k ‘i’ [)1 df'
Noting that
* 1 exp[_7'2ﬂf(k+l)1df = / A<;)e""["2"f(k+lndf
e 1 . : i 1 ex [j21rf(l—k)ldf
/ a‘ “2""”“"“"f /1 114w p
- ' fthe integrand (due to A(‘f) = AW”):
which follows from the Hermitian property 0
we have
exp[j21rf(k ' [)1 df
2 exp[j21rf(k + [)1 df-
2 evaluated at n = k l >
- ' 1 se uences
. ' lution of two anticausa
~ the se uence 1s the convo
0. 1s term 1s zero since
thatis,if f_1{%}:{ gm] gig
then
Therefore,
1110111. = $114k - l1?»
= ~25 l Aw’) exp(]21rfk)df
where again we have used the Hermitian propert of the inte rand and the anticausalit
of .7‘1{1/A*(f)}. Finally, ior k = p + 1;l = p+1
lliallm = 3L, Ttdf = m
so that
FIR-an 0
2a:
where [Rm], = ruli — j] is a p x p Toeplitz autocorrelation matrix and 0 is a p x 1
vector 0i zeros. Upon inverting the Fisher information matrix we have that
vai-(&[k]) 3  [n;;],, k = 1,2,...,p
var(¢;§) 3 2,‘? (3.45)
As an illustration, if p = 1,
var(a[1]) Z N330]
so that
varwill) 2 — (1 - ‘fun
indicating that it is easier to estimate the ﬁlter parameter when |a[1]| is closer to one
than to zero. Since the pole of the ﬁlter is at —a[1], this means that the ﬁlter parameters
of processes with PSDs having sharp peaks are more easily estimated (see also Problem
Qq WYbyg‘  t,   a i ,,. - ,.._-_»_
$3,» *.~",§I-_ ‘gr-iv '-,'>.xv$!l¢j:31<; a‘,
.<-.iiv;.__,.-t,,,.i,,-ﬁv ix€£a§.‘§,g; 1- ~.¢,~w
References
BOX, G.E.P., G.M. Jenkins, Time Series Analysis: Forecasting and Control, Holden-Day, San
Francisco, 1970.
Brockwell, P.J., R.A. Davis, Time Series: Theory and Methods, Springer-Verlag, New York, 1987.
Kay, S.M., Modern Spectral Estimation: Theory and Application, Prentice-Hall, Englewood Cliffs,
Kendall, Sir M., A. Stuart, The Advanced Theory of Statistics, Vol. 2, Macmillan, New York, 1979.
Makhoul, J., “Linear Prediction: A Tutorial Review,” IEEE Proc., Vol. 63, pp. 561-580, April
McAulay, R.J., E.M. Hofstetter, “Barankin Bounds on Parameter Estimation,” IEEE Trans. 1n-
form. Theory, Vol. 17, pp. 669-676, Nov. 1971.
Porat, B., B. Friedlander, “Computation of the Exact Information Matrix of Gaussian Time Series
With Stationary Random Components,” IEEE Trans. AcousL, Speech, Signal Process., Vol.
34, pp. 118-130, Feb. 19st.
Porat, B., B. Friedlander, “The Exact Cramer-Rao Bound for Gaussian Autoregressive Processes,”
IEEE Trans. Aerosp. Electron. Syst, Vol. 23, pp. 537-541, July 1987.
Seidman, L.P., “Performance Limitations and Error Calculations for Parameter Estimation,” Proc.
EE, Vol. 5s, pp. 644-652, May 1970.
Stoica, P., R.L. Moses, B. Friedlander, T. Soderstrom, “Maximum Likelihood Estimation of the
Parameters of Multiple Sinusoids from Noisy Measurements,” IEEE Trans. Acoast., Speech,
Signal Process. Vol. 37, pp. 378—392, March 1989.
Van Trees, H.L., Detection, Estimation, and Modulation Theory, Part I, J . Wiley, New York, 1968.
Ziv, J., M. Zakai, “Some Lower Bounds on Signal Parameter Estimation,” IEEE Tmns. Inform.
Theory, Vol. 15, pp. 386~391, May 1969.
Problems
3.1 If :c[n] for n = O, 1, . . . , N — 1 are IID according to Ll [0, 0], show that the regularity
condition does not hold or that
El l #0 forall9>O.
Hence, the CRLB cannot be applied to this problem.
3.2 In Example 3.1 assume that u/[O] has the PDF p(w[O]) which can now be arbitrary.
Show that the CRLB for A is
(duo)?
var(A) Z / Ldu
Evaluate this for the Laplacian PDF
PWlOl) = if’ exp Qﬂg-Hl)
and compare the result to the Gaussian case.
3.3 The data :c[n] = Ar" + u/[n] for n = 0,1, . . .,N — 1 are observed, where w[n] is
WGN with variance a2 and 1- > O is known. Find the CRLB for A. Show that an
efficient estimator exists and ﬁnd its variance. What happens to the variance as
N --> oo for various values of r?
a
3.4 If :c[n] = r" + wln] for n = O, 1,. . . , N — 1 are observed, where w[n] is WGN with
variance a2 and r is to be estimated, ﬁnd the CRLB. Does an eﬁicient estimator
exist and if so ﬁnd its variance?
3.5 If a:[n] = A+w[n] for n = O, 1,... ,N— l are observed and w = [w[O] w[1] . . . w[N —
l]]T ~ N (0, C), ﬁnd the CRLB for A. Does an eﬁicient estimator exist and if so
what is its variance?
3.6 For Example 2.3 compute the CRLB. Does it agree with the results given?  l
3.7 Prove that in Example 3.4
% Z cos(41rfon + 2¢) z O.
What conditions on f0 are required for this to hold? Hint: Note that
Z cos(om + 5) = Re  expLj(an + 
and use the geometric progression sum formula.
3.8 Repeat the computation of the CRLB for Example 3.3 by using the alternative
expression (3.12).
3.9 We observe two samples of a DC level in correlated Gaussian noise
am = A+wm
where w = [w[O] w[1]]T is zero mean with covariance matrix
The parameter p is the correlation coefﬁcient between w[O] and w[1]. Compute
the CRLB for A and compare it to the case when w[n] is WGN or p = O. Also,
explain what happens when p --> i1. Finally, comment on the additivity property
of the Fisher information for nonindependent observations.
3.10 By using (3.23) prove that the Fisher information matrix is positive semideﬁnite
for all 0. In practice, we assume it to be positive deﬁnite and hence invertible,
although this is not always the case. Consider the data model in Problem 3.3
with the modiﬁcation that r is unknown. Find the Fisher information matrix for
0 = [A r]T. Are there any values of 0 for which [(0) is not positive deﬁnite?
3.11 Fora 2 X 2 Fisher information matrix
which is positive definite, show that
What does this say about estimating a parameter when a second parameter is
either known or unknown? When does equality hold and why?
ac—b7
3.12 Prove that 1
This generalizes the result of Problem 3.11. Additionally, it provides another
lower bound on the variance, although it is usually not attainable. Under what
conditions will the new bound be achieved? Hint: Apply the Cauchy-Schwarz
inequality to e,7\/I(0)\/I—1(0)e,-, where e, is the vectors of all zeros except for a
1 as the ith element. The square root of a positive deﬁnite matrix A is deﬁned
to be the matrix with the same eigenvectors as A but whose eigenvalues are the
square roots of those of A.
3.13 Consider a generalization of the line ﬁtting problem as described in Example 3.7,
termed polynomial or curve ﬁtting. The data model is
for n = O, 1,. . . ,N — 1. As before, u/[n] is WGN with variance o2. It is desired to
estimate {A0, A1, . . . , A,,_1  Find the Fisher information matrix for this problem.
3.14 For the data model in Example 3.11 consider the estimator a2, = (AV, where A
is the sample mean. Assume we observe a given data set in which the realization
of the random variable A is the value A0. Show that A —> A0 as N —> oo by
verifying that
var(A|A = A0)
Hence, ca?‘ —> Ag as N —> oo for the given realization A = A0. Next ﬁnd the
variance of chi as N —-> oo by determining var(A2), where A ~ N (0, aft), and
compare it to the CRLB. Explain why crfl cannot be estimated without error even
3.15 Independent bivariate Gaussian samples {x[0],x[1],...,x[N — 1]} are observed.
Each observation is a 2 >< 1 vector which is distributed as x[n] ~ N (0, C) and
Find the CRLB for the correlation coeﬁicient p. Hint: Use (3.32).
3.16 It is desired to estimate the total power PO of a WSS random process, whose PSD
is given as
P110.) = P0Q(f)
where 1
i Q(f) dr = 1
and Q( f ) is known. If N observations are available, ﬁnd the CRLB for the total
power using the exact form (3.32) as well as the asymptotic approximation (3.34)
and compare.
3.17 If in Example 3.14 the data are observed over the interval 11:: —M, . . . , 0, . . . ,M,
ﬁnd the Fisher information matrix. What is the CRLB for the sinusoidal param-
eters? You can use the same approximations as in the example by assuming M
is large. Compare the results to that of the example.
3.18 Using the results of Example 3.13, determine the best range estimation accuracy
of a sonar if
(t) _ 1- 100|a - 0.01| 0 g t g 0.02
s _ 0 otherwise.
Let N0/2 = 10"6 and c = 1500 m/s.
3.19 A line array of antennas with d = /\ / 2 spacing for a 1 MHz electromagnetic signal
is to be designed to yield a 5° standard deviation at ﬂ = 90°. If the SNR 17 is 0
dB, comment on the feasibility of the requirement. Assume that the line array is
to be mounted on the wing of an aircraft. Use c = 3 x 108 m/ s.
3.20 In Example 3.16 we found the CRLB for the ﬁlter parameter of an AR(1) process.
Assuming a: is known, ﬁnd the asymptotic CRLB for P,,( f ) at a given frequency
by using (3.16) and (3.44). For a[1] = —0.9, 0i = 1, and N = 100, plot the CRLB
versus frequency. Explain your results.
Appendix 3A
Derivation of Scalar Parameter CRLB
In this a endix we derive the C LB for a scalar arameter a = 0 where the
PDF is parameterized by 0. We consider all unbiased estimato ‘ or those for which
15(6) = <1 = 9(9)
f 5111069) dx = amt (BA-l)
Before beginning the derivation we ﬁrst examine the regularity condition
6 ln p(x; 0)]
which is assumed to hold. Note that
= ﬁ p(x;0)dx
Hence, we conclude that the regularity condition will be satisﬁed if the order of differ-
entiation and integration may be interchanged. This is generally true except when the
domain of the PDF for which it is nonzero depends on the unknown parametersuch as
an ro lem 3.1.
Now differentiating both sides of (3A.1) with respect to 0 and interchanging the
partial differentiation and integration produces
f 681K139) d! __ 8g(0)i;’
a0 as”
a bPENDIX 3A.
8 8lnp(x;0) _ _
°' ﬁlnpwtm _ _ 89w) 56 / —T ”(""l“‘ - °
a p(x,0) dx - ———. (3A.3) -
86 80 821nP(X;9) (t 0) + wnpww) ammo] dx z 0
We can modify this using the regularity conditi n to roduc f 602 p ’ 80 80
{(0 — a) p(x;0) ;-—- 9T2)- (3A.4) .V
_E  p(x; 0) dx
[8 lnp(x; 0) 8 ln p(x; 0)
since
/a p(x; 6) dx = 04E  = O.
a0 a0 = EKalg-(gﬁ) 
[fw(x)g(x)h(x)dx] g fw(x)g2(x)dx/w(x)hz(x)dx (3A.5) . us @>
which holds with euality if and only if w x = ch x for c some constant not deendent‘
on x. The functions g and h are arbitrar scalar functions while w _>_ O for all x, _
Now let
Qhjgh is (3.16). If a = 9(0) = 0, we have (3.6).
9(1) = 5 -<1 ~   _-_ Q5, _ a”
m) ‘a‘“1"<*;@>.> - a” °
a0 ‘ $-
l ere c can depend on 0 but not on x. If a = 9(0) = 0, we have
8lnp(x;9) _ _1_
80 _ 0(0)
and apply the Cauchy-Schwarz ine ualit to 3A.4 to produce
 s / (<1 —Q>*p<x;@>dx/  putmdx
(é - @1-jf*
E [(8lnp(x;9))2] = _E [82lnp(x;0)]i 
Now note that
ﬁnally
a0 am ;>
This follows from (3A.2l as
all!  :7
[ p(x; 0) dx = O hich agrees with (3.7).
Appendix 3B
Derivation of Vector Parameter CRLBZ’
In this appendix the CRLB for a vector arameter a =
is c aracterized by 0. We consider unbiased estimators such that
 =ai =  i=112a"'1r' 
is derived. The PDF
so that a1 . 0 a 0
fa.- - a.-)—‘%§§1—)p<x;v>dx = lﬂailgi (313.1)
Now consider for j 75 i
/(d,- — a,) p(x;0)dx
/(d4 —— aifapézgg) dx
Kai /a,~p(x;9)dx
v 8lnp(x; 9)
alE i aej ]
a0,-
aigig
a0,
. (313.2)
Combining (33.1) and (33.2) into matrix form, we have
f“! _ 008111210X, 0) Mme) d! = 022?)?
NOW premultiply by aT and postmultiply by b, where a and b are arbitrar r >< 1 and
x 1 vectors, respective , to ield
fawn“: — a)—————a 11122:; a) Tbp(x; 0) dx = aT biri
Now let
w(x) = p(x;0) w
9(1) = ﬂd-a)”
Mx) = dlniaagxﬂ) a?‘
and apply the Cauchy-Schwarz inequality of (3A.5)
(“Taﬂmblzi w” aw - am - afamx; v) dx
.fbT  bp(x;g)dx
= aTC,-,abTI(0)b”
since as in the scalar case
a1 (x;9)8lnp(x;0) __ 8’lnp(X;9) z __
Since b was arbitrary, let
b = 1-1(0) 82g’) Ta j.
to yield
(aragaflrl<e>%Ta)z 5 8T0.» (aTa'j,fll-l<vi%7¥lTa),
Since I(0 is ositive deﬁnite, so is I'1(9), and ﬁﬂldwﬁiT i5 at 168511 05m“?
semideﬁnite. The term inside the arentheses is there ore nonne ative and we have
0 a a T -
ar< 6 __ 3§(0H-1(9)% )a_>_0. 
Recall that a was arbitrary, so that (3.30) follows. If a = g(0) = 0, then 53%;) = I and
(3.24) follows. The conditions for equality are g(x) = ch(x), Where C iS a COIIStaIIt 110$
dependent on x. This condition becomes
aT(d—a) = c———i-—ng(9x )b
_ 3lnp(x;9)T _1 3g(9)T
._ cT I (9)——80 a.
Since a was arbitrary
Consider the case when a = 510! = 0, so that lag) = I. Then,
5 111P(1;9)
a0
Noting that c may depend on 0, we have
3111 ( H9) _ p [Iwllik
g0: _Z c(0)
= 36 — a)-
= ;1(o)(a - 0),.
(ék — 9k)
and differentiating once more
a2 p a Clwllik)
Finally, we have
[I(9)L~j = —E ’
0(9)
since  = 9k. Clearly, 0(0) = 1 and the condition for equality follows.
Appendix 3C
Derivation of General Gaussian CRLB *
Assume that x ~ N 9 ,C 0 where 0 is the N >< 1 mean ve
the N >< N covariance matrix, both of which depend on 9. Then the PDF is
__i1___ [-1 - a Tc-l 0 - 0 )}"
(My; detﬂcm] BXP 2 (I #( )) ( )(I #( )
We will make use of the following identities
  = tr (c-lunagélf”) (30.1)
where 3C(0)/89k is the N >< N matrix with [i,j element ¢9[C(9)],-j/89k and
8C4“) = _c-1(0)iai)c-1(o). . (30.2)
To establish (3C.1) we ﬁrst note
3lndet[C(9)] _ 1 8det[C(9)]
if‘ ‘ det[C(6)] 60k ' (3%)
Since det[C(0)] depends on all the elements of C(9)
8d qcun] N ” 8det[C(0)] a{c(a)]1,
8det[C(0)] acTw)
"< acm) aak l
where 8det[C(9)]/3C(9) is an N >< N matrix with [i,j] element
3det[C(9)]/3{C(0)],~j and the identity

[ _to('140517191424') ]

has been used Now by t e e n1t1on o t Ae determ1nant b z aiak Z 2mm _ UL(0)L)[C—1(0)L] (arm _ [Mann
d@tlC(9)l = [Cwlli-[Mlr ‘ i ,
 J J  a =  w] — mum.) [KTWWL-j (ﬁlﬁﬁll)
where M is the N >< N cofaictor matrix and j can take on any value from 1 to N. Thus, i} ' m‘ F‘
ﬁdetlcwﬂ I [My + liagbtvm — [Mann]
or adetlcwn _ M  + (— 80k ) [C 1(0)lu($l.7l l#(0)l1)
8°”) _ “  a a ac-wm
It is well known, however, that __ = ‘(x - u(9))TC“(9) 55k) + (x ‘ VMDT 39k (x _ “(an
MT u ¢9u(9)T -1
C (“l detlcwﬂ > 39* ( Xx M ac 1 0
so that ad WW = -2a“6(00) c-1(0)(x - u(0)) + <1 - ma)?" 8,: ls — ma»;
e _ _1 k
i-iacw) - C (0)det[C(0)]. I
Using this in (3C.3) and (3C.4), we have the desired result. I
8lndet[C(0)] _ 1 (CA 9 d C a 60(0))
ask " det[C(0)]tr l l “l ( )7’ aek f
z tr <C_1(0)3C(g)) _ j (30.5)
The second identity (3C.2) is easily established as follows. Consider '  g
Differentiating each element of the matrices and expressing in matrix form, we have
which is equivalent to (3.23), yields
8C(0) 8C'1(0)
0(a) = o
—- + 1 _ acw _ ac 0)
89k 89k men“ = in (c IunTJ) w (C 1(0)?’
which leads to the desired result. 1
  +%tr(C_.(6)aa;k))E<yT 80f a)
—-"-‘i—) = --—“5l-‘—” - -— [<1 - yunfc ‘(aux - men?- . T
60k 2 30k 2 39;, ; 3pm) C_1(0)E[ T]C_1(0)8p(9)
The ﬁrst term has already been evaluated using 3C.1 so that we consider the secon » 39k y 891
term: e _1 _1
“T 53;[<==-@-<v>>’"c-*<v)<xmo)?’ +lE[yT”° l”) T“ ‘mil
where we note that all odd order m0 -
 0- Continuing, we have
lI(9)]u =
where E(yTz) =1; E T f
  [flproratzalgdxﬁliegrlaztlczirjr 1158161302) have been used. To
EWTAYYTBYJ = tr(AC)tr(BC) + zmAcnc)
whereC=E(yyT adA dB . . _
) n an are Symmetric matrices. Thus, this term becomes
étr(ac"(e)c(g)) tr(ac-1(0)C(9))
80k a9!
1 ac-lw) -1
+5“( ask Cwlacaafahxalzl
NeXt, using the relationshi 3C2 this term becomes
(so?)
dfial ' ' .
Appendix 3D
Derivation of Asymptotic CRLB
It can be proven that almost an WSS Gaussian random rocess I n ma be re re-
sented as t e out ut o a causal linear shift invariant ﬁlter driven at the in ut b white
Gaussian noise u[n] [Brockwell and Davis 1987] or
:c[n] = i h[k]u[n - kl»: (3D.1)
where mo] = 1.  fy
/ilnP,,(f)df > -co.
With this representation the PSD of z n is
P"(f) = |H(f)|’"§ '
where a2 is the variance of u n and Ht f} = 2E0 hlk] 6XP(_J'27Tfk) is the ﬁl f
guencx response. If the observations are {rm}, a:[1],. . . ,z[N — 1]} and N is large, then
the representation is approximated by
z h[k]u[n - kf (3112)
This is e uivalent to setting u n = 0 for n < 0. As n —> o0, the approximate repre-
e poorly represented un ess the impulse response h[k] is small for k > n. For large
impulse response length. Since
r,,[k] = a3, i h[n]h[n + k]_ 
the correlation time or effective duration of T“ is the same as the impulse response 1 l 2
ength. ence, ecause t e CRLB to e erive 1S ased on (3D.2 , the asymptotic = g5 1 lUifll df
CRLB will be a good approximation if the data record length is much greater than the “l '5
correlation time.  N /5 |X(f)|2 df
To ﬁnd the PDF ofx we use ]3D.2!, which is a transformation from u = u 0 u 1 ...  N _; g3|H(f)|2
l] 0x M 1x z }" )<~ H“ [i |X(f)l’,1f. ' (3114)
h[N - 1] h[N - 2] h[N - s] mo] 7 i
j ln 0i = / ln oi df
Note that H has a deter ‘ N = 1 and hence is invertible. Since 11 N i‘ _ i 1 PM”) d
(0, Jul), the PDF of x is NULUZHHT) or i — _% n |H(f)|2 f
p x;0 = i-i-wxp [—-— (cruHH ) x ' = lflpuhfldf- lI1|H(f)| df-
( ) mo‘? detﬂagnnT) 2 ’ a a
det(a,2,HHT) = cr§Ndet2(H) = 0i” . it A
Also, f i 1n |H<f>|’dr = / l 111w] + 1n Hm 11f
xT(a§HHT)’1x = -l;(H’1x)T(H'1x) = -L2l£1ll1 2 é
“u " = 2116/ 1nH(f)df
so that 1 1  -%
;0 =-- (-_- T  3113 - = 74 dz
m: > and); exp 20511 u c > 2Re C the) 2m
From (3D.2) we have approximately
XU) =H(f)U(f)
where
2 m [z-l {ln'H(z)}|n___0]
nvhere C’ is the unit circle in the z plane. Since H] z] corresponds to the system function
of a causal ﬁlter it conver es outside a circle of radius 1' < 1 (since 'H(z) is assumed
to exist on the unit circle for the frequency response to exist). Hence, lnﬂgz] also
converges outside a circle of radius r < 1 so that the corres ondin se uence is causal.
By the initial value theorem which is valid for a causal sequence
z[n]exp(—j21rfn)
f; Z_1{ln'H(z)}|,,=0 2 211330 ln’H(z)
HlRlBXPPJZWfH) = 11121310 11(2)
are the Fourier transforms of the truncated sequences. By Parseval’s theorem
an
Therefore,
—  [v] /_§ln|H(f)|2df=-.0
80 APPENDIX an. DERIVATION OF ASYMPTOTIC CRLB APPENDIX 3D- DERIVATION 0F ASYMPTOTIC CRLB
and ﬁnally AS N -—> o0,
<1—  Talk] ~> rmﬂc]
11mg: / lnP“(f)df. (3115)
2 assuming that the ACF dies out sufficiently rapidly. Hence,
Substituting (3D.4) and (3D.5) into 3D.3 roduces for the 10 PDF
Upon taking expectations in §3D.7§, the ﬁrst term is zero: and ﬁnally,
g 1 8Pxz(f)aPzz(f)
2 _%P3,(f> a0,» as,
lnp(l; o) .-. -%ln21r - g /_ i 1nP,,( f) df - é Li lg df;
Hence, the asymptotic log PDF is
a ‘if
lnp(x;0) = —?ln21r — 
ln P,,,(f) +  df. (313.6)
g  81111241‘) 81:11am")
2 _ a0,- a0,-
T0 determine the CRLB
alnpix; 9) z _E /% 1 ___ ‘iwvlxifn? EPHU) df which is (3.34) without the explicit dependence of the PSD on 9 shown.
891- 2 _% Pm(f) Pfx(f) 30¢
1 —’—|X(f>|’ aPmmwaPnui
i P241) Pam a0.- wj f ( l
In taking the expected value we encounter the term E X 2 N . For lar e N this
1s now s own to e I, . Note that X 2 N is termed the eriodo ram s ectral
estimator.
E (Ewan?) = E (N Z  w[m]w[n]e><p[—121rf(m— m)
= N 1',,[m — n] exp[—j21rf(m — 11)]
= (1 ——  rmUc] exp(—j21rfk) (3D.8)
k=-(N—1)
where we have used the identity
Chapter 4
Linear Models
4.1 Introduction
The determination of the MVU estimator is in eneml a difficult task. It is fortunate,
however, that a large number of signal processing estimation problems can be repre-
sented b a data th t allows us to easil de ermine this stimator. This class of
models is the linear model. Not only is the MVU estimator immediately evident once
the linear model has been identiﬁed, but in addition, the statistical performance follows
naturally. The key; then, to ﬁnding the optimal estimator is in structuring the problem
4.2 Summary?’
 . When this data model can be assumed, the MVU
an also e cient) estimator is given by (4.9), and the covariance matrix by (4.10).
arbitrary covariance matrix, as o osed to 021 for the linear model. The MVU (and also
eiiicienti estimator for this model is given by (4.25), and its corresponding covariance
matrix by (4.26). A ﬁnal extension allows for known si nal components in the data to
yield the MVU (an also e cient estimator of 4.31 . The covariance matrix is the
same as for t e general linear model.
4.3 Deﬁnition and Properties
The linear model has already been encountered in the line ﬁtting problem discussed in
Example 3.7. Recall that the problem was to ﬁt a strai t ine t rou h noise corru ted
data. Ks our model of the data we chose
where w[n] is WGN and the slope B and intercept A were to be estimated. In matrix
x=H0+vr° (4-1)
where
[210] a:[1] . ..a:[N — INT
and
The matrix H is a known matrix of dimension N >< 2 and is referred to as the observatior?
 ata x are observed after 0 is operated upon by H. Note also that the
noise vector has the statistical characterization w ~ N (0, 021? The data model in
4.1 is termed the linear mode?  e
vector ' ssian althou h other authors use the term more generally for any noise
111E [Graybill 1976]. _
As discussed in Chapter 3, it is sometimes possible to determine the MVU estimator
if t f the CRLB theorem are satisﬁed. From Theorem 3.2,
0 = g(x) will be the MVU estimator if
6lnp(x;0)
for some function g. Furthermore, the covariance matrix of é will be I'1(0). To
determine ii this condition is satisﬁed lor the linear model of (4.1), we have
=1<v>a<==> -v>  (H)
81 (x; 9) 8 _ 1
lg? = a? —ln(21ra2)g - We - new; - H0)
Using the identities
696:” = 2A0 (4.3)
for A a symmetric matrix, we have
5lnp(x;0) _
a9 _ 315m’; - nTnai;
Assuming that HTH is invertible
5lnp(x; 0) _ HTH
60 a2 [(H H) H x 0k (4.4)
which is exactly in the form of (4.2) with
é = (nTm-‘HT; “ (4.5)
1(0) = U2 . _ (4.6)
Hence, the MVU estimator o ' ‘ b 4.5 and its covariance matrix is
c, = I"‘(0) = ﬂnTln-ll‘ (4.7)
Additionally, the MVU estimator for the linear model is efficient in th
the CRLB. e reader may now verify the result of (3.29) by substituting H for the
line ﬁtting problem into (4.4).  
invertibility of HTH. For the line ﬁtting example a direct c cu ation will verify that the
inverse exists (compute the determinant of the matrix given in (3.29)). Alternatively,
this follows from the linea.r independence of the columns of H (see Problem 4.2). If the
columns of H are not linearly independent, as for example,
and x = [22 . . . 2]T so that x lies in the range space of H, then even in the absence of
Fifi»: the model parameters will not be identi able. For then
and for this choice of H we will have for z[n] _
As illustrated in Figure 4.1 it is clear that an inﬁnite number of choices can be made for
A and B that will result in the same observations or iven a noiseless x, 0 is not uni ue.
e si ua ion can ar y ope to improve when the observations are corrupted by noise.
Although rarely occurring in practice, this degeneracy sometimes nearly occurs when
H H is ill-conditioned (see Problem 4.3).
e previous discussion, although illustrated by the line ﬁtting example, is com-
pletely general, as summarized by the following theorem.
Theorem 4.1 (Minimum Variance Unbiased Estimator for the Linear Model) i
If the data observed can be modeled as
x = H0 + w (4.8)
All 9 on this
line produce the same
observations
Figure 4.1 Nonidentiﬁability of linear model parameters
when: x is an N >< 1 vector o observations H is a known N >< observation matrix
with N > p and rank p, 9 is a p >< 1 vector o rameters to be estimated and w is
an ‘ >< 1 1107.86 vector wt ' I H1163] then the M VU estimat 's
é = (nTnrlnTx (4.9)
and the covariance matrix of é is
c; = maﬁa)“.
For the linear model the M VU estimator is efficient in that it attains the CRLB.
That é is unbiased easily follows by substituting (4.8) into (4.9). Also, the statistical
performance of 0 is completely speci ed (not just the mean and covariance) because 0
is a znear rans ormation o a Gaussian vector x and hence
é ~ N(o,@’(nTn)-1). (4.11)
allows us t0 determine
The Gaussian nature of the MVU estimator for the linear model
tlie exact statistical performance if desired lsee Problem 11$. ln the next section we
present some examples illustrating the use of the linear model.
(4.10)
4.4 Linear Model Examples»
of line ﬁtting is easily handled once we recog-
We have already seen how the problem
ension is to the problem of ﬁtting a curve to
nize it as a linear model. A simple ext
experimental data.
Example 4.1 - Curve Fitting
eek to determine an em irical relationshi between
In many experimental situations we s
Figure 4.2 we present the results of a experiment
a pair o varia es. For instance, in
Voltage
;' 9 <- Measured voltage, 1Q")
‘fm Hypothesized relationship, s(t)
Time, t
t0 i1 f2 ta tN_1
Figure 4.2 Experimental data
in which ‘mltage measurements are taken at the time instants t — t t t B
plotting the measurements it appears that the underlying volta e nil; 1i;- i l’ 1:5 .y
function of time. That the points do not lie exactly on a curv ‘g i;  e a qu ran-c
mental error or noise. Hence, a reasonable model for the dataeiss a tn uted to expen-
$(t11)=91 n=0,1,___’N_1_
To avail ourselves of the utility of the linear model we assume that (t ) I11)
Gaussi d ' ' - w " are
l an  om variables with zero mean and variance a2 or that they are WGN
Samp 6S. en, we have the usual linear model farm
where
x = l$(t0)$(t1)~--$(t1v-1)]T
and
1 to tg
In general’ if we Seek t0 ﬁt a (P " llst-Qrder Polynomial to experimental data we will
have
W") =91 +92tn+---+0,,¢£,"1 +w(t,,) n=0,1,...,1v_1_
The MVU estimator for 0 = [01 02 . . . 0p]T follows from (4.9) as
é = (HTH)"HTX
.¢lhh)‘)hlﬂldahzlnihviktlnhlrmt--\...
Note that H has dimension N >< 2M where p = 2 M Hence for H to Satisfy N >
’ ' v p we
require < N 2. n eterminin the MV estimator we can s1m if the computations
by noting that the columns of H are orthogonal. Let H be represented in column form
where
x = [a:(t@) r(t1) . . .x(tN_1)]T
1 to ... ti,” as
H ___  1‘ (Nxp) H—-[h1h2...h2M]
' ‘ p'_1 where m denotes the ith column of H. Then, it follows that
The observation matrix for this example has the special form of a Vandermonde matrix. h‘ h? _ 0 f“ 1 9e J-
Note that the resultant curve ﬁt is This property is quite useful in that
where s(t) denotes the underlying curve or signal. <> hgnM
Example 4.2 - Fourier Analysis — 3 . __ _
Many signals exhibit cyclical behavior. It is common ractice to determine the presence
of stron c c ic com onen s em oyin aFourier anal sis. Large Fourier coefficients
are indicative of strong components. In this example we show that a Fourier analysis
is really just an estimation of the linear model parameters. Consider a data model
consisting of sinusoids in white Gaussian noise:
a:[n]=Zakcos(—AT-)+Zbksin( N )+w[n]~ n=0,1,...,N—1 (4.12)
where w[n is WGN. The freguencies are assumed to be harmonically related or multi-
ples o the fundamental f1 = 1/N as fk = k/N f“ The amplitudes ahbk of the cosines
and sines are to be estimated. To reformulate the pro em in erms o t e inear model
becomes a dia onal matrix which is easily in e ted Th ‘
results from the discrete Fourier transform  reIatiiIiJsrhi Z ti??? fit? 60135128
we let  (413)
0=[(l1(lg...(lMb1bg...bM]T 
and  in outline of the orthogonality proof is given in Problem 4.5. Usin this ro ert , we
H 2  ave N
1 1 0 0 f) a 0 M
COS (i?) c“ (17+) Si“ (it) Si“ (Li?) 11TH =° . 'f’ '_" _ = 
cos [_i__-_1z] cos [My] S1,, [h -1 ] s“, [Zﬂiﬁltn] 0 0 %
so that the MVU estimator of the amplitudes is
lillhKcnhu-i-xaw-t \ M.’
0 _ (HTHYlHTx
or ﬁnally,
’ (4.14)
te Fourier transform coefficients. From the properties
These are recognized as the discre
of the linear model we can immediately conclude that the means are
E(b,,) = bk?» (4.15)
and the covariance matrix is
C, = U2(HTH)_1
= a 
= 3121. (4.1s)
Because é is Gaussian and the covariance matrix is dia onal the am litude estimates
are in ependent (see Problem 4.6 for an application to sinusoidal detection).
It 1s seen from this example that a key ingredient in simplifying the computation of
the MVU estimator and its covariance matrix is the orthogonality of the columns of H.
Note that this property does not hold if the frequencies are arbitrarily chosen. <>
Example 4.3 - System Identiﬁcation
e able to identify the model of a s stem from in ut out ut
ta eddela line TDL) or ﬁnite impulse res onse FIR
' is provided to “probe” the
system. Ideally, at the output the sequence Zf_____u hlk uln — kl is observed from which
(a) Tapped delay line
(b) Model for noise-corrupted output data
Figure 4.3 System identiﬁcation model
we would like t t' t th ‘ - .
the ten 152x113; (:36 hiigilrwﬁklihﬁlslth ktﬂor equivalently, the impulse response of
in Figure 43b is mare péropriate Assumerlikilaés corrupted  30:86, so that the model
- unisprovie 0rn=(),1,___,N_1
and that the output is observed over the same interval. We then have
W ere 1t 1s assumed that u n - 0 for n < 0. In matrix form we have
(4.11)
(4.1s)
Assuming w[n] is WGN, (4.18) is in the form
‘Murat.
estimator of the impulse response iS
é z (HTH)-_lHTX.
The covariance matrix is
It is now shown that the signal should
[MacWilliams and Sloane 1976]. Since t
where e1 = [00...010...0]
tored as DTD with 1) an invertible p >< p matrix, we ca
fac
Co; = 0'2(HTH)_1.
A ke uestion in system identiﬁcation is how to choose the robing si Hal 1t Ti ~
be c A
he variance of 91' is
var(0i,-) = eﬂircgei
T with the 1 occupying the ith 913136, and C5‘
n use the Cauchy-SchWafZ
inequality as follows. Noting that
we can let £1 = Dee and £2
1 = (GTDTDT-leilz)
-_— DT-lei to yield the inequality
(s? e212 s 1:211:95»
Because ﬁlTﬁz = 1, We have
or ﬁnally
constant or
or, equivalently, the conditions for all the varianc
Noting that
we have
<e?nTne1><e?D"‘DT“ei>
(e?C;‘e1~)(e?C@e@-)
De,‘ z (HDT- e1
0T1) = c; = a2
e,- = Ciei-
es to be minimized are
of the linear model, and so the MVU
can be
If we combine these equations in matrix form, then the conditions for achieving the
minimum possible variances are
nTHza-z 0 C2  0
It is now clear that to minimize the variance of the MVQ estimator, u n should be
chosen to make HTH diagonal. Since [HLJ- =  — j]
[HTHLj = 21411-111411 —j] 1= 1,2,...,p;j = 1,2,...,p (4.19)
and for N large we have (see Problem 4.7)
iiliiliil" + Ii — ill (420)
which can be recognized as a correlation function of the deterministic sequence u[n].
where
may be viewed as an autocorrelation function of u n]. For HTH to be diagonal we
require
which is approximately realized if we use a PRN se uence as our input signal. Finally,
under these conditions H7 H = N13“, [O]I, and hence
var(h[i])———L- i-O 1 p 1 (421)
Nrmiol/az , ,..., .
and the TDL weight estimators are independent.
As a ﬁnal consequence of choosing a PRN sequence, we obtain as our MVU estimator
é = (HTm-‘HTX
l '-i§m.dﬁa.i‘£aaqi
where HTH = N Tuu [011 Hence, we have
ma] =  who] i 14” - Mn]
= undo] 5 (4.22)
since u[n] = 0 for n < 0. The numerator in (4.22) is just the crosscorrelation r“, 
between the input and out ut se uences. Hence, if a PRN input is used to identi the
system, then the approximate (for large N) MVU estimator is
i»[i1="“l'l i=0,1,...,p-1_. » (4.23)
where
See also Problem 4.8 for a spectral interpretation of the system identiﬁcation problem.
4.5 Extension to the Linear Model l
al form of the linear model allows for noise that is not white. The general
A more gener
linear model assumes that
w ~ N (0, C’
where C is not necessarily a scaled identity matrix. To determine the MVU estimator,
we can repeat t e erivation 1n ection 4.3 (see Problem 4.9). Klternatively, we can
use a whitenin a roach as follows. Since C is assumed to be positive deﬁnite, C“ is
positive deﬁnite and so can be factored as
0-‘ = DTD’ (4.24)
where D is an N >< N invertible matrix. The matrix D acts as a whitening transformation
w en app 1e to w since
E [(Dw)(1>w>'-'>1
As a consequence, if we transform our eneralized model
the noise will be whitened since w’ = Dw ~ A/(g I) and the usual lineal, model win
result. The MVU estimator of 0 is, from (4.9),
é = (H’TH’)"H’Tx'
— (HTDTDHYIHTDTD:
so that
9 = (HTC"‘H>-‘HT¢-‘==- r (4.25)
c, = (n"n')-1
or ﬁnally
C6 = (H’¢-‘H)"‘f‘ (4.26)
is iﬁﬁlsltrfzéé by a; ggzvrigllive our previous results. The use of the general linear model
Example 4.4 - DC Level in Colored Noise 1X
We now extend Example 3.3 to the colored noise case If z[n] — A + w[n] for
07 l) 1 n a # l i ' ‘ . i u n :-
  itlzis.421.:iszfsffhiasiri;"“sr  N X l! comes  C»
. a, W1 = _—_ '
of the DC level is [1 1 I l ' l] ’ the MVU estlmator
A = (HTC-1H)—1HTC-1x
and the variance is, from (4.26),
 = (HTC~1H)—1
VU estimator the sample mean with a variance of a2 /N-
of the MW estimator follows FY rmdei‘? $3.138?)
ted previously that D 1S a whitening ma r .
1f C = JZI, we have as Ollf M
An interesting interpretation
ization of C“ as DTD. We no
estimator is expressed as
(Eli
z N21‘ dnﬂn] (4.21)
. _ 7) the data are ﬁrst prewhitened t0 _
where d" =[D1]n/1TDTD1. according to (4 2 , _ . ht d ' The preyvhitenlng
aQ/[a] and then “averagetlisﬁusing grtellllliltizrlire; tiiHYEIEIiECZSIEf tie noises at each obsep
has the effect of decorre a m5 an
vation time before averaging (see Problems 4.10 and 4 1 ) o
hat are known.
_ . f ' nal com onents t
Another extenslon to the hnear model alglhlaithredlign the data. Then, a linear model
Assume t at s re resents a nown s1 na
To determine the MVU estimator let x X 5, 5° a
. ' he MVU estimator follows a5
 is now 1n the fOfIn Of the linear InOdeL  
é = (HTHPHTOK _ s) > (4.2s)
with covariance (429)
C; = oﬂnTnl-l- a
E ample 4 5 DC Level and Exponential in White NOisQW
_ _ ' 1m , A is to be estimated»
HIM] =A+Tn+w[n] forn -0,1,...,N 1, where r is own
and w[n] is WGN» the model is
where s = [1 r . . . rN'1]T. The MVU estimator, is from (4.28),
with variance, from (4.29), as
It should be clear that the two extensions described can be combined to produce the
general linear model summarized by the following theorem.
Theorern 4.2 (Minimum Variance Unbiased Estimator for General Linear
Model)‘5"7f the data can be modeled as
x = H0 + s + w *3‘ (4.30)
where x is an N X 1 vector of observations, H is a known N X Q observation matrix
(N > of rank 0 is a >< 1 vector of parameters to be estimated, s is an N >< 1
vector o known si nal sam les w is an >< 1 noise vector wit PDF 0, C),
then the M VU estimator is
é = (nTc-lnrlnTc-‘(x - s)  (4.31)
c, = (nTc-‘nrl. i’ (4.32)
For the general linear model the M VU estimator is eﬁcient in that it attains the CRLB.
This theorem is quite powerful in practice since many signal processing problems can
be modeled by (4.30).
References
Graybill, F.A.,Theo1~y and Application of the Linear Model, Duxbury Press, North Scituate, Mass.,
MacWilliams, F.J., NJ. Sloane, “Pseudo-Random Sequences and Arrays,” Proc. IEEE, Vol. 64,
pp. 1715—1729, Dec. 1976.
Problems
4.1 We wish to estimate the amplitudes of exponentials in noise. The observed data
are
uvegkiﬁlﬁri.» kill,‘ -&Ag(i“r‘wilﬁ\h
2. Find the MVU estimator of the amplitudes
where w[n] is WGN with variance a its for the case when p z 2J1 _
and also their covariance. Evaluate your r8811
1J2 _—_ -1, and N is even.
4 2 Prove that the inverse of HTH exists if and only if the columns of 111 are linearly
independent. Hint; The problem 1s eqqvalent t0 Provmg ltllat E1 EH11: 23318111‘?
'ble if and only if the columns are inear y 1 P ~
4.3 Consider the observation matrix
deﬁnite and hence inverti
1 1 + e
where e is small. Compute  1 and examine whaitl happelljsaesi 6*
x = [2 2 2]T, ﬁnd the MVU estimator and describe what appens ) -
4.4 In the linear model it is desired to estimate the signal s H0. If an MVU es ima 0r
of 0 is found, then the signal may be estimated as s - H0. What is the P o
s? Apply your results to the linear model in Example 4.2.
4.5 Prove that for k,l = 1,2,---,M < N/2
Z¢0s( N)cos N —2 u
by using the trigonometric identity
coswl cos w; = écosﬁul + we) + 5 cos(w1 - W2)
and noting that
2 cos an = Re  BXPUYITQ) -
4 6 Assume that in Example 4.2 we have a single slnus°ldal component at fk : k/N'
The model as given by (4.12) is
a:[n] = ak cos(21rfkn) + bk SlIIQFfk") + wlnl
Using the identity A cos w+B sinw = V A’ + B’ cos(w—¢), where ¢ = arctan(B/A),
we can rewrite the model as
1m]: (/a{+ bi cos(21rfkn -— Q5) + 10W]-
bk so that the estimated power of the sinusoid
An MVU estimator is used for ak,
A measure of detectability is Ez(l5)/var(l5). Compare the measure when a sinu-
soid is present to the case when only noise is present or ak = bk = 0. Could you
use the estimated power to decide if a signal is present?
4.7 Verify (4.20) by letting the limits of the summation in (4.19) be n = —oo to n = oo
and noting that u[n] = 0 for n < 0 and n > N — 1.
4.8 We wish to estimate the frequency response H ( f ) of a causal linear system based
on the observed input and output processes. The input process u[n] is assumed
to be a WSS random process, as is the output a:[n]. Prove that the cross~power
spectral density between the input and output is
PtAf) = H(f)P...i(f)
where Pu,( f) = f{7'ux[k]} and ru,[k] = E(u[n]a:[n +  If the input process
is white noise with PSD Pu,,( f ) = a2, propose an estimator for the frequency
response. Could you estimate the frequency response if P,m( f ) were zero for a
band of frequencies? Can you relate your estimator to (4.23) for the case when
the linear system is a TDL?
4.9 Derive (4.25) and (4.26) by repeating the derivation in Section 4.3.
4.10 If in Example 4.4 the noise samples are uncorrelated but of unequal variance or
C = diag(a§, of, . . . , U)2v_1)
ﬁnd d" and interpret the results. What would happen to A if a single 0i were
equal to zero?
4.11 Assume the data model described in Problem 3.9. Find the MVU estimator of
A and its variance. Why is the estimator independent of p? What happens if
4.12 In this problem we investigate the estimation of a linear function of 0. Letting
this new parameter be a = A0, where A is a known r >< p matrix with r < p and
rank r, show that the MVU estimator is
a=Aé
where 9 is the MVU estimator of 0. Also, ﬁnd the covariance matrix. Hint:
Replace x by
x’ = A(HTH)'1HTx
where x’ is r >< 1. This results in the reduced linear model. It can be shown that
x’ contains all the information about 0 so that we may base our estimator on this
lower dimensionality data set [Graybill 1976].
“linear model” x = H0 + W but with
- ‘ t r the
4'13 In prance we Sometlmes encoun e gnore this difference and use our
H composed of random variables. Suppose W8 i
usual estimator é z (HTHYIHTX,
ealization of H is known to us. Show that
where we assume that the partiﬁlllaf T
d covariance of 0 are
if H and w are independent, the mean an
C9 = 625s [(HTH)"l
where E H denotes the expectation with respect to the PDF of H at aPPenS
if the independence assumption is not made‘?
  we  a   1.5 r1:"§;".*?.‘:::11%~izzsrzzeiiaimzs.
the fading process as resulting in a sigriv, vaGN or xM ___ A + Mn] for n =
simple illustration, consider the DC leve 111
0, 1, .. . , N — 1. When the signal fades, the data model becomes
. . - _ A m'n we know when the signal has
when? théetfnohscfelhlilseotflh: fzgiflt: 06f Prosljlleml 4g13 to determine an estimatfll‘ 0f
experienc a , '
A and also its variance. Compare your results to the case of no fading.
Chapter 5
General Minimum Variance Unbiased
Estimation
5.1 Introduction
We have seen that the evaluation of the CRLB sometimes results in an efficient and
hence MVU estimator. In articular, the linear model rovides a useful exam le of
this approach. If, however, an efficient estimator does not exist, it is still of interest
to be ahle to ﬁnd the MVU estimator (assuming of course that it exists). To do so
requires the concept of sufficient statistics and the important Rao-Blackwell-Lehmann-
Scheffe theorem. Krmed with this theory it is possible in many cases to determine the
MVU estimator by a simple inspection of the PDF. Flow this is done is explored in this
chapter.
5.2 Summary ‘
In Section 5.3 we deﬁne a sufficient statistic as one which summarizes the data in the
sense that if we are given the sufficient statistic, the PDF of the data no longer de ends
on the unknown arameter. The Neyman-Fisher factorization theorem lsee Theorem
5.1l enables us to easily ﬁnd sufficient statistics by examining the PDF. Once the
sufficient statistic has been found, the MVU estimator ma be determined by ﬁnding
a function of the sufficient statistic that is an unbiased estimator. The essence of the
approach is summarized in Theorem 5.5 and is known as the Rao-Blackwell-Lehmann-
Scheffe theorem. In using this theorem we must also ensure that the sufficient statistic
is com lete or that there exists only one function of it that is an unbiased estimator.
The extension to the vector parameter case lS given in Iheorems 5.3 and 5.4. The
approach is basically the same as for a scalar parameter. We must ﬁrst ﬁnd as many
sufficient statistics as unknown parameters and then determine an unbiased estimator
based on the sufficient statistics.
_Zi§lilaaInhibits”;ixwaismiai-iamiuiu-isapsas.’
5.3 Sufficient Statistics
In a previous chapter we found that for the problem of estimating a DC level A lg
___l:I_ see Example 3.3 , the samp e mean
A = F 7;] a:[n] .
was the MVU estimator, having minimum variance az/N. if, on the other hand, Le
had chosen
A = a:[0]
as our estimator, it is immediately clear that even though A is unbiased, its variance is
much larger (being a2) than the minimum. Intuitively, the poor performance is a direct
result of discarding the data oints {z[l],a:[2], . . . ,a:[N — 1]} which carr information
about A. reasona e question to as is ic a a sam les are ertinent to the
estimation problem? or Is there a set of data that is sufficient?. The following data
sets may be claimed to be sufficient in that they may 5e used to compute A.
S1 = {x[0],2[1],...,a:[N—1]}
S; = {x[0] +x[l],x[2],x[3], . . . ,a:[N — 1]}
S3 - {$311.1}.
S; represents the original data set, which as expected, is always sufficient for the prob-
lem. S2 and S3 are also sufficient. It is obvious that for this roblem there are man
sufficient data sets. The data set that contains the least number of elements is called
the minimal ona If we now t in o the elements of these sets as statistics, we say
that the N statistics of 1 are su cien as we as t e statistics of S2 an
the single statistic of S3. This latter statistic, jug,‘ a:[n], in addition to being a suf-
ﬁcient statistic, is the minimal suﬂicient statistic? For estimation of A, once we know
25:“; a:[n], we no longer need the individual data values since all information has been
sum arize in t e su cient statistic. o quanti what we mean by this, consider the
DF of the data
110K; A) = ————-@XP [— 2 Zfwlﬂl — A) ] (51)
(211172 ) % i? “=0
and assume that T(x) = L‘; a:[n] = To has been observed. Knowledge of the value of
this statistic will change the PDF to the conditional one p(x} 25:"; a:[n] = To; A), which
e su cien s a is ic as een o serve .
now ives the PDF of the observations aifer EH FE i t t t H 5 5 d
Since the statistic is sufficient for the estimation of A, this conditional PDF should not
P<x=xo Za:[n]=T0;A) P<x=x0 Z¢[n]=TO;A)
(a) Observations provide information after (b) No information from observations after
Tfx) °b59FV9d—T(X) is not sufficient T(x) observed—T(x) is sufficient
Figure 5.1 Sufficient statistic deﬁnition
depend on A. If it did, then we could infer some additional information about A from
the data in ad ition to that alread r vided b the sufficient statistic As an examp e
111 F1811" 51a, 1f X -— X0 for an arbitrary! 31?, then values of A near A0 would be more
likely. _Th1s violates our notion that 27,20 a:[n] is a sufficient statistic. On the other
an 7 111 E1811"! 515, any value of A is as liliely as any other, so that after observing
T(x) the data may be discarded. Hence, to verify that a statistic is sufficient we need
to determine the conditional PDF and conﬁrm that there is no dependence on A.
Example 5.1 - Veriﬁcation of a Sufficient Statistic
Consider the PDF 0f (5-1)- T0 PYOVB that 25:’; x[n] is a sufficient statistic we need
to determine p(x|T(x) = T ;A ’ h T = N-l - -
Conditional PDF we have o ) W 6Y6 (X) 2nd, a:[n]. By the deﬁnition of the
p(x7  : T0; 
P(X|T(X)=T@;A)= P(T(X)=T_A) .
But note that T(x) is functionally dependent on x, so that the joint PDF p(x, T(x) =
TO7A) takes on nonzero values only when x satisﬁes T(x) = To. The joint PDF is
gf/fgefore Pfx; A)5(_T(X) _— To), where 6 is the Dirac delta function (see also Appendix
or a further discussion). Thus, we have that
"Klrfrryjlgafféhlggaglﬂyﬁ)i|i'.'a§'ﬁlﬂ.§Eiﬂ\iIq
Clearly, T(X) ~ A/(NA, N02), so that
PUG A)5(T(X) - To)
= ———~—e><i> “Tr-Zhllﬂl-A) 5(T(X) 0)
: _-_--e)(p ——- 2 III [TL] —    T0)
(27Ta2)¥ 20-2 n=0
= ii exp -——1—  Izlnl "  +   _ 
(27razl-g 20-2 n=0
From (5.2) we have
p(x|T(x) = ToiA) ’
1 lfzflzqn] ex [__1-(-2AT0 + NAQ]
(21rJ2)% exp _ 2'72 =0 p 2G2
exp ‘i- 2Na2 (T0 — NAP]
x/ 21rNa2
=  exp  i; flail exp [WW] we) - To)
6(T(x) — To)
(21ra2)_i"
which as claimed does not depend on A. Therefore, we can conclude that 25;; r[n]
is a sufficient statistic for the estimation of A. o
This example indicates the procedure for verifying that a SliatlSlilC-IS sufficient. For
many problems the task of evaluating the conditional PDF 1S formidable,N s_o1 that an
easier approach is needed. Additionally, in Example 5.1 the Cl101C8 of End,  for
examination as a sufficient statistic was fortuitous. In general an even more difficult
problem would be to identify potential sufficient statistics. The approach of guessing at
a sufficient statistic and then verifying it is, of course, quite unsatisfactory in pkractice.
To alleviate the guesswork we can employ the Neyman-Fisher factoriaation t eorem,
which is a simple “turn-the-cran ” procedure for ﬁnding sufficient statistics.
5.4 Finding Sufficient Statistics
The Neyman-Fisher factorization theorem is now stated, after which we will use it to
ﬁnd sufficient statistics in several examples.
Theorem 5.1 (Neyman-Fisher Factorization) If we can factor the PDF p(x; 0) as
P(I; 9) = 9(T(X)» 9)h(X) (53)
where g is a function depending on x only through T(x) and h is a function depending
only on x, then T(x) is a sufficient statistic for 0. Conversely, if T(x) is a sufficient
statistic for 0, then the PDF can be factored as in (5.3).
A proof of this theorem is contained in Appendix 5A. It should be mentioned that at
times it is not obvious if the PDF can be factored in the required form. If this is the
case, then a sufficient statistic may not exist. Some examples are now given to illustrate
the use of this powerful theorem.
Example 5.2 - DC Level in WGN
We now reexamine the problem discussed in the previous section. There the PDF
was given by (5.1), where we note that a2 is assumed known. To demonstrate that a
factorization exists we observe that the exponent of the PDF may be rewritten as
Z (x[n] — A)2 = Z z2[n] — 2A Z r[n] + NA2
so that the PDF is factorable as
p(x; A) =   exp [-535 (NAz — 2A g  exp IE5}; “=0 x2[n]] .
9(T(X), A) MK)
Clearly then, T(x) = YIN-lain] is a sufficient statistic for A. Note that T'(x) =
225:“; a:[n] is also a sufficient statistic for A, and in fact any one-to-one function of
25:11am] is a suﬂicient statistic (see Problem 5.12). Hence, sufficient statistics are
unique only to within one-to-one transformations. <>
Example 5.3 - Power of WGN
Now consider the PDF of (5.1) with A = 0 and a2 as the unknown parameter. Then,
p(X;O'2) =  exp [~1- 531.19g] - 1
Q(T(X), <12) hf‘)
Again it is immediately obvious from the factorization theorem that T(x) = EL‘; 2:2 [n]
is a sufficient statistic for a2. See also Problem 5.1. <>
,ij)ib_i‘iﬁegmlkiniqi-wmw-i-i
Example 5.4 - Phase of Sinusoid
_ . . - ' h f ' Soid
Recall the problem in Example 3.4 1n which we w1sh to estimate the P 358 ° a Sm“
embedded in WGN 0r
q;[n] = Acos(21rf0n + r15) + winl
of the sinusoid are known, as is the noise
n—_-(),1,...,N—l.
Here, the amplitude A and frequency f0
variance a2. The PDF is
p“; (p) ___ __1__I_v_ exp {__l-   — A cos(21rf0n + qgnz} .
The exponent may be expanded as
2 a [n1 — 2A Y. win] cosfhfo" + ¢> + 2 A  Wfo" + l’)
= Ni $2 [n1 - 2A  In arrow) cow
+ 2A  z-[n] sin 21mm) sin ¢ + Nil A2 Coszfwfv" + ‘bl
at the PDF is factorable as required by the
In this example it does not appear th _ _ _ _ _
le sufficient statistic exists. However, it Can be
Neyman-Fisher theorem. Hence, no sing
factored as
Pfx; a5) =
  exP {—é%; U; A2 cosf (21rfon + 45) _ 2471(1) 99S <15 + 2A1‘? (x) Sin ‘bl }
Q(T1(X), T2 (xla 
~exp LET; $2
where
T1 (x) -_-. E z[n] cos 21rf0n
T2(;) = E E[n] sin 21rf0n.
By a slight generalization of the Neyman-Fisher theorem we can conclude that T1 (x)
and T2(x) are jointly sufficient statistics for the estimation of ¢>. However, no single
sufficient statistic exists. The reason why we wish to restrict our attention to single
sufficient statistics will become clear in the next section. <>
The concept of jointly sufficient statistics is a simple extension of our previous deﬁnition.
The r statistics T1(x), T2(x), . . . , T,(x) are jointly sufficient statistics if the conditional
PDF p(x|T1(x),T2(x), . . .,T,(x);9) does not depend on 9. The generalization of the
Neyman-Fisher theorem asserts that if p(x; 9) can be factored as [Kendall and Stuart
P(X;9) = QLTAXLTAX). - - '1TT(x)!0)h(x) (5-4)
then {T1(x),T2(x), . . . ,T,(x)} are sufficient statistics for 0. It is clear then that the
original data are always sufficient statistics since we can let 1' = N and
T,,+1(x)=z[n] n=0,1,...,N—1
so that
and (5.4) holds identically. Of course, they are seldom the minimal set of sufficient
statistics.
5.5 Using Sufﬁciency to Find the MVU Estimator
Assuming that we have been able to find a sufficient statistic T(x) for 9, we can make use
of the Rao-Blackwell-Lehmann-Scheffe (RBLS) theorem to find the MVU estimator. We
will ﬁrst illustrate the approach with an example and then state the theorem formally.
Example 5.5 - DC Level in WGN
We will continue Example 5.2. Although we already know that A = :2 is the MVU
estimator (since it is efficient), we will use the RBLS theorem, which can be used even
when an efficient estimator does not exist and hence the CRLB method is no longer
viable. The procedure for ﬁnding the MVU estimator A may be implemented in two
different ways. They are both based on the sufficient statistic T(x) = 212:0‘ a:[n].
1. Find any unbiased estimator of A, say Av: a:[0], and determine A = E(A]T). The
expectation is taken with respect to p(A|T).
2. Find some function g so that A = g(T) is an unbiased estimator of A.
Eor the ﬁrst approach we can let the unbiased estimator be A = a:[0] and determine
A = E(a:[0]| ZNA  To do so we will need some properties of the conditional
s is). )_ -».».> y) 2 g,._i_i,___iﬁhggg_kgalnmlamamnawnj 1FPIPFFPPFPWPPWPFVFVVFPPI"???
108 T Class of unbiased
Gaussian PDF. For l1; 111T a Gaussian random vector with mean vector H = lE (-75) E (yll Variance / estimawrs (all 9)
and covariance matrix C z [ varw) covhny) ] ,
cov(y, 2:) Vafly)
it may be shown that (see Appendix 10A) Not possible Effeet of
since variance
has increased
conditional
expectation
E(a:ly) = f” Immune
operation
: [co $13k?  dx é = MVU estimator
_°° p y (I ) Figure 5.2 RBLS argument for MVU estimator
= E(a:) + -“—”’—o - Ea»- <55)
vady)
is an unbiased estimator of A. By inspection this is g(a:) = x/N, which yields
N“ n] and note that N
Applying this result, we let a: = a:[0] and 1U = 211:0 ~73 l
Ilol
Nflo] 1 0 0 ... 0 “[1] "=0
[ ac 1 = 23in] = l 1 1 1  1 1 a as the MVU estimator. This alternative method is much easier to apply, and therefore
y 0 a MN _ 1] in practice, it is the one we generally employ. <>
s a linear transformation of a We now formally state the RBLS theorem.
Hence, the PDF of [z y]T is N (u, C) since this represent
Gaussian vector, where Theorem 5.2 (Rao-Blackwell-Lehmann-Scheﬁe) If ti’ is an unbiased estimator of
A 9 and T(x) is a sufficient statistic for 9, then d = E(9']T(x)) is
c = a2LLT=<72l1 N)‘
1. a valid estimator for 9 (not dependent on 0)
2. unbiased
3. of lesser or equal variance than that of bl, for all 0.
Hence, we have ﬁnally from (5.5) that
A = E<w|y>=A+;,%(Zwl"1-NA)
Additionally, if the suﬁlcient statistic is complete, then é is the M VU estimator.
A proof is given in Appendix 5B. In the previous example we saw that E 25:‘; a:[n])
= i did not depend on A, making it a valid estimator, was unbiased, and had less vari-
ance than a:[0]. That there is no other estimator with less variance, as Theorem 5.2
asserts, follows from the property that the sufficient statistic 25:‘; z[n] is a complete
sufficient statistic. In essence, a statistic is complete if there is only one function of the
statistic that is unbiased. The argument that 0 = E(0'|T(x)) is the MVU estimator is
now given. Consider all possible unbiased estimators of 0, as depicted in Figure 5.2. By
determining E (6l|T(x)) we can lower the variance of the estimator (property 3 of Theo-
rem 5.2) and still remain within the class (property 2 of Theorem 5.2). But E(0'|T(x))
is solely a function of the sufficient statistic T (x) since
h‘ h " th MVU estimator This approach requiring evaluation of a conditional
w 1c 1s e - *
‘ ' ll th matically intractable. _
expflsitzrzifirlf;ldliruzztznltlibriiatoethe second aPPTOa-Ch, We need to ﬁnd some functlon g so
that N_,
E<é|T<==>> = f épaimx» d9
gave)- (so
_iﬁi§kiyummiauixwi—v~i"
_ . - ' b’ destimator. Hence,
If T(x) 1s complete, there is but ene function of T that 1S an un iase F‘ uee e e Every
é is unique independent of the 0 we choose from the class shewn 1n bigl e-h- e fee
. . ’ - ‘ ' f 6 t e ess an
6 maps into the same estimator 0. Because the variance o mus
-_ , ld maémmtmim
any 0 within thelclasﬁ (Dropertﬁge 31>“ (g geieeeeczfeeieieeflieilee were F§e§l§dl1b$eakeng any unbiased
MVU estimator. n summary, . . h is (m1
_ - ' ' ,6). Alternatively, since t ere Y
estimator and carrying 0111'» the OPeYalFIQnS m (5 _ . d (m1
. - ' ' ds to an unbiased estimator, we I188 y
one function of the sufﬁcient statistic that lea _ _ F eh. l tter approach we
find the unique g to maelece theeeseililﬁjeiept]etat§wjfielieezﬁelg or is a
found in Example 5.5 t at g "=0 i" n = n=0 ' . h . t determines
a d on the PDF of 1:, whw m Pr"
th eehigFpieliiitzuﬁsifeiilpiigiiliii 535111111211}, practical cases of interest it holds. In par-
e . . . .
. - 5.14 d 5.15 this condition
ticular, f: telliee expgntenteieelefamilyeeofelelPgiegzeeelgeezeeeiegiee is ielneeneeeg quite diﬁieule,
issatisﬁe . ovaiae a asu c f
- - ' d St t 1979 . A ﬂavor or
and we refer the reader to the discussions in lKendau an uer l
the concePt of comPleteness 1S Pmvlded by the next two examp e )
f v’('r)w(Ag — T) d1" ¢ 0
Gaussian pulse, w(A1 -— r)
Gaussian pulse, 1U(Ag — r)
Choose i/(r) to make
f v’(r)w(A1 — T) d1" = 0
(a) Integral equals zero (b) Integral does not equal zero
Figure 5.3 Completeness condition for sufficient statistic (satisﬁed)
Example 5 6 - Completeness of a Sufficient Statistic Example 5.7 - Incomplete Sufficient Statistic
h ‘ t’ f A th sufﬁcient statistic ZN? xlnl is complete or there is but
For t e estima ion o , e n= .
. - __ , th t there exists
one function 9 for which Elyhilo‘  ~ A._S2Pl¥;:eh;weee‘ee1rld feelew that with
a second function h for which E[h(Zn=o  " - ’
T = EN-el-Tlnlv
Consider the estimation of A for the datum
where w[0] ~ LIP-é,  A sufﬁcient statistic is 110], being the only available data, and
furthermore, a:[0] is an unbiased estimator of A. We may conclude that g(a:[0]) = a:[0]
is a viable candidate for the MVU estimator. That it is actually the MVU estimator
still requires us to verify that it is a complete sufﬁcient statistic. As in the previous
example, we suppose that there exists another function h with the unbiased property
h(a:[0]) = A and attempt to prove that h = g. Again letting v(T) = g(T) — h(T), we
examine the possible solutions for v of the equation
EMU) _ MT“ = A _. A = 0 for all A
or sinceT~N(NA,NU2l
[e“T\ﬁmwﬁ”pi@N#
where v(T) = g(T) — h(T). Letting T — T/N and 1/(7) v(NT) we ave
“. N eX,“§
_ . ' (T) with a Gaussian pulse
which may be recognized as the convolution of a fUIICtIOIIIU e b e e any zero.
w(r) (see Figure 5.3). For the result to be zero for all A, v (T) mus e 1 en 1°
T th‘ call that a signal is zero if and only if its Fourier transform is identically
o see is re
zero, resulting in the condition
. - f h G ' pulse in
where V'(f) = _7-'{v’(r)} and f) 1S thehFoierier tZZiIieSiEJeIIEQeOQtI l‘: “guises: that the
(5.7). Since W( f) is also Gaussian an,d t efeeorfe Pen f Hence eve must have that
condition is satisﬁed if and only if V (f) - 0r h f- e0 Yes unique‘. O
v’(1-) = 0 for all T. This implies that g = h or that t e unc i n g
(T _ NAf] dT = l) for all A
(A_T)2] d¢=0 for allA (5-7) / v( )p(x’A) x 0 Ora A
For this problem, however, x = :c[0] = T, so that
foo v(T)p(T; A) dT = O for all A.
p(T’A)_{ 0 otherbrise 2
so that the condition reduces to
~ilmiilw-~miiiéai  i‘  ' i“
/ v(T) dT = 0 for all A.
, ﬁn_i2kslalﬁilnkkkfillklumiluiawp.s,..,..‘_‘__
I v(T)p(T; A1) dT = 0 f v(T)p(T', Ag) dT = 0
(a) Integral equals zero (b) Integral equals zero
Figure 5.4 Completeness condition for sufficient statistic (not satisﬁed) )
The nonzero function v(T) = sin 21rT will satisfy this condition as illustrated in Fig-
ure 5.4. Hence a solution is
v(T) = g(T) — h(T) = sin 21rT
h(T) = T — sin 21rT.
As a result, the estimator A
nd is unbiased for A. Having found at least
a function of the sufficient statistic, we may
t complete. The RBLS theorem no longer
is also based on the sufficient statistic a
one other unbiased estimator that is also
conclude that the sufficient statistic is no A
holds, and it is not possible to assert that A = a:[0] is the MVU estimator.
To summarize the completeness condition, we say that a sufficient statistic is complete
if the condition w
f v(T)p(T;6) dT = o for all 0 (5.8)
= 0 for all T.
results and then apply them to an
is satisfied only by the zero function or by v(T)
mator. The procedure is
orthwhile to review our
At this point it is w
estimation problem for which we do not know the MVU esti
as follows (see also Figure 5.5):
1. Find a single sufficient statistic for 6, that is, T(x), by using the Neyman-Fisher
factorization theorem.
2. Determine if the sufficient statistic is complete and, if so, proceed; if not, this
approach cannot be used.
Use geyman-Fisfier factorization
eﬂrem to find sufficient
statistic
Determine if T(x) is Complete
See (5.8)
Find function of T(x) that
is unbiased
- Figure 5 5 P d ~
v = g<T<x>> = MVU estimator ' m“ "e f“ ﬁmimg
MVU estimator (scalar
parameter)
3. Find a function g of the sufficient stat' t’
9(T(X))- The MVU estimator is then 1;. 1c a yle an unblased estlmator 0 z
As an alternative implementation of step 3 we ma
“a e (9|T(X)), Where 0 is any unbiased estimator,
HQWQVQT, in Practice the condition '
_ a1 expectatio 1 t‘ ' .
next example illustrates the overall procedure. n eva “a Ion 1S usually too tedious The
ExamPle 5-8 - Mean of Uniform Noise
We observe the data
a:[n]=w[n] n=(),1,___7N_1
where w[n] is IID noise with PDF U [0
,5]forﬂ>0W '11 -
for the mean 9 = 5/2. Our i ‘t’ l .' e WIS to ﬁnd the MVU estlmator
g1? hence MVU estimator czrlilllzt iplgrrlolacezhtoiff:  for Ending an efficient
F does not satisfy the requ‘ d 1 - . . 0 em" 1S is 599M158 the
estimator of 6 is the Sample mg; 5:5“ ﬂflty conditions (See Problem 3.1). A natural
The sample mean is easily shown to be unbiased and to have variance
pvarUvPll)
121v‘ (5.9)
var(§)
To determine if the sample mean is the MVU estimator for this problem we will follow
the procedure outlined in Figure 5.5. Deﬁne the unit step function as
1 forz>O
uh): 0 fora:<0.
Then,
where ,6 = 26, and therefore the PDF of the data is
This PDF will be nonzero only if 0 < a:[n] < B for all 111i], so that
pa”): IF, 0<a:[n]<,6 n-0,1,...,N—1
0 otherwise.
Alternatively, we can write
MTG) ___ I87 maxz[n] < ,6, minz[n] > O
0 otherwise
so that 1
p(x;6) = Bwuw — maxa:[n]) u(mina:[n]).
imam
9(T(X)» 9) M!)
By the Neyman-Fisher factorization theorem T(x) = max z[n] is a suﬁicient statistic
for 6. Furthermore, it can be shown that the sufﬁcient statistic is complete. We omit
the proof.
Next we need to determine a function of the sufﬁcient statistic to make it unbiased.
To do so requires us to determine the expected value of T = max The statistic T
is known as an order statistic. Its PDF is now derived. Evaluating ﬁrst the cumulative
distribution function
H Pr{a:[n] g ﬁ}
since the random variables are IID. The PDF follows as
PT(€) = i-dprgrgsé}
= NPrlfwlnl s s“ (K
But dPr{a:[n] 5 {}/d§ is the PDF of a:[n] or
pﬂnﬂéia) ={ E ° <5 < 5
0 otherwise.
Integrating, we obtain
Pr{a:[n]§§}= goqq,
which ﬁnally yields
We now have
Eu") = fispﬂod:
dPr{a:[n] 5 ﬁ}.
To make this unbiased we let é = [(N + 1) / 2N ]T, so that ﬁnally the MVU estimator is
9 = TX-Tmaxzhi].
fSOmewhat contrary to intuition, thesample mean is not the MVU estimator of the mean
or uniformly distributed noise! It 1S of interest to compare the minimum variance to
that of the sample mean estimator variance. Noting that
var(0A) = ( )2var(T)
and
(N + 1)2(N + 2)
we have that the minimum variance is
var(6A) =  . (5.10)
This minimum variance is smaller than that of the sample mean estimator (see (5.9))
for N _>_ 2 (for N = 1 the estimators are identical). The difference in the variances
is substantial for large N since the MVU estimator variance goes down as 1/N2 as
compared to the 1/N dependence for the sample mean estimator variance. , O
The success of the previous example hinges on being able to ﬁnd a single sufficient
statistic. If this is not possible, then this approach cannot be employed. For instance,
recalling the phase estimation example (Example 5.4) we required two statistics, T, (x)
and T2(x). Theorem 5.2 may be extended to address this case. However, the MVU
estimator (assuming complete sufficient statistics or E(g(T1,T2)) = 0 for all <15 implies
that g = 0) would need to be determined as  = E(<f>|T1,T2), where d) is any unbiased
estimator of the phase. Otherwise, we will need to guess at the form of the function g
that combines T1 and T; into a single unbiased estimator for g5 (see Problem 5.10). As
mentioned previously, the evaluation of the conditional expectation is difficult.
5.6 Extension to a Vector Parameter
We now extend our results to search for the MVU estimator for a p >< 1 vector parameter
0. In this case we are seeking an unbiased vector estimator (each element is unbiased),
such that each element has minimum variance. Many of the results of the previous
sections extend naturally. In the vector case we may have more sufficient statistics
than the number of parameters (r > p), exactly the same number (r = p), or even
fewer sufficient statistics than parameters to be estimated (r < p). The cases of r > p
and r = p will be discussed in the following examples. For an example of r < p see
Problem 5.16. From our previous discussions a desirable situation would be to have
r = p. This would allow us to determine the MVU estimator by transforming the
sufficient statistics to be unbiased. The RBLS theorem may be extended to justify this
approach for a vector parameter. Unfortunately, this approach is not always possible.
Before presenting some examples to illustrate these possibilities, extensions of the basic
deﬁnitions and theorems will be given.
A vector statistic T(x) = [T1(x) T2(x) ...T,(x)]T is said to be sufficient for the
estimation of 0 if the PDF of the data conditioned on the statistic or p(x|T(x); 0) does
not depend on 0. This is to say that p(x|T(x);0) = p(x|T(x)). In general, many such
T(x) exist (including the data x), so that we are interested in the minimal sufficient
statistic or the T of minimum dimension. To ﬁnd the minimal sufficient statistic we can
again resort to the N eyman-Fisher factorization theorem which for the vector parameter
case is as follows.
Theorem 5.3 (Neyman-Fisher Factorization Theorem (Vector Parameter))
If we can factor the PDF p(x; 0) as
P(X; 9) = 9(T(X). Blhfx) (5.11)
where g is afunction ‘depending only on x through T(x), an r >< 1 statistic, and also
on 0, and h is a function depending only on x, then T(x) is a sufficient statistic for 0.
Conversely, if T(x) is a sufficient statistic for 0, then the PDF can be factored as in
(5.11 ).
A proof can be found in [Kendall and Stuart 1979]. Note that there always exists a set
of sufficient statistics, the data set x itself being sufficient. However, it is the dimen-
sionality of the sufficient statistic that is in question, and that is what the factorization
theorem will tell us. Some examples now follow.
Example 5.9 - Sinusoidal Parameter Estimation
Assume that a sinusoidal signal is embedded in WGN
xlnlzftcwwffoﬂ-i-whl] n=O,1,...,N—1
where the amplitude A, frequency f0, and noise variance a’ are unknown. The unknown
parameter vector is therefore 0 =  f0 02F. The PDF is
1101.8) =   exp ~ig5 Z  — Acos 21rf0n)2 .
Expanding the exponent, we have
Z (z-[n] - Acos 21rfoﬂ)2 = Z z2[n] — 2A Z a:[n] cos 21rf0n + A2 Z cos’ 21rf0n.
Now because of the term EH0 a:[n] cos 21rf0n, where f0 1s unknown, we cannot reduce
the PDF to have the form expressed in (5.11). If, on the other hand, the frequency
were known, the unknown parameter vector would be 0 = [A a2]T and the PDF could
be expressed as
(Frazﬁ; eXP [-2fﬂ(2 1' [n] — 2A Z z'[n] cos 27ffg7l + A2 Z cosz 21rf0n)] - 1
y(T(X). B) ha)
where _ _ _ 119
Iilwlﬂlms 21rfon Additionally’ if the sumcient statistic is Complete, then é is the M VU estimator.
n: In the vector t
Tb!) = o N21 q 1 function of T’ pfarame er case completeness means that for v(T), an arbitrary r >< 1
o t a ( ( )) v( )P(T,6)dT _ 0 for all a (5.12)
r A and a2, but only 1f f0 1s known (see also then it must be true that
Hence, T(x) is a sufficient statistic fo
t example continues this problem. O
v(T) = 0 for all T.
As before. this can be difficult to veri
. ' fy- Also to be able to d t '
estlmator directly from T(x) without havin t ’ 1 ' e ermine the MVU
the sufficient statistic should be equal to thi d? Eva ‘fate E(9|T(X))1 the dimension of
mens1on of the unknown parameter or
Problem 5.17). The nex
wn Noise Power
Example 5 10 -
If r = p. If this is satisﬁed, th d - . _
m] = A + wit] n = 0, 1, . . . , N - 1 en we nee ‘my ﬁnd a“ 1* d““°“~”“°“a1 functwn s such that
where wln] is WGN with variance a2 and the unknown parameters are A and a2, we E(g(T)) = 0
le to ﬁnd a sufficient statistic of dimension assuming that T is a complete sufficient st ' ‘
at1st1c. We illustrate this appmeeh b
can use the results of the previous examp
2. Letting the unknown vector parameter be 0 = tinuing Example 5,10,
previous example, we have as our sufficient statistic
[A azlT and letting f0 = 0 in the
Example 5.11 - DC L l‘ . _
eve 1n WGN with Unknown Noise Power (continued)
[ffifillii f~'1.é*‘iiv$‘-i~ﬂsfsip~<w¢lgezes-   . x,
Th) =  We ﬁrst need to ﬁnd the mean of the sufficient, Statistic
"=0 T1 (x) 2 z[n]
Note that we had already observed that when a2 is known, 1,201 a:[n] is a sufficient Th) = [ T20!) l = 
A is known (A = 0), 25:‘; x2[n] is 2013M]
statistic for A (see Example 5.2), and that when
a sufficient statistic for a2 (see Example 5.3). In this example, the same statistics are
jointly sufficient. This will not always be true, however. Since we have two parameters
to be estimated and we have found two sufficient statistics, we should be able to ﬁnd
the MVU estimator for this example. O
Taking the expected value produgeg
Emu» = l NElhélnl) l = l N(Ug:A2) 
Clearly, we could remove the bias of th ﬁ 1; . .
by N. However, this would not help the rssecﬁfildxzgxpldrfi-"fnflofll; ‘lifting  Statiitic
- 11 e o v1ous t at
mator for the previous example, we need to state
Before actually ﬁnding the MVU esti
"‘~"#'I!r!.4i§ﬂ1h( 4-..m-,.~,‘,,3__'~i-r'-. Iglipﬁvrlk 1.35s.“ If.” -‘ : r-l," v T  ~ i; ‘
the RBLS theorem for vector parameters.
         as  i
E(0\T(x)) is
1. a valid estimator forO (not dependent on 0) “Th” z [ LT hi“? 2 ]
2. unbiased N 2(X)E[WT1(X)]
3. of lesser or equal variance than that of é (each element of l; has lesser or equal = 1 N_1 z
variance )
120 CHAPTER s. GENERAL MVU ESTIMATION M EXTENSION To A VECTOR PARAMETER 121
the“ Ea) = A and d2 are independent and that
and i ~ N (A, 
E (i? 2 $2M ~ i’) = U2 + A’ ~ E69). (N —1)U2 2
But we know that :7: ~ N (A, a2 / N ), so that Hence’ the ‘mvariaﬂﬁe matrix is
EU?) =A2+a2/N L’ 0
and we have that 0 a
E  N2“ $2M] _ i2) = U20 _ gi- = N]; 1G2. while the CRLB is (see Example 3.6)
1r we multiply this statistic by N/(N - 1), at will then be unbiased for a2. Finally, the I"(6) = N 204
appropriate transformation is 0 T
LT (x) Thus, the MVU estimator could not have been obtained b ‘ '
gab!» = N 1  Finally, we should observe that if we had known beiloreeililailidublg the CRLB‘
7%; [T2 (x) — N (§T1(x))2]  the sample mean and sample variance were the MVU ' r Suspected that
i  reduced our work‘ The PDF estimators, then we could have
TIL-T [Egan] __ N52]  P(X,0) =   exp ~F Ztsﬂn] _ A)2
‘  can be factored more directly by noting that
However, since  N_1 N_1
= Nil arzlﬂl ~ N5?  _ n=O(z[n] _ I) + 2h ‘ A) iolrln] _ i) + NW‘ s Alz-
n=0 The middle term is zero, so that we have the factorization
this may be written as (r a) _ 1 1 N—1 2
i ” ’ ‘ (was ex? T; Delta-aw ewe-Ari}- 1
N__"i (Zlnl - i) 9(T'(X)»9) h(x)
“=0 where
which is the MVU estimator of 0 = [A 02F. The normalizing factor of 1/ (N _- 1) for d? i
(the sample variance) is due to the one degree of freedom lost in estimating the mean. T'(x) = N-1
In this example 0 is not efﬁcient. It can be shown [Hoel, Port, and Stone 1971] that i‘ Z  — i)’
_ Q Qf s T’ x and T(x) 5.5 The IID observations a:[n] for n =.0,1,...,_N -- 1 are distributed according to
Dividing the SQCOHd COmPQnent by (N t1) wiilllllgtgggiigcznce agZI-(Qlrﬁfat 15h: Sufficient g Ll [-6, 6], where 6 > 0. Find a sufficient statistic for 6.
are related by a one-to-one transforma 10111 _ ' _
statistic is unique to within these transformations. ‘ 5.6 If z-[n] = A + w[n] for n = 0, 1, . . . , N — 1 are observed, where w[n] is WGN with
In asserting that 6 is the MVU estimator w; have not  thesgggalilitazlgeisf variance 02, ﬁnd the MVU estimator for a2 assuming that A is known. You maY
. . 1g a, . . . . .
of the statistic. Cotmarlilfeteiileisogogglgs bvevﬁﬁeaireeknﬁvlvllslsigznbe complete [Kendall and assume that the sufficient statistic is complete
the Vector expmen 1 am ’ ~ ~ ' f ‘l f . 5 7 C s'd th fr t' t' f ' 'd b dd d ' WGN
fth 1 x onential army o . on 1 er e equency es ima ion o a sinusoi em e e in or
Smart 1979] (see Problem 5.14 for the deﬁnition o e sca ar e P O [ 1 [ 1
PDFS), _ acn =cos21rf0n+wn n=0,1,...,N—1
where w[n] is WGN of known variance a2. Show that it is not possible to ﬁnd a
sufficient statistic for the frequency f0.
Refere _ _ Mm, Boston 5.8 In a similar fashion to Problem 5.7 consider the estimation of the damping constant
Hoel, P.G., S.C. Port, C.J. Stone, Intmduction to Statistical Theory, Houghton i l)!» Y  r for the data n
1971. F _ J W1 y New York 1979  a:[n] = r + w[n] n = 0, 1, . . . , N — 1
' ' i Y - l e ’ ' ' . . . . . . .
Hoskinﬁf  dgnégrdlazre: Tzttjglllasnced Theoi-y of Statistics, Vol. 2, Macmillan, New York, 1979.  where mm} 1S WGN wlth known valance U2‘ Agam Show that a Sufﬁclent Statlstlc
gigging. 2., Pmliability, iRandom Variables, and Stochastic Pmcessesi MCGraw-Hlu’ New York’ i: does not exist for 1’.
1965- “* 5.9 Assume that a:[n] is the result of a Bernoulli trial (a coin toss) with
 mm] = 1} = e
Problems '
N—1 - - ' 1;‘ f 2 b sing the
5.1 For Example 5.3 prove that EH0 z2[n] is a sufficient statis ic or a y u
_ _ . 2. H‘ t: N t that
the PDF of s = "=0 972°’ 1S a chksquar 1S n
freedom or 1
11(5) ={ ﬁﬂi)
and that N IID observations have been made. Assuming the Neyman-Fisher
factorization theorem holds for discrete random variables, ﬁnd a sufficient statistic
for 6. Then, assuming completeness, ﬁnd the MVU estimator of 6.
5.10 For Example 5.4 the following estimator is proposed
Show that at high SNR or a2 —> 0, for which
exp  S%_1 S > 0
() s<0.
Q3 = — arctan [
. _ _ Ra l ' h PDF
5.2 The IID observations 17in] f“ " — 0, 1» - - - iN 1 have the y elg
ilexp ($12M) m] > o
Pl-Tlnl; U2) = U2 2 U2 ' A T1 (X) z N21 Acos(21rfon + <15) cos 21rfon
() :r[n] < O. 
Find a sufficient statistic for a2.
5_3 The IID observations :r[n] for n - 0, 1, . . . ,N 1 have the exponen ia
p(rlnl;»\) ={ “expﬁmhll  iii.
T2(x) z Z A cos(21rf0n + <15) sin 21rf0n
the estimator satisﬁes qi z qﬁ. Use any approximations you deem appropriate. Is
the proposed estimator the MVU estimator?
Find a Sufficient Statistic for ),_ 5.11 For Example 5.2 it is desired to estimate 6 = 2A + 1 instead of A. Find the MVU
_ _ ' ‘b t d rding to  _ estimator of 6. What if the parameter 6 = A3 is desired? Hint: Reparameterize
" 0’ 1’ ' ‘ ' ’ N 1 are dlstrl u e “co the PDF in terms of 6 and use the standard approach.
vsv..se-i$i.-j=iie.iii-flws;ltswm.iiwiﬁﬂi7.". ‘y’ l   “'72
5.4 The IID observations x[n] for n . _ _ f 0
N’ (6, 6), where 6 > 0. Find a sufficient statistic or .
-_.-.- :4.‘ . J1me.
iiiiiiiiliiiiailwul
sufficient statistics are unique to within one-
consider Example 5.2 but with the sufficient 5'15 If we observe xln] for n z o’ l’ ' ' ‘ ’ N _ l’ which are IID and WhQSe PDF bekmgs
5.12 This problem examines the idea that
to the exponential family of PDFs, show that
to-one transformations. Once again,
statistics
is a sufficient statistic for 6. Next a l this lt t d ' '
T (x) ___ mm] _ _ _ _ _ PP Y Iesu o etermine the sufficient
2 (g stalilzistic for ‘the ﬁaussian, Rayleigh, and exponential PDFs in Problem 5.14. Fi-
na y, assuming t at the sufficient statistics are complete, ﬁnd the MVU estimator
in each case (if possible). (You may want to compare your results to Example 5.2
Find functions g1 and g2 so that the unbiased condition is satisﬁed or
and Problems 5.2 and 5.3, respectively.)
Elg1(T1(x))l = A
El92(T2(X)ll = A
and hence the MVU estimator. Is the transformation from T1 to T2 one-to-one?
What would happen if the statistic T3 (x) = (if; a:[n])2 were considered? Could
5.16  this problem an example is given where there are fewer sufficient statistics
an parameters to be estimated [Kendall and Stuart 1979]. Assume that a:[n] for
" — 0»1,---,N — 1 are observed, where x ~ A/(y, C) and
you ﬁnd a function g3 to satisfy the unbiased constraint? Np
5.13 In this problem we derive the MVU estimator for an example that we have not 0
encountered previously. If N IID observations are made according to the PDF u = 0
. _ exvi-(winl - 9)] win] > 6 f
Pimlnlﬁ) - { 0 ﬁn] < 9’ ()
ﬁnd the MVU estimator for 6. Note that 6 represents the minimum value that C = [ N ' 11+ U2 ‘If } _
 may attain. Assume that the sufficient statistic is complete.
The covariance matrix C has the dimensions
[ 1><1 1><(N—1)
(N~1)><1(N-1)><(1v-1)l'
5.14 For the random variable a: consider the PDF
WW9) = eXP lA(9)B (1) + 9(1) + 13(9)]-
A PDF of this type is said to belong to the scalar exponential family of PDFs.
Many useful properties are associated with this family, and in particular, that of
sufﬁcient statistics, which is explored in this and the next problem. Show that
the following PDFs are within this class.
a. Gaussian
(Tlll-‘L expll a]
Show that with 6 = [ii azlT
What are the sufficient statistics for 6? Hint: You will need to ﬁnd the inverse
and determinant of a partitioned matrix and to use Woodbury’s identity.
b. Rayleigh
~ 5.17 Consider a sinusoid of known frequency embedded in WGN or
c. Exponential h _ _ _ 2 _
w ere w[n] 1S WGN with variance a . Find the MVU estimator of
a. the amplitude A, assuming a2 is knqwn
w ere > b. the amplitude A and noise variance a .
You may assume that the sufficient statistics are complete. Hint: The results of
Examples 5.9 and 5.11 may prove helpful.
5J8 If mm] for n = 0,1,...,N — 1 are observed, where_the samples aree ITID and
distributed agcgrding to (A61, 62], find a sufficient statistic for 0 = [91 2l -
5 19 Recall the linear model in Chapter 4 in which the data are modeled as
Appendix 5A
here w ~ N (0 0'21) with a2 known. Find the MVU estimator of 0 by using the
§ man-Fisher land RBLS theorems. Assume the sufficient statistic is complete.
Cgilnpare your result with that described in Chapter 4. Hint: First prove that the
following identity holds:
(x _ H0)T(x _ H9) = (x - new - H6) + w — MTHTHW — 5)
Proof of Neyman-Fisher Factorization
Theorem (Scalar Parameter)
where Consider the joint PDF p(x,T(x);6). In evaluating this PDF it should be noted
that T(x) is functionally dependent on x. Hence, the joint PDF must be zero when
evaluated at x = x0, T(x) = To, unless T(x0) = To. This is analogous to the situation
where we have two random variables z and y with z = y always. The joint PDF
p(z, y) is p(z)5 (y -— at), a line impulse in the two-dimensional plane, and is a degenerate
two-dimensional PDF. In our case we have p(x,T(x) = T156) = p(x;6)6(T(x) — To),
which we will use in our proof. A second result that we will need is a shorthand way
of representing the PDF of a function of several random variables. If y = g(x), for x a
vector random variable, the PDF of y can be written as
é = (HTHYIHTx.
m) = fpooao - goo) d» (SA-l)
Using properties of the Dirac delta function [Hoskins 1979], this can be shown to be
equivalent to the usual formula for transformation of random variables [Papoulis 1965].
If, for example, x is a scalar random variable, then
where {z1,z2,. . .,a:;,} are all the solutions of y =  Substituting this into (5A.1)
and evaluating produces the usual formula.
We begin by proving that T (x) is a sufficient statistic when the factorization holds.
p(x, T(x) = To; 0)
P(T(X) = To; 9)
110K; 9)6(T(X) — To)
P(T(X) = To; 9) I
P(X|T(X) = To; 9)
Using the factorization so that (5A.4) is satisﬁed. Then, (5A.5) becomes
p(X|T(X) = T0;6) =  (am) m; 6)6(T(x) - To) =  ,,(T(x) = To”)
But, from (5A.1), Mme) = gab!) = To; with)
where
(m) = To; v) = (x; Q><Y<T< )— To) d»
p [p x g<T<x> = 11w)- “Tw = T“ a)
' I h(X)6(T(X) - To) ax"
In this form we see how, in principle, the PDF of the sufficient statistic can be found
based on the factorization since
Again using the factorization
pew) = To; v) f 9mm = To, 9)h(1<)6(T(x) - To) dx
P(T(X) = TM) = y(T(x) = To; 6) f h(x)6(T(x) - To) dx.
The latter step is possible because the integral is zero except over the surface in RN
for which T(x) = To. Over this surface g is a constant. Using this in (5A.2) produces
_ _ _ h()6(T(x)_T0)
MT“) ‘ T“ 6) ‘ Iholiacmx) - To) dx
which does not depend on 0. Hence, we conclude that T(x) is a sufficient statistic.
Next we prove that if T(x) is a sufficient statistic, then the factorization holds.
Consider the joint PDF
PU» Th!) = To; 9) = P(XIT(X) = To; 9)P(T(X) = To; 9) (5A-3) i.
and note that 
P(X, Tlx) = To; 9) = P(X; 9)5(T(X) ~ Tol- 
Because T(x) is a sufficient statistic, the conditional PDF does not depend on 6. Hence, 
we write it as p(x|T(x) = To). Now since we are given T(x) = To, which defines -
a surface in RN, the conditional PDF is nonzero only on that surface. We can let 
p(x\T(x) = To) = w(x)6(T(x) — To), where 
f w(x)6(T(x) - To) dx = 1. (5A.4) 
Thus, substituting in (5A.3) produces 
P(X; 9)5(T(X) — To) = w(X)5(T(X) — To)P(T(X) = To; 9)- (5A-5) 
We can let v 
h(x) ’
we have
var(6) = E [(9 - 5(9)?)
= E [(9 -é
(;_
= EK ‘ — (9)21 + 2BR (M; - 0)] + E[(6 - 0y].
Appendix 5B
The cross-term in the above expression, E [(6 — 6)(6 - 6)], is 119w Shawn to be zero We
know that 6 is solely a function of T. Thus, the ex ectat'o f th -t ' ‘
respect to the joint PDF of T and 6 or p l n o e cross erm 1S with
Rao-BlackWell-Lehmann-Scheffe
Theorem (Scalar Parameter) >
and also
EélTlé " 9A] = E§|T(6[T) — 6 = 6 — 6 = 0.
To prove (1) of Theorem 5.2, that 6 is a valid estimator of 6 (not a function of 6), Thus’ we have the desired result
note that
we) Em —@>’1+var<@>
9 = Ewlllxi) 2 var(6).
= I 01x)!’ (xlfmx)? 6) dx‘ (531) Finally that 6 is the MVU estimator if it is complete follows from the discussio g’
’ n 1ven
in Section 5.5.
By the deﬁnition of a sufficient statistic, p(x|T(x); 6) does not depend on 6 but only
on x and T(x). Therefore, after the integration is performed over x, the result will be
solely a function of T and therefore of x only. A
To prove (2), that 6 is unbiased, we use (5B.1) and the fact that 6 is a function of
Eu?) ff 9(X)1>(XlT(X);6)dxp(T(X);6)dT
f 9e) f p<xtr<x>¢1>p<rr<x>w> d1" dx
= [6(x)p(x; 6) dx
= E(6).
But 6 is unbiased by assumption, and therefore
5(9) = 5(9) = o.
To prove (3), that A _
WW) S var(9)
Chapter 6
Best Linear Unbiased Estimators
6.1 Introduction
feafa-P  » .
It frequently occurs in practice that the MVU estimator, if it exists, cannot be found.
For instance, we may not know the PDF of the data or even be willing to assume a
model for it. In this case our previous methods, which rely on the CRLB and the
theory of sufﬁcient statistics, cannot be applied. Even if the PDF is known, the latter
approaches are not guaranteed to produce the MVU estimator. Faced with our inability
to determine the optimal MVU estimator, it is reasonable to resort to a suboptimal
estimator. In doing so, we are never sure how much performance we may have lost
(since the minimum variance of the MVU estimator is unknown). However, if the
variance of the suboptimal estimator can be ascertained and if it meets our system
speciﬁcations, then its use may be justified as being adequate for the problem at hand.
If its variance is too large, then we will need to look at other suboptimal estimators,
hoping to ﬁnd one that meets our speciﬁcations. A common approach is to restrict the
estimator to be linear in the data and ﬁnd the linear estimator that is unbiased and
has minimum variance. As we will see, this estimator, which is termed the best linear
unbiased estimator (BLUE), can be determined with knowledge of only the ﬁrst and
second moments of the PDF. Since complete knowledge of the PDF is not necessary,
the BLUE is frequently more suitable for practical implementation.
6.2 Summary
The BLUE is based on the linear estimator deﬁned in (6.1). If we constrain this
estimator to be unbiased as in (6.2) and to minimize the variance of (6.3), then the
BLUE is given by (6.5). The minimum variance of the BLUE is (6.6). To determine the
BLUE requires knowledge of only the mean and covariance of the data. In the vector
parameter case the BLUE is given by (6.16) and the covariance by (6.17). In either the
Scalar parameter or vector parameter case, if the data are Gaussian, then the BLUE is
also the MVU estimator.
6.3 Deﬁnition of the BLUE
We observe the data set {a:[0],a:[1], . . . ,a:[N - 1]} whose PDF p(x;6) depends on an
unknown parameter 6. The BLUE restricts the estimator to be linear in the data or
6A: Z a,,a:[n] (6.1)
where the a,,’s are constants yet to be determined. (See also Problem 6.6 for a similar
deﬁnition except for the addition of a constant.) Depending on the a,,’s chosen, we
may generate a large number of different estimators of 6. However, the best estimator
or BLUE is deﬁned to be the one that is unbiased and has minimum variance. Before
determining the a,,’s that yield the BLUE, some comments about the optimality of
the BLUE are in order. Since we are restricting the class of estimators to be linear,
the BLUE will be optimal (that is to say, the MVU estimator) only when the MVU
estimator turns out to be linear. For example, for the problem of estimating the value
of a DC level in WGN (see Example 3.3) the MVU estimator is the sample mean
which is clearly linear in the data. Hence, if we restrict our attention to only linear
estimators, then we will lose nothing since the MVU estimator is within this class.
Figure 6.1a depicts this idea. On the other hand, for the problem of estimating the
mean of uniformly distributed noise'(see Example 5.8), the MVU estimator was found
to be N + 1
which is nonlinear in the data. If we restrict our estimator to be linear, then the BLUE
is the sample mean, as we will see shortly. The BLUE for this problem is suboptimal,
as illustrated in Figure 6.1b. As further shown in Example 5.8, the difference in per-
formance is substantial. Unfortunately, without knowledge of the PDF there is no way
to determine the loss in performance by resorting to a BLUE.
Finally, for some estimation problems the use of a BLUE can be totally inappropri-
ate. Consider the estimation of the power of WGN. It is easily shown that the MVU
estimator is (see Example 3.6)
a =ﬁZa:[n]
which is nonlinear in the data. If we force the estimator to be linear as per (6.1), so
that N 1
0A2 = Z anzhi],
a 1;;  f“\\,_f€1*k$yi4}:j>§_ p,
—__——F——P—*_F’F?C$tty——v_—,
All unbiased
estimators
All unbiased
estimators
Linear
estimators
Linear
estimators
Nonlinear
estimators estimators
(a) DC level in WGN; BLUE is opti- (b) Mean of uniform noise; BLUE is
mal suboptimal
Figure 6.1 Optimality of BLUE
the expected value of the estimator becomes
Eve) = Z a.E<1ini>= o
 ES? :1 0 for all n. Thus,  cannot even ﬁnd a single linear estimator that is
ase , e one one that has minimum variance. Although the BLUE is unsuitable
fQT this Problem, a BLUE utilizing the transformed data y[n] = x2[n] would produce a
viable estimator since for
<72 = Z anyhl] = i Gnizlﬂ)
n=()
the unbiased constraint yields
E672) = Z anaz = a2.
xggietialre many‘ valliiies of the a,,’s that could satisfy this constraint. Can you guess
e an s s ou be to yield the BLUE? (See also Problem 6.5 for an example of
b 1S daga transformation approach.) Hence, with enough ingenuity the BLUE may still
e use if the data are ﬁrst transformed suitably.
6.4 Finding the BLUE
To determine the BLUE we constrain 6 to be linear and unbiased and then ﬁnd the
an’s to minimize the variance. The unbiased constraint, is from (6.1),
E(6) = Ni anemia) = e. (6.2)
The variance of 6 is
var(6) = E ana:[n] — E a,,a:[n])) ] .
But by using (6.2) and letting a = [a0 a1 . . . aN_1] we have >
var(6) = E[(aTx— TE(X))2]
= E [($66 - E(x)))’]
= E [aT(x — E(x))(x — E(x))Ta]
= aTCa. (6.3)
The vector a of weights is found by minimizing (6.3) subject to the constraint of (6.2).
Before proceeding we need to assume some form for  In order to satisfy the
unbiased constraint, E must be linear in 6 or
E(z[n]) = s[n]6 (6.4)
where the s[n]’s are known. Otherwise, it may be impossible to satisfy the constraint.
As an example, if  = cos 6, then the unbiased constraint is Efgo‘ an cos6 = 6
for all 6. Clearly, there do not exist a,,’s that will satisfy the unbiased constraint. Note
that if we write z[n] as
1W = E(Il"l) + [IP11 - Eﬁvlﬂlll
then by viewing a:[n] — E as noise or w[n] we have
The assumption of (6.4) means that the BLUE is applicable to amplitude estimation of
known signals in noise. To generalize its use we require a nonlinear transformation of
the data as already described.
With the assumption given by (6.4) we now summarize the estimation problem. To
ﬁnd the BLUE we need to minimize the variance
var(6) = aTCa
subject to the unbiased constraint, which from (6.2) and (6.4) becomes
ganEl-Tlnl) = 6
Zans[n]6 = 6
Zanshi] = 1
aTs=1
where S = (s[0] s[1] . . . s[N — 1]]T. The solution to this minimization problem is derived
in Appendix 6A as
so that the BLUE is
(6.5)
and has minimum variance
Note from (6.4) that since E (x) = 6s, the BLUE is unbiased
sTC‘1E(x)
W09) = (6.6)
E(6)
Also, as we asserted earlier, to determine the BLUE we only require knowledge only of
1. s or the scaled mean
2- C, the covariance
or the ﬁrst two moments but not the entire PDF. Some examples now follow.
Example 6.1 - DC Level in White Noise
Ifweobserve
ghtefe wln] is white noise with variance a2 (and of unspeciﬁed PDF), then the problem
o estimate A. Since w[n] is not necessarily Gaussian, the noise samples may be
138 — A we Therefore
ed. Because Eiilnl) " * ’
_ h h they are uncorrelat _
Statistically depirligsxistzlrflxlinltaiiidgtherefore s = 1. From (6.5) the BLUE 1s N_1 EM
have from (6-4) 1 A _0 U?‘
1T ~Ix A = "- (6.7)
1 ‘a? n=0 U"
N n: m] var(A) = N_, 1 . (as)
and has minimum VHIIHJICG: from (6.6): 1 n=0 U?»
vii-WA) “ T 1 The BLUE weights those samples most heavily with smallest variances in an attempt
“E1 ‘ to e ualize the noise contribution from each sample. The denominator in 6.7 is the
a q _
U2 scale factor needed to make the estimator unbiased. See also Problem‘ 6.2 for some
=  further results. O
‘ . t}, PDF of the data. In addition,
le mean ts the BLUE mdepeﬂdent of e n noise. O
f G 31a In general, the presence of C“ in the BLUE acts to prewhiten the data prior to
the MVU estimator 0r @113
averaging. This was previously encountered in Example 4.4 which discussed estimation
of a DC level in colored Gaussian noise. Also, in that example the identical estimator
for A was obtained. Because of the Gaussian noise assumption, however, the estimator
could be said to be eﬁicient and hence MVU. It is not just coincidental that the MVU
estimator for Gaussian noise and the BLUE are identical for this problem. This is a
general result which says that for estimation of the parameters of a linear model (see
Chapter 4) the BLUE is identical to the MVU estimator for Gaussian noise. We will
say more about this in the next section.
Hence. the sump . .
as already discussed, 1t 13
6.5 Extension to a Vector Parameter
If the parameter to be estimated is a p >< 1 vector parameter, then for the estimator to
The covariance InatﬂX 15 be linear in the data we require
0 a? 0 N—1
C = 3 '._ ’ 9,- = Z a,‘,,a:[n] i = 1, 2, . . . ,p (6.9)
O 0 Gav-l n=0
where the aﬂﬁs are weighting coefficients. In matrix form this is
and its inverse is l2 O 0 é = Ax
a0 1
0 g5 0 where A is a p >< N matrix. In order for l; to be unbiased we require
0 0 1 5(a) = Z a,-,,E(a:[n]) = 0,- ¢= 1,2, . . . ,p (6.10)
or in matrix form  : AEU) : 9_ (6.11)
e appropriate only for problems in which
Keep in mind that a linear estimator will b we must have that
the unbiased constraint can be satisﬁed. From (6.11)
for H a known N >< p matrix. In the scalar parameter case (see (6.4))
so that (6.12) is truly the vector parameter generalization. Now, substitution of (6.12)
into (6.1 1) produces the unbiased constraint
AH = I. (6-13)
If we deﬁne 8i = [am an _ _ _ a,-(N_1)]T, so that 01 = aiTx, the unbiased constraint may be
rewritten for each a,- by noting that
and letting h,- denote the ith column of H, so that
With these deﬁnitions the unbiased constraint of (6.13) reduces 110
8-Thj:6ij i=1a2u-"ip;j:1Y2Y"'7p' 
Also, the variance 1s varwi) z aiTCm (6.15)
using the results for the scalaricase (see (6.3)). The  for a vezcrtzlortkpzrrzzilflififzg?
found by minimizing (6.15)dsubéeBctﬂt!o the constrain: i: ‘(agrielé T3551) ygield the BLUE
tion for each i. In Appen ix 1S mllllmlza 1°
as g = (HTC-1H)'1HTC“x ' (6-16)
and a covariance matrix of C9 z (HTC_1H)-1_ (617)
The form of the BLUE is identical to the MVU estimator for the general linear model
(see (4.25)). This extends the observations of Examples 6.1 and 6.2. That this should
be the case follows from the deﬁnition of the general linear model
where w ~ N (0, C). With this model we are attempting to estimate 0, where E(x) =
H0. This is exactly the assumption made in (6.12). But from (4.25) the MVU estimator
for Gaussian data is A
0 = (HTC"H)"1HTC“1x
which is clearly linear in x. Hence, restricting our estimator to be linear does not lead
to a suboptimal estimator since the MVU estimator is within the linear class. We may
conclude that if the data are truly Gaussian, then the BLUE is also the M VU estimator.
To summarize our discussions we now state the general BLUE for a vector parameter
of a general linear model. In the present context the general linear model does not
assume Gaussian noise. We refer to the data as having the general linear model form.
The following theorem is termed the Gauss-Markov theorem.
Theorem 6.1 (Gauss-Markov Theorem) If the data are of the general linear model
x = H0 + W (6.18)
where H is a known N >< p matrix, 0 is a p>< 1 vector of parameters to be estimated, and
w is aN >< 1 noise vector with zero mean and covariance C (the PDF of w is otherwise
arbitrary), then the BLUE of 0 is
é = (nTc-lnrlnTc-lx (6.19)
and the minimum variance of 6A,- is
var(9f,~) = [mTc-lln-qii. (6.20)
In addition, the covariance matrix of 9 is
0,; = (HTc"1H)-‘. (6.21)
Some properties of BLUEs are given in Problems 6.12 and 6.13. Further examples
of the computation of BLUEs are given in Chapter 4.
6.6 Signal Processing Example
In the design of many signal processing systems we are given measurements that may
not correspond to the “raw data.” For instance, in optical interferometry the input to
the system is light, and the output is the photon count as measured by a photodetector
[Chamberlain 1979]. The system is designed to mix the incoming light signal with a
delayed version of itself (through spatially separated mirrors and a nonlinear device).
Aircraft
H3 """°Antenna3 t2_t1=0 H _R _H
H1 t3_t2:0 = i— 2- a
t,- = Time of received signal (emitted
by aircraft) at antenna i
O Antenna 1 l Antenna 2
Figure 6.2 Localization of aircraft by time difference
of arrival measurements
The data then is more nearly modeled as an autocorrelation function. The Jraw data,
that is, the light intensity, is unavailable. From the given information it is desired
to estimate the spectrum of the incoming light. To assume that the measurements
are Gaussian would probably be unrealistic. This is because if a random process is
Gaussian, then the autocorrelation data are certainly non-Gaussian [Anderson 1971].
Furthermore, the exact PDF is impossible to ﬁnd in general, being mathematically
intractable. Such a situation lends itself to the use of the BLUE. A second example is
explored in some detail next.
Example 6.3 - Source Localization
A problem of considerable interest (especially to anyone who has traveled by airplane) is
that of determining the position of a source based on the emitted signal of the source. In
order to track the position of an aircraft we typically use several antennas. A common
method as shown in Figure 6.2 is to base our estimate on the time difference of arrival
(TDOA) measurements. In the ﬁgure the signal emitted by the aircraft is received at
the same time at all three antennas, so that t1 = t; = t3. The TDOA t2 — t1 between
antennas 1 and 2 is zero, as is the TDOA t3 — t2 between antennas 2 and 3. Hence, we
may conclude that the ranges R, to all three antennas are the same. The dashed line
between antennas 1 and 2 indicates the possible positions of the aircraft in order for
R1 = R2, and likewise for the dashed line between antennas 2 and 3. The intersection
of these lines gives the aircraft position. In the more general situation for which the
TDOAs are not zero, the dashed lines become hyperbolas, the intersection again giving
the position. It is necessary to have at least three antennas for localization, and when
noise is present, it is desirable to have more.
We now examine the localization problem using estimation theory [Lee 1975]. To
do so we assume that N antennas have been placed at known locations and that the
time of arrival measurements t,» for i = 0,1,... ,N — 1 are available. The problem is to
estimate the source position (z,,y,) as illustrated in Figure 6.3. The arrival times are
assumed to be corrupted by noise with zero mean and known covariance but otherwise
Source (1,, ya)
Nominal position (In, y")
Antenna
Figure 6.3 Source localization geometry
unknown PDF. For a signal emitt d b th '
are modeled by e y e Source at tlme t = To, the measurements
ti=T0+Ri/C+6i i=(),1’___’N_1 (622)
Where the eﬂs are measurement ' d .
samples are assumed to be zerdlﬁﬁnagvitchdfzilfites thezpropagatlon Speed‘ The ‘wise
other‘ No assumptions are made about the ppFiaiifcetahg and u¥correlated with each
must relate the range from each ant R. noise. o Pr-oceed further we
Letting the position of the ith antenrfgugz (w. toihehuiﬁuiown position 0 = [mi M11‘.
have that " y’ ’ w 1c 1S assumed m be kmwn, We
(is r a)’ + (.118 — 11.02- (6.23)
Substituting (6 23) into (6 22) we see t
- ~ , hat th d 1 ~ - ~
parameters w, and 11,- T0 apply the Gauss-Margo?! fheksirelrfiniiirxeeziilln the unkfllown
nominal source position (z y ) is available Th‘ - _ _ afssume t at a
the true Source position coglid gave been -. is nominal P08112011: which is close to
. _ _ i obtained from previous m
situation is typical when the source ' b ' t Fasurements’ Such a
position we require an estimate of 01S:  fie???) Iienclgidiesglmice tThe new Sourlm
Figure 6.3. Denote the nominal ranges by Rn“ %e usse ayfilrst-indexfl‘ gal] l as Sh?“ m
R,- (considered as a two-dimensional function of a: and ) b r ay or éxpansllnl of
z. = m... y. = y.» ’ y’ a m“ the mmmal Pesltlon
$11 _ $1 .11.. — y.-
Rm 61173 + Rm 6y, (624)
See also Chapter 8 for a more corn '
plete description of the line ' t‘ -
model W-th th- . _ _ _ . ariza ion of a nonlinear
1 1S approximation we have upon substitution in (6.22)
Rn» I —a:~ _ .
which is now linear in the k
deﬁned in Figure 6.3 u“ “Qwn parameters 51?, and 5313- Alternatively, since as
the model simpliﬁes to i
ti = To +  + coiaih, + Since“ 61/8 + 6.»
The term R", / c is a known constant which can be incorporated into the measurement
by letting (see also Problem 6.6)
Hence, we have for our linear model
‘r,- = To + cos “tar, + Sm “'61,, + e,- (6.25)
where the unknown parameters are T0,6a:,,6y,. By assuming the time To when the
source emits a signal is unknown, we are bowing to practical considerations. Knowledge
of To would require accurate clock synchronization between the source and the receiver,
which for economic reasons is avoided. It is customary to consider time diﬁerence of
arrivals or TDOA measurements to eliminate To in (6.25). We generate the TDOAs as
£1 = Ti “To
Then, from (6.25) we have as our ﬁnal linear model
5, = —(cosa, -— cos a,-_1)6a:, + -c-(sina,- -- s1na,¢_1)6y, + c, — c,-_1 (6.26)
for i = 1,2,. . . ,N - 1. An alternative but equivalent approach is to use (6.25) and
estimate To as well as the source position. Now we have reduced the estimation problem
to that described by the Gauss-Markov theorem where
cos a1 — cos a0 sin a1 — sin a0
sin a; — sin a1
1 cos a; — cosal
cosaN_1 — cosaN_2 sinaN_1 — sinaN_2
_.W,_,d_,,____aﬁ_'mv  y "zi; -.-;~,;w._  _
I Source
Nominal position
Figure 6.4 Example of source lo-
“fhzatlfﬂl geﬁlnetry for line array
with minimum number of antennas
The noise vector w is zero mean but i
variables. To ﬁnd the covariance matrix iilcfteilliietr composed of uncorrelated random
A e
where A has dimension N - 1 N ' - . _
that ( ) >< . Since the covariance matrix of e is U21, we have
c = E[AeeTAT] = U2AAT_
0m (6 19) the BLUE of the source position parameters is
(HTC"H)“‘HTC*1§
[HT(AAT)“H]~1HT(AAT)-1g (627)
and the minimum variance, is from (6 20)
ma)” = "2 llﬁTlAATwnldl .. (6.2s)
or the covariance matrix, is from (6.21)
lHT(AAT>"‘H]“- <62.»
As an example, for the three antenna li
. ne ar th ' '
required) shown in Figure 6.4 we have that my ( e mmlmum number of antennas
H ___ l —cosa 1—sina
c —cosa —(1—sina)
After substituting in (6.29) we have for the covariance matrix
C6 z U202 coos a 3/2
(1 —sina)2
For the best localization we would like a to be small. This is accomplished by making
the spacing between antennas d large, so that the baseline of the array, which is the
total length, is large. Note also that the localization accuracy is range dependent, with
the best accuracy for short ranges or for a small.
References
Anderson, T.W., The Statistical Analysis of Time Series, J. Wiley, New York, 1971. 
Chamberlain, .l., The Principles of Interferometric Spectmscopy, J. Wiley, New York, 1979.
Lee, H.B., “A Novel Procedure for Assessing the Accuracy of Hyperbolic Multilateration Systems,” g5
IEEE Trans. Aemsp. Electmn. Syst., Vol. 11, pp. 2-15, Jan. 1975.
Problems
6.1 If a:[n] = Ar" + w[n] for n = 0, 1, . . . , N -— 1, where A is an unknown parameter, r
is a known constant, and w[n] is zero mean white noise with variance a2, ﬁnd the
BLUE of A and the minimum variance. Does the minimum variance approach
zero as N —> 0o?
6.2 In Example 6.2 let the noise variances be given by of, = n + 1 and examine what
happens to the variance of the BLUE as N —> oo. Repeat for of, = (n + l)’ and
ex plain your results.
6.3 Consider the estimation problem described in Example 6.2 but now assume that
the noise samples are correlated with the covariance matrix
where ]p| < 1 and N, the dimension of the matrix, is assumed to beleven. C is
a block-diagonal matrix and so is easily inverted (see (Appendix 1)). Find the
BLUE and its variance and interpret your results.
Flags: erved Samples {$]0]»$]1l» - - - ,$]N — 1]} are IID according to the following
a. Laplacian
Mela]; l1) = 5 exvl-lrln] — ul]
b. Gaussian
Mwlnli/i) = fr... [gen] _ My] ,
esltximatgr forUu? of the mean p m both cases’ What can Y9" Say about the MVU
He); Served samPles 19:10]» $11]. - - - ,$[N — 1]} are IID according to the lognormal
P(I[1i];0)= {  exp ]_2(lnz]n] _ 6y] $171] > 0
mve a t e mean 1S eXPw + 1/ 2) and therefore that the unbiased constraint
cannot be satisﬁed. Using th t f ' f - .
i/[n] = inzln], ﬁnd the BLUEe £5112? ormatlon 0 random variables appmch Wm‘
6.6 In this problem we extend the scalar BLUE results. Assume that E  — 6
,6, where 0 is the unknown parameter to be estimated and ,6 is a knozn 
The data vector x has covariance matr' C W d fi ' '
aﬁine) estimator for this problem as 1X ' e e ne a modlﬁed hnear (actually
0 : 2 an-Thl] 
Prove that the BLUE is given by
g: STC"‘(X — 51)
Also, ﬁnd the minimum variance.
ssume t at 15M] As[n]  w[n] for n _ 0,1,...,N — 1 are observed, where
lélgi] is zero mean noise with covariance matrix C and s[n] is a known signal
e am? ltllde A 1S t0 be estimated using a BLUE. Find the BLUE and discuss
giliimllﬁpveélfsiaftiife. Mo] Sm ' ‘ ‘ SW lllT 1S an elgenvector of C. A180. ﬁnd the
czmgliglaxzrion ‘Sf 1311: :57. We (éan always represent the signal vector s as a linear
_ genvec ors o C. This is because we can always ﬁnd N
eigenvectors that are linearly independent. Furthermore it can be shown that
these eigenvectors are orthonormal due to the symmetric nature of C. Hence, an
orthogonal representation of the signal is
where {v0,v1, . . . ,vN_1} are the orthonormal eigenvectors of C. Prove that the
minimum variance of the BLUE is
var(A) =
where A,- is the eigenvalue of C corresponding to the eigenvector vi. Next show
that the signal energy 8 = sTs is given by 2i? af. Finally, prove that if the
energy is constrained to be some value 80, then the smallest pOSSibIQ variance of
the BLUE is obtained by choosing the signal
where vmin is the eigenvector of C corresponding to the minimum eigenvalue and
c is chosen to satisfy the energy constraint £0 = sTs = c2. Assume that the
eigenvalues of C are distinct. Explain why this result makes sense.
6.9 In an on-off keyed (OOK) communication system we transmit one of two signals
s0(t)=0 OﬁtﬁT
to represent a binary 0 or
to represent a binary 1. The amplitude A is assumed to be positive. To determine
which bit was transmitted we sample the received waveform to produce
a:[n]=s,»[n]+w[n] n=O,1,...,N—1.
If a 0 was transmitted, we would have s,-[n] = 0, and if a 1 was transmitted, we
would have s,-[n] = Acos 21rf1n (assuming a sampling rate of 1 sample/s). The
noise samples given by w[n] are used to model channel noise. A simple receiver
might estimate the sinusoidal amplitude A and decide a 1 was sent if A > y, and
a 0 if A < 'y. Hence, an equivalent problem is to estimate the amplitude A for
the received data
a:[n]=Acos21rf1n+w[n] n=0,1,...,N—1
where w[n] is zero mean noise with covariance matrix C = 021. Find the BLUE
for this problem and interpret the resultant detector. Find the best frequency in
the range 0 5 f1 < 1/2 to use at the transmitter.
Sample at
“(zl = 2 hlklfk Figure 6.5 FIR ﬁlter estimator
'°=° for Problem 6.11
6.10 We continue Problem 6.9 by examining the question of signal selection for an
OOK system in the presence of colored noise. Let the noise w[n] be a zero mean
WSS random process with ACF
o k=1
Find the PSD and plot it for frequencies 0 5 f 5 1/2. As in the previous
problem, ﬁnd the frequency which yields the smallest value of the BLUE variance
for N = 50. Explain your results. Hint: You will need a computer to do this.
6.11 Consider the problem of estimating a DC level in WSS noise or given
where w[n] is a zero mean WSS random process with ACF rum, [k], estimate A. It
is proposed to estimate A by using the output of the FIR ﬁlter shown in Figure 6.5
at time n = N — 1. Note that the estimator is given by
The input a:[n] is assumed to be zero for n < 0. To obtain a good estimator we
wish to have the filter pass the DC signal A but block the noise w[n]. Hence,
we choose (exp(j0)) = 1 as a constraint and minimize the noise power at the
output ‘at time n = N — 1 by our choice of the FIR ﬁlter coeﬁicients h[k]. Find
the optimal ﬁlter coeﬁicients and the minimum noise power at the output of the
optimal ﬁlter. Explain your results.
6.12 Prove that the BLUE commutes over linear (actually affine) transformations of
0. Thus, if we wish to estimate
a=B0+b
where B is a known p >< p invertible matrix and b is a known p >< 1 vector, prove
that the BLUE is given by
a=Bé+b
where 6 is the BLUE for 0. Assume that x = H0 + w, where E(w) = 0 and
E(wwT) = C. Hint: Replace 0 in the data model by a.
6.13 In this problem it is shown that the BLUE is identical to the weighted least squares
estimator to be discussed in Chapter 8. In particular, the weighted least squares
estimator is found by minimizing the criterion
J = (x - H0)TC_1(X - H0). Appendix 6A
Prove that the 0 that minimizes J is the BLUE.
6.14 A noise process is composed of IID zero mean random variables with PDF
MWjsaexpl~é<wiil>l+asexplétitli
where 0 < e < 1. Such a PDF is called a Gaussian mixture. It is used to model
noise that is Gaussian with variance 0'1’; for 100(1 — e)% of the time and Gaussian
with variance 0? the remaining time. Typically, 0? >> 0% and e << 1, so that
the noise is predominately due to background noise with variance 0f; but also
contains occasional high level events or interferences modeled as Gaussian noise
with variance 0?. Show that the variance of this PDF is
Derivation of Scalar BLUE
To minimize var((§) = aTCa subject to the constrai T
. _ _ nt a = 1
Lagrangian multipliers. The Lagrangian function J becomess we use the method of
J = aTCa + }\(aTs -1)_
Using (4.3), the gradient with respect to a is
a = 2C3 + AS.
Setting this equai to the zero vector and solving produces
02 = (1 — Q02]; + 60?.
If {w2[0], w2[1], . . . , w2[N — 1]} is considered to be the data and 0%, e are assumed
known, ﬁnd the BLUE of 0?. Hint: Use the results of Problem 6.12.
6.15 For the general linear model A
a = —§'C_1S.
e agrangian multiplier A is found using the constraint equation as
where s is a known N x 1 vector and E(w) = 0, E(wwT) = C, ﬁnd the BLUE.
6.16 In implementing the BLUE for the linear model described in Theorem 6.1, we 8T8 = —%STC‘1s = 1
may unknowingly use the wrong covariance matrix to form the estimate or
é = (HTC"‘H)‘1HTC"x. i = 1
To examine the effect of this modeling error assume that we wish to estimate A  5° that the gradient is Zero with the constraint Satisﬁed for
as in Example 6.2. Find the variance of this estimator and that of the BLUE for  C 1
the case where N = 2 and a0 =
C = [ a (l) 1 The variance for this value of a is found from
c = l; 2i a” = 
Compare the variances for a —> 0, a = 1, and a —> oo. _ (sTC-isy
- " ' ' 'ﬁed by
. . ' ' d not _]l1St a local minimum 1s verl
That 30m is indeed the global minimum an
considering the func ion G z (a _ aOpt)TC(a _ aopt)
Expanding G, we have
0 — eTca-2afpeca+agpica°p“ Appendix 6B
— aTCa—2SSTCC_1CSa + STC-ls
= ewa- Téas Derivation of Vector BLUE
where we have used the constraint equa-ti°n' Hence’
aTCa = (a — a0pt)TC(a _ 30m) + STC—1S
To derive the BLUE for a vector parameter we need to minimize
This is because the positive deﬁnite PFOPeFtY
greater than zero for all la“ 30m) 7e 0' m“ ) 1 a1
which is uniquely minimized for a = Hep:-
1; 1; the right-hand side
of C makes the ﬁrs erm on for i = 1,2,. . . , p subject to the constraints
afhj=aij i=1,2,...,p;j=l,2,...,p.
This problem is much the same as that of the scalar BLUE (see Appendix 6A) except
for the additional constraints on a,» Now we have p constraints on a; instead of just
one. Since each a,- is free to assume any value, independently of the others, we actually
have p separate minimization problems linked only by the constraints. Proceeding in a
similar manner to that of Appendix 6A, we consider the Lagrangian function for a,»
Taking the gradient of the Lagrangian using (4.3)
We now let A,» = [AP Ag“ . . . ,\§;'>]T and note that H = [n1 h, . . 11,] to yield
Setting the gradient equal to zero yields
a,- = —§C IHA,» (6B.1)
To ﬁnd the vector of Lagrangian multipliers we use the constraint equations Since [e1 e2 _ _ _ eplT is the identity matrix Also’ the Covariance matrix of é is
aiTh,-=6,, j=1,2,...,p
or in combined form C‘; z E  _  _ EwenTl
111T o
_ _ where
hi1 o é - 12(9) = (HTc-IHyIHTc-‘(Ho + w) - E((§)
hi a,‘- = ]_ z (HTC-IED-IHTC-lw.
h; 0 c9 = E[(HTC*‘H)“‘HTC"‘wwTC"H(HTC“H)“].
Letting e,- denote the vector of all zeros except in the ith place, we have the constraint Since the covariance matrix of w is C’ we have
“mam” 0,; = (HTC"H)"‘HTC“CC"‘H(HTC‘1H)"
= (HTC-lln-l
HTa,~ = e,»
Using (6B.1), the constraint equations become
HTai ___ _%HTC_1HA1_ : ei with the minimum variances given by the diagonal elements of C‘; or
so that assuming invertibility of HTC‘1H, the Lagrangian multiplier vector is varwei) = eircéei : KHTCAHYIl“
_lx = (HTC»1H)—1e_ as previously derived.
Using this in (6B.1) yields the ﬁnal result
a”, = C‘1H(HTC“H)'1e,» (613.2)
and a corresponding variance of
var((§,») = aimCm-OP,
= eI-T(HTC'1H)'1HTC‘1CC'1H(HTC'1H)'1ei
"' E?(HTC_IH)_IB,'.
In like fashion to Appendix 6A, adopt can be shown to produce the global minimum.
We can express the vector BLUE in a more compact form by letting 
at.» eﬂﬂTc-lﬂrlﬂTrrlx 1%
A aim}: e§(HTC‘1H)'1HTC‘1x 
e; 
e2 T -1 -1 T -1 
= _ (H C H) H C x 
e}; 
= (HTC'1H)“HTC"x 
Chapter 7
Maximum Likelihood Estimation
7.1 Introduction
We now investigate an alternative to the MVU estimator, which is desirable in situations
where the MVU estimator does not exist or cannot be found even if it does exist. This
estimator, which is based on the maximum likelihood principle, is overwhelmingly the
most popular approach to obtaining practical estimators. It has the distinct advantage
of being a “turn-the-crank” procedure, allowing it to be implemented for complicated
estimation problems. Additionally, for most cases of practical interest its performance
is optimal for large enough data records. Speciﬁcally, it is approximately the MVU
estimator due to its approximate efficiency. For these reasons almost all practical
estimators are based on the maximum likelihood principle.
7.2 Summary
In Section 7.3 we discuss an estimation problem for which the CRLB cannot be achieved
and hence an efficient estimator does not exist, and for which the sufﬁcient statistic
approach in Chapter 5 to ﬁnding the MVU estimator cannot be implemented. An es-
timator is examined, however, that is asymptotically (for large data records) efﬁcient.
This is the maximum likelihood estimator (MLE), deﬁned as the value of 0 that maxi-
mizes the likelihood function. In general, as stated 1n Iheorem 7.1, the MLE has the
asymptotic properties of being unbiased, achieving the CRLB, and having a Gaussian
PDF. Thus, it can be said to be asymptotically optimal. In Example 7.6 it is also
shown that for signal in noise problems, the MLE achieves the CRLB for high SNRs.
The MLE of l9 can be used to ﬁnd the MLE of a function of l9 by invoking the invari-
ance property as described by Theorem 7.2. When a closed form expression cannot
be found for the MLE, a numerical approach employs either a grid search or an itera-
tive maximization of the likelihood function. The iterative approaches, which are used
only when a grid search is not practical, are the Newton-Raphson (7.29) and scoring
(7.33) methods. Convergence to the MLE, however, is not guaranteed. The MLE for
a vector parameter 0 is the value maximizing the likelihood function, which is now a
function of the components of 0. The asymptotic properties of the vector parameter
MLE are summarized in Theorem 7.3, and the invariance property in Theorem 7.4. For
the linear model the MLE achieves the CRLB for ﬁnite data records. The resultant
estimator is thus the usual MVU estimator as summarized in Theorem 7.5. Numerical
determination of the vector parameter MLE is based on the Newton-Raphson (7.48)
or scoring (7.50) approach. Another technique is the expectation-maximization 
algorithm of (7.56) and (7.57), which is iterative in nature. Finally, for WSS Gaussian
random processes the MLE can be found approximately by maximizing the asymptotic
log-likelihood function of (7.60). This approach is computationally less intensive than
determination of the exact MLE.
7.3 An Example
As a rationale for why we might be interested in an approximately optimal estimator,
we now examine an estimation problem for which there is no obvious way to ﬁnd
the MVU estimator. By resorting to the estimator based on the maximum likelihood
principle, termed the maximum likelihood estimator (MLE), we can obtain an estimator
that is approximately the MVU estimator. The nature of the approximation relies on
the property that the MLE is asymptotically (for large data records) efficient.
Example 7.1 - DC Level in White Gaussian Noise - Modiﬁed
Consider the observed data set
a:[n]=A+w[n] n=0,1,...,N—1
where A is an unknown level, which is assumed to be positive (A > 0), and w[n] is
WGN with unknown variance A. This problem differs from our usual one (see Example
3.3) in that the unknown parameter A is reﬂected in the mean and the variance. In
searching for the MVU estimator of A we ﬁrst determine the CRLB (see Chapter 3) to
see if it is satisﬁed with equality. The PDF is
P(X§A)= Qmﬁ exp ——— (rlnPAlz - (7-1)
Taking the derivative of the log-likelihood function (the logarithm of the PDF consid-
ered as a function of A), we have
  — —%+%Z(:c[n]—A)+§%5Z(r[n]—A)2
é I(A)(A - A).
It is certainly not obvious if the derivative of the log-likelihood function can be put in
the required form. It appears from a casual observation that it cannot, and therefore,
an efficient estimator does not exist. We can still determine the CRLB for this problem
to ﬁnd that (see Problem 7.1)
var(A) 3 N A2 (7.2)
(A+§)'
We next try to ﬁnd the MVU estimator by resorting to the theory of sufficient statistics
(see Chapter 5). Attempting to factor (7.1) into the form of (5.3), we note that
so that the PDF factors as
Based on the Neyman-Fisher factorization theorem a single sufﬁcient statistic for A is
T(x) = ZN '1 m2[n]. The next step is to ﬁnd a function of the sufficient statistic that
produces an unbiased estimator, assuming that T(x) is a complete sufficient statistic.
To do so we need to ﬁnd a function g such that
E [g  mqnﬂ] = A for all A > 0.
Since
E  1:2[n]] NE[122(n]]
= N(A + A2)
it is not obvious how to choose g. We cannot simply scale the sufficient statistic to
make it unbiased as we did in Example 5.8. _ . _ A N_1 2
A second approach would be to determine the conditional expectation E (A| EH20 r [n]),
where A is any unbiased estimator. As an example, if we were to choose the unbiased
estimator A = a:[0], then the MVU estimator would take the form
E  1539M) . (73)
Unfortunately, the evaluation of the conditional expectation appears to be a formidable
task.
We have now exhausted our possible optimal approaches. This is not to say that
we could not propose some estimators. One possibility considers A to be the mean, so
that
since we know that A > 0. Another estimator considers A to be the variance, to yield
(see also Problem 7.2). However, these estimators cannot be claimed to be optimal in
any sense. <>
Faced with our inability to ﬁnd the MVU estimator we will propose an estimator that
is approximately optimal. We claim that for large data records or as N —> oo, the
proposed estimator is efﬁcient. This means that as N —> oo
E(A) -> A (7.4)
var(A) -> CRLB, (1.5)
the CRLB being given by (7.2). An estimator A that satisﬁes (7.4) is said to be
asymptotically unbiased. If, in addition, the estimator satisﬁes (7.5), then it is said to
be asymptotically efficient. For ﬁnite data records, however, we can say nothing about
its optimality (see Problem 7.4). Better estimators may exist, but ﬁnding them may
not be easy!
Example 7.2 - DC Level in White Gaussian Noise - Modiﬁed (continued)
We propose the following estimator:
(7.6)
This estimator is biased since
E(A) =
for all A
It is nonetheless a reasonable estimator in that as N —> oo, we have by the law of large
numbers
and therefore from (7.6)
The estimator A is said to be a consistent estimator (see Problems 7.5 and 7 6) To ﬁnd
the mean and variance of A as N —> oo we use the statistical linearization argument
described in Section 3.6. For this example the PDF of i ZNII z2[n] will be concen-
trated about its mean’ A ‘l’ A2, for large data records. Tvhis Talltows us to linearize the
fumition given in (7.6) that transforms ﬁ Zfjol $2M] into  Let g be that function
so t at _ ’
Where u = — m‘, z2[n], and therefore,
g(u)=_5+ Urbz-
Linearizing about a0 =  = A + A?’ we have
9W) z 9W0) +  (u — a0)
It now follows that the asymptotic mean is
E(A) = A
so that A is asymptotically unbiased. Additionally, the asymptotic variance becomes,
from (7.7),
A+é var N a;[n]
vaﬂfl) ( i  (lfvi 2 )
=  var(a:2[n]).
But var(z2[n]) can be shown to be 4A3 + 2A2 (see Section 3.6), so that
var( ) N(A+%)24A (A+2)
N(A+%)
p(1o; 9)
9 Figure 7.1 Rationale for maxi-
92 91 mum likelihood estimator
which from (7.2) is the CRLB!
Summarizing our results, the proposed estimator given by (7.6) is asymptotically
unbiased and asymptotically achieves the CRLB. Hence, it is asymptotically efficient.
Furthermore, by the central limit theorem the random variable § 22:01 $2M] is Gaus-
sian as N -+ oo. Because A is a linear function of this Gaussian random variable for
large data records (as per (7.7)), it too will have a Gaussian PDF. <>
The proposed estimator is termed the MLE. How it was found is discussed in the next
section.
7.4 Finding the MLE
The MLE for a scalar parameter is deﬁned to be the value of 9 that maximizes p x;9
or x e , i.e. t e va ue that maximizes t e likelihoo unction. The maximization is
performed over the allowable range of 9. In the previous example this was A > 0. Since
p(x; 9) will also be a function of x, the maximization produces a 9 that is a function of x.
The rationale for the MLE hinges on the observation that p(x; 9) dx gives the probability
of observing x in a small volume for a given 9. In Figure 7.1 the PDF is evaluated for
x = x0 and then plotted versus 9. The value of p(x = x0; 9) dx for each 9 tells us the
probability of observing x in the region in RN centered around x0 with volume dx,
assuming the given value of 9. If x = x0 had indeed been observed, then inferring that
9 = 91 would be unreasonable. Because if 9 = 91, the probability of actually observing
x = x0 would be small. It is more “likely” that 9 = 9; is the true value. It yields a high
probability of observing x = x0, the data that were actually observed. Thus, we choose
9 = 92 as our estimate or the value that maximizes p(x = x0;9) over the allowable
range of 9. We now continue with our example.
Example 7.3 - DC Level in White Gaussian Noise - Modiﬁed (continued)
To actually ﬁnd the MLE for this problem we ﬁrst write the PDF from (7.1) as
(27TA)_¥  _ Alz '
exp -
Considering this as a function of A, it becomes the likelihood function. Differentiating
the log-likelihood function, we have
6lnp(x; A) N 1 NT 1 N71
"sf = ‘if; 2W ‘mm (“"1 W’
and setting it equal to zero produces
Solving for A produces the two solutions
We choose the solution
A=—§+ ﬁzas2[n]+z
to correspond to the permissible range of A or A > 0. Note that A > 0 for all possible
yalues of ﬁ 2,1220! x2[n]. Finally, that A indeed maximizes the log-likelihood function
1s veriﬁed by examining the second derivative. <>
Not only does the maximum likelihood procedure yield an estimator that is asymptot-
ically. efficient, it also sometimes yields an efficient estimator for ﬁnite data records.
This 1s illustrated in the following example.
Example 7.4 - DC Level in White Gaussian Noise
For the received data
where A is the unknown level to be estimated and w[n] is WGN with known variance
a2, the PDF is
MIRA) =  eXP ’% glwlnl ~ Alz
Taking the derivative of the log-likelihood function produces
"1"—§ffi-‘-‘l=§ jails-ﬁn
which being set equal to zero yields the MLE
B t we have already seen that the sample mean is an efﬁcient estimator (see Example
3.3). Hence, the MLE is efﬁcient. O
elihood
esult is true in general. If an eﬁiczent estimator BiEZStS, e mdmmum
This r e Problem 7.12 for an outline of the proof.
procedure will produce it. Se
7.5 Properties of the MLE A
- - ~ ‘ that for large data records
The example discussed in Section 7.3 ‘led to an estimator G _ PDF. In
(or asymptotically) was unbiased, achieved the CRLB, and had a aussian
summary, the MLE was distributed as
a e N(@.I'1(9)) "-8)
' ‘ t ‘b t d cordin to.” This result is quite general
where i’ denotes ‘asymptotically dls n uliety aotf the lVglLE. Of course, in practice it is
and forms the basis for claiming optima be in Order for (7.8) to hold. An analytical
Seldom knoll,“ 1.11113 (lglgllicbfkl (this Milli isriislisially impossible to derive. As an alternative
expression or _ _ _ . d- d
- lation 1S usually required, as iscusse
means of assessing performance, a computer simu
next.
Example 7 5 - DC Level in White Gaussian Noise - Modiﬁed (continued)
- ' d t d t ' e how large the data record had to be
ter simulation was perfolgulflln piiniiglgltlﬁe exact PDF of A (see (7.6)) could be
Carlo method (see Appendix
A compu l 1
t t‘ u ts to app
lglirildebaiftyxvrilguldll)Zegxtremely tedious. Using the Monte
7 A)’ M _-_- 1000 realizations ofQA were generated for various data record lengths. The
mean  and variance var(A) were estimated by
Ei; _ i i‘: ,1. (1.9)
( ) — M i=1 1
varlAl = M- 2 Q41‘ _  ' (710)
For a value of A equal to 1 the resul
lengths. Instead of the asymptotic variance or the CRLB 0f (7 2), We ta u a e
Nvar(A) =
ts are shown in Table 7.1 for various data record _
TABLE 7.1 Theoretical Asymptotic and Actual Mean and
Variance for Estimator in Example 7.2
Data Record ~ N XVariance,
Length, N Mean’ EU‘) Nvar(A)
20 0.996 (0.987) 0.707 (0.669)
Theoretical asymptotic value 1 0.667
since this is independent of N, allowing us to check the convergence more readily. The
theoretical asymptotic values of the mean and normalized variance are
E(A) = A=1
Nvar(,4) = 
It is observed from Table 7.1 that the mean converges at about N = 20 samples, while
the variance jumps around somewhat for N Z 15 samples. The latter is due to the
statistical ﬂuctutations in estimating the variance via a computer simulation, as well
as possible inaccuracies in the random number generator. To check this the number
of realizations was increased to M = 5000 for a data record length of N = 20. This
resulted in the mean and normalized variance shown in parentheses. The normalized
variance is now nearly identical to its asymptotic value, whereas for some unknown
reason (presumably the random number generator) the mean is off slightly from its
asymptotic value. (See also Problem 9.8 for a more accurate formula for 
Next, the PDF of A was determined using a Monte Carlo computer simulation.
This was done for data record lengths of N = 5 and N = 20. According to (7.8), the
asymptotic PDF is A
A 3' A/(A, F100).
which for A = 1 becomes, upon using (7.2),
In Figure 7.2 the theoretical PDF and estimated PDF or histogram (see Appendix 7A)
are shown. To construct the histogram we used M = 5000 realizations of A and divided
the horizontal axis into 100 cells or divisions. Note that for N = 5 the estimated PDF
is somewhat displaced to the left, in accordance with the mean being too small (see
Table 7.1). For N = 20, however, the match is better, although the estimated PDF still
appears to be skewed to the left. Presumably for larger data records the asymptotic
PDF will more closely match the true one. <>
/Theoretical asymptotic PDF
Histogram
/Theoretical asymptotic PDF
Histogram
(b) N = 2O
Figure 7.2 Theoretical PDF and histogram
In describing the performance of estimators whose PDFs cannot be determined analyt-
ically, we must frequently resort to computer simulations, as in the previous example.
In practice, the computer has become the dominant tool for analyzing the performance
of nonlinear estimators. For this reason it is important t0 be able to carry out such _
a simulation. In Appendix 7A a description of the computer methods used in the
previous example is given. Of course, the subject of Monte Carlo computer methods
for statistical evaluation warrants a more complete discussion. The interested reader
should consult the following references: [Bendat and Piersol 1971, Schwartz and Shaw
1975]. Some practice in performing these simulations is provided by Problems 7.13 and
7.14. We now summarize the asymptotic properties of the MLE in a theorem.
Theorem 7.1 (Asymptotic Properties of the MLE) I the PDF x;l9 of the
data x satisﬁes some “re ularit ” conditions, then the MLE 0 the unknown parameter
0 is asymptotically distributed (for large ata recor s according to
é 1 .N'(0, 1-1(o)) (7.11)
where I (l9) is the Fisher information evaluated at the true value of the unknown param-
eter.
The regularity conditions require the existence of the derivatives of the log-likelihood
function, as well as the Fisher information being nonzero, as described more fully in
Appendix 7B. An outline of the proof for IID observations is also given there.
From the asymptotic distribution, the MLE is seen to be asymptotically unbiased
and asymptotically attains the CRLB. It is therefore asymptotically eﬂicient, and hence
asymptotically optimal; Of course, in practice the key question is always How large
does N have to be for the asymptotic properties to apply’? Fortunately, for many cases
o interest the data record lengt s are not excessive, as illustrated by Example 7.5.
Another example follows.
Example 7.6 - MLE of the Sinusoidal Phase
We now reconsider the problem in Example 3.4 in which we wish to estimate the phase
4S of a sinusoid embedded in noise or
a:[n]=Acos(21rf0n+¢)+w[n] n=0,1,...,N——1
where w[n] is WGN with variance o2 and the amplitude A and frequency f0 are assumed
to be known. We saw in Chapter 5 that no single sufficient statistic exists for this
problem. The sufficient statistics were
T1(x) = ZIZI[TLlCOS(2TI'fQTL)
T2(x) :r[n] sin(21rf0n). (7.12)
The MLE is found by maximizing p(x; qS) or
p(x; ¢) = (EmTﬁ exp w?” "=0 (a:[n] - Acos(21rfOn + w)’
Or, equivalently, by minimizing
m) = Z (:c[n] - Acos(21rfQn + w)’. (7.13)
Differentiating with respect to ¢> produces
  = 2 g  - Acos(2vrfon + ¢>)) Asin(21rfOn + ¢>)
and setting it equal to zero yields
Z a:[n] sin(21rfOn + <15) = A Z sin(21rfOn + a5) cos(21rf@n + d2). (7.14)
But the right-hand side may be approximated since (see Problem 3.7)
% z sin(21rfQn +  cos(21rfQn +  = % Z sin(41rf0n + 2d) Q: 0 (7.15)
for f0 not near 0 or 1/2. Thus, the left-hand side of (7.14) when divided by N and set
equal to zero will produce an approximate MLE, which satisﬁes
z a:[n] sin(21rfOn +  = 0. (7.16)
Upon expanding this we have
Z :c[n] sin 21rfOn cos  = ~ Z x[n] cos 21rfOn sin 
or ﬁnally the MLE of phase is given approximately as
Z ."c[n] sin Zrrfon
(i5: —arctan%——i———. (7,17)
Z z[n] cos 21rfnn
It is interesting to note that the MLE is a function of the sufficient statistics. In hind-
sight, this should not be surprising if we keep in mind the Neyman-Fisher factorization
theorem. In this example there are two sufficient statistics, effecting a factorization as
14X; <15) = 9(T1(X), Tzfx)» WINK)-
Clearly, maximizing p(x; 4S) is equivalent to maximizing g (h(x) can always be chosen
so that h(x) > 0), and thus (i) must be a function of T1 (x) and T;
According to Theorem 7.1, the asymptotic PDF of the phase estimator is
éiN(¢.I-‘(¢>)>- (1-18)
TABLE 7.2 Theoretical Asymptotic and Actual Mean and
Variance for Estimator in Example 7.6
Data Record ~ N ><Variance,
Length, N Mm’ E0”) N var(q§)
Theoretical asymptotic value ¢ = 0.785 1/1] = 0.1
From Example 3.4
so that the asymptotic variance is
var(¢) = = — (7.19)
where 17 = (A2 / 2) /a2 is the SNR. To determine the data record length for the asymp-
totic mean and variance to apply we performed a computer simulation using A = 1, f0 =
0.08, ¢> = 1r/4, and a2 = 0.05. The results are listed in Table 7.2. It is seen that the
asymptotic mean and normalized variance are attained for N = 80. For shorter data
records the estimator is considerably biased. Part of the bias is due to the assumption
made in (7.15). The MLE given by (7.17) is actually valid only for large N. To ﬁnd the
exact MLE we would have to minimize J as given by (7.13) by evaluating it for all ¢>.
This could be done by using a grid search to ﬁnd the minimum. Also, observe from the
table that the variance for N = 20 is below the CRLB. This is possible due to the bias
of the estimator, thereby invalidating the CRLB which assumes an unbiased estimator.
Next we ﬁxed the data record length at N = 80 and varied the SNR. Plots of the
mean and variance versus the SNR, as well as the asymptotic values, are shown in
Figures 7.3 and 7.4. As shown in Figure 7.3, the estimator attains the asymptotic
mean above about —10 dB. In Figure 7.4 we have plotted 10logm  This has the
desirable effect of causing the CRLB to be a straight line when plotted versus the SNR
in dB. In particular, from (7.19) the asymptotic variance or CRLB is
10logmvar(¢) = 10logmN—n
= —10logmN— 10l0gm17.
Examining the results, we see a peculiar trend. For low SNRs the variance is higher
than the CRLB. Only at higher SNRs is the CRLB attained. Hence, the required
data record length for the asymptotic results to apply also depends on the SNR. To
Mean
Log-likelihood function
Phase, ¢
SNR (dB)
Figure 7.3 Actual vs. asymptotic mean for phase estimator (a) High SNR (10 dB)
l0 log“, variance
Log-likelihood function
e L", é
(J
Ti-ue d:
Phase, (p
SNR (dB)
(b) Low SNR (-15 as)
Figure 7.5 Typical realizations of log-likelihood function for phase
Figure 7.4 Actual vs. asymptotic variance for phase estimator
FIQISlSIiIIIIIIIQTIPiy, the asymkpltotic PDF of the MLE is valid for large enough data records.
the S £11 is Tlllinﬁlse PFOh errlns the CRLB. may be attained even for short data records if
g enoug . 0 see why this is so the phase estimator can be written from
understand why this occurs we plot typical realizations of the log-likelihood function (7 17)
- as
for different SNRs. As seen in Figure 7.5 for a high SNR, the maximum is relatively
stable from realization to realization, and hence the MLE exhibits a low variance. For
lower SNRs, however, the effect of the increased noise is to cause other peaks to occur.
Occasionally these peaks are larger than the peak near the true value, causing a large
estimation error and ultimately a larger variance. These large error estimates are said
to be outliers and cause the threshold effect seen in Figures 7.3 and 7.4. Nonlinear
estimators nearly always exhibit this effect. O
¢ = —arctan
o-IO
[A ¢0$(21Tfo'rl + ¢>) + w[n]] cos 21rfOn
P(wl"l) Pfwlﬂl; A)
(a) (b)
Figure 7.6 Non-Gaussian PDF for Example 7.7
z — arctan n
9% cos d1 + z w[n] cos 21rf0n
where we have used the same type of approximation as in (7.15) and some standard
trigonometric identities. Simplifying, we have
13f? . (7.20)
cos ¢> + Ni}, Z w[n] cos 21rf0n
¢A> z arctan
If the data record is large and/or the sinusoidal power is large, the noise terms will be
small. It is this condition, that the estimation error is small, that allows the MLE to
attain its asymptotic distribution. See also Problem 7.15 for a further discussion of this
point.
In some cases the asymptotic distribution does not hold, no matter how large the
data record and/or the SNR becomes. This tends to occur when the estimation error
cannot be reduced due to a lack of averaging in the estimator. An example follows.
Example 7.7 - DC Level in Nonindependent Non-Gaussian Noise
Consider the observations
where each sample of w[n] has the PDF p(w[n]) as shown in Figure 7.6a. The PDF is
symmetric about w[n] = 0 and has a maximum at w[n] = 0. Furthermore, we assume
all the noise samples are equal or w[0] = w[1] =  = w[N — 1]. In estimating A we
need to consider only a single observation since all observations are identical. Utilizing
only 1:[O], we ﬁrst note that the PDF of r[0] is a shifted version of p(w[n]), where the
shift is A as shown in Figure 7.6b. This is because p,[0](a:[0]; A) = pw[0]($[0] — A). The
MLE of A is the value that maximizes p,,,[ol(a:[0] — A), which because the PDF of w[0]
has a maximum at w[0] = 0 becomes
This estimator has the mean
E(A) = E(¢[0]) = A
since the noise PDF is symmetric about w[0] = 0. The variance of A is the same as the
variance of a:[0] or of w[0]. Hence,
var(A) =l uzpwgiﬂu) du
while the CRLB, is from Problem 3.2,
<dpw[0](u)>2
var(A) Z Loo pwmmu) u
and the two are not in general equal (see Problem 7.16). In this example, then, the
estimation error does not decrease as the data record length increases but remains the
same. Furthermore, the PDF of A = 1;[0] as shown in Figure 7.6b is a shifted version
of p(w[n]), clearly not Gaussian. Finally, A is not even consistent as N —> oo. <>
7 .6 MLE for Transformed Parameters
In many instances we wish to estimate a function of 9 the arameter charac erizin
the P . or example, we may not e 1nteres e in the value of a DC level A in WGN
but only in the power A2. In such a situation the MLE of A2 is easily found from the
MLE of A. Some examples illustrate how this is done.
Example 7.8 - Transformed DC Level in WGN
Consider the data
where w[n] is WGN with variance a2. We wish to ﬁnd the MLE of a = exp(A). The
PDF is given as
1 xp —~1—N_1(w{n]—A)’
(21ra2)1¥ e 202 0o < 0o ( )
"Yﬁttna-i-p-
meter 9 = A. However, since a is a one-to-one 3 ‘ p(x;A)
and is parameterized by the para
lently parameterize the PDF as
transformation of A, we can equiva
pT(x;a) =   exp —i'—2 11:0 - lna)2 a > 0 (7.22) A
where the subscript T indicates that the PDF is parameterized according to the trans-
formed parameter. Clearly, pT(x; a) is the PDF of the data set
:c[n]=lna+w[n] =0,1,...,N—1
valent. The MLE of a is found by maximizing  v
and (7.21) and (7.22) are entirely equi
(x; a) with respect to a equal to zero yields
(7.22) over a. Setting the derivative of pT
Aﬁ(1:[n]—lnd)% = 0
67 = exp(:f:).
But i: is just the MLE of A, so that
d = exp(A) = exp(0).
e transformed parameter is found by substituting the MLE of the original
The MLE of th
parame er i ns ormat1on. is property o the MLE is terme the invariance-
Example 7.9 - Transformed DC Level in WGN (Another Example)
ata set in the previous example. If Q0
Now consider the transformation a = A2 for the d
blem. Attempting to parameterize
we try to repeat the steps, we soon encounter a pro
p(x; A) with respect to a, we ﬁnd that
one. lf we choose A = J5, then some of the
lly require two sets of PDFs
Figure 7.7 Construction of modiﬁed likelihood function
1. For a ' e 1 f - _
for i1; £512,118 o a, say a0, determine whether p11 (x; a) or pT, (x; a) is larger. If,
since the transformation is not one-to-
PT1 (I; Q0) > Pr, (x; a0),
possible PDFs of (7.21) will be missing. We actua
then denote the value of pT (x-q ) as I; - R f
I3T(x;a)_ (Note that ﬁﬂma : 0) 0: p“; 351)). epeat or all a > 0 to form
1m (x. a) = —-—(2na2) g2;
PT: (ma) =   exp {_5g_2  + ﬂy] a > 0 (723) - The MLE is given as the a that maximizes pT(x;a) over a Z 0_
n=0 This procedure is illustrated in Fi ' -
_ gure 7.7. The function pT(x' a) can be th h f
amod dim/i d ' - - r. . . “gm”
‘ﬁe Z e I 0° functwn» hﬂvlng been derived from the original likelihood function
It is possible to ﬁnd the MLE of a as the value of a
by transforming the value of A that yields the maximum value for a given a I th'
example, for each a the possible values of A are dq/E Now from (7 24) the MLE " i
' 1 - a s
é‘ = “g ‘i135; {PM ~/5),P(X; —~/5)}
to characterize all possible PDFs.
that yields the maximum of pTl (x; a) and pT, (x; a) or
d = arg {I121 {pT1(x;a),pT, (x; a)}. (7.24)
Alternatively, we can ﬁnd the maximum in two steps as
= larg ‘I/Iéf) {Mm \/51l»P(X; ﬂ/i-lllz
= arg Ina-X pixiAll
—oo<A<o<>
' t h ld . The understanding is that d maximizes
nvarlance proper y Osirlce the standard likelihood function with a
so that again the i _ _
the modiﬁed likelihood function pﬂx; 01)
as a parameter cannot be deﬁned-
W ummarize the preceding discussion in the following theorem.
e s
Th 7 2 (Invariance PrOPQrW 0f the MLE) The MLE of the parameter a z
eorem - j—-——f""'_——”‘
9(0), where the PDF p(X;9l is pammeiemed b 9 is ‘"6" l’
é = 9(9)
h re (i is the MLE o l9 The MLE ofﬁ is obtained by mggimizing 91x30). Ifg is
w e -
  then d maximizes the modi ed likelihood nction 13T(X; 01),
no a o - - i
e ne as
pTixla) = {inggzlfinlpkwl
We complete our discussion by giving another 8148-1111918-
Example 7.10 - Power of WGN in dB
' ' 2 hose power in dB is to be estimated.
$76 ((1) bserve Ili Szilrllpifstiloli 1E}; vTrlizhlmivZ uvsve the invariance principle to ﬁnd the
o o so we rs n - i
power P in dB, which is deﬁned as
P = 10logm a2.
The PDF is given by
as”) =  "X" law  ” M '
Differentiating the log-likelihood fllnﬁtiml Pmduces
ainpsai - 1 -§...2._-iw2--. shill
P(X; 9)
a é = MLE b Figure 7.8 Grid search for MLE
and upon setting it equal to zero yields the MLE
a 
The MLE of the power in dB readily follows as
P = 10log10aA2
l0logm F z z2[n].
7.7 Numerical Determination of the MLE
A distinct advantage of the MLE is that we can always ﬁnd it for a given data set
numerically. This is because the MLE is determined as the maximum of a known
function, namely, the likelihood function. If, for example, the allowable values of l9 lie
in the interval [a, b], then we need only maximize p(x; 9) over that interval. The “safest”
way to do this is to perform a grid search over the [a, b] interval as shown in Figure 7.8.
As long as the spacing between l9 values is small enough, we are guaranteed to ﬁnd the
MLE for the given set of data. Of course, for a new set of data we will have to repeat
the search since the likelihood function will undoubtedly change. If, however, the range
of l9 is not conﬁned to a ﬁnite interval, as in estimating the variance of a noise process
for which 02 > 0, then a grid search may not be computationally feasible. In such a
case, we are forced to resort to iterative maximization procedures. Some typical ones are
the Newton-Raphson method, the scoring approach, and the expectation-maximization
algorithm. In general, these methods will produce the MLE if the initial guess is close
to the true maximum. If not, convergence may not be attained, or only convergence to a
iocal maximum. The difficulty with the use of these iterative methods is that in general
we do not know beforehand if they will converge and, even if convergence is attained,
whether the value produced is the MLE. An important distinction of our estimation
problem which sets it apart from other maximization problems is that the function to
be maximized is not known a priori. The likelihood function changes for each data set,
requiring the maximization of a random function. Nevertheless, these methods can at
times produce good results. We now describe some of the more common ones. The
interested reader is referred to [Bard 1974] for a more complete description of methods
for nonlinear optimization as applied to estimation problems.
As a means of comparison, we will apply the methods to the following example.
Example 7.11 - Exponential in WGN
Consider the data
where wln] is WGN with variance a2. The parameter r, the exponential factor, is to
be estimated. Allowable values are r > 0. The MLE of r is the value that maximizes
the likelihood function ‘t
P(X;1‘) = m
or, equivalently, the value that minimizes
1w = Z (m1 —r">’.
Differentiating J (r) and setting it equal to zero produces
(7.25) ~
This is a nonlinear equation in r and cannot be solved directly. We will now consider »
the iterative methods of Newton-Raphson and scoring. <>
The iterative methods attempt to maximize the log-likelihood function by ﬁnding a _.
zero of the derivative function. To do so the derivative is taken and set equal to zero,
yielding
3lnp(x; 9)
Then, the methods attempt to solve this equation iteratively. Let
and assume that we have an initial guess for the solution to (7.26). Call this guess 90.
Then, if g(9) is approximately linear near 9O, we can approximate it by
9(9) z 9W0) + @-
= 0. (7.2s)
(9 — 9o)
(7.27)
Figure 7.9 Newton-Raphson
method for ﬁnding zero of function
as shown in Figure 7.9. Next, we use (7.27) to solve for the zero 91, so that upon setting
g(01) equal to zero and solving for 91 we have
(90)
1 ° 49(9)
Again we linearize g but use the new guess, 91, as our point of linearization and repeat
the previous procedure to ﬁnd the new zero. As shown in Figure 7.9, the sequence of
guesses will converge to the true zero of g(9). In general, the Newton-Raphson iteration
ﬁnds the new guess, 9H1, based on the previous one, 9k, using
9k+1 = 9k - (7.28)
Note that at convergence 9H1 = 0,,’ and from (738) gwk) _—_ 0’ as deSjI-ed_ Since 9(9)
is the derivative of the log-likelihood function, we ﬁnd the MLE as
(1.29)
<921I1P(X;9) '1 191M101; 9)
6*“ Z 0'“ — liaézil ~60?‘
Several points need to be raised concerning the Newton-Raphson iterative procedure.
1. The iteration may not converge. This will be particularly evident when the second
derivative of the log-likelihood function is small. In this case it is seen from (7.29)
that the correction term may ﬂuctuate wildly from iteration to iteration.
2- Even if the iteration converges, the point found may not be the global maximum but
possibly only a local maximum or even a local minimum. Hence, to avoid these
possibilities it is best to use several starting points and at convergence choose the
one that yields the maximum. Generally, if the initial point is close to the global
maximum, the iteration will converge to it. The importance of a good initial guess
cannot be overemphasized. An illustration is given in Problem 7.18.
Applying the Newton-Raphson iteration to the problem in Example 7.11, we have
3lnp(x;r) __ 1 N-l n 1h,
62lnp(x;r) 1 N-l n_ N“ n_
T = F Z n(n —1):r[n]r 2 —— Z n(2n —— 1)r2 2
— Z nr"'2 [(11 —  — (2n — 1)r"] (7.30)
and thus the Newton-Raphson iteration becomes
 . (7.31)
A second common iterative procedure is the method of scoring. It recognizes that
32 lnp(x; 9)
(7.32)
where I (0) is the Fisher information. Indeed, for IID samples we have
1 Nd 32lnp(a:[n];l9)
62 lnp(x[n]; 0)
—Ni(0)
-I(0)
by the law of large numbers. Presumably, replacing the second derivative by its expected
value will increase the stability of the iteration. The method then becomes
6lnp(x; 9)
(7.33)
This approach is termed the method of scoring. It too suffers from the same convergence
problems as the Newton-Raphson iteration. As applied to the problem in Example 7.11,
TABLE 7.3 Sequence of Iterates
for Newton-Raphson Method
Iteration Initial Guess, r0
it produces upon using (7.30)
so that
Z (rlnl - TEMP?“
As an example of the iterative approach, we implement the Newton-Raphson method
for Example 7.11 using a computer simulation. Using N = 50, 1' = 0.5, and a2 = 0.01,
we generated a realization of the process. For a particular outcome —J (1') is plotted
in Figure 7.10 for 0 < r < 1. The peak of the function, which is the MLE, occurs
at r = 0.493. It is seen that the function is fairly broad for r < 0.5 but rather sharp
for larger values of r. In fact, for r > 1 it was difficult to plot due to the exponential
signal r", which causes the function to become very large and negative. We applied the
N ewton-Raphson method as given by (7.31) using several initial guesses. The resulting
iterates are listed in Table 7.3. For r0 = 0.8 or r0 = 0.2 the iteration quickly converged
to the true maximum. However, for r0 = 1.2 the convergence was much slower, although
the true maximum was attained after 29 iterations. If the initial guess was less than
about 0.18 or greater than about 1.2, the succeeding iterates exceeded 1 and kept
7A‘ = 0.493 “ /Tr\1e r = Q5
Figure 7.10 Function to be maximized for MLE
increasing, causing a computer overﬂow condition in the algorithm. In this case the
Newton-Raphson method fails to converge. As expected, the method works well when
the initial guess is close to the true MLE but may produce poor results otherwise.
A third method that has recently been proposed is the expectation-maximization
algorithm. Unlike the ﬁrst two methods, it requires some ingenuity on the part of
the designer. Its advantage, however, is its guaranteed convergence to at least a local
maximum under some mild conditions. Convergence to the global maximum is, as
with all iterative schemes, not assured. The expectation-maximization algorithm, not
being a “turn-the-crank” procedure, does not lend itself to all estimation problems. A
particular class of problems for which it is useful involves a vector parameter, and so
we defer our discussion until the next section.
7.8 Extension to a Vector Parameter
The MLE for a vector parameter 0 is deﬁned to be the value that maximizes the
likelihood function p(x; 0) over the allowable domain for 0. Assuming a differentiable
likelihood function, the MLE is found from
3lnp(x; 9)
a0 = 0. (7.35)
If multiple solutions exist, then the one that maximizes the likelihood function is the
MLE. We now ﬁnd the MLE for the problem in Example 3.6.
Example 7.12 - DC Level in WGN
Consider the data
Ila] =A+w[n] n=0,1,...,N—-1
zziziirszfif]iziﬁlgiiiaiiiszf.“     a = [Aer  w b.
3lnp(x;0) 1 N“
W- = gzhrlal-A)
wnpote) _ N 1 "-1
7 - _50_—2+2~0_—4Z(r[n]—fl)2.
Solving for A from the ﬁrst equation, we have the MLE
Solving for a2 in the second equation and using A = i: produces
U” = N (rial - if.
The MLE is O
a:
For a vector parameter the asymptotic properties of the MLE are summarized in the
following theorem.
Theorem 7.3 (Asymptotic Properties of the MLE (Vector Parameter» If the
PDF Pilﬁa) 0f the data x satisﬁes some “regularity” conditions then the MLE of the
unknown parameter 0 is asymptotically distributed according to
é i Nu». Pa» (1.36)
w ere 1(0) is the Fisher information matrix evaluated at the true value of the unknown
parameter.
The regularity conditions are similar to those discussed in Appendix 7B for the Scalar
parameter case. As an illustration, consider the e ' 1 It
Fhat [H0617 Port’ and Stone 1971] pr vious examp e. can be shQwn
53 ~ N(A,a2/N)
kiwi ~ Xlv-i (1.37)
11:0 a
and that the two statistics are independent. For large N it is well known from the
central limit theorem that 2 a
XN ~ N (N , 2N )
so that a
T NA/(N - 1,2(N — 1))
(N—1)a2 zuv- n04) (738)
a? = § grin] - a2 W  N.
The asymptotic PDF of é is jointly Gaussian since the statistics are individually Gaus-
sian and independent. Furthermore, from (7.37) and (7.38) for large N
Ew) _ (N—1)a2 —) l U2 l z
9(9) = 2(N—1)cr“‘ “’ 0 204 =1 (B)
where the inverse of the Fisher information matrix has previously been given in Example
3.6. Hence, the asymptotic distribution is described by (7.36).
In some instances, the asymptotic PDF of the MLE is not given by (7.36). These
situations generally occur when the number of parameters to be estimated is too large
relative to the number of data samples available. As in Example 7.7, this situation
restricts the averaging in the estimator.
Example 7.13 - Signal in Non-Gaussian Noise
Consider the data
where w[n] is zero mean IID noise with the Laplacian PDF
p<wini>=§e><p[-§|win1|] -
The signal samples {s[O], s[1], . . . , s[N — 1]} are t0 be estimated. The PDF of the data
11ml?) = Aﬁl 28w l-élrlnl - slnlll - (7-39)
The MLE of 9 -_- M0] s[1] . . . s[N — 1]]T is easily seen to be
or, equivalently,
It is clear that the MLE is not Gaussian even as N —> oo. The PDF of é is given by
(7.39) with x replaced by d. The difﬁculty is that no averaging is possible since we
have chosen to estimate as many parameters as data points. Therefore, the central limit
theorem, which accounts for the asymptotic Gaussian PDF of the MLE (see Appendix
7B), does not apply. <>
As in the scalar case, the invariance property holds, as summarized by the following
theorem.
Theorem 7.4 (Invariance Property of MLE (Vector Parameter)) The MLE of
the parameter a = g(0), where g is an r-dimensional function of the p >< 1 parameter
0, and the PDF p(x; 0) is parameterized by B, is given by
d = 2(9)
for 0A, the MLE of 0. If g is not an invertible function, then d maximizes the modiﬁed
likelihood function pT(x; a), deﬁned as
{ 011x113 on 110K; 9)-
An example of the use of this theorem can be found in Problem 7.21.
In Chapter 3 we discussed the computation of the CRLB for the general Gaussian
case in which the observed data vector had the PDF
X ~N(#(9)»C(9))-
In Appendix 3C (see (3C.5)) it was shown that the partial derivatives of the log-
likelihood function were
6lnp(x;0) 1 ( _, 6C(9)> 6,u(0)T _1
60k 2 I ( ) 69k + 69k ( XX M l)
-5<x- sunfTj-lix - ma» (r40)
for k = 1, 2,. . . , p. By setting (7.40) equal to zero we can obtain necessary conditions
for the MLE and on occasion, if solvable, the exact MLE. A somewhat more convenient
form uses (3C.2) in Appendix 3C to replace 6C'1(9)/6l9;, in (7.40). An important
example of the latter is the linear model described in detail in Chapter 4. Recall that
the general linear data model is
x = H0+w (7.41)
where H is a known N >< p matrix and w is a noise vector of dimension N >< 1 with PDF
N (0, C). Under these conditions the PDF is
exp [—él~(x - H0)TC‘1(x - 110)]
so that the MLE of 0 is found by minimizing
He) = (x - H0)Tc"(x — H9) (1.42)
is a positive deﬁnite
Since this is a quadratic function of the elements of 0 and C mg (7.40) and noting
matrix, differentiation will produce the global minimum. Now, us
a w) = H0
and that the covariance matrix does not depend on 0, we have
3lnp(x; 0) z 3(H9)T C—1(x _ H9)
Combining the partial derivatives to form the gradient, we have
6lnp(x; 9) : 3(H0)T C—1(x _ H0)
so that upon setting the gradient equal t0 Zen) we have
HTC‘1(x - us) = o.
Solving for é produces the MLE
o“ _ (HTC-IID-IHTC-lx (7.43)
_ _ - ' t l1 as an
But this estimator was shownin ChﬂPteT 4 t0 be the MyUcisgéma or as we
efficient estimator. As such, 0 is unblased and has a Comma“
C9 = (HTC-IH)“, (7.44)
. ' ' ' ' function of x) so that the
Finally, the PDF of the MLE is Gaussian (belng a lmea-r _ ' d F th
asymptotic properties of Theorem 7.3 are attained even for ﬁniteldata recormsgrizzri be
lineal- mode] it w“ be said that the MLE is optimal. These resu ts are sum Y
the following theorem.
Theorem 7.5 (Optimality 0f a“? MLE for th
data x are described by the geneﬂll 111E611?‘ model
e Linear Model) If the observed
(7.45)
0 is a p >< 1 parameter
where H is a known N >< p mairiw with N > P and 0f "Ink P, than the MLE ofa
vector to be estimated, and w is a noise vector with PDF Ni": C)’
é = (HTC-lfn-IHTC-IL (7.46)
0 is also an efficient estimator in that it attains the CRLB and hence is t e
estimator. The PDF of 0 is
0" ~  (HTC-‘1H)-1)_ 
Many examples are given in Chapter 4. The preceding result can be generalized to
assert that if an eﬂicient estimator exists, it is given by the MLE (see Problem 7.12).
In ﬁnding the MLE it is quite common to have to resort to numerical techniques of
maximization. The Newton-Raphson and scoring methods were described in Section
7.7. They are easily extended to the vector parameter case. The Newton-Raphson
iteration becomes
621m ,0) "a1 (,0)
where
[62lnp(x;0)] z 62lnp(x;0) i= 1,2,...,p
is the Hessian of the log-likelihood function and alngffo) is the p >< 1 gradient vector.
In implementing (7.48) inversion of the Hessian is not required. Rewriting (7.48) as
62lnp(x;0) 0 _ 82 lnp(x; 0) 0 _ 6lnp(x;0)
aaaoT ,=,,_ H‘ 7 aaaoT H, k a0
we see that the new iterate, 9H1, can be found from the previous iterate, 0k, by solving
a set of p simultaneous linear equations.
The scoring method is obtained from the Newton-Raphson method by replacing the
Hessian by the negative of the Fisher information matrix to yield
6 ln p(x; 9)
(7.49)
0k+1 = 0k + I_1(0) 
As explained previously, the inversion may be avoided by writing (7.50) in the form
of (7.49). As in the scalar case, the Newton-Raphson and scoring methods may suffer
from convergence problems (see Section 7.7). Care must be exercised in using them.
Typically, as the data record becomes large, the log-likelihood function becomes more
nearly quadratic near the maximum, and the iterative procedure will produce the MLE.
A third method of numerically determining the MLE is the expectation-maximization
(EM) algorithm [Dempster, Laird, and Rubin 1977]. This method, although iterative
in nature, is guaranteed under certain mild conditions to converge, and at convergence
to produce at least a local maximum. It has the desirable property of increasing the
likelihood at each step. The EM algorithm exploits the observation that some data
sets may allow easier determination of the MLE than the given one. As an example,
consider
r[n]=2cos21rf,-n+w[n] n=0,1,...,N—1
where w[n] is WGN with variance a2 and the frequencies f = [f1 f2 . . . fp]T are to be
estimated. The MLE would require a multidimensional minimization of
J(f) = z  - zcoﬂirfm) .
On the other hand, if the original data could be replaced by the independent data sets
ih-[n] = cos 27ff¢n + w,- (7.51)
where w,-[n] is WGN with variance 0,2, then the problem would be decoupled. It is easily
shown that due to the independence assumption the PDF would factor into the PDF
for each data set. Consequently, the MLE of f,- could be obtained from a minimization
J(f,’)=Z(y,~[n]—cos21rf,-n)2 i=1,2,...,p.
The original p-dimensional minimization has been reduced to p separate one-dimensional
minimizations, in general, an easier problem. The new data set {g1 [n], g2 [n], . . . , y?
is termed the complete data and can be related to the original data as
win] =  yini (w)
if the noise is decomposed as
For this decomposition to hold we will assume that the w, noise processes are inde-
pendent of each other and
0'2 = Z62. (7.53)
The key question remains as to how to obtain the complete data from the original or
incomplete data. The reader should also note that the decomposition is not unique.
We could have just as easily hypothesized the complete data to be
and thus
as a signal and noise decomposition.
In general, we suppose that there is a complete to incomplete data transformation
given as
X = s(yi, n, - - - . m) = g0) (7-54)
where in the previous example M = p and the elements of y, are given by (7.51).
The function g is a many-to-one transformation. We wish to ﬁnd the MLE of 0 by
maximizing ln p,(x; 9). Finding this too difficult, we instead maximize ln py(y; 9). Since
y is unavailable, we replace the log-likelihood function by its conditional expectation
Ellllllnpl/(yigll = l1I1Py(y;0)p(y|x;0) dy. (7.55)
Finally, since we need to know 0 to determine p(y|x;0) and hence the expected log-
likelihood function, we use the current guess. Letting 0;, denote the kth guess of the
MLE of 0, we then have the following iterative algorithm:
Expectation  Determine the average log-likelihood of the complete data
w. 0k) = /1npi<y;@>p<y|x;ai>dy- (1.56)
Maximization  Maximize the average log-likelihood function of the complete
data
9k+1 = 358111314019» ail- (7.57)
At convergence we hopefully will have the MLE. This approach is termed the EM
algorithm. Applying it to our frequency estimation problem, we have the following
iteration as derived in Appendix 7C.
E Step: Fori=1,2,...,p
yiln] = cos 21rfim + ,8,(z[n] - Z cos 21rf,~,,n). (7.5s)
M Step: Fori=1,2,...,p
f1,“ = arg  z i], [n] cos 27Tfi7l (Z59)
where the ,6,’s can be arbitrarily chosen as long as 2P ,5, = 1_
N°te_thﬂt_5i($[_n] — €:1C0S27Tfikn) in (7.58) is an estimate of w,»[n]. Hence, the
algorithm iteratively decouples the original data set into p separate data sets, with each
imetlrzlonls/ilsltgig <f)f a single sinusoid in.WGN. The maximization given above corresponds
data? P obla siéiglle sinusoid with the data set given by the estimated complete
disadvzeei ro emth.  Good results have been obtained with this approach. Its
form anrgﬂtglf: arst e' i cu tyhof determining the conditional expectation in closed
Gaussian ar 1 rariness in t e choice of the complete data. Nonetheless, for the
‘ problem this method can easily be applied. The reader should consult [Feder
and Weinstein 1988] for further details.
7.9 Asymptotic MLE
In many cases it is difIicult to evaluate the MLE of a parameter whose PDF is Gaussian
due to the need to invert a large dimension covariance matrix. For example, if x ~
N(0, C(0)), the MLE of 0 is obtained by maximizing
(21% deté (can)
If the covariance matrix cannot be inverted in closed form, then a. search technique
will require inversion of the N >< N matrix for each value of 0 to be searched. An
alternative approximate method can be applied when x is data from a zero mean WSS
random process, so that the covariance matrix is Toeplitz (see Appendix 1). In such
a. case, it is shown in Appendix 3D that the asymptotic (for large data records) log-
likelihood function is given by (see (3D.6))
exp [— —xTC'1(0)x] .
P(X; 9) = 2
lnp(x;0) = —%ln21r - g /_ [lnPm(f) + Pi df (7.60)
where 2
11f) = w Z w») eXIK-ﬂﬂf")
is the periodogram of the data and P,,( f) is the PSD. The dependence of the log-
likelihood function on 0 is through the PSD. Differentiation of (7.60) produces the
necessary conditions for the MLE
alnpctwzl  1 _ 1(1) F1241) d].
i 1 1(1) 6Pzz(f) _
[1 lP..<f)'Pz.<r)l a1. "*0" "'61)
The second derivative is given by (3D.7) in Appendix 3D, so that the Newton- Raphson
or scoring method may be implemented using the asymptotic likelihood function. This
leads to simpler iterative procedures and is commonly used in practice. An example
Example 7.14 - Gaussian Moving Average Process
A common WSS random process has the ACF
The PSD is easily shown to be
Pug) = 11+ b[1] exp(—j21rf) + b[2] exp(—j41rf)|2.
The process  is obtained by passing WGN of variance 1 through a ﬁlter with system
function B(z) = 1 + b[1]z'1 + b[2]z'2. It is usually assumed that the ﬁlter B(z) is
minimum-phase or that the zeros z1,z2 are within the unit circle. This is termed
a moving average (MA) process of order 2. In ﬁnding the MLE of the MA ﬁlter
parameters b[1], b[2], we will need to invert the covariance matrix. Instead, we can use
(7.60) to obtain an approximate MLE. Noting that the ﬁlter is minimum-phase
f lnPm(f)df=0
for this process (see Problem 7.22), the approximate MLE is found by minimizing
)5 1(1) df
s 11 + 1>[11e><p<—121rf) + 1)21e><p(-141rf)|’ '
However,
B(z) = (1 -— z1z'1)(1— z2z'1)
with |z1| < 1, Izg] < 1. The MLE is found by minimizing
)1 1(1) df
_g |1— zl exp(-j21rf)|2|1— z; exp(—j21rf)|2
over the allowable zero values and then converting to the MA parameters by
bill = -(Z1+Z2)
For values of z1 that are complex we would employ the constraint z; = zf. For real
values of zl we must ensure that Z2 is also real. These constraints are necessitated by
the coeﬁicients of B( z) being real. Either a grid search or one of the iterative techniques
could then be used. <>
Another example of the asymptotic MLE is given in the next section.
7.10 Signal Processing Examples
In Section 3.11 we discussed four signal processing estimation examples. The CRLB
Was determined for each one. By the asymptotic properties of the MLE the asymptotic
PDF of the MLE is given by A/(O, I"1(0)), where I'1(0) is given in Section 3.11. We
110W determine the explicit form for the MLEs. The reader should consult Section 3.11
for the problem statements and notation.
Example 7.15 - Range Estimation (Example 3.13)
From (3.36) the PDF is seen to be
P(X¥Tl0) = H
V21ra2 exp i
¢m exp l“
v 21m? ex
In this case the continuous parameter To
has been discretized as no = TO/A.
likelihood function simpliﬁes to
The "i
202 _ (—2r[n]s[n — no] + s2[n — n0])
exp [-
or, equivalently, by minimizing
Z (—2r[n]s[n — no] + s2[n — 120]).
But g [n-no] =2
[n] and is not a function of no. Hence, the MLE 0f 
no is found by maximizing
(7.62) 
By the invariance principle, since R
(cA/2)fi@. Note that the MLE of the
possible received signals and then ch
= 01-0/2 = cn0A/2, the MLE of range is R = l
delay no is found by correlating the data with all
oosing the maximum.
Example 7.16 - Sinusoidal Parameter Estimation (EXamPle 3'14)
The PDF is given as
pg; g) = (Big exp —2%2 (rlnl ~ Acos(21rfon + ¢llé
here A > 0 and 0 < f0 < i/2. The MLE of amplitude A, frequency f0. and Phase ¢
is found by minimizing
“A, foyrb) =_-  - Acos(21rfOn + ¢))2.
We ﬁrst expand the cosine to yield
m. f0. ¢> = Z (m1 - Aces mos 2mm + AS111 win Qrfonf-
A1 h h J ' nonquadratic in A and d; we may transform it to a quadratic function
t oug 1s i
by letting
a1 _—. Acos¢
a2 = —ASlIl¢
which is a one-to—one transformation. The inverse transformation is given by
A = ﬂaf+a§
¢> = arctan (i) . (7.63)
Also, let
c = [1 cos 21rf0...cos 21rf0(N — l)]T
[0 sin21rf0 . ..sin21rfo(N — lllT-
Then, we have
(x — oqc — a2s)T(x — 011C - 0125)
= (x — Ha)T(x — H0)
J’(a1aa2af0)
: T d H = c5, Hence, the function to be minimized over ‘or iS
gxlzlitacltiycltthatgridbziintgled in the[lii]1ear model in (7.42) with C = I. The minimizing
solution is, from (7.43),
d = (HTHWHTX (7.64)
so that
1102,, 02,, f0) (x - Hd)T(x - Ha)
(x — H(HTH)'1HTx)T(x — H(HTH)'1HTX)
= xT(I — H(HTH)_1HT)x
since I — H(HTH)_1HT is an idempotent matrix. (A matrix is idempotent if A2 = A.)
Hence, to ﬁnd f0 we need to minimize J’ over f0 or, equivalently, maximize
xTH(HTH)‘1HTx.
Using the deﬁnition of H, we have the MLE for frequency as the value that maximizes
STX sTc sTs STX ‘ (T65)
Once f0 is found from the above expression, d can be found from (7.64), and then A43
from (7.63). An approximate MLE can be obtained if f0 is not near 0 or 1 / 2 (see also
Problem 7.24) since in this case
—- —- cos 1r Onsin 1r n
2N nzo
e: 0
and, similarly, cTc/N w 1/2 and sTs/N w 1/2 (see Problem 3.7). Then, (7.65) becomes
approximately
KCTXV _+_ (STx)2]
  cos 21rf0n) +   sin 21rf0n> 5]
Z win] exm-nwron)
0r the MLE of frequency is obtained by maximizing the periodogram
2  exp(-j21rfn)
2 a:[n] cos 21rf0n
(v.66)
over f. Then, from (7.64)
(Q)
a:[n] sin 21rf0n
and thus ﬁnally we have
z  cos 21rf0n
Z w[n1exp<—121rf0n>
Q5 = arctan
Example 7.17 - Bearing Estimation (Example 3.15)
The bearing B of a sinusoidal signal impinging on a line array is related to the spatial
frequency f, as
f, = Ecosﬁ.
Assuming the amplitude, phase, and spatial frequency are to be estimated, we have the
sinusoidal parameter estimation problem just described. It is assumed, however, that
the temporal frequency F0 is known, as well as d and c. Once f, is estimated using an
MLE, the MLE of bearing follows from the invariance property as
3 = arccos  .
To ﬁnd f, it is common practice to use the approximate MLE obtained from the peak Substituting this into (7.67) results in
of the periodogram (7.66) or
2 lnp(x'aqz)=—ﬂln21r—ﬂlnqz—ﬂ.
I(f1)= M Z rlnlexp(_.727rfsn) A
"=0 To ﬁnd a we must minimize 0i or
for M sensors. Here, ﬁn] is the sample obtained from sensor n. Alternatively, the
approximate MLE of bearing can be found directly by maximizing J (a) = / 21 |A( f)|2I ( f ) df.
1 : _ _ ~2 F _ l o e a 1s unction 1s qua rat1c in a, resu ting 1n t e g o a minimum upon differ-
I w) M g) mm] exp < J 7r Ocncos g) entiation. For k = 1, 2, . . . ,p we have
over a. <> am) _ % 8A*(f) 8A(f) ,
Example 7.18 - Autoregressive Parameter Estimation (Example 3.16) =  [A(f)eXp(j21|-fk) + A*(f) eXp(--j21ffk)] [(f) df_
-a
Tolenn Sitn; thlé Pvsvgviirsi use t e asymptotic orm o t e og 1 ei oo given y ( ) Since A(_f) = A1“) and I(__f) : Hf), We can rewrite this as
‘f’ MU)!’ a (Z) = 2 ’ A(f)I(f)eX1>(i21rfk) df-
we have a[ 1 E
N N % 5 I ( f Setting it equal to zero produces
lnp(x; a, a3) = —?ln21r — 3 /_% ln IAEW + IAYFL df- l p
As shown in Problem 7 22 since .A(z) is minimum-phase (required for the stability of /_1 (1 + all] exp(_j27rfl)) Hf) expwaﬂfk) df = 0 k : 1’2""’p (T68)
1/.A(z)), then 2 l=l
_% n‘ m‘ f 2am 21 1m expuwnk — 1)] df = — 1 I(f)e><1>U21rfk]df-
and therefore 1 [:1 2 2
N N N é But ffé I ( f) exp[j21r fk]df is just the inverse Fourier transform of the periodogram
ln p(x; a, oi) = -5 ln 21r — 5 ln a3 — 202 f I |A(f)|2I(f) df. (7.67) k evaluated at k. This can be shown to be the estimated ACF (see Problem 7.25):
u "a
Differentiating with respect to of: and setting the result equal to zero produces firm z % Z  + | |k| S N _ 1
7% +  [i lA(f)I’I(f)df = 0 ° ‘k’ 2 N"
u u j l The set of equations to be solved for the approximate MLE of the AR ﬁlter parameters
and thus a becomes
(£3 =/_% [A(f)|2I(f)df. Zé[l]f'11[k—l] = audit] k: 1,2,...,p
or in matrix form
fzrlol Swill 72ml? _ ll all] frzlll
imlll frrlol Pulp _ 2l él2l Pzxlfzl
I Z I 2 = — I . (7.69)
12m — ll mlv — 2] iwlol ﬁlrl ma]
These are the so-called estimated Yule- Walker equations and this is the autocorrelation
method of linear prediction. Note the special form of the matrix and the right-hand
vector, thereby allowing a recursive solution known as the Levinson recursion [Kay
1988]. To complete the discussion we determine an explicit form for the MLE of o5.
Hom (7.68) we note that
i A<f>r<f>exp<j2rfk>df =0 k =1.2.....p.
Since
a
a» b
we note that all terms in the sum are zero except for k = 0 due to (7.69). It follows
that (we let a[0] = 1)
m’. Amara‘) d;
ilk] é I(f)@Xi>(—J'21rfk)df
Finally, the MLE is given by
(7.70)
References
Bard, Y., Nonlinear Parameter Estimation, Academic Press, New York, 1974.
Bendat, J.S., A.G. Piersol, Random Data: Analysis and Measurement Procedures, J. Wiley, New
York, 1971.
Bickel, P.J., K.A. Doksum, lblathematical Statistics, Holden-Day, San Francisco, 1977.
Dempster, A.P., NM. Laird, D.B. Rubin, “hlaximum Likelihood From Incomplete Data via the
EM Algorithm,” Ann. Roy. Statist. Soc., Vol. 39, pp. 1-38, Dec. 1977.
Dudewicz, E.J., Introduction to Probability and Statistics, Holt, Rinehart, and Winston, New York,
l-‘eder. M., E. Weinstein, “Parameter Estimation of Superimposed Signals Using the EM Algo-
rithm,” IEEE Trans. Acoust, Speech, Signal Process, Vol. 36, pp. 4774189, April 1988.
I-loel, P.G., S.C. Port, C.J. Stone, Introduction to Statistical Theory, Houghton Mifﬂin, Boston,
Kay, S.M, Modern Spectral Estimation: Theory and Application, Prentice-Hall, Englewood Cliffs,
Rao, C.R. Linear Statistical Inference and Its Applications, J. Wiley, New York, 1973.
Schwartz, M., L. Shaw, Signal Processing: Discrete Spectral Analysis, Detection, and Estimation,
McGraw-Hill, New York. 1975.
Problems
7.1 Show that the CRLB for A in Example 7.1 is given by (7.2).
7.2 Consider the sample mean estimator for the problem in Example 7.1. Find the
variance and compare it to the CRLB. Does the sample mean estimator attain
the CRLB for ﬁnite N? How about if N —-+ oo? Is the MLE or the sample mean
a better estimator?
7.3 We observe N IID samples from the PDFs:
a. Gaussian 1
 z mexp  _ lilzil '
b. Exponential
92- pa,» ___{ Aexpé-Ax) 
In each case ﬁnd the MLE of the unknown parameter and be sure to verify that
it indeed maximizes the likelihood function. Do the estimators make sense?
i .4 Asymptotic results can sometimes be misleading and so should be carefully applied.
As an example, for two unbiased estimators of 0 the variances are given by
var(01) = 7,-
var(02) = 7V" + W .
Plot the variances versus N to determine the better estimator.
7 5 A formal deﬁnition of the consistency of an estimator is given as follows. An
estimator 0 is consistent if, given any 6 > 0,
Prove that the sample mean is a consistent estimator for the problem of estimating
a DC level A in white Gaussian noise of known variance. Hint: Use Chebychev’s
inequality.
7 6 Another consistency result asserts that if a = 9(9) for 9v a" Continuous function’
l and é is consistent for 0, then 64 = g(9i) is consistent for a. Using the statistical
linearization argument and the formal deﬁnition of consistency, show why this 1s
true. Hint: Linearize g about the true value of 0.
7 7 Consider N IID observations from the exponential family of PDFs
p(z; 0) = exp [A(0)B(w) + C(15) ‘l’ 0(9)]
where A B C and D are functions of their respective arguments. Find an equa-
tion to be solved for the MLE. Now apply your results to the PDFs in Problem
7 8 If we observe N IID samples from a Bernoulli experiment (coin toss) with the
probabilities
ﬁnd the IVILE Of p.
7 9 For N IID observations from a Ll [0, 9] PDF ﬁnd the MLE of 9-
7.10 If the data set
is observed, where s[n] is known and w[n] is WGN with known variance a2, ﬁnd
the MLE of A. Determine the PDF of the MLE and whether or not the asymptotlC
7.12 In this problem we prove that if an efficient estimator exists, the maximum like-
lihood method will produce it. Assuming a scalar parameter, if an efficient esti-
mator exists, then we have
3 lnp(x; 9)
60 = 1(0)(é - 0).
Use this to prove the theorem.
7.13 We observe N IID samples of a DC level in WGN or
where A is to be estimated and w[n] is WGN with known variance a2. Using a
Monte Carlo computer simulation, verify that the PDF of the MLE or sample
mean is N(A,az/N Plot the theoretical and computer-generated PDFs for
comparison. Use A = 1, a2 = 0.1, N = 50, and M = 1000 realizations. What
happens if M is increased to 5000? Hint: Use the computer subroutines given in
Appendix 7A.
7.14 Consider the IID data samples r[n] for n = 0,1,..., N— 1, where  ~ N(0, a2).
Then, according to Slutsky’s theorem [Bickel and Doksum 1977], if :7: denotes the
sample mean and
denotes the sample variance, then
a:
Although this may be proven analytically, it requires familiarity with convergence
of random variables. Instead, implement a Monte Carlo computer simulation to
ﬁnd the PDF of s/(a/JN") for a2 = 1 and N = 10, N = 100. Compare your
results to the theoretical asymptotic PDF. Hint: Use the computer subroutines
given in Appendix 7A.
7.15 In this problem we show that the MLE attains its asymptotic PDF when the
estimation error is small. Consider Example 7.6 and let
e, = ——-— w[n]sin21rf0n
ea = —- w[n]cos21rf0n
NA nzo
7.11 Find an equation to be solved for the MLE of the correlation coefﬁcient p in
Problem 3.15. Can you ﬁnd an approximate solution if N —> oo? Hint: Let
277;‘? 1ﬂnl/N —> 1 and 21:01 x§[n]/N —> 1 in the equation where x[n] =
in (7.20). Assume that f0 is not near 0 or 1 / 2, so that the same approximations
can be made as in Example 3.4. Prove ﬁrst that e, and ct are approximately
uncorrelated and hence independent Gaussian random variables. Next, determine
the PDF of e_,,ec. Then, assuming c, and ec to be small, use a truncated Taylor
series expansion of  about the true value 4'2. This will yield
<25 = 90sec)
a S? C
z “(L0H 3905.6.) L(E_E_)
3e 68 +
a 6‘
e,=O.r.c=0 6C c,=0.ec=O
where the function g is given in (7.20). Use this expansion to ﬁnd the PDF of 
Compare the variance with the CRLB in Example 3.4. Note that for es, cc to be
small, either N —> oo and/or A —+ oo.
7.16 For Example 7.7 determine the var(A) as well as the CRLB for the non-Gaussian
PDF (termed a Laplacian PDF)
P011101) = §eXP(—lw[0l|)-
Does the MLE attain the CRLB as N —> oo?
7.17 For N IID observations from a N(0, l/9) PDF, Where 9 > 0, ﬁnd the MLE 0f 9
and its asymptotic PDF.
7.18 Plot the function
g(x) = exp —§z + 0.1exp ~§(x — 10)
over the domain -3 5 x _<_ 13. Find the maximum of the function from the graph.
Then, use a Newton-Raphson iteration to ﬁnd the maximum. In doing so use the
initial guesses of x0 = 0.5 and x0 = 9.5. What can you say about the importance
of the initial guess?
x[n]=cos21rf0n+w[n] n=0,1,...,N-—1
where w[n] is WGN with known variance a2, show that the MLE of frequency is
obtained approximately by maximizing (f0 not near 0 or 1 / 2)
Z  cos 21rf0n
over the interval 0 < f0 < 1/2. Next, use a Monte Carlo computer simulation
(see Appendix 7A) for N = 10, f0 = 025,02 = 0.01 and plot the function to be
maximized. Apply the Newton-Raphson method for determining the maximum
and compare the results to those from a grid search.
7.20 Consider the data set
where s[n] is unknown for n = 0,1,...,N — 1 and w[n] is WGN with known
variance 02. Determine the MLE of s[n] and also its PDF. Do the asymptotic
MLE properties hold? That is, is the MLE unbiased, efficient, Gaussian, and
consistent?
7.21 For N IID observation from the PDF N (A, a2), where A and a2 are both un-
known, ﬁnd the MLE of the SNR a = AQ/az.
7.22 Prove that if
.A(z) = 1+ Z a[k]z"°
is a minimum-phase polynomial (all roots inside the unit circle of the z plane),
then
f ln|A(f)l2df=0.
To do so ﬁrst show that
%ln|A(f)|2df = 211% 1 fln/KA-i-Z}
a 271'
where the contour is the unit circle in the z plane. Next, note that
is the inverse z transform of ln .A(z) evaluated at n = 0. Finally, use the fact that
a minimum-phase .A(z) results in an inverse z transform of ln A(z) that is causal.
7.23 Find the asymptotic MLE for the total power P0 of the PSD
where l
/_ Q(f) df = 1.
If Q( f) = 1 for all f so that the process is WGN, simplify your results. Hint: Use
the results from Problem 7.25 for the second part.
7.24 The peak of the periodogram was shown in Example 7.16 to be the MLE for
frequency under the condition that f0 is not near 0 or 1/ 2. Plot the periodogram
for N = 10 for the frequencies f0 = 0.25 and f0 = 0.05. Use the noiseless data
x[n]=cos21rf0n n=0,1,...,N—1.
What happens if this approximation is not valid? Repeat the problem using the
exact function
xTH(HTH)_1HTx
where H is deﬁned in Example 7.16.
7.25 Prove that the inverse Fourier transform of the periodogram is
Hint: Note that the periodogram can be written as
where X’ (f) is the Fourier transform of the sequence
x  _ { O otherwise.
7.26 An AR(1) PSD given as
11+ a[11e><p<-121rf>|"
If a[1] and a2 are unknown, ﬁnd the MLE of P,,( f) for f = f0 by using (759)
and (7.70). What is the asymptotic variance of the MLE’? Hint: Use (3.30) and
(3.44).
Appendix 7A
Monte Carlo Methods
We now describe the computer methods employed to generate realizations of the
random variable (see Example 7.1)
where  = A + w[n§ for w[n] WGN with variance A > 0. Additionally, how the
statistical properties of A such as the mean, variance, and PDF are determined is
discussed. The description applies more generally in that the methods are valid for any
estimator whose performance is to be determined.
The steps are summarized as follows.
Data Generation:
1. Generate N independent Ll[0, 1] random variates.
2. Convert these variates to Gaussian random variates using the Box-Mueller
transformation to produce w[n].
3. Add A to w[n] to yield z[n] and then compute A.
4. Repeat the procedure M times to yield the M realizations of A.
Statistical Properties:
1. Determine the mean using (7.9).
2. Determine the variance using (7.10).
3. Determine the PDF using a histogram.
This approach is implemented in the accompanying Fortran computer program, MON-
TECARLO, which we now describe.
Histogram, p(:x:,')
Figure 7A.1 Histogram of com-
puter generated data
In the data generation we ﬁrst use a standard pseudorandom noise generator which
produces independent Ll[0,1] random variates. As an example, on a VAX 11/780 the
intrinsic function RAN can be used. In MONTECARLO the subroutine RANDOM
calls the RAN function N times (for N even). Next, we convert these independent
uniformly distributed random variates into independent Gaussian random variates with
mean 0 and variance 1 by using the Box-Mueller transformation
wl = \/—2lnu1 cos21rug
where ubu; are independent Ll[0,1] random variables and whw; are independent
N (0, 1) random variables. Since the transformation operates on two random variables
at a time, N needs to be even (if odd, just increment by 1 and discard the extra random
variate). To convert to N (0,a2) random variates simply multiply by a. This entire
procedure is implemented in subroutine WGN. Next, A is added to wln] to generate
one time series realization  for n = 0, 1, . . . , N — 1. For this realization of xln], A is
computed. We repeat the procedure M times to yield the M realizations of A.
The mean and variance are determined using (7.9) and (7.10), respectively. The
alternative form of the variance estimate
— 5(4)
var(A) = M Z A
is used by the subroutine STATS. The number of realizations, M, needed t0 provide an
accurate estimate of the mean and variance can be determined by ﬁnding the variance
of the estimators since (7.9) and (7.10) are nothing more than estimators themselves. A
simpler procedure is to keep increasing M until the numbers given by (7.9) and (7.10)
converge. A
Finally, to determine the PDF of A we use an estimated PDF called the histogram.
The histogram estimates the PDF by determining the number of times A falls within
a speciﬁed interval. Then, a division by the total number of realizations to yield the
probability, followed by a division by the interval length, produces the PDF estimate. A
typical histogram is shown in Figure 7A.1 where the PDF is estimated over the interval
(Imimelmax). Each sub' t 1 ._A -
ith can is found as m erva ($1 W2, $1 +Aa$/2) 1s called a cell. The value for the
where L is th b f x
_ i e num er o realizations of x- that lie with' th '
estlmates the probability that x. will fan iin the ith in e 2th cell. Hence, Li/M
' A cell, and by dividing by Ax we
obtain the estimated PDF. If ' ~
interpolating lzhese points as slviljwihssificliiigeufeiaj7iAvlili>liotdif PDF at lhe Gen center’ the"
nous PDF. It is apparent fr th' d' ' i Ces an éstlmate of the contin-
Would like the can Width toeg: Snlizn lS:([I‘ll'11Si§l(i)SI1 ttgrzltuizr “gsod estimates of the PDF we
' are es imating not the PDF
p(z) but
w./ . Pee
01‘ the average PDF over the c ll U f .
(more cells), the probability of z: realizliitilntrlnfgltlirliyi  m; Ce“ Width becomes Smaller
Yields highly variable PDF estimates As before ganglolbd satifizjtu beclimles Smaller. This
until the estimated PDF appears t i ’ . 98y eeps increasing M
o converge. The histogram approach is implemented
in the subroutine HISTOG. A f h ' " -
{Bendat and Piersol 1971]. urt er discussion of PDF estimation can be found in
Fortran Program MONTECARLQ
. .. msizszaiiezesznzi   
C , PDF (see Examples 7.1-7.3, 7_5)_
The array dimen ' - .
with numerical ‘Sliiizsarerglzzn gazlsrvarlable. Replace them
_ - e 1s -
a plotting subroutine to replace PLOTI-Zida: Zziildiiiiill nkeged
num er
generator to replace the i t - - . _
subroutine RANDOM. n nnslc function RAN m the
QQQQ(
DIMENSION X01) .w(N) ,AHAT(M) PDF
,, JMNCELLS) » (NCELLSLHIsTQicELLs)
PI=4 . *ATAN(1 .)
C Input the value f A
° , the number of data points N, and the
number of reallzatlons M.
WRITE(6,10)
READ(5.*)A,N,M
C Generate M realizations of the estimate of A.
no 4o K=1,M
C Generate the noise samples and add A t h
o eac one.
CALL WGN(N,A,W)
20 X(I)=A+W(I)
C Generate the estimate of A.
C XMEAN — Sample mean
C XVAR — Sample variance
DIMENSION X(1)
no 3o I=1,N XSQ=O.
3o XSQ=XSQ+X(I)*X(I)/N no 1o I=1,N
AHAT(K)=-O.5+SQRT(XSQ+O.25)
C Compute the mean and variance of the estimates of A.
CALL STATS(AHAT,M,AMEAN,AVAR)
C Normalize the variance by N.
XMEAN=XMEAN+X(I)/N
10 XSQ=XSQ+X(I)*X(I)/N
SUBROUTINE HISTOG(X,N,XMIN,XMAX,NCELLS,XX,HIST)
IF(ARG.LT.50.)PDF(I)=(1./SQRT(2.*PI*SIG2))*EXP(-O.5*ARG)
C Compare graphically the asymptotic PDF to the histogram.
CALL PLOT(XX,HIST,PDF)
SUBROUTINE STATS(X,N,XMEAN,XVAR)
is the value for XX(I), I=1,2,..,NCELLS
WRITE(6,*)AMEAN,AVAR C This subroutine computes a histogram of a set of data.
C Input the interval (XMIN,XMAX) and the number of cells C
C for the histogram. C Input parameters:
WRITE(6,50) C
50 FORMAT(’ INPUT XMIN,XMAX,NCELLS FOR HISTOGRAM’) C X — Input data array of dimension Nx1
READ(5,*)XMIN,XMAX,NCELLS C N - Number of data samples
C Compute the histogram. C XMIN — Minimum value for histogram horizontal axis
CALL HISTOG(AHAT,M,XMIN,XMAX,NCELLS,XX,HIST) C XMAX - Maximum value for histogram horizontal axis
C Compute the asymptotic variance. C NCELLS — Number of cells desired
SIG2=A*A/(N*(A+O.5)) C
C Compute the asymptotic Gaussian PDF for comparison to C Output parameters:
C the histogram. C
DO 60 I=1,NCELLS C XX - Array of dimension NCELLSx1 containing
ARG=(XX(I)—A)*(XX(I)-A)/SIG2 C cell centers
IF(ARG.GT.50.)PDF(I)=O. C HIST — Histogram values for each cell where HIST(I)
DIMENSION X(1),XX(1),HIST(1)
CELLWID=(XMAX—XMIN)/FLOAT(NCELLS)
10 XX(I)=XMIN+CELLWID/2.+(I—1.)*CELLWID
C This program computes the sample mean and variance HIST(K)=O.
C of a data set. CELLMIN=XX(K)-CELLWID/2.
C CELLMAX=XX(K)+CELLWID/2.
C Input parameters: DO 20 I=1,N
C 20 IF(X(I).GT.CELLMIN.AND.X(I).LT.CELLMAX)HIST(K)=
C X - Array of dimension Nxl containing data * HIST(K)+1./(N*CELLWID)
C N - Number of data set 30 CONTINUE
C Output parameters: END
SUBROUTINE PLOT(X,Y1,Y2)
DIMENSION x(1).Y1(1),Y2(1)
Replace this subroutine with any standard Fortran-compatible
plotting routine.
SUBROUTINE WGN(N,VAR,W)
This subroutine generates samples of zero mean white
Gaussian noise.
Input parameters:
N — Number of noise samples desired
VAR — Variance of noise desired
Output parameters z
W — Array of dimension Nxl containing noise samples
DIMENSION W(1)
PI=4.*ATAN(1.)
Add 1 to desired number of samples if N is odd.
IF(MOD(N,2).NE.O)N1=N+1
Generate N1 independent and uniformly distributed random
variates on [0,1].
CALL RANDOM(N1,VAR,W)
Convert uniformly distributed random variates to Gaussian
ones using a Box—Mueller transformation.
U1=W(2*I—1)
U2=W(2*I)
TEMP=SQRT(-2.*ALOG(U1))
W(2*I-1)=TEMP*COS(2.*PI*U2)*SQRT(VAR)
w(2*I)=TEMP*SIN(2.*PI*U2)*SQRT(VAR)
SUBROUTINE RANDOM(N1,VAR,W)
DIMENSION W(1)
For machines other than DEC VAX 11/780 replace RAN(ISEED)
with a random number generator.
W(I)=RAN(11111)
Appendix 7B
Asymptotic PDF of MLE for a Scalar
Parameter
We now give an outline of the proof of Theorem 7.1. A rigorous proof of consistency
can be found in [Dudewicz 1976] and of the asymptotic Gaussian property in [Ran
1973]. To simplify the discussion the observations are assumed to be IID. We further
assume the following regularity conditions.
1. The ﬁrst-order and second-order derivatives of the log-likelihood function are well
deﬁned.
Epmaglle] 
We ﬁrst show that the MLE is consistent. To do so we will need the following inequality
(see [Dudewicz 1976] for proof), which is related to the Kullback-Leibler information.
/ln  p(x[n];01) dx[n] Z 0 (7B.1)
with equality if and only if 61 = 02. Now, in maximizing the log-likelihood function,
we are equivalently maximizing
1111(6) lllﬁluw)
But as N —> oo, this converges to the expected value by the law of large numbers.
Hence, if 00 denotes the true value of 0, we have
W i lnp(r[n];0) —> /lnp(x[n];0)p(z[n];60)  (7B.2)
___-_-,—»——\II—$—§§I—_§C—--
However, from (7B.1)
f 111 [Pﬁvln];Qillpﬁﬂlnlﬂildrlnl 2 f1I1Lv($l"l;92)l1>(1l"l;91)drlnl-
The right-hand side of (7B.2) is therefore maximized for 0 = 90. By a suitable continuity
argument, the left-hand side of (7B.2) 0r the normalized log-likelihood function must
also be maximized for 0 = 00 or as N —> oo, the MLE is  = 00. Thus, the MLE is
consistent.
To derive the asymptotic PDF of the MLE we ﬁrst use a Taylor expansion about
the true value of 9, which is 00. Then, by the mean value theorem
8lnp(x; 0) _ 8lnp(x; 0)
+ 62 lnp(x; 0)
a0 6:6 a0
where 00 < é <  But
5lnp(x; 0)
by the deﬁnition of the MLE, so that
0 = 8lnp(x; 6)
32 lnp(x; 6)
_(é - 00). (713.3)
Now consider \/:7_V— — 00), so that (7B.3) becomes
1 3Inp(x; 0)
(113.4)
0=é N 692 0=0o
h re the last convergence is due to the law of large numbers. Also, the numerator
term 1s 1 Nil alnpuhke)
m “:0 a0
NW z 3lnp(r[n]; 0)
g5 a random variable, being a function of  Additionally, since the z[n]’s are IID,
5Q are the ifs. By the central limit theorem the numerator term in (7B.4) has a PDF
that converges to a Gaussian with mean
E[_1_ ’§8lnp(r[n];0) l:
Wn=0 a6 0:00
andvariance
1“"1a1np(¢[n];a) 2 _ 1”" ﬁlnmwlnlwh)’ l
E (ﬁ-YWF l - W?“ 86 
= i(00)
due to the independence of the random variables. We now resort to Slutsky’s theorem
B’ kel and Doksum 1977] which says that if the sequence of random variables 2,, has
ihécasymptotic PDF of the random variable z and the sequence of random variables
y" converges to a constant c, then zn/yn has the Same asymptolilc PDF as the random
variable r/c. In our case,
a: ~ Name»)
yn _> c:i(60)
so that (7B.4) becomes A
WW0 _ 00) i 1W0, i"(00))
A a 1
9N” ia°vwwo>l
or, equivalently,
or ﬁnally A
0 4N(9<>J‘1(9o))-
Concatenating the cﬁs and yfs as
Appendix 7C
Derivation of Conditional
Log-Likelihood for EM Algorithm
Example
we have
From (7.56) we write the conditional expectation as
(109,911) = El1I1Py(y;9)|X;0kl
= E(h(y))x;0k)+cTE(y|x;0k). (7C.2)
Since we wish to maximize U (0, 0k) with respect to 0, we can omit the expected value
of h(y) since it does not depend on 0. Now, y and x are jointly Gaussian since, from
Hom (7.51) we have, upon noting the independent data set assumption, (7 52)
where I is the N >< N identity matrix and the transformation matrix is composed of
p identity matrices. Using the standard result for conditional expectations of jointly
Gaussian random vectors (see Appendix 10A), we have
lflpyllﬁe)
j exp ii- 2; Z (y,-[n] — cos 21rf,-n)2] }
P 1 "-1 E ;e =E +043; —E .
z c _ Z F 2  _ COS 27rfin)2  k)  U (x 
i=1 U1‘ n=O The means are given by
= 9(1) + Z —2 Z (111111 cos 2111211 - - c052 211121») "1
where c is a constant and g(y) does not depend on the frequencies. Making the ap-   c‘?
proximation that 211:0‘ cosz 21rfin z N / 2 for f,» not near 0 or 1 / 2, we have p
E(x) = Z c,-
mp1! (y i a) = My) + 2 Q Z yilnl COS 2711f in while the covariance matrices are
C“ = oil
or letting c,~ = [1 cos 21rfi . . . cos 21rf,-(N — 1)]T, we have W1
lnvdy; 6') = My) + Z Egciryi. (79-1) V‘;
W1 W1 T which is maximized over 0 by maximizing each term in the sum separately or
= E [I I I] _ fwd = arg  czlyi. (7C.4)
Finally, since the 0i s are not unique, they can be chosen ar itrari y as ong as (see
of! 0 0 I (153))
0 agl 0 I p 2 2
21 UP I or, equivalently, p p 2
ail ZE=Z$=1
so that
EGIIX; 0k) = ; + (7 (X — Z m)
c, ail l
E(yl-lx;0k)=c,-+a—g(x—zci) i=1,2,...,p
where c,~ is computed using 0k. Note that E(y,»|x; 0k) can be thought of as an estimate
of the yi [n] data set since, letting $3 = E(y,-|x; 0k),
 = cos Zvrfikn + g  —  cos 21rfikn) . (7C.3)
Using ("(C2) and dropping the term that is independent of 0, we have
Um at) = Z at
yaprwwwwwIwIIIIIYIIIIIIIIIII1
Chapter 8
Least Squares
8.1 Introduction
In previous chapters we have attempted to ﬁnd an optimal or nearly optimal (for large
data records) estimator by considering the class of unbiased estimators and determining
the one exhibiting minimum variance, the so-called MVU estimator. We now depart
from this philosophy to investigate a class of estimators that in general have no op-
timality properties associated with them but make good sense for many problems of
interest. This is the method of least squares and dates back to 1795 when Gauss used
the method to study planetary motions. A salient feature of the method is that no
probabilistic assumptions are made about the data, only a signal model is assumed.
The advantage, then, is its broader range of possible applications. On the negative
side, no claims about optimality can be made, and furthermore, the statistical perfor-
mance cannot be assessed without some speciﬁc assumptions about the probabilistic
structure of the data. Nonetheless, the least squares estimator is widely used in practice
due to its ease of implementation, amounting to the minimization of a least squares
error criterion.
8.2 Summary
The least squares approach to parameter estimation chooses 0 to minimize (8.1), where
the signal depends on 6. Linear versus nonlinear least squares problems are described
in Section 8.3. The general linear least squares problem which minimizes (8.9) leads
to the least squares estimator of (8.10) and the minimum least squares error of (8.11)-
(8.13). A weighted least squares error criterion of (8.14) results in the estimator of
(8.16) and the minimum least squares error of (8.17). A geometrical interpretation
of least squares is described in Section 8.5 and leads to the important orthogonality
principle. When the dimensionality of the vector parameter is not known, an order-
recursive least squares approach can be useful. It computes the least squares estimator
recursively as the number of unknown parameters increases. It is summarized by (8.28)-
(8.31). If it is desired to update in time the least squares estimator as more data become
Model
0 Noise inaccuracies
Error = e[n]
(b) Least squares error
Figure 8.1 Least squares approach
available, then a sequential approach can be employed. It cliletermiiéest ther 1122.85 
estimator based on the estimate at the previous time and _t e new a a. q _
(8 46)—(8 48) summarize the calculations required. At times the parameter vector is
Constrained, as in (850). In such a case the constrained leasst sqléares estitrlirligsgrf 01s
given by (8.52). Nonlinear least squares 1S discussed in Sectiioliji  . tomcmrlilsimization
converting the problem to a linear one are described, followe y 1 era ive h
approaches if this is not possible. The two methods that are generally usgd are t e
Newton-Raphson iteration of (8.61) and the Gauss-Newton iteration of (8.6 ).
8.3 The Least Squares Approach
Our focus in determining a good estimator has been to ﬁnd one that was unbiased and
had minimum variance. In choosing the variance as our measure of goodness we 1mg
plicitly sought to minimize the discrepancy (on the average) Ibetweerti ourtesglrritrilartlce
the true parameter value. In the least squares (LS) approac We a efflp _ l
the squared difference between the given data a:[n] and the assumed signal or nisehess
data. This is illustrated in Figure 8.1. The signal is genezrjited by somel mgijrlwmllélstilgl
turn depends upon our unknown parameter .0. The sign s[n] is pure y _ f -
Due to observation noise or model inaccuracies we observe a perturbed version o 
which we denote by  The least squares estimator (LSE) of 0 chooses the value t a
makes s[n] closest to the observed data  Closeness is measured by the LS error
criterion
1(0) = Zeta] — Sh]? (8.1)
where the observation interval is assumed to be n = 0, 1,. . . , N — 1, and the dependence
of J on 0 is via s[n]. The value of 0 that minimizes J (0) is the LSE. Note that no
probabilistic assumptions have been made about the data  The method is equally
valid for Gaussian as well as non-Gaussian noise. Of course, the performance of the
LSE will undoubtedly depend upon the properties of the corrupting noise as well as
any modeling errors. LSEs are usually applied in situations where a precise statistical
characterization of the data is unknown or where an optimal estimator cannot be found
or may be too complicated to apply in practice.
Example 8.1 - DC Level Signal
Assume that the signal model in Figure 8.1 is s[n] = A and we observe r[n] for n =
0, 1,. . . , N — 1. Then, according to the LS approach, we can estimate A by minimizing
(8.1) or
1w) = Z (W11 - AV-
Differentiating with respect to A and setting the result equal to zero produces
or the sample mean estimator. Our familiar estimator, however, cannot be claimed to
be optimal in the MVU sense but only in that it minimizes the LS error. We know,
however, from our previous discussions that if  = A+ w[n], where w[n] is zero mean
WGN, then the LSE will also be the MVU estimator, but otherwise not. To underscore
the potential diﬂiculties, consider what would happen if the noise were not zero mean.
Then, the sample mean estimator would actually be an estimator of A + E(w[n]) since
w[n] could be written as
where w’ is zero mean noise. The data are more appropriately described by
It should be clear that in using this approach, it must be assumed that the observed
data are composed of a deterministic signal and zero mean noise. If this is the case,
the error e[n] =  - s[n] will tend to be zero on the average for the correct choice of
the signal parameters. The minimization of (8.1) is then a reasonable approach. The
reader might also consider what would happen if the assumed DC level signal model
were incorrect, as for instance if z[n] = A+Bn+w[n] described the data. This modeling
error would also cause the LSE to be biased. <>
Example 8.2 - Sinusoidal Frequency Estimation
Consider the signal model
sln] = cos 21rf0n
in which the frequency f0 is to be estimated. The LSE is found by minimizing
Jot) = Zeta — C0S27Tf0n)2'
In contrast to the DC level signal for which the minimum is easily found, here the LS
error is highly nonlinear in f0. The minimization cannot be done in closed form. Since
the error criterion is a quadratic function of the signal, a signal that is linear in the
unknown parameter yields a quadratic function for J, as in the previous example. The
minimization is then easily carried out. A signal model that is linear in the unknown
parameter is said to generate a linear least squares problem. Otherwise, as in this
example, the problem is a nonlinear least squares problem. Nonlinear LS problems are
solved via grid searches or iterative minimization methods as described in Section 8.9.
It should be noted that the signal itself need not be linear but only in the unknown
parameter, as the next example illustrates. <>
Example 8.3 - Sinusoidal Amplitude Estimation
If the signal is s[n] = Acos 21rf0n, where f0 is known and A is to be estimated, then
the LSE minimizes N 1
J(A) = 2  — Acos 21rf0n)2
over A. This is easily accomplished by differentiation since J (A) is quadratic in A. This
linear LS problem is clearly very desirable from a practical viewpoint. If, however, A
were known and the frequency were to be estimated, the problem would be equivalent
to that in Example 8.2, that is to say, it would be a nonlinear LS problem. A ﬁnal
possibility, in the vector parameter case, is that both A and f0 might need to be
estimated. Then, the error criterion
J(A, f0) = 2am] - Acos21rf0n)2
is quadratic in A but nonquadratic in f0. The net result is that J can be minimized
in closed form with respect to A for a given f0, reducing the minimization of J to one
over onl . Th‘ t ' - - - . .
f0 y 1S YPe of Problem, III WhlCh the signal is linear in some parameters
but nonlinear in others, is termed a bl l t -
will discuss this further. 36mm e gas squares probleln In Sectlon 89 We
8.4 Linear Least Squares
r1 aPP ylrlg t e linear LS approach fOr a Scalar Para-meter we must assume that
Where hlnl is a known sequence (The reader ma
' fer back to Ch t 6
the BLUE f ~ - - - y want t° re . . a!’ er °“
or a COmPar 1S0" t0 Slmllar Slgrlal Irwdels.) The LS error criterion becomes
1(0) = Zea] — 01414)’. (83)
T_i—'—- (8.4)
e minimum LS error, obtained by substituting (8.4) into (8.3), is
1m = J<@> = Z (rial - éhlnlXwlnl - éhinii
Z zlnKrlnl — 911M) — 6 Ni h['fl]($[n] 4911M)
Z xzlnl _ é  (85)
The last step follows because the s S ' ' ‘ - .
by using (8.4) We can rewrite Jmln ‘in ls Zero (Substitute 0 to venfy). Altematwely’
~_. ( 21mins)
(8.6)
The minimum LS error is the original energy of the data 0r 25;; izlnl less that due
to the signal ﬁtting. For Example 8.1 in which 0 - A we have h[n] 1» 5° that Tom
(8.4) 14 = i‘ and from (8.5)
. . _ fect
If the data were noiseless so that  f A, then Jmin - 0 0T We Wougd have a ger
LS ﬁt to the data. On the other hand, if  = A + win], Where EC" [nil >> A » the“
25;)‘ xﬂn] / N >> i’. The minimum LS error would then be
or not much different than the original error. It can be shown (see Prob em 8 ) a
the minimum LS error is always between these two extremes or
0 s Jmin s Nilﬁlnl- (87)
The extension of these results to a vector parameter 0 of dimension p x 1 is straight-
forward and of great Practlcal utlhty- For the Slgnal 5 lslol 5 slN  O e
linear in the unknown parameters, we assume, using matrix notation,
s = H0 (3-8)
~ t ' N > of full rank p. The matrix H is referred to
Zshisalli: Iolbgfsriiuziiilziivrrlnzjziiapﬁizis  (if coufsb, the linear model, albeit without the usual
noise PDF assumption. Many examples of signals satisfying this model can be found
in Chapter 4. The LSE is found by minimizing
1(0) = Z (r1111 — SM)’
= (x- H0)T(x - H0). (8-9)
This is easily accomplished (since J is a quadratic function of 0) by using (4.3). Since
1(9) = xTx — XTHO — OTHTX + QTHTHQ
(note that xTHO is a scalar), the gradient is
3J(0)
a0 = —2HTX + 
Setting the gradient equal to zero yields the LSE
é = (HTHYlHTx. (s10)
The equations HTHO = HTx to be solved for é are termed the normal equations.
The assumed full rank of H guarantees the invertibility of HTH. See also Problem
8.4 for another derivation. Somewhat surprisingly, we obtain an estimator that has
the identical functional form as the eﬂicient estimator for the linear model as well as
the BLUE. That é as given by (8.10) is not the identical estimator stems from the
assumptions made about the data. For it to be the BLUE would require E(x) = H0
and C, = 021 (see Chapter 6), and to be eﬂicient would in addition to these properties
require x to be Gaussian (see Chapter 4). As a side issue, if these assumptions hold,
we can easily determine the statistical properties of the LSE (see Problem 8.6), having
been given in Chapters 4 and 6. Otherwise, this may be quite diﬂicult. The minimum
LS error is found from (8.9) and (8.10) as
= (x — H0)T(x — HOA)
= (x - H(HTH)"HTx)T (x - H(HTH)‘1HTx)
(I - H(HTH)_‘HT) (I - H(HTH)_1HT) x
(I - H(HTH)"HT) x. (8.11)
The last step results from the fact that I — H(HTH)—1HT is an idempotent matrix or
it has the property A2 = A. Other forms for Jmin are
Jrnin = xTx - xTH(HTii)-1HTx (8.12)
= xT(x - He). (8.1s)
An extension of the linear LS problem is to weighted LS. Instead of minimizing (8.9),
we include an N >< N positive deﬁnite (and by deﬁnition therefore symmetric) weighting
matrix W, so that
1(0) = (x - iie)Tw(x - H0). (s14)
If, for instance, W is diagonal with diagonal elements [W],-,- = w,- > 0, then the LS error
for Example 8.1 will be
J(A) = E w,,(x[n] —- A)?
The rationale for introducing weighting factors into the error criterion is to emphasize
the contributions of those data samples that are deemed to be more reliable. Again,
considering Example 8.1, if r[n] = A+w[n], where w[n] is zero mean uncorrelated noise
with variance 03,, then it is reasonable to choose w" = 1/03,. This choice will result in
I1IIIIIIwIIwiInwwiIi-—i----—i--i-Pi(
the estimator (see Problem 8.8)
A = 1»_=<>__"__ (8.15)
This familiar estimator is of course the BLUE since the w[n]’s are uncorrelated so that
W = C“ (see Example 6.2).
The general form of the weighted LSE is readily shown to be
9 = (HTWH)“HTWx (8.16)
and its minimum LS error is
1,... = XT (w - WH(HTWH)'1HTW)x (8-1?)
(see Problem 8.9).
8.5 Geometrical Interpretations
We now reexamine the linear LS approach from a geometrical perspective. This has the
advantage of more clearly revealing the essence of the approach and leads to additional
useful properties and insights into the estimator. Recall the general signal model s =
H0. If we denote the columns of H by hi, we have
so that the signal model is seen to be a linear combination of the “signal” vectors
Example 8.4 - Fourier Analysis
Referring to Example 4.2 (with M = l), we suppose the signal model to be i
s[n]=acos21rf0n+bsin21rf0n n=0,1,...,N-1
where f0 is a known frequency and 0 = [a b]T is to be estimated. Then, in vector form
we have
s[:1] = cos Svrfo sin Srfo [ L: ] I (8.18)
.s[N.- 1] cos 21rf0.(N - 1) sin 21rf0-(N - 1)
It is seen that the columns of H are composed of the samples of the cosinusoidal and
sinusoidal sequences. Alternatively, since
cos21rf0(N— 1) ]T
sin21rj'O(N—1)]T,
we have
S I (lhl + 
The LS error was deﬁned to be
J(0) = (x — H0)T(x — H0).
If we further deﬁne the Euclidean length of an N >< 1 vector ﬁ = [f1 f; . . . {NF as
then the LS error can be also written as
= ||x-Z0.-ii,-||2. (8.19)
We now see that the linear LS approach attempts to minimize the square of the distance
from the data vector x to a signal vector 2L1 0ih4, which must be a linear combination
of the columns of H. The data vector can lie anywhere in an N-dimensional space,
termed RN , while all possible signal vectors, being linear combinations of p < N vectors,
must lie in a p-dimensional subspace of RN , termed S". (The full rank of H assumption
assures us that the columns are linearly independent and hence the subspace spanned
is truly p-dimensional.) For N = 3 and p = 2 we illustrate this in Figure 8.2. Note
that all possible choices of 01, 02 (where we assume —oo < 01 < o0 and —oo < 62 < 0o)
produce signal vectors constrained to lie in the subspace S’ and that in general x does
not lie in the subspace. It should be intuitively clear that the vector s that lies in S 2 and
m Subspace spanned
(a) Signal SUbSPaCB (b) Orthogonal projection to deter-
mine signal estimate
Figure 8.2 Geometrical viewpoint of linear least squares in R3 l
that is closest to x in the Euclidean sense is the component of x in S2. Alternatively, s
1s the orthogonal projection of x onto S 2. This means that the error vector x — s must
be orthogonal to all vectors in S2. Two vectors in RN are deﬁned to be orthogonal if
xTy = 0. To actually determine s for this example we use the orthogonality condition.
This says that the error vector is orthogonal to the signal subspace or
(x — s) _L S 2
where _l_ denotes orthogonal (or perpendicular). For this to be true we must have
(X—§) _l_ b1
(X—§) d. h;
since then the error vector will be orthogonal to any linear combination of b1 and b2.
Using the deﬁnition of orthogonality, we have
(x — s)Tb1 =
(X — §lTh2
Letting s = 01h1 + 02b2, we have
(X - 91111 — 62h2)Th1
(X — 01th — 9gb2)Th2
In matrix form this is
(X “' H9)Tb1
(x — H0)Tb2 = O.
Combining the two equations yields
(x—H6)T[ b1 h; 1:07‘
(x - H0)TH = 0T. (3.20)
Finally, we have as our LSE A
0 = (HTHYIHTx.
Note that if e1: x — H0 denotes the error vector, then the LSE is found from (8.20) by
invoking the condition
6TH = 0T. (8.21)
The error vector must be orthogonal to the columns of H. This is the well-known
orthogonality principle. In effect, the error represents the part of x that cannot be
described by the signal model. A similar orthogonality principle will arise in Chapter 12
in our study of estimation of random parameters.
Again referring to Figure 82b, the minimum LS error is [|x — s||2 or
= (x-Hé)T(x-Hé).
In evaluating this error we can make use of (8.21) (we have already done so for the
scalar case in arriving at (8.5)). This produces
1m... = (x - H0A)T(x - H6l)
= xT(x _ Ho) _ éTHTe
= xT (1 - H(HTH)"HT) x. (8.22)
In summary, the LS approach can be interpreted as the problem of ﬁtting or ap-
proximating a data vector x in RN by another vector s, which is a linear combination
of vectors {b1,b2, . . . ,h,,} that lie in a p-dimensional subspace of RN. The problem is
solved by choosing s in the subspace to be the orthogonal projection of x. Many of
our intuitive notions about vector geometry may be used to our advantage once this
connection is made. We now discuss some of these consequences.
Referring to Figure 8.3a, if it had happened that b1 and b2 were orthogonal, then s
could have easily been found. This is because the component of s along b1 or s1 does
not contain a component of s along b2. If it did, then we would have the situation
in Figure 8.3b. Making the orthogonality assumption and also assuming that ]|h1|| =
Hhg“ = 1 (orthonormal vectors), we have
= (b,Tx)b1 + (bgx)b2
(_*—--v—\—ftp?wﬂiﬁﬁﬁ§Pﬂ¥ﬁiﬁtiiiiiiii‘III‘
and also
so that hl and h; are orthogonal but not orthonormal. Combining these results produces
HTH = (N/2)I, and therefore,
(a) Orthogonal h,» ’s
(b) Nonorthogonal h.~’s
= (HTm-IHTX
Figure 8.3 Effect of nonorthogonal columns of observation matrix 2 T
where hTx is the length of the vector x along h,» In matrix notation this is 2 NA k
' F :r[n] cos 21rﬁn
If we had instead deﬁned the signal as
s[n] = aw % cos (21rﬁn) + U‘) i? sin (21rgn)
then the columns of H would have been orthonormal. <>
so that
In general, the columns of H will not be orthogonal, so that the signal vector estimate
is obtained as
This result is due to the orthonormal columns of H. As a result, we have
s = H9 = H(HTH)_1HTx.
The signal estimate is the orthogonal projection of x onto the p-dimensional subspace.
The N >< N matrix P = H(HTH)‘1HT is known as the orthogonal projection matrix or
just the projection matrix. It has the properties
(HTHP = <1)“ = I
and therefore
0 = (HTHYlHTx = HTx.
No inversion is necessary. An example follows. L PT = P’ Symmetric
Example 8.5 - Fourier Analysis (continued) 2- P2 = P» idempotent-
That the projection matrix must be symmetric is shown in Problem 8.11, that it must be
idempotent follows from the observation that if P is applied to Px, then the same vector
must result since Px is already in the subspace. Additionally, the projection matrix
must be singular (for independent columns of H it has rank p, as shown in Problem
8.12). If it were not, then x could be recovered from s, which is clearly impossible since
many x’s have the same projection, as shown in Figure 8.4.
Continuing Example 8.4, if f0 = k/N, where k is an integer taking on any of the values
k = 1,2,. . . ,N/2 — l, it is easily shown (see (4.13)) that
hlThg = Z cos (Zwﬁn) sin (21rﬁn) = 0
_  5'19. ¢1;6"P\‘.:< .125"; apsxit. ,-
Figure 8.4 Vectors with same
projection onto subspace
Likewise, the error vector e = x —§ = (I — P)x is the projection of x onto the
complement subspace or the subspace orthogonal to the signal subspace. The matrix
PJ- = I — P is also a projection matrix, as can be easily veriﬁed from the properties
given above. As a result, the minimum LS error is, from (8.22), >
Jmin = xT(I—P)x
which is just Hell’.
In the next section we further uti
recursive LS solution.
lize the geometrical theory to derive an order-
8.6 Order-Recursive Least Squares
nknown and must be assumed. For example, consider
In many cases the signal model is u
ure 8.5a. The following models might be assumed
the experimental data shown in Fig
s2(t) = A + Bt
for 0 g t _<_ T. If data zzrln] are obtained by sampling :z:(t) at times t = nA, where A = 1
and n = 0, 1,. . . , N - 1, the corresponding discrete-time signal models become
Using a. LSE with
(a) Experimental data
Time, t
Time, t
Time, t
Figure 8.5 Experimental data ﬁtting by least squares
100 ‘Z Constant
Number of parameters, k
Figure 8.6 Effect of chosen number of parameters oii minimum least
squares error
would produce the estimates for the intercept and slope as
A. = :2 (8.23)
and (see Problem 8.13)
A _ 2(2N—1)N_1$n_ s N-‘n
A’ _ N(N+1)g l] N(N+1)g zln]
In Figures 8.5b and 8.5c we have plotted §1(t) = A; and é2(t) = Ag + Sgt, where
T = 100. The ﬁt using two parameters is better, as expected. We will later show that
the minimum LS error must decrease as we add more parameters. The question might
be asked whether or not we should add a quadratic term to the signal model. If we
did, the ﬁt would become better yet. Realizing that the data are subject to error, we
may very well be ﬁtting the noise. Such a situation is undesirable but in the absence of
a known signal model may be unavoidable to some extent. In practice, we choose the
simplest signal model that adequately describes the data. One such scheme might be to
increase the order of the polynomial until the minimum LS error decreases only slightly
as the order is increased. For the data in Figure 8.5 the minimum LS error versus
the number of parameters is shown in Figure 8.6. Models s1(t) and s2(t) correspond
to k = 1 and k = 2, respectively. It is seen that a large drop occurs at k = 2. For
larger orders there is only a slight decrease, indicating a modeling of the noise. In this
example the signal was actually
3(a) = 1 + 0.0m
and the noise w[n] was WGN with ' 2 = 0 1 -
estimated perfectly, the minimum LSl/elrrrlgzlfivaoiild have begiote that If A’ B had been
so that when the true order is reach d J - z 10 T ' ' - - -
also increases our conﬁdence in the claiosenmiiliodel l hls 1S venﬁed m Figure 8'6 and
In the preceding example we saw that ‘t ' l t .
LSE for several signal models. A straightfilarvlrvalrsdnalgiiriagcrlﬁ vcxﬁiiijlilzldbliiipziietczlaifenllilgg tfhe
l1 a 1 " _ - - °’
1332675226 jrszivsiiitg (8 10). Alternatively, the computation may be reduced by using an
e S approach. In this method we update the LSE in ord r S ‘ﬁ ll
we are able to compute the LSE based on an H of dimension N x (ke-l- up‘? cat}?
so u ion ase on an H of dimension N x k. For the previous example this update in
order would have been exceedingly 'm l h d th l
see why assume the signal models tzlbile) e a e co umns of H2 been orthogonaL To
igzerval Fg/I-M] 85W; higzélaw altered the observation interval to be the symmetric
assum tion igt th PP _ 0 e Oflglllﬂl [UJY — 1] interval. The effect of this
P 0 0r ogonalize the columns of H2 since now
The LSE is readily found since
is a diagonal matrix. The solutions are
Projection of h;
1 2 Figure 8.7 Gram-Schmidt
orthogonalization
and
In this case the LSE of A does not change as we add a parameter to the model and
follows algebraically from the diagonal nature of HgHq. In geometric terms this result
is readily apparent from Figure 8.3a. The orthogonality of the h,’s allows us to project
x along b1 and h; separately and then add the results. Each projection is independent
of the other. In general, the column vectors will not be orthogonal but can be replaced
by another set of p vectors that are orthogonal. The procedure for doing so is shown
in Figure 8.7 and is called a Gram-Schmidt orthogonalization. The new column h; is
projected onto the subspace orthogonal to hl. Then, since hl and hg are orthogonal,
the LS signal estimate becomes '
where 11181 is also the LSE for the signal based on H = hl only. This procedure
recursively adds terms to the signal model; it updates the order. In Problem 8.14
this geometrical viewpoint is used together with a Gram-Schmidt orthogonalization to
derive the order-update equations. A purely algebraic derivation is given in Appendix
8A. It is now summarized. A
Denote the N >< k observation matrix as Hk, and the LSE based on H), as 0), or
9,. = (HfHkYlHf x. (8.25)
The minimum LS error based on H), is
1m, = (x - Hkék)T(x - Hkék). (8.26)
By increasing the order t0 k + 1 We add a column to the observation matrix This
generates a new observation matrix which in partitioned form is l
Hk+l=lHk ht+1]=[Nxk Nxl]. (827)
To update 0A,, and Jmink we use
hZHPtX _ 1>< 1 (828)
where
Pi = 1 _ H,,(HfHk)"Hf
is the projection matrix onto the subspace ortho nal i; th t
of Hk. To avoid inverting HZH), we let go o a spanned by the columns
Di = (HZHkW (8.29)
and use the recursive formula
1 >< k 1 >< 1 (8-30)
Where Pi‘ = I — HkDkHZ". The minimum LS error is updated by using
k+1 a 8+1
The entire algorithm requires no matrix inversions The recursion begins by determini
61, Jmilliv and D1 115mg (825), (825), and (8.29), respectively. We term (8.28) (8 30)
and (8.31) the order-recursive least squares method. To illustrate the computation;
involved we apply the method to the previous line ﬁtting example.
Example 8.6 - Line Fitting
Since 51h] = A1 and s2[n] = A2 +3211 for n = 0,1,___,N _ 1, We have
Where 0 Upon substitution we have
Using (8.25) and (8.26) to begin the recursion
= (HfHlflHlTx
and
Jmim = (X — H1é1)T(X — H1031) J
= (IBM ~ i)’ _
a 4H¥H1>~1H¥h2h2TPw ~~1 ~~1
The necessary terms are "=0 "=0
(H1TH1)~1 = % 57mph] N(N - 1) _
P} = 1- H,(H1TH,)"‘H1T = “=0 2
1 N(N2 -1)/12
z x_g N(N+1) "=0 N(Nz_1)gn:v[n]
hgPlix = hgx — 21151 and
hzP1h2 — 112112 N(h21> = ;;_ 2 B2
2(2N - 1) N" 6 N-l
These are seen to agree with (8.24). The minimum LS error is, from (8.31),
z n$[n] — i z n
mml Nil 2 1 Nil 2 Span {hl,hz’nrn,hk} \‘ \
N-i N 2 = new information in hk+r
2 narln] — €(N — 1):?
= Jmlﬂl T “:0 N (N 2 ' 1)/12 l Figure 8.8 Near colliiiearity of columns of observation matrix
O and the recursive procedure will “blow up” (see (8.28)). In essence, since h“;
nearly lliies in the subspace spanned by the COIIIIITIIIS of H,“ the new observation
matrix I H1 will be nearly of rank k and hence HkHHkH will be nearly singular.
In practice, we could monitor the term hT PgLhkH and exclude from the recursion
those column vectors that produce smalclbl/alues of this term.
In solving for the LSE of the parameters of the second-order model we ﬁrst solved for
the LSE of the ﬁrst-order model. In general, the recursive procedure solves successive
LS problems until the desired order is attained. Hence, the recursive order-update
solution not only determines the LSE for the desired model but also determines the
LSE for all l0wer~0rder models as well. As discussed previously, this property is useful
when the model order is not known a priori.
Several interesting observations can be made based on (8.28).
4. The ' ' ' - - - - - .
minimum LS error can also be written in a more suggestive fashion, indicating
the contribution of the new parameter in reducing the error. From (8.22) and
(8.31) we have
1. If the new column hkH is orthogonal to all the previous ones, then Hfhkﬂ = 0 and
from (8.2s) é 1mm“ z xTPtx _ (hZHPkLXf
[n+1 [e hlc+l X Pk xhkHPk hk+1
or the LSE for the ﬁrst k components of 0],“ remains the same. This generalizes _ 1mm». (1 Tin) (8.32)
the geometrical illustration in Figure 8.3a. Where
2. The term Pix = (I — H;,(HZH;,)‘IHZ)x = x — Hkék = 6;, is the LS error vector 2 [(Pi-hk+,)T(PI-CLX)]Z
or the data residual that cannot be modeled by the columns of Hk. It represents Tk+1 : "_T—‘_TL_7- (8.33)
the component of x orthogonal to the space spanned by the columns of He (see
Problem 8.17). We can view Pfc-x, the residual, as the part of x yet to be modeled. Letting (X, Y) = XTy denote the inner product in RN , we have
3. If hi,“ is nearly in the space spanned by the columns of Hk, then, as shown in i l 2
(Pr. hlwi» Pk x)
Figure 8.8, Pﬁhwr will be small. Consequently, 1f“ _—.  
H‘ k H‘ “l1 '“ 2f h“ where if +1 is seen to be the square of a correlation coefﬁcient and as such has the
= “Pk hk+1|l property
Intuitively, Pix represents the residual or error in modeling x based on k pa-
rameters, while PﬁhkH represents the new model information contributed by
the (k + 1)st parameter (see Figure 8.8). For instance, if Pix and Pkihkﬂ are
collinear, then r: +1 = 1 and Jmink +1 = 0. This says that the part of x that could
not be modeled by the columns of H). can be perfectly modeled by hkH. Assum-
ing that n+1 74 0, the expression of (8.32) also shows that the minimum LS error
monotonically decreases with order as in Figure 8.6.
5. Recall that the LS signal estimate is
where P is the projection matrix. It is shown in Appendix 8B that P may be
recursively updated by using (8.28). The result is
(I - pk)hk+lh{+l(l _ Pk) (834)
This is termed the recursive orthogonal projection matrix. See Problem 8.18 for
its utility in determining the recursive formula for the minimum LS error. If we
deﬁne the unit length vector
uh 1 : (I — Pk)hk+1
A" |l(I - Pilhwll
the recursive projection operator becomes
where m,“ points in the direction of the new information (see Figure 8.8), or by
letting s. = Pkx, we have
= Pkx + (uk+1x)uk+1
= s. + (uZHxhkH.
8.7 Sequential Least Squares
In many signal processing applications the received data are obtained by sampling a
continuous-time waveform. Such data are on-going in that as time progresses, more
data become available. We have the option of either waiting for all the available data
or, as we now describe, of processing the data sequentially in time. This yields a
sequence of LSEs in time. Speciﬁcally, assume we have determined the LSE é based on
{:z:]0], $[1], . . . ,:r[N —  If we now observe z[N], can we update é (in time) without
having to resolve the linear equations of (8.10)? The answer is yes, and the procedure
is termed sequential least squares to distinguish it from our original approach in which
we processed all the data at once, termed the batch approach.
Consider Example 8.1 in which the DC signal level is to be estimated. We saw that
the LSE is
where the argument of A denotes the index of the most recent data point observed If
we now observe the new data sample :zr[N], then the LSE becomes l
In computing this new estimator we do not have to recompute the sum of the observa-
tions since
AW] = frl w[n] +ar[N]>
The new LSE is found by using the previous one and the new observation. The sequen-
tial approach also lends itself to an interesting interpretation. Rearranging (8 35) we
have ' ’
rvlNl- (8.35)
N+1(wlN1—/i[N— 11). (8.36)
The new estimate is equal to the old one plus a correction term. The correction term
decreases with N , reﬂecting the fact that the estimate A[N ~ 1] is based on many
more data samples and therefore should be given more weight. Also, :z:[N] — A[N — 1]
can be thought of as the error in predicting :z:[N] by the previous samples, which are
summarized by  — 1]. If this error is zero, then no correction takes place for that
update. Otherwise, the new estimate differs from the old one.
The minimum LS error may also be computed recursively. Based on data samples
up to time N — l, the error is
and thus using (8.36)
(I
+ (Nf,,,1111~1- Aw - 11>A+<A1N1- Aw1>2.
Noting that the middle term on the right-hand side is zero, we have after some simpli-
cation
Jfnin[N] = JminlN - 1] + ($[N] _ Apv - 1])?
The apparent paradoxical behavior of an increase in the minimum LS error is readily
explained if we note that with each new data point, the number of squared error terms
increases. Thus, we need to ﬁt more points with the same number of parameters.
A more interesting example of a sequential LS approach arises in the weighted LS
problem. For the present example, if the weighting matrix W is diagonal, with  =
l/af, then the weighted LSE is, from (8.15),
To ﬁnd the sequential version
S’: an]
”"‘ m] + $[N]
n=0 a?‘
= n=O a" + 012V
or ﬁnally
A[N] = A[N - 1] + NU” (¢]1v] _ A[N _ 1]). (8.31)
As expected, if of, = a2 for all n, we have our previous result. The gain factor that
multiplies the correction term now depends on our conﬁdence in the new data sample.
If the new sample is noisy or afv —> oo, we do not correct the previous LSE. On the
other hand, if the new sample is noise-free or 0K, —> 0, then /l[N] —> :z:[N We discard
all the previous samples. The gain factor then represents our conﬁdence in the new
data sample relative to the previous ones. We can make a further interpretation of
our results if indeed  = A + w[n], where w[n] is zero mean uncorrelated noise with
variance of, Then, we know from Chapter 6 (see Example 6.2) that the LSE is actually
the BLUE, and therefore,
var(/l[N - 1]) =
and the gain factor for the Nth correction is (see (8.37))
var(/l[N -1])
var(/i[N — 1]) + afv.
(8.3s)
Since 0 f K [N] f 1, the correction is large if K [N] is large or var(/l[N — 1]) is large.
Likewise, if the variance of the previous estimator is small, then so is the correction.
Further expressions may be developed for determining the gain recursively, since K [N]
depends on var(A]N - 1]). We can express the latter as
var(A[N]) = N1 1
var(A[N -1]) + a].
var(A[N — l])a§v
var(A[N — 1]) + oi, >
= ( -  var(A[N -1])
var(A[N —1])+ 0],,
or ﬁnally A
var(A[N]) = (1 - K[N])var(A[N - 1]). (8.39)
To ﬁnd K [N] recursively we can use (8.38) and (8.39) as summarized below. In the
process of recursively ﬁnding the gain, the variance of the LSE is also found recursively.
Summarizing our results, we have
Estimator Update:
Apv] = Apv - 1] + K[N](z[N] - Apv - 1]) (8.40)
Where A
K[N] =  . (8.41)
var(A[N — 1]) + 012v
Variance Update: A
var(A[N]) = (1 - K[N])var(A[N - 1]). (8.42)
To start the recursion we use
var(/l[0]) = a3.
Then, K[1] is found from (8.41), and A]1] from (8.40). Next, var(A[1]) is determined
from (8.42). Continuing in this manner, we ﬁnd K [2], A[2], var(A]2]), etc. An example
of the sequential LS approach is shown in Figure 8.9 for A = 10, of, = 1 using a Monte
Carlo computer simulation. The variance and gain sequences have been computed
Variance
Gain
Estimate
s0 70 8o 9o 100
Current sample, N
°~°° r- -—r —T —1 —r r- —1- a
Current sample, N
0 10 20 30 40 50 s0 70 80 9o 100
Current sample, N
Figure 8.9 Sequential least squares for DC level in white Gaussian
noise
recursively from (8.42) and (8.41), respectively. Note that they decrease to zero since
VaF(AlNl)= N 1 =ﬁ
and from (8.41)
Also, as seen in Figure 8.9c, the estimate appears to be converging to the true value
of A = 10. This is in agreement with the variance approaching zero. Finally, it can be
shown (see Problem 8.19) that the minimum LS error can be computed recursively as
(am _ Apv - 11);’ ’
(8.43)
We now generalize our results to obtain the sequential LSE for a vector parameter.
The derivation is given in Appendix 8C. Consider the minimization of the weighted LS
error criterion J with W = C“, where C denotes the covariance matrix of the zero
mean noise or
J = (x - H0)Tc-1(x - H0).
This is implicitly the same assumptions as in the BLUE. We know from (8.16) that the
weighted LSE is
é =(HTc-1H)-1HTc-1x
and since C is the covariance matrix of the noise, we have from (6.17)
c, = (HTC"H)"
where C); is the covariance matrix of If C is diagonal or the noise is uncorrelated, then
0 may be computed sequentially in time, but otherwise not. Assuming this condition
to hold, let
C[n] = diag(ag,of,...,a§,)
x]n] = [ so} $]1]  m] f
and denote the weighted LSE of 0 based on x[n] or the (n + 1) data samples as 0A[n].
Then, the batch estimator is ‘
0A[n] = (HT[n]C“l[n]H[n])_lHT[n]C_1]n]x[n] (8.44)
Figure 8.10 Sequential least squares estimator
with covariance matrix
C5 = E[n] = (HT[n]C'1[n]H[n])—1. (8.45)
It should be kept in mind that C[n] is the covariance matrix of the noise, while E[n] is
the covariance matrix of the LSE. The sequential LSE becomes
Estimator Update:
ma] = 9]“ - 1] + K]n] (m) - hTpqépi - 1]) (8.4s)
where
KW 2 0,2, + hT]n]E]n - 11am"
(8.47)
Covariance Update:
E[n] = (I — K[n]hT]n]) E]n — 1]. (8.48)
The gain factor  is a pxl vector, and the covariance matrix E[n] has dimension px p.
It is of great interest that no matrix inversions are required. The estimator update is
summarized in Figure 8.10, where the thick arrows indicate vector processing. To start
the recursion we need to specify initial values for 0A[n — 1] and E[n — 1], so that  can
be determined from (8.47) and then 0A[n] from (8.46). In deriving (8.46)—(8.48) it was
assumed that 0A]n — 1] and E[n — 1] were available or that HT[n — 1]C_1]n — 1]H[n - 1]
was invertible as per (8.44) and (8.45). For this to be invertible H[n — 1] must have
rank greater than or equal to p. Since H[n — 1] is an n X p matrix, we must have n Z p
(assuming all its columns are linearly independent). Hence, the sequential LS procedure
normally determines 08]}: - 1] and 2L2) — 1] using the batch estimator (8.44) and (8.45),
and then employs the sequential equations (8.46)—(8.48) for n Z p. A second method of
initializing the recursion is to assign values for 0A[—1] and E[—1]. Then, the sequential
LS estimator is computed for n Z 0. This has the effect of biasing the estimator toward
0[-—1]. Typically, to minimize the biasing effect we choose E[—1] to be large (little
conﬁdence in 0[—1]) or E[-1] = aI, where a is large, and also 0[—1] = 0. The LSE for
n Z p will be the same as when the batch estimator is used for initialization if a —> oo
(see Problem 8.23). In the next example we show how to set up the equations. In
Example 8.13 we apply this procedure to a signal processing problem.
Example 8.7 - Fourier Analysis
We now continue Example 8.4 in which the signal model is
s[n] = acos21rfon+bsin21rfon n Z 0
and 0 = [a b]T is to be estimated by a sequential LSE. We furthermore assume that the
noise is uncorrelated (C[n] must be diagonal for sequential LS to apply) and ,has equal
variance a2 for each data sample. Since there are two parameters to be estimated,
we need at least two observations or :z:[0], z[1] to initialize the procedure using a batch
initialization approach. We compute our ﬁrst LSE, using (8.44), as
(HT]1]  H[1])_1HT[1]  x]1]
(flTlllfllllYlflTlllxlll
an]
where
cos 21rf0 sin 21rfQ
(H]1] is the 2 x 2 partition of H given in (818).) The initial covariance matrix is, from
(8.45),
= a2(HT[1]H[1])“.
Next we determine 0[2]. To do so we ﬁrst compute the 2 x 1 gain vector from (8.47) as
m] z a2 + hT[2]2[1]h[2]
where hT [2] is the new row of the H[2] matrix or
hT[2] = [ cos41rf0 sin41rf0 ] .
Once the gain vector has been found, 0]2] is determined from (8.46) as
Finally, the 2 x 2 LSE covariance matrix is updated as per (8.48) for use in the next
computation of the gain vector or
ElQl = (I — KlQlhTIQDEUl-
It should be clear that, in general, computer evaluation of 0[n] is necessary. Also, except
for the initialization procedure, no matrix inversion is required. Alternatively, we could
have avoided even the matrix inversion of the initialization by assuming 0[—1] = 0 and
Z]—1] = aI with a large. The recursion then would have begun at n = 0, and for n Z 2
we would have the same result as before for large enough a. <>
Finally, we remark that if the minimum LS error is desired, it too can be found sequen-
tially as (see Appendix 8C)
(wlnl — hTlnléln — ll)’
J‘“‘"l"l z Jmml" ' l] + oi + hT[n]E[n - 1]h[n]'
(8.49)
8.8 Constrained Least Squares
At times we are confronted with LS problems whose unknown parameters must be
constrained. Such would be the case if we wished to estimate the amplitudes of several
signals but knew a priori that some of the amplitudes were equal. Then, the total
number of parameters to be estimated should be reduced to take advantage of the a
priori knowledge. For this example the parameters are linearly related, leading to a
linear least squares problem with linear constraints. This problem is easily solved, as
we now show.
Assume that the parameter 0 is subject to r < p linear constraints. The constraints
must be independent, ruling out the possibility of redundant constraints such as 01 +
02 = 0, 201 + 202 = 0, where 0,- is the ith element of 0. We summarize the constraints
A0 = b (8.50)
where A is a known r x p matrix and b is a known r >< 1 vector. If, for instance,
p = 2 and one parameter is known to be the negative of the other, then the constraint
would be 01 + 02 = 0. We would then have A = [1 1] and b = 0. It is always assumed
that the matrix A is full rank (equal to r), which is necessary for the constraints to
be independent. It should be realized that in the constrained LS problem there are
actually only (p — 1") independent parameters.
To ﬁnd the LSE subject to the linear constraints we use the technique of Lagrangian
multipliers. We determine 0C (c denotes the constrained LSE) by minimizing the La-
grangian
J. = (x - H0)T(x - H0) + ,\T(A0 _ b)
252 CHAPTER 8_ LEAST SQUARES as. CONSTRAINED LEAST SQUARES
where ,\ is a r >< 1 vector of Lagrangian multipliers. Expanding this expression, we have
1. = XTX ~ aaTnTi + QTHTHB + ATAH ~ Vi»
Taking the gradient with respect to 0 and using (4.3) yields
a0 —2HTx + 2HTH0 + ATA
and setting it equal to zero produces K
. Signal vector
lies here
Constraint
‘i / subspace
a. (HTHWHTX — QHTHWATA
(a) Unconstrained least squares (b) Constrained least squares
é -(HTH)-1ATg (8-51)
where é is the unconstrained LSE and /\ is yet to be determined. To ﬁnd ,\ we impose
the constraint of (8.50), so that
Figure 8.11 Comparison of least squares and constrained least squares
must lie in the plane shown in Figure 8.lla. The unconstrained LSE is
Observe that the signal vector
and hence
g = [A(HTH)-1AT]" (A9 - b).
Substituting into (8.51) produces the solution 9 = (HTHYIHTX = [ d l
éc = é - (HTH)“AT [MHTHYIAT] 1 (A0 — b) (8-52)
and the signal estimate is
Where é = (HTfD-IHTX. The constrained LSE is a corrected version of the uncon- ‘Biol
strained LSE. If it happens that the constraint is fortuitously satisﬁed by 0 or A0 = b, g = H9‘ = aim
then according to (8.52) the estimators are identical. Such is usually not the case, 0
however. We consider a simple example.
As shown in Figure 8.11a, this is intuitively reasonable. Now assume that we know a
Example 8.8 - Constrained Signal priori that 01 = 02. In terms of (8.50) we have
If the signal model is l l _1 l 0 = 0
so that A = [1 — 1] and b = 0. Noting that HTH = I, we have from (8.52)
0 " = 2 9C = é - AT(AAT)-1Aé
and we observe {w[0], :z:[1], :z:[2]}, then the observation matrix is = [I _ AT(AAT)—iA] a
1 0 After some simple algebra this becomes
%(-'v[0l + fvlll)
and the constrained signal estimate becomes
as illustrated in Figure 8.11b. Since 01 = 62, we just average the two observations,
which is again intuitively reasonable. In this simple problem we could have just as
easily incorporated our parameter constraints into the signal model directly to yield
and estimated 0. This would have produced the same result, as it must. This new model
is sometimes referred to as the reduced model [Graybill 1976] for obvious reasons. It
is worthwhile to view the constrained LS problem geometrically. Again referring to
Figure 8.11b, if 01 = 02, then the signal model is
and s must lie in the constraint subspace shown. The constrained signal estimate 5,. may
be viewed as the projection of the unconstrained signal estimate s onto the constrained
subspace. This accounts for the correction term of (8.52). In fact, (8.52) may be
obtained geometrically using projection theory. See also Problem 8.24. <>
8.9 Nonlinear Least Squares
In Section 8.3 we introduced the nonlinear LS problem. We now investigate this in more
detail. Recall that the LS procedure estimates model parameters 0 by minimizing the
LS error criterion
J = (X — S(9))T(X — 8(9))
where s(6) is the signal model for x, with its dependence on 0 explicitly shown. (Note
that if x — s(0) ~ A/(O, azl), the LSE is also the MLE.) In the linear LS problem the
signal takes on the special form s(0) = H0, which leads to the simple linear LSE. In
general, 5(0) cannot be expressed in this manner but is an N -dimensional nonlinear
function of 0. In such a case the minimization of J becomes much more difﬁcult, if not
impossible. This type of nonlinear LS problem is often termed a nonlinear regression
problem in statistics [Bard 1974, Seber and Wild 1989], and much theoretical work on
it can be found. Practically, the determination of the nonlinear LSE must be based on
iterative approaches and so suffers from the same limitations discussed in Chapter 7
for MLE determination by numerical methods. The preferable method of a grid search
is practical only if the dimensionality of 0 is small, perhaps p g 5. _
Before discussing general methods for determining nonlinear LSEs we ﬁrst describe
two methods that can reduce the complexity of the problem. They are
1. transformation of parameters
2. separability of parameters.
In the ﬁrst case we seek a one-to-one transformation of 0 that produces a linear signal
model in the new space. To do so we let
a = 2(9)
where g is a p-dimensional function of 0 whose inverse exists. If a g can be found so
that
S(9(¢1)) = S (:“(¢1)) = H“
then the signal model will be linear in a. We can then easily ﬁnd the linear LSE of a
and thus the nonlinear LSE of 9 by
9 = s“(<i)
where
d = (HTHYlHTx.
This approach relies on the property that the minimization can be carried out in any
transformed space that is obtained by a one-to-one mapping and then converted back
to the original space (see Problem 8.26). The determination of the transformation g, if
it exists, is usually quite difﬁcult. Sufﬁce it to say, only a few nonlinear LS problems
may be solved in this manner.
Example 8.9 - Sinusoidal Parameter Estimation
For a sinusoidal signal model
s[n]=Acos(21rf0n+¢) Tl=0,1,---»N_1
it is desired to estimate the amplitude A, where A > 0, and phase <15. The frequency f0
is assumed known. The LSE is obtained by minimizing
J = [iii (:z:[n] — Acos(21rf0n + <15))2
over A and <15, a nonlinear LS problem. However, because
A cos(21rf0n + <15) = A cos ¢ cos 21rf0n — A sin ¢ sin 21rf0n
if we let
where ]
a1 = Acos¢  9=[a]=[(p_q)xl
and H(a) is an N >< q matrix dependent on a. This model is linear in B but nonlinear
in a. As a result, the LS error may be minimized with respect to B and thus reduced
to a function of a only. Since
1mm = (x - H<a>a>T (x — Helm)
the B that minimizes J for a given a is
[a = (HT(a)H(a))_1HT(a)x (8.5s)
a2 = —A sin <15,
then the signal model becomes
slnl = <11 C05 21Ffo" + a; sin 21rfon.
In matrix form this is
s = Ha
where
H : C05 ?7rf0 sin 2i1rfo )
and the resulting LS error is, from (8.22),
1w, is) = XT [i - H(a) (HT(a)H(a))
cos 21rf0:(N — 1) sin 21rf0:(N — 1) HT(a)l x'
which is 110W linear in the new parameters, The LSE of a is The problem now reduces to a maximization of
xTH(a) (HT(a)H(a))_l HT(Q)X (8.54)
over a. If, for instance, q = p — 1, so that a is a scalar, then a grid search can possibly
be used. This should be contrasted with the original minimization of a p-dimensional
function. (See also Example 7.16.)
a = (HTm-IHTX
and to ﬁnd é we must ﬁnd the inverse transformation g‘1(a) This is
A = \/Q¥+a§
<15 = arctan (i)
Example 8.10 - Damped Exponentials
Assume we have a signal model of the form
so that th l‘ L ' - -
e non inear SE for this problem 1S given by Sm] z Al”, + AZTZ" + A373,,
where the unknown parameters are {A1, A2, A3, r}. It is known that 0 < r < 1. Then,
the model is linear in the amplitudes B = [A1 A2 A3]T, and nonlinear in the damping
factor a = r. Using (8.54), the nonlinear LSE is obtained by maximizing
xTH<r> (HT<T>H<1~>)“ HT<T>==
arctan 
a‘ over 0 < r < 1,where
Where a = (HTHTIHTX The reader may Wish to refer back t
. ' o Exam l 7.16' h' h 1 1 l
thls aPproach Was used to ﬁnd the MLE of frequency. p e m w 1% T T2 7.3
H(1") =
A second type of nonlinear LS problem that is less com ' ' '
_ _ plex than the en 1 h'b' ‘ ' '
the separability property. Although the signal model is nonlineaqgit 121:), itelierllteir  N-l 7.2(N—1) 7.3(N—1)
some of the paramet , '11 t t d ' - .
model has the form erg as 1 us ta e 1n Example 3-3- In general, a separable signal
Once f- is found we have the LSE for the amplitudes
s = “(a)” B = <HT<r>H<1=>>"‘HT<r>x-
This maximization is easily carried out on a digital computer. <>
It may also be possible to combine the approaches to reduce a nonseparable problem
to a separable one by an appropriate transformation. When these approaches fail, we
are forced to tackle the original nonlinear LS problem or to minimize
1 = (x—s(9))T(X—s(9))-
Necessary conditions can be obtained by differentiation of J. They are
5,; — -2 gﬂlll — sill) aaj = 0
for j = 1, 2, . . . ,p. If we deﬁne an N x p Jacobian matrix as
(M9)) zﬁsii] i=0,1,...,N—1 >
a0 ij  j=1i2i"'ipi
then the necessary conditions become
or in matrix form
(X ~ 5(9)) = 0 (8.55)
which is a set of p simultaneous nonlinear equations. (If the signal is linear in the
unknown parameters or 5(9) = H0 so that 3s(0)/30 = H, We have our usual LS
equations.) They may be solved using the Newton-Raphson iteration described in
Chapter 7. Letting
s(9)= as (x—s(0)) (8.5s)
the iteration becomes
(8.57)
9H1 = 0k — < )_1g(0)
To ﬁnd the J acobian of g, which is actually the scaled Hessian of J , We need
al (9)li _ 5 N-l 6 [n]
20], W; (gain-Sim) 501,]
which follows from (8.56). Evaluating this, we have
al (0)1. __ N-l 62s[n] 6s[n] 6 n
gait _ .2... [Mn] “ smnaiiaej ' a0, Sigill
To put this in more succinct form let
[H(0)l.~,~ = (Gig), = 6353,)? (8.58)
for i = 0,1,...,N —1;j=1,2,...,p. (Again if s(0) = H0, then H(0) =  Also let
ia..(0)]ij =  (8.59)
for i = 1,2,...,p; j = 1,2,...,p. Then,
Gigi‘? =  (wliil ~ s[ii])[G..(0)1.-,- - iHuiit, inwii...
= i2 lG5(9)].-,- (wliil — sliil) ~ iHTuiii iHuni...
and Nd
9% = Z Gnwﬂwinl - SW) - HT<9)H(0)- (860)
a0 “=0
In summary, we have upon using (8.57) with (8.56) and (8.60) the Newton-Raphson
iteration
0k+1 = 9i + (HT(0k)H(0k) ‘ If G~(9i=)(-'vl"l — {SW/JLJ) S
~  — S(9k)) 
where H(0) is given by (8.58) and G,,(0) is given by (8.59). These are the ﬁrst-order
and second-order partials of the signal with respect to the unknown parameters. It is
interesting to note that if 5(0) = H0, then G,,(0) = 0 and H(0) = H and we have
0k+1 = 0k + (HTH)-1HT(X - 
= (HTHYlHTx
or convergence is attained in one step. This is, of course, due to the exact quadratic
nature of the LS error criterion, which results in a linear g(0). It would be expected then
that for signal models that are approximately linear, convergence will occur rapidly.
A second method for solving a nonlinear LS problem is to linearize the signal model
about some nominal 0 and then apply the linear LS procedure. This differs from the
Newton-Raphson method in which the derivative of J is linearized about the current
iterate. To appreciate the difference consider a scalar parameter and let 00 be the
nominal value of 0. Then, linearizing about 00 we have
s[n; 0] z s[n; 0Q] + (0 — 00)
where the dependence of s[n] on 0 is now shown. The LS error becomes
=  5(a)) +  —  (X - 5(a)) +  ~  .
Since x —— s(00) + H(0O)00 is known, we have as the LSE
(9 = (HT<@@>H<@@>)“ HWQO) (x ~ 5(a) + Hwowo)
= 0» + (HT<@O>H<@Q>)" Hm) (x - sum)- >
If we now iterate the solution, it becomes
0,... = 0,. + (HT(0,,)H(0,,))_1 HT(0k)(x - s(e.))
which is identical to the Newton-Raphson iteration except for the omission of the second
derivatives or for the presence of Gn. The linearization method is termed the Gauss-
Newton method and can easily be generalized to the vector parameter case as
(at = a + <HT<@t>H<@.>>" 1mm (x - w.» (s-sa
where
In Example 8.14 the Gauss method is illustrated. Both the Newton-Raphson and
the Gauss methods can have convergence problems. It has been argued that neither
method is reliable enough to use without safeguards. The interested reader should
consult [Seber and Wild 1989] for additional implementation details.
8.10 Signal Processing Examples
We now describe some typical signal processing problems for which a LSE is used. In
these applications the optimal MVU estimator is unavailable. The statistical charac-
terization of the noise may not be known or, even if it is, the optimal MVU estimator
cannot be found. For known noise statistics the asymptotically optimal MLE‘ is gener-
ally too complicated to implement. Faced with these practical difficulties we resort to
least squares.
Example 8.11 - Digital Filter Design
A common problem in digital signal processing is to design a digital ﬁlter whose fre-
quency response closely matches a given frequency response speciﬁcation [Oppenheim
and Schafer 1975]. Alternatively, we could attempt to match the impulse response,
which is the approach examined in this example. A general inﬁnite impulse response
(IIR) ﬁlter has the system function
B(z)
1 + a[1]z‘1 + - - - +a[p]z_P '
'H(z)
If the desired frequency response is Hd( f ) = 'Hd(exp[,j21rf]), then the inverse Fourier
transform is the desired impulse response or
hdlﬂl = ~7:_1{Hd(f)}-
The digital ﬁlter impulse response is given as a recursive difference equation, obtained
by taking the inverse z transform of 71(2) to produce
hm] z —ia[k]h[n — k] + ZqIb[k]5[n — k] n Z 0
A straightforward LS solution would seek to choose {a[k],  to minimize
1 = Z (ham - hlnl)’
where N is some suﬁiciently large integer for which hd[n] is essentially zero. Unfortu-
nately, this approach produces a nonlinear LS problem (see Problem 8.28 for the exact
solution). (The reader should note that hd[n] plays the role of the “data,” and h[n]
that of the “signal” model. Also, the “noise” in the data is attributable to modeling
error whose statistical characterization is unknown.) As an example, if
the“ _ blOK-alll)" n z 0
and
J = E (hdlﬂl — bl0](—al1])")2
which is now a quadratic function of the a[k]’s and b[k]’s. Alternatively,
J = Z [hﬂn] - (—za[k]hd[n—k] +b[n]):l .
7min] In minimizing this over the ﬁlter coefficients note that the b[n]’s appear only in the
ﬁrst (q + 1) terms since b[n] = O for n > q. As a result, we have upon letting a =
(a) True 1°35‘ Squares “m” [a[1] a[2] . . . a[p]]T and b = [b[0] b[1] . . . b[q]]T,
6m Jf(a,b) = Z ma] - (- Zai/qhdpi - k] + 14111)]
+ Z [ma] - (- Z a[/€]hd[n - to]
(b) Filtered least squares error
Figure 8.12 Conversion of nonlinear least squares ﬁlter design to lin- k_1
ear least squares _
In matrix form the LSE of the numerator coeﬂicients is
which is clearly very nonlinear in a[1]. In fact, it is the presence of A(z) that causes b = h ‘i’ H05
the LS error to be nonquadratic. To alleviate this problem we can ﬁlter both hd[n] and whet
h[n] by .A(z), as shown in Figure 8.12. Then, we minimize the ﬁltered LS error e
h = [ Mo] hd[1]  hd[q] 1T
f g} "M [ l) mo] 0  0
Where hd, is given as i I § f f
ha, in] = E alkihdi" ' kl  . . . . . .
to Z‘ The vector h has dimension (q + 1) X 1, while the matrix HO has dimension (q + 1) >< p.
4 To ﬁnd the LSE of the denominator coefficients or a we must minimize
and a[0] = 1. The ﬁltered LS error then becomes  N4 P 2
' Jf(a,b) = Z [ma] - (- Z a[/€]hd[ﬂ - MN
(x — H0)T(x — H0) =
1, =  (iaikihiin - k1 - hm)
where 0 = a and
x = [h.,i[q+1] hd[q+2] hd[N-1]]T (N—1—qx1)
H z _ ilqr+ l dlq] dlq :P+2l (N_l_qxp),
The LSE of the denominator coefficients is
a = (HTin-‘HTX.
This method for designing digital ﬁlters is termed the least squares Prony method [Parks
and Burrus 1987]. As an example, consider the design of a low-pass ﬁlter for which the
desired frequency response is
where fc is the cutoff frequency. The corresponding impulse response is
hlilnl = ism fife" ~ oo < n < oo
and as expected is not causal (due to the zero phase frequency response assumption).
To ensure causality we delay the impulse response by no samples and then set the
resultant impulse response equal to zero for n < 0. Next, to approximate the desired
impulse response using the LS Prony method we assume that N samples are available
_ sin 21rfc(n —- no)
1r(n — no)
For a cutoff frequency of fc = 0.1 and a delay of no = 25 samples, the desired impulse
response is shown in Figure 8.13a for N = 50. The corresponding desired magnitude
frequency response is shown in Figure 8.13b. The effects of truncating the impulse
response are manifested by the sidelobe structure of the response. Using the LS Prony
method with p = q = 10, a digital ﬁlter was designed to match the desired low-pass
ﬁlter frequency response. The result is shown in Figure 8.13c, where the magnitude
of the frequency response or |'H(exp( j21r f ))| has been plotted in dB. The agreement is
generally good, with the Prony digital ﬁlter exhibiting a peaky structure in the pass-
band (frequencies below the cutoff) and a smooth rolloff in the stopband (frequencies
above the cutoff) in contrast to the desired response. Larger values for p and q would
presumably improve the match. <>
"my magi" u e frequency 11951101159 (dB) Deslred magnitude response (dB) Desired impulse response
$115+ i i i i i i
Sample number, n
(b)
Frequency
5 (c)
Frequency
Figure 8.13 Low pass ﬁlter design by least squares Prony method
Example 8.12 - AR Parameter Estimation for the ARMA Model
We now describe a method for estimation of the AR parameters of an autoregressive
moving average (ARMA) model. An ARMA model for a WSS random process assumes
a PSD of
_<T§lB(f)|2
where
Bu) = Hijblklexpejwrrk)
A(f) = 1+ia[k]exp(—j21rfk).
a-
The b[k]’s are termed the MA ﬁlter parameters, the a[lc]’s the AR ﬁlter parameters, and
oi the driving white noise variance. The process is obtained by exciting a causal ﬁlter
whose frequency response is B (f) /A(f) with white noise of variance 0,2,. If the b[k]’s are
zero, then we have the AR process introduced in Example 3.16. The estimation of the
AR parameters of an AR model using an asymptotic MLE was discussed in Example
7.18. For the ARMA process even the asymptotic MLE proves to be intractable. An
alternative approach relies on equation error modeling of the ACF. It estimates the
AR ﬁlter parameters, leaving the MA ﬁlter parameters and white noise variance to be
found by other means.
To determine the ACF we can take the inverse z transform of the PSD extended to
the z plane or
a2B(z)B(z_1)
A(z)A(z'1)
where B(f) = B(exp(j21rf)), A(f) = A(exp(j21rf)). The usual PSD is obtained by
evaluating 'P,,x(z) on the unit circle of the z plane or for z = exp( j21r f ). It is convenient
to develop a difference equation for the ACF. This difference equation will serve as the
basis for the equation error modeling approach. Taking the inverse z transform of
A(z)'P¢1(Z), we have
Since the ﬁlter impulse response is causal, it follows that
 e Z1212}
for n < 0. Consequently, we have that
htnezalsiziaia
for n > O and hence is anticausal. Thus,
Z—‘{<#B<Z>B"Il} = a’b[n]*h[—nl
Continuing, we have
A(z“1)
Z-*{A<z>1>.,<z>} = 2-1 {U2B(Z)B(z—1)}
Finally, a difference equation for the ACF can be written as
Zalklnmln » k1= 0 for n > q (8.63)
zvhere a[0] = 1. These equations are called the rnodiﬁed Yule- Walker equations. Recall
rom Example 7.18 that the Yule-Walker equations for an AR process were identical
except that they held for n > 0 (since q = 0 for an AR process). Only the AR ﬁlter
parameters appear 1n these equations.
In practice, we must estimate the ACF lags as
assuming z[n] is available for n = 0, 1,. . . , N - 1. Substituting these into (8.63) yields
2 a[k]1‘,,[n ~ k] = e[n] n > q
where e[n] denotes the error due to the effect of errors in the ACF function estimate
The model becomes
111M] = — ia[k]f'm[n — k] + e[n] n > q
which can be seen to be linear in the unknown AR ﬁlter parameters. If the ACF is
estimated for lags n = 0,1, . . .,M (where we must have M < N — 1), then a LSE of
a[k] will minimize _
J = Z lmlnl-(—ia[k]n=[n—k])] (8.64)
(x — H0)T(x —- H0)
where
a[1]
The LSE of 0 is (HTH)‘1HTx and is termed the least squares modiﬁed Yule- Walker
equations. It is interesting to observe that in this problem we use a LSE for the
estimated ACF data, not the original data. Additionally, the observation matrix H,
which is usually a known deterministic matrix, is now a random matrix. As expected,
the statistics of the LSE are difficult to determine. In practice, M should not be chosen
too large since the ACF estimate is less reliable at higher lags due to the (N — k)
lag products averaged in the estimate of rm Some researchers advocate a weighted
LSE to reﬂect the tendency for the errors of (8.64) to increase as n increases. As an
example, the choice w" = 1 - (n/ (M + 1)) might be made and the weighted LSE of
(8.16) implemented Further discussions of this problem can be found in [Kay 1988].
Example 8.13 - Adaptive Noise Canceler
A common problem in signal processing is to reduce unwanted noise. It arises in such
diverse problems as the suppression of a 60 Hz interference in an electronic circuit and
the suppression of a mother’s heartbeat masking the fetal heartbeat in an EKG trace
[Widrow and Stearns 1985]. A particularly effective means of accomplishing this is
the adaptive noise canceler (ANC). It assumes the availability of a reference noise that
after suitable ﬁltering can be subtracted from the noise that is to be canceled. As an
example, to cancel a 60 Hz interference we need to know the amplitude and phase of the
interference, both of which are generally unknown. A suitable adaptive ﬁlter as shown
in Figure 8.14 can modify the amplitude and phase so as to make it nearly identical
to the interference to be canceled. A discrete-time ANC is shown in Figure 8.15. The
primary channel contains noise  to be canceled. To effect cancelation a reference
channel containing a known sequence arﬂn] that is similar but not identical to the noise
is then ﬁltered to improve the match. The ﬁlter 'H,,(z) has weights that are determined
pIr-aIIIIYIIIIIIIYIIIIIIIIIIII
6O Hz interference
Primary T_
channel ‘me
Error
signal
Reference
channel
Adaptive
ﬁlter
Reference
oscillator
Figure 8.14 Adaptive noise canceler for 6O Hz interference
Primary If")
channel
e[n]
Reference iRlﬂl
channel
Figure 8.15 Generic adaptive noise canceler
at each time n to make  z  for k = 0, 1, . . . , n. Since we want e[n] z 0, it makes
sense to choose the weights at time n to minimize
i (rvlkl - ma)"
Z (an -  ammk - l1)
Time, t Time, t
(a) Fixed interference model (station- (b) Changing interference model (non-
ary) stationary)
Figure 8.16 Stationarity of interference model
where hn]l] are the ﬁlter weights at time n. It is immediately recognized that the
weights can be determined as the solution of a sequential LS problem. Before giving
the solution we remark that the LSE assumes that the interference is ﬁxed over time
or that it is stationary. For instance, the primary channel containing the sinusoid is
assumed to appear as shown in Figure 8.16a. If, however, the interference appears as
in Figure 8.16b, then the adaptive ﬁlter must quickly change its coefficients to respond
to the changing interference for t > to. How fast it can change depends upon how
many error terms are included in J [n] before and after the transition at t = to. The
chosen weights for t > to will be a compromise between the old weights and the desired
new ones. If the transition occurs in discrete time at n = no, then we should probably
expect the wrong weights until n >> no. To allow the ﬁlter to adapt more quickly
we can downweight previous errors in J [n] by incorporating a weighting or “forgetting
factor” /\, where 0 < /\ < 1, as follows:
This modiﬁcation downweights previous errors exponentially, allowing the ﬁlter to react
more quickly to interference changes. The penalty paid is that the estimates of the ﬁlter
weights are noisier, relying on fewer effective LS errors. Note that the solution will not
change if we minimize instead
J'[n] = i %  — lg) h,,[l]zR[k — 1]) (8.65)
for each n. Now we have our standard sequential weighted LS problem described in
Section 8.7. Referring to (8.46), we identify the sequential LSE of the ﬁlter weights as
é[n]=]i.,[o1 Ann]  h,,[p-1]]T.
810 mcNALPR0cE$nN0£wAinmEs 271
The data vector h[n] is given as (not to be confused with the impulse response h[n])
h[n]= [ $3M] zﬂn-l] zR[n—p+1]]T
and the weights 0i are given by /\". Also, note that
is the error at time n based on the ﬁlter weights at time n — 1. (This is not the Same as
Einl = $lnl _ ilni =  — hT The algorithm is summarized from (8.46)—(8.48)
as
9W = 9i" - ll + Klnlelnl
where
hlnl = l Main] $12M — 1]
EM] = (I — K[n]hT[n]) E[n -1],
$R[n—p+ 1] ]T
The forgetting factor is chosen as 0 < /\ < 1, with /\ near 1 or 0.9 < /\ < 1 being typical
As an example, for a sinusoidal interference
 = 10 cos(21r(0.1)n + 1r/4)
and a reference signal
$3M] = cos(21r(0.1)n)
we implement the ANC using two ﬁlter coefﬁcients (p = 2) since the reference signal
need only be modiﬁed in amplitude and phase to match the interference. To initialize
the sequential LS estimator we choose
é[-1] = o
and a forgetting factor of /\ = 0.99 is assumed. The interference :r[n] and output
e[n] of the ANC are shown in Figures 817a and 8.17b, respectively. As expected, the
interference is canceled. The LSE of the ﬁlter coefficients is shown in Figure 8.17c and
is seento rapidly converge to the steady-state value. To ﬁnd the steady state values of
the weights we need to solve
'H[exp(21r(0.  = 10 exp(j1r/4)
10 which says that the adaptive ﬁlter must increase the gain of the reference signal by 10
8i and the phase by 1r/ 4 t0 match the interference. Solving, We have
6 i h[0] + h[1]exp(-—j21r(0.1))= 10exp(j1r/4)
g 2 which results in h[0] = 16.8 and h[1] = —12.0. <>
"‘ -4 Example 8.14 - Phase-Locked Loop
.8 We now consider the problem of carrier recovery in a communication system, which is
-10 1- 1 I I necessary for coherent demodulation. It is assumed that the carrier is received but is
2 30 40 50 60 70 80 90 100 embedded in noise. The received noise-free carrier is
(a) 0 10 0 Sample number, n
s[n]=cos(21rf0n+¢) n=—1\/I,...,0,...,M
l: where the frequency f0 and phase qb are to be estimated. A symmetric observation
interval is chosen to simplify the algebra. A LSE is to be used. Due to the nonlinearity
6 we employ the linearization technique of (8.62). Determining ﬁrst H(0), where 0 =
3 [f0 ¢]T, we have
m _2 afo n 1rs1n( 1rfgn + qb)
_6  = — s1n(21rfQn + qb)
~10 r—- —r— r~ —T- 1   5° that
(b) 0 10 20 30 gllmplesﬁumber n —M21rsin(—21rfQM + qb) l sin(—21rfQM + qb)
’ —(lVI — l)21rs1n(—21rfQ(M — 1) + qb) s1n(—21rfQ(M — 1) + qb)
20 mo) z _ 5 1
Isl] hi0] M21rsin(21rfQM + (p) 5mm 10M + (p)
J2 1° and
g 0 41r2 Z n2 sin2(21rfQn + qb) 21r Z nsin2(2vrfon + ¢)
i‘: "10 flu] 21r Z nsin2(21rf0n + qb) Z sin2(21rfQn + ¢)
( ) 0 10 20 30 40 50 60 70 80 90 100
Sample number» T‘ Z n2 sin2(21rf@n + qb) = Z  —- 1g- cos(41rfon + 2¢)]
Figure 8.17 Interference cancellation example
Z nsin2(21rfo" + <1?) = Z  _ 5 coswwfon + 2¢ll
f Sinﬂgﬂfon .4. 4,) = 2 [5 — 5 cos(41rf0n + 249]
and since [Stoica et al. 1989]
1 Z ni c05(47"f0" "l" 2115)” O Z = o’ l’ 2
we have from (3.22)
TM(M + 1)( + ) )
for M >> 1. We have then from (8.62)
f0 _ f0 _ 313 2 nSin(27|-fokn+¢k)(.’l2[n]—COS(27Tf0;,n+¢k))
1 M ~ ¢  — cos(21rf@ n+ 115k»
¢k+l - (pk _ H SlI1(27TfgkTl + k k
or since
2M1+ 1 Z sin(2irfq,,n + 115k) COSQFfOk" ‘l’ W)
= i;  Slll(47l'fgkn+2¢kl
2(2M + 1) ,,=_M
it follows that
fowl = f0, — “M, ":21" nilnl su1(21rfo,,n + W)
¢k+l = ¢k _ % Z z[n] sin(21rfo,,n + 115k)-
The LSE of frequency and phase can be implemented as a phase-locked loop [Proakis
1983]. In Problem 8.29 it is shown that at convergence for a high enough SNR the
solution will be
a-
References
Bard, Y., Nonlinear Parameter Estimation, Academic Press, New York, 1974.
Graybill, F.A., Theory and Application of the Linear Model, Duxbury Press, North Scituate, Mass.,
Kay, S.M., Modern Spectral Estimation.‘ Theory and Application, Prentice-Hall, Englewood Cliffs,
Oppenheim, A.V., R.W. Schafer, Digital Signal Processing, Prentice-Hall, Englewood Cliffs, N.J.,
Parks, T.W., C.S. Burrus, Digital Filter Design, J. Wiley, New York, 1987.
Proakis, .l.G., Digital Communications, McGraw-Hill, New York, 1983.
Scharf, L.L., Statistical Signal Processing, Addison-Wesley, New York, 1991.
Seber, G.A.F., C..l. Wild, Nonlinear Regression, J. Wiley, New York, 1989.
Stoica, P., R.L. Moses, B. Friedlarider, T. Soderstrom, “Maximum Likelihood Estimation of the
Parameters of Multiple Sinusoids from Noisy Measurements,” IEEE Trans. Acoust, Speech,
Signal Process., Vol. 37, pp. 378—392, March 1989.
Widrow, B., S.D. Stearns, Adaptive Signal Processing, Prentice-Hall, Englewood Cliffs, N..l., 1985.
Problems
8.1 The LS error
J = Z  — Acos Zirfgn — Br")2
is to be minimized to ﬁnd the LSE of 0 = [A fOBr]T, where 0 < r < 1. Is
this a linear or nonlinear LS problem? Is the LS error quadratic in any of the
parameters, and if so, which ones? How could you solve this minimization problem
using a digital computer?
8.2 Show that the inequality given in (8.7) holds.
8.3 For the signal model
_ A 0$n$M—1
ﬁnd the LSE 0f A and the minimum LS error. Assume that  = s[n] + w[n] for
n = 0, 1, . . . , N — 1 are observed. If now w[n] is WGN with variance a2, ﬁnd the
PDF of the LSE.
8.4 Derive the LSE for a vector parameter as given by (8.10) by verifying the identity
J = (x - H0A)T(x - H6) + (é - afnTrué - a)
where A
0 = (HTHYIHTx.
To complete the proof show that J is minimized when 0 = 0, assuming that H is
full rank and therefore that HTH is positive deﬁnite.
8.5 For the signal model
s[n] = Z A,- cos 21rf,-n )
where the frequencies f,» are known and the amplitudes A,- are to be estimated,
ﬁnd the LSE normal equations (do not attempt to solve them). Then, if the
frequencies are speciﬁcally known to be f,» = i/N, explicitly ﬁnd the LSE and
the minimum LS error. Finally, if  = s[n] + w[n], where w[n] is WGN with
variance 0'2, determine the PDF of the LSE, assuming the given frequencies. Hint:
The columns of H are orthogonal for the given frequencies.
8.6 For the general LSE of (8.10) ﬁnd the PDF of the LSE if it is known that x ~
N(H0,o'2I). Is the LSE unbiased?
8.7 In this problem we consider the estimation of the noise variance 0'2 in the model
x = H0 + w, where w is zero mean with covariance matrix 021. The estimator
62 = %Jfnin = 315a - H0A)T(x - H9)
where é is the LSE of (8.10), is proposed. Is this estimator unbiased, and if
not, can you propose one that is? Explain your results. Hint: The identities
E(xTy) = E(tr(yxT)) = tr(E(yxT)) and tr(AB) = tr(BA) will be useful.
8.8 Verify the LSE for A given in (8.15). Find the mean and variance for A if z[n] =
A + w[n], where w[n] is zero mean uncorrelated noise with variance a2
8.9 Verify (8.16) and (8.17) for the weighted LSE by noting that if W is positive
definite, we can write it as W = DTD, where D is an invertible N X N matrix.
8.10 Referring to Figure 82b, prove that
This can be thought of as the least squares Pythagorean theorem.
8.11 In this problem we prove that a projection matrix P must be symmetric. Let
x = § +§*, where ﬁ lies in a subspace which is the range of the projection matrix
or Px = § and {i lies in the orthogonal subspace or Pﬁi = 0. For arbitrary
vectors x1_,x2 in RN show that
by decomposing x1 and x2 as discussed above. Finally, prove the desired result.
8.12 Prove the following properties of the projection matrix
P = H(HTH)'1HT.
a. P is idempotent.
b. P is positive semideﬁnite.
c. The eigenvalues of P are either 1 or 0.
d. The rank of P is p. Use the fact that the trace of a matrix is equal to the sum
of its eigenvalues.
8.13 Verify the result given in (8.24).
8.14 In this problem we derive the order-update LS equations using a geometrical
argument. Assume that 0;, is available and therefore
a=mm
is known. Now, if hk+1 is used, the LS signal estimate becomes
ék+1 = 5;, + ahjwl
where h}, +1 is the component of hk+1 orthogonal to the subspace spanned by
{h1,h;, . . .,h;,}. First, ﬁnd hi,“ and then determine a by noting that ahfwl is
the projection of x onto h; +1. Finally, since
determine 0H1.
8.15 Use order-recursive LS to ﬁnd the values of A and B that minimize
J = Z (m1 - A - Br")?
The parameter r is assumed to be known.
8.16 Consider the models
The second signal is useful for modeling a jump in level at n = M. We can express
the second model in the alternative form
s[n] = Au1[n]+ (B — A)u2[n]
where
0 ogngM-i ,
Now let 01 = A and 02 = [A (B — A)]T and ﬁnd the LSE and minimum LS error
for each model using order-recursive LS. Discuss how you might use it to detect
a jump in level.
8.17 Prove that PkLx is orthogonal to the space spanned by the columns of Hk.
8.18 Using the orthogonal recursive projection matrix formula (8.34), derive the update
formula for the minimum LS error (8.31).
8.19 Verify the sequential formula for the minimum LS error (8.43) by using the se-
quential LSE update (8.40).
8.20 Let z[n] = Ar" + w[n], where w[n] is WGN with variance 0'2 = 1. Find the
sequential LSE for A, assuming that r is known. Also, determine the variance of
the LSE in sequential form and then solve explicitly for the variance as a function
of n. Let A[0] = z[0] and var(A[0]) = a2 = 1.
8.21 Using the sequential update formulas for the gain (8.41) and variance (8.42), solve
for the gain and variance sequences if a}, = r". Use var(A[0]) = var(z[0]) = a5 =
1 for initialization. Then, examine what happens to the gain and variance as
N -—> oo ifr = 1, 0 < r < 1, and r > 1. Hint: Solve for 1/var(A[N]).
8.22 By implementing a Monte Carlo computer simulation plot A[N] as given by (8.40).
Assume that the data are given by
where w[n] is zero mean WGN with of, = r". Use A = 10 and r = 1,0-.95,1.05.
Initialize the estimator by using A[0] = z[0] and var(A[0]) = var(z[0]) = a3 = 1.
Also, plot the gain and variance sequences.
8.23 In this problem we examine the initialization of a sequential LSE. Assume that
we choose 0[—1] and 2[—1] = aI to initialize the LSE. We will show that, as
a —> oo, the batch LSE
93w = (liTlnliﬁinllilnlYlHqnliﬁinlxln]
(i aighlklhTlkl) i1 (2 éwikihﬁki)
for n Z p is identical to the sequential LSE. First, assume that the initial observa-
tion vectors {h[—p], h[—(p — 1)], . . . , h[—1]} and noise variances {crfw 019b,), . . .,
c731} exist, so that for the chosen initial LSE and covariance we can use a batch
estimator. Hence,
9H1 = 95H] = (liTl-IJCW-IJHI-IIYI HT[~11¢-1i-11==[—11
and
El-l] = (HTI-HCW-llﬂl-ll)“
where
C[—-1] = diag(crip,a'i(p_1),..wail).
Thus, we may view the initial estimate for the sequential LSE as the result of
applying a batch estimator to the initial observation vectors. Since the sequential
LSE initialized using a batch estimator is identical to the batch LSE using all
the observation vectors, we have for the sequential LSE with the assumed initial
conditions
05m] = (Z ;5h[k]hT[k]) (Z E-Ewptjiﬂzq) .
Prove that this can be rewritten as
(2“‘[—1] + Zn: Uiih[k]hT[k]> _ (2-1[-1]é[-1]+ f": iﬂpqhﬁzq) .
Then examine what happens as a —> oo for 0 ﬁ n 5 p - 1 and n Z p.
8.24 In Example 8.8 determine s. by ﬁrst projecting x onto the subspace spanned by
b1 and h; to produce s, and then projecting é onto the constrained subspace.
8.25 If the signal model is
s[n]=A+B(——1)" n=0,1,...,N—1
and N is even, ﬁnd the LSE of 0 = [A BlT. Now assume that A = B and repeat
the problem using a constrained LS approach. Compare your results.
8.26 Consider the minimization of h(9) with respect to 9 and assume that
0A: arg main h(9).
Let a = g(9), where g is a one-to-one mapping. Prove that if d minimizes
h(.<1"(<1)), then 9 = Q"(d)- ’
8.27 Let the observed data be
x[n]=exp(9)+w[n] n=0,1,...,N—1.
Set up a Newton-Raphson iteration to ﬁnd the LSE of 9. Can you avoid the
nonlinear optimization and ﬁnd the LSE analytically?
8.28 In Example 8.11 the true LSE must minimize
In this problem we derive the equations that must be optimized to ﬁnd the true
LSE [Scharf 1991]. It is assumed that p = q+ 1. First, show that the signal model
can be written as
all] glﬁl 0 Mo]
ylq + ll ylq] all] bk]
where
9M = 2'1 {Ala}
and G is N >< (q + 1). Letting x = [hd[0] hd[1] . . . hd[N — 1]]T, show that
J(a,5) = 1T (1 _ G(GTG)_1GT) x
where b is the LSE of b for a. given a. Finally, prove that
1- G(GTG)_1GT = A(ATA)_‘AT (8156)
where AT is the (N-q- 1) >< N=(N—p) >< N matrix
a[p] a[p—1]  1 0 U 0
T U a ]  a[1] 1 0 0
b b 6 afp] a[p.—1] f. i
Hence, the LSE of a is found by minimizing
xTA(ATA)_1ATx.
Once this is found (using a nonlinear optimization), we have
b = (GTGYlGTx
= (I — A(ATA)'1AT) x
where the elements in A are replaced by the LSE of a. Note that this problem is
an example of a separable nonlinear LS problem. Hint: To prove (8.66) consider
L = [A G], which is invertible (since it is full rank), and compute
L(LTL)"1LT =1.
You will also need to observe that ATG = 0, which follows from a[n] *g[n] = 6
8.29 In Example 8.14 assume that the phase-locked loop converges so that fqk +1 = fok
and ¢>k+1 = (ﬁk. For a high SNR so that z[n] z cos(21rfQn + ¢), show that the
ﬁnal iterates will be the true values of frequency and phase.
we have
(H£+1Hk+1)_1 z E);
Appendix 8A
where Dk = (HZHkYI or
0 o f D +
Derlvatlon 0 D k hgﬂpthkn hgﬂhkﬂ
Order-Recursive * _h£+1HkE/¢ 1
res » * *
Least  Simplifying the off-diagonal block of DkH, we have
- ' ' h d - ' 1 t formulas (8.28)—(8.31). + +
Refirlirtlsrllugstipéasegglgx ‘y: ldaelve t e or er recurs1ve eas squares — DkHZhkH [hgwLlpkLhkwLl + hail (I _ Pﬂhkﬁ]
gm =(H{+1Hk+1)‘1Hf+1x. z DkHZhkH I
‘lamp/J; hk+1
Bu?» Finally, we have
D a -i--
Using the inverse 0f a partitioned matrix formula (see Appendix l) _ [ 1 x k 1 x 1 ] '
To ﬁnd 01H we make use of the previous result
(A_t_).bL) -1 (A-PEL) b
‘Eb (A ” T) c —bTA'1b = Dk+1 k
for A a symmetric k >< k matrix, b a k >< 1 vector, and c a scalar, and also Woodbury’s DkHTx + Dkﬂzhkwqhgﬁuﬂkpkﬂfx _ Dkﬂfhkﬂhgﬂx
identity (see Appendix 1) z '° hfﬂmghk“ hfﬁPﬁhkﬂ
( bbT)“ = A_l + A“bbTA"
ék _ DkH£hk+1h£+1 (x — HkDkHZX) g
ha} (x — HkDkHfx)
_ '° hg+lpthk+l Appendlx 8B
hﬂlPihke . - f - P - .
Finally, the update for the minimum LS error is Derlvatlon 0  roJectlon
Jmim¢+1 = (X — HI=+19Ak+1)T(X — Hk+1ék+1) Matrlx
_ xTx _ KT [ H’: hk 1 1 h£+1PLLhk+1 We now derive the recursive update for the projection matrix given by (8.34). Since
hkHPk hk+1 where Dk+1 = (HkHHkH) , we can use the update for Dk derived 1n Appendix 8A.
hT Pi, As a result, we have
_ J . _ (hﬂfix)? k k k hg+lpicLhk+l
(1- Pk)hk+1h£+1(I'" Pk)
The ﬁrst term in parentheses is just Sh]. We can use W00dbury’s identity (see
Appendix 1) to obtain
E]n] = (E'1[n — 1] + §5h[n]hT]n])_1
= (I —- K[n]hT]n]) E]n -1]
Appendix 8C
where — 2M _ llhm]
Applying these results t0  yields
91”] = (1 - K[”111T]”]) 2]” - 1]
- (2-11” - 1191” - 11 + a—1121h]n]z[n])
Derivation of Sequential Least Squares KM
In this appendix We derive (8.46)—(8.48). D
61”] = (11T1”1c-11”]111”1)"11T1”1c-11”]x1”1
= (1 HT[”_1] hm] l ] CD351] :1 ]_1]HE;[:L]1] ]>_
since
91” -11 = (11T1” - 110-11” - 11111” - 111-‘ 11T1” - 1111-11” - 1]x[n - 11
Continuing, we have
Since the covariance matrix is diagonal, it is easily inverted to yield 0111] = 9]” - 1] + izﬁn - 1]h]n];z]n] - K]n]hT]n]9A]n — 1] '
a
9W z (HTW _ “Cam _ 11m” _ 11+ ihhllhqyﬂ) -1 _ Ulilﬂnlhﬂnlmn — 111111111111].
. (H7111 — 1]C'1[n —1]x]n — 1] + U—Lh]n]:r]n]) . But
Let s1” -11 = (11% _ 11w] -11111 _ 11)" =  (“i + “T1121” ' “m” KW
which is the covariance matrix of 9[n — 1]. Then, [ 1 an
 = (2—1[n _ l] + ﬁhlnlhTkrll) '1 811d thEIGfOTE
= 191” - 11+ K1”1(”1”1- 11T1”1é1” - 11).
- (11% - 1111-11” -11>=1~ - 11+ 11111111) -
Finally, to update the minimum LS error we have
1.1.1111 = (x1111 - Hiniéwif c1111 (x1111 - 11111111111)
11T1111c-11111 (111111 - 11111161111)
= £111 _11c—1111 _ 11 (11111 - 11- H111 - 1191111)
Let e[n] = z[n] — hT]n]0A[n — 1]. Using the update for  produces )
Jmhdn] z x117, _ 116-11,, _1] (x111 a 11 - H111 - 119111 - 11- H111 - 11K1111111111)
+ $11111 (11111 _ hT1111é111 - 11 - 11T1111K1111@1111)
= Jmm]n - 1] - xT[n — 1]C'1]n —1]H[n —1]K[n]e[n]
+ $111111 (1 - 11T1111K1111) @1111.
so that
+ 6-1211111 (1 - 11T1111K1111) @1111
and also
Elm ( _ of, + hT[n]E[n —1lhl"l F31 + hTlnlzln " llhlnl
e[n]
of, + hT]n]E[n — l]h[n]'
Hence, we have ﬁnally
ezlnl
Chapter 9
Method of Moments
9.1 Introduction
The approach known as the method of moments is described in this chapter. It produces
an estimator that is easy to determine and sim le to implement. Although the estimator
has no optimality properties, it is useful if the data record is long enough. lhis is
ecause the method of moments estimator is usuall consistent. If the performance
is not satisfactory, then it can be used as an initial estimate, which is subsequently
improved through a Newton-Raphson implementation of the MLE. After obtaining the
method of moments estimator, we describe some approximate approaches for analyzing
its statistical performance. The techniques are general enough to be used to evaluate
the performance of many other estimators and are therefore useful in their own right.
9.2 Summary
The method of moments approach to estimation is illustrated in Section 9.3 with some
examples. In general, the estimator is given by ]9.11] for a vector parameter. The
performance of the met o o moments estimator may be partially characterized by
the approximate mean of (9.15) and the approximate variance of (9.16). Also, g1);
estimator that depends on a group of statistics whose PDF is concentrated about its
H1881) C8D malZe U58 Ol tlie S8II18 8X f8SS1OI1S. AIIOlIHGI‘ C8188 WE8T8 8J1 8QEIOXIIII8I8 II188.I1
and variance evaluation is useful involves signal in noise problems when the SNR is
hig . Then, the estimation performance can be partia y described by the approximate
mean of (9.18) and the approximate variance of (9.19).
9.3 Method of Moments "
The method of moments approach to estimation is based on the solution of a theoretical
equation involvin t e moments o a . s an examp e, assume we observe z[n] for
n = 0,1,. . . ,N — 1, which are IID samples from the Gaussian mixture PDF (see also
Problem 6 .14)
or in more succinct form
Pﬁlnl; 6) = (1 - E)¢1(Il"l) + @¢2(@l"l)
Mwlnl) =  exp ($2111 ) .
The parameter e is termed the mixture parameter, which satisﬁes 0 < e < 1, and of, a?
are the variances of the individual Gaussian PDFs. The Gaussian mixture PDF may
be thought of as the PDF of a random variable obtained from a N (0, of) PDF with
probability 1 — e and from a N (0, 0g) PDF with probability e. Now if of, 0% are known
and e is to be estimated, all our usual MVU estimation methods will fail. The MLE
will require the maximization of a very nonlinear function of c. Although this can
be implemented using a grid search, the method of moments provides a much simpler
estimator. Note that
Ewan = f” mm [<1 - @>¢.<w[n1>+ agent] dun]
where
= (1 — 5):)? + ca; (9.1)
since the mean of zrln] is zero. This theoretical equation relates the unknown param-
eter e to the second moment. If we now replace  by its natural estimator
f 25;; 212M], we have
% i z2[n] = (1 -— e)o'f + sag (9.2)
and solving for c, we have our method of moments estimator
w 22M] — a?
e - -—-ﬂg _ U? . (9.3)
The ease with which the estimator was found is due to the linear nature of the theoretical
moment equation (9.1). The estimator is easily shown to be unbiased (although this is
not generally the case). The variance of é is found as
var(e) =  var  Z z 
1 znzo
= N<<1%-<r%>*"“’(“["l)
= .1 2 [E(1“[nl)—E’(w’[~1)l
But it is easily shown that
E(z4[n]) = (1 — Q30’? + e30’;
which when combined with (9.1) produces the variance
3(1 — e)a'f + 3mg — [(1 — do’? + e032
To determine the loss in performance we could evaluate the CRLB (we would need to do
this numerically) and compare it to (9.4). If the increase in variance were substantial,
we could attempt to implement the MLE, which would attain the bound for large data
records. It should be observed that the method of moments estimator is consistent in
that é —+ e as N —+ oo. This is because as N —> oo, from (9.3) E(€) = e and from (9.4)
var(€) —+ 0. Since the estimates of the moments that are substituted into the theoretical
equation approach the true moments for large data records, the equation to be solved
approaches the theoretical equation. As a result, the method of moments estimator
will in general be consistent (see Problem 7.5). In (9.2) for example, as N —> oo, we
have
var(é) = (9.4)
and the theoretical equation results. When solved for c, the true value is obtained.
We now summarize the method of moments for a scalar parameter. Assume that
the kth moment m, = E(:z"[n])i'depends upon the unknown parameter 9 according to
m, = 12(0). (9.5)
We ﬁrst solve for 9 as
-"——'"-*“* 0=h*mo
assuming h“ exists. We then replace the theoretical moment by its natural estimator
m=§iHM we
to yield the method of moments estimator
6 = h“ l Nils/Tn] (9.7)
Some examples follow.
Example 9.1 - DC Level in WGN
If z[n] = A+w[n] is observed for n = 0, 1,. . . , N — 1, where w[n] is WGN with variance
0'2 and A is to be estimated, then we know that
This is the theoretical equation of (9.5). According to (9.7), we replace p1 by its natural
estimator, resulting in
In this case h is the identity transformation or h(:r) = at. <>
Example 9.2 - Exponential PDF
Consider N IID observations from the exponential PDF
_ _ /\exp(-—/\z[n]) z[n] > 0
pew. i) - { 0 M < 0_
We wish to estimate the parameter /\, where /\ > 0. The ﬁrst moment is >
l), =  = / :r[n]/\exp(——/\z[n])dz[n]
§ wsexm-od:
Solving for A
and substituting the natural estimator result in the method of moments estimator
9.4 Extension to a Vector Parameter
Now consider a vector parameter 0 of dimension p >< 1. It is obvious that to solve for
6 requires p theoretical moment equations. Hence, we suppose
11,1 = h1(01,02,...,0p)
[L2 h2(91,9g,...,0p)
[LP hp(91, 02, . . . ,0?) 
or in matrix form
p = h(0). (9.9)
We then solve for 0 as
0 = h'1(p) (9.10)
and determine the method of moments estimator as
é=1r1(p) (9.11)
where
It may occur that the ﬁrst p moments are insufficient to determine all the parameters
to be estimated (see Example 9.3). In this case we need to ﬁnd some set of p moment
equations allowing (9.9) to be solved for 0 to obtain (9.10). In practice, it is desirable to
use the lowest order moments possible. This is because the variance of the moment es-
timator generally increases with order. Also, to be able to solve the resulting equations
we would like them to be linear or at least mildly nonlinear. Otherwise, a nonlinear
optimization may be needed, defeating the original motivation for the method of mo-
ments estimator as an easily implemented estimator. In the vector parameter case we
may also need cross-moments, as the signal processing example in Section 9.6 illustrates
(see also Problem 9.5). We now continue the Gaussian mixture example.
Example 9.3 - Gaussian Mixture PDF
Returning to the introductory example of a Gaussian mixture PDF, assume now that
in addition to e, the Gaussian variances of and a; are unknown as well. To estimate
all three parameters we require three moment equations. Noting that the PDF is an
even function so that all odd order moments are zero, we utilize
p; = E(:r2[n]) = (1 — 6):)? + ca;
p4 = E(z4[n]) = 3(1— Q01‘ + 360';
p6 = E(z6[n]) = 15(1— do? + 15603.
Although nonlinear, these equations may be solved by letting [Rider 1961]
v = crfcrg. (9.12)
Then it can be shown by direct substitution that
Once u is found, v can be determined. Then, of and a; are obtained by solving (9.12)
to yield
o’ i
and ﬁnally, the mixture parameter becomes (see (9.3))
The method of moments performs the same operations but with the theoretical mo-
ments {p2, p4, p6} replaced by their natural estimators.
9.5 Statistical Evaluation of Estimators
In the method of moments we do not know beforehand whether the estimator will
perform well. Once the estimator has been obtained we may be able, if we are lucky,
to determine the statistical properties of mean and variance as in the introductory
Gaussian mixture example. Since
é = 11w») = gm (913)
we can in principle determine the PDF of 9 using standard formulas for the transforma-
tion of random variables. In practice, this is usually impossible due to its mathematical
intractibility. Because (9.13) is the general form for any estimator, the methods to be
described apply to other estimators in addition to the method of moments estimator.
They allow us to assess the performance of an estimator by determining approximate
expressions for the mean and variance. The approximation generally improves as the
data record length increases, being an asymptotic approach. For determination of the
exact mean and variance a Monte Carlo computer simulation as discussed in Chapter 7
must be employed.
Consider a scalar parameter that has been estimated using (9.13). To determine the
approximate mean and variance of 9 we must assume that it depends upon the '1' < N
statistics {T1(x),T2(x), . . . ,T,(x)}, whose variances and covariances are small. With
this latter assumption the PDF of [T1 T 2 . . . T ,]T will be concentrated about its mean.
In Example 9.2, for instance, the estimator of /\ can be written as
1) = Q(T1(x))
where T1(x) = ﬁzgz-Olzrln] and g(T1) = 1/T1. For large N the PDF of T1 will be
heavily concentrated about its mean since var(T 1) = 1/ (N A2), as will be shown 1n
Example 9.4. Using the statistical linearization argument of Chapter 3, we can use a
ﬁrst-order Taylor expansion of g about the mean of T1. In general, we assume that
9 = 9(T)
where T = [T1 T; . . . T,]T. We then perform a ﬁrst-order Taylor expansion of g about
the point T = E(T) = p to yield
e=g<r> ~9(H)+g m hum m.)- (914)
Assuming this to hold, the mean becomes
5(5) = 900- (9-15)
To this degree of approximation  = E(g(T)) =  or the expectation com-
mutes over the nonlinear function g. Note that we requireithe ﬁrst-order moment of T
to determine the mean. To determine the variance we again use (9.14) to obtain
var(d) = E{ 900+ 3% (T—u) —E(@)] 
But from (9.15) we have that  = g(p). Therefore,
var(l9) _ 5f TZ” CT 6T h” ( .1 l
where CT is the covariance matrix of T. Now we require both the mean p and covariance
matrix CT to determine the variance. Similar derivations can be used to- extend the
Taylor expansion to higher order terms, although the algebra becomes increasingly
tedious (see Problems 9.8 and 9.9). An example follows.
Example 9.4 - Exponential PDF (continued)
Continuing Example 9.2, suppose we have the method of moments estimator
where the :zt[n]’s are IID and each has an exponential distribution. To ﬁnd the approx- and from (9.16) the approximate variance is
imate mean and variance we use (9.15) and (9.16). In this case we have
~ _ var(/\) — var(T ) —
were INA (A)NX2( A)
T1 — W 113(7).) _ A2
and . . . . . .
T _ 1 The estimator is seen to be approximately unbiased and has an approximate variance
g( 1) _ F, decreasing with N. It should be emphasized that these expressions are only approxi-
The mean of T is mate. They are only as accurate as is the linearization of g. In fact, for this problem
1 /\ can be easily shown to be the MLE. As such, its asymptotic PDF (as N —> oo) can
_ , be shown to be (see (7.8))
n1 = E(T1) = w E(z[n]) ,\ .“c/\/(,\,,\2/N)_
=  Hence, the approximation is accurate only as N —> oo. (In Problem 9.10 quadratic
1 expansions of g are employed to derive the second-order approximations to the mean
= X and variance of /\.) To determine how large N must be for the mean and variance
vaﬂwlnl)
so that
Since g(T1) =1/T1,
From (9.15) we have the approximate mean
var(T1)
expressions to hold requires a Monte Carlo computer simulation. <>
The basic premise behind the Taylor series approach is that the function g be approx-
imately linear over the range of T for which p(T; 9) is essentially nonzero. This occurs
= Va‘ — ZW) t ll h
N “=0 na ura y w en
_ Valiilnl) 1. The data record is large so that p(T; 9) is concentrated about its mean, as in the pre-
N vious example. There the PDF of T1 = i 25:’; :r[n] becomes more concentrated
about E(T1) as N —+ oo since var(T1) =1/(N/\2) —> 0 as N —> oo.
1 2. The problem is to estimate the parameter of a signal in noise, and the SNR is
high. In this case, by expanding the function about the signal we can obtain the
/0m z2[n]/\exp(—/\z[n]) dz[n]
2 1 1 A approximate mean and variance. For a high SNR the results will be quite accurate.
= 7 - 7 = 7 This is because at a high SNR the noise causes only a slight perturbation of the
’\ ’\ ’\ estimator value obtained in the case of no noise. We now develop this second case.
1 We consider the data
var(T1) = W
_6_9 _ __}_ : _/\2‘ where w[n] is zero mean noise with covariance matrix C. A general estimator of the
3T1 13:,“ [If scalar parameter 9 is
9 — 9(1)
* 1 = 9(S(9) + W)
i = h(w)-
In this case we may choose the statistic T as the original data since as the SNE becomes
larger, the PDF of T = x becomes more concentrated about its mean, which is the
signal. We now use a ﬁrst-order Taylor expansion of g about the mean of x, which is
p = s(0), or, equivalently, of h about w = 0. This results In
9w h(0) + Nil 6h w[n]. (917)
As before, it follows that approximately
5(9) = h(0) = 9(S(9)) (9-18)
- an T an
9 = __ C _ (9.19)
var( ) 8w W20 8w w=0
In most cases if there is no noise, the estimator yields the true value since g(s(9)) = 9.
Hence, for a high SNR it will be unbiased. An example follows.
Example 9.5 - Exponential Signal in White Noise
For the data
:r[n] =r"-l-w[7ll Tl=oili2
where w[n] is zero mean uncorrelated noise with variance (r2, the damping factor r is
to be estimated. Wishing to avoid a maximization 0f the likelihood to ﬁnd the MLE
(see Example 7.11), we propose the estimatm‘
In terms of the signal and noise we have
and according to (9.18)
(T) - < > - ,, H -
or the estimator is approximately unbiased. To ﬁnd the variance
8w[0] u, _ (r + will + 1 + will)? m.
7 (r +1)’
Similarly,
Since C = c721, we have from (9.19)
var(r) = a2i(6g}[ln] )2
z <ri1>2l’2+("1)2+1l
It is also worthwhile to remark that from (9.14) and (9.17) the estimators are approx-
imately linear functions of T and W, respectively. If these are Gaussian, then 0 will
also be Gaussian — at least to within the approximation assumed by the Taylor expan-
sion. Finally, we should note that for a vector parameter the approximate mean and
variance for each component can be obtained by applying these techniques. Obtaining
the covariance matrix of 9 is possible using a ﬁrst-order Taylor expansion but can be
extremely tedious.
9.6 Signal Processing Example
We now apply the method of moments and the approximate performance analysis to
the problem of frequency estimation. Assume that we observe
z[n]=Acos(21rf0n+¢)+w[n] n=0,1,...,N—1
where w[n] is zero mean white noise with variance 0'2. The frequency f0 is to be
estimated. This problem was discussed in Example 7.16 in which the MLE of frequency
was shown to be approximately given by the peak location of a periodogram. In an
effort to reduce the computation involved in searching for the peak location, we now
describe a method of moments estimator. To do so we depart slightly from the usual
sinusoidal model to assume that the phase d: is a random variable independent of w[n]
and distributed as qb ~ Ll[0, 2w]. With this assumption the signal s[n] = Acos(21r fon +
qb) can be viewed as the realization of a WSS random process. That this is the case is
veriﬁed by determining the mean and ACF of s[n]. The mean is
Bow) = E{Acos(21rf@n+¢)1
0 cos( 1r on 27f
and the ACF is
= E[A2 cos(21rfQn + ¢)cos(211'fo(" + k) + ¢ll
= A2E écosﬂlvrfon + 2am + w) + 5 cos 21mm
= i;- cos 21rf0k. i
The ACF of the observed process becomes
= —§—cos21rf0k + a 
To simplify the discussion and the subsequent estimator we will assume that the signal
amplitude is known (see Problem 9.12 for the unknown amplitude case). We w1l let
A = J5, so that the ACF becomes
rmﬂc] = cos Qvrfgk + 026M].
To estimate the frequency by the method of moments approach we observe that
r,,[1] = cos Zvrfg,
and therefore without having to assume knowledge of 0'2 we can implement the method
of moments estimator 1
f0 = g arccos 1*,,[1]
where FIJI] is an estimator of rmﬂ]. A reasonable estimator of the ACF for k = 1 is
so that ﬁnally we have for our frequency estimator
f0 = 2-” arccos N _ 1 1;) z[n]w[n + l] - (920)
The argument of the arccos function may exceed 1 in magnitude, usually occurring
when the SNR is low. If this is the case, the estimate is meaningless and it is indicative
of an unreliable estimator.
Although motivated by the random phase sinusoidal model, this estimator remains
valid if qb is deterministic and unknown (see Problem 9.11). In determining the mean
and variance of f0 we assume that qb is deterministic. To assess the performance of our
estimator we use the ﬁrst-order Taylor expansion approach and expand about the point
w = 0. Thus, the obtained performance is valid for high enough SNR. From (9.18) we
have
E(fAQ) = 517? arccos ] 1 \/§cos(2vrfgn + <15)\/§ cos(21rfQ(n + 1) + M]
g arccos [N _ 1 Z (cos(41rfQn + Qvrfo + 2gb) + cos 21rfQ)] .
The double-frequency term when summed is approximately zero, so that at a high SNR
E(fo) = 51; arccos [cos Qvrfg]
= f0. (9.21)
To ﬁnd the variance we note that
f0 = h(w) = 2—7r arccos N _1 Z(s[n] + w[n])(s[n + 1] + w[n +1])
so that the ﬁrst-order partials are
3111b] ‘:0 _ 21r t/l - u? 3w[i] w=o'
a 1 a '2
awlfi] =  Z (s[n]w[n + 1] + w[n]s[n +1])
with the other terms in the sum resulting in zero after differentiating and letting W = 0.
Continuing, we have
= F1_—l(s[2'—1]+s[i+1]) i=1,2,...,N—2
Also,
arccos(z)
cos 21rf0
as already shown. Thus,
Figure 9.1 Explanation for in-
6h _ __ 1 6U _1 1 Z z COS 27d!) creased variance of frequency esti-
3w[i] w=o 21rsin 21rf0 3112M w=o mat!"
From (9.19) with C = 021 we obtain 0220
vafifo) ~ 0' "E20 (awml W20) s g 0310
(21r)2(N — 1)2sin221rf0 0.200
+ (s[n —~ 1] + s[n +1])2 + s2[N — 2]} . 0.195
But sln - l] + s[n + 1] = 2cos 21rf@s[n], as can easily be veriﬁed, so that ﬁnally the -10 l5 (I, y To 1g l
variance of the frequency estimator is for a high SNR SNR (dB) 20
var(]f ) "2 2[1]+ 4cos2 21f f 1552p] + 11v 2] (9 22) (a)
Where sin] = \/2cos(21rf0n + <15). The evaluation of the variance is straightforward,
although tedious. Note that the variance decreases most rapidly as 1 /N 2 (for f0 = 1/4)
versus the 1/N3 dependence for the CRLB (see (3.41)). Additionally, for f0 near 0 or g
1/2 the variance increases rapidly. This is due to the arccos operation required to g
determine f0. As shown in Figure 9.1, a slight perturbation in the argument for z near >0
i1 (f0 near O or 1/2) will cause large ﬂuctuations in the function value due to the steep g3
slope. g
As an example of the accuracy of the approximate mean and variance expressions,
we consider the data set with parameters SNR = 242/202 = 1/02 for A = \/2 and
f0 = 0.2, qb = 0, and N = 50. For w[n] being WGN a Monte Carlo computer simulation
in which 1000 realizations of f0 as given by (9.20) were generated for each of several
SNRs. In Figure 9.2a the actual mean E(f0) obtained, as well as the theoretical mean
of f0 = 0.2, are shown versus SNR. As expected for a high enough SNR, the estimator
is essentially unbiased. The slight discrepancy is due to the approximations made in
deriving the estimator, principally that N was large enough to neglect the double-
frequency term. For larger N it can be veriﬁed that the mean equals the true value
SNR (dB)
(b)
Figure 9.2 Method of moments frequency estimator
for a high SNR. Correspondingly, in Figure 9.2b the actual variance obtained and
the theoretical variance as determined from (9.22) are shown versus SNR. The same
behavior is evident. Some further discussions of this example can be found in Problem
9.13. For further discussions of this estimation approach the reader should consult
[Lank et al. 1973, Kay 1989].
References
Kay, S., “A Fast and Accurate Single Frequency Estimator,” IEEE Trans. Acoust, Speech, Signal
Process., Vol. 37, pp. 1987—1990, Dec. 1989.
Lank, G.W., LS. Reed, G.E. Pollon, “A Semicoherent Detection and Doppler Estimation Statistic,”
IEEE Trans. Aerosp. Electron. Systems, Vol. 9, pp. 151-165, March 1973.
Rider, P.R., “Estimating the Parameters of Mixed Poisson, Binomial, and Weibull Distributions
by the Method of Moments,” Bull. Int. Statist. Inst, Vol. 38, pp. 1-8, 1961. D
Problems
9.1 If N IID observations {z[0],z[1], . . . ,:r[N — 1]} are made from the Rayleigh PDF
2 —m—exp(——m—2) z>0
P(@;F)= a’ 2v
ﬁnd a method of moments estimator for a2.
9.2 If N IID observations {:r[0], :r[1], . . . ,2:[N — 1]} are made from the Laplacian PDF
P(@;<1)= # exp (- ﬁlm)
2a U
ﬁnd a method of moments estimator for a.
9.3 Assume that N IID samples from a bivariate Gaussian PDF are observed or
{xQ,x1,...xN_1}, where each x is a 2 >< 1 random vector with PDF x ~ A/(O, C).
ﬁnd a method of moments estimator for p. Also, determine a cubic equation to be
solved for the MLE of p. Comment on the ease of implementation of the different
estimators.
9.4 If N IID observations {z[0],z[1], . . . ,:r[N — 1]} are made from the N(p,az) PDF,
ﬁnd a method of moments estimator for 6 = [p crzlT.
9.5 In Example-Bl? we determined that the ACF of an ARMA process satisﬁes the
recursive difference equation
Trrlnl = _ ialklﬁzln — k] n > q
Where ialll, 11m, - - - , a[p]} denote the AR ﬁlter parameters and q is the MA order.
Propose a method of moments estimator for the AR ﬁlter parameters.
9.6 For a DC level in WGN or
where w[n] is WGN with variance 0'2, the parameter A2 is to be estimated. It is
proposed to use
For this estimator ﬁnd the approximate mean and variance using a ﬁrst-order
Taylor expansion approach.
9.7 For the observed data
where  is WGN with variance 0'2, ﬁnd a method of moments estimator for
qb. Assuming the SNR is high, determine the approximate mean and variance for
your estimator using the ﬁrst-order Taylor expansion approach.
9.8 In this problem we determine the mean of an estimator to second order by using a
second-order Taylor expansion. To do so expand 9 = g(T) about the mean of T
or T = p, retaining the terhms up to and including the quadratic term. Show that
the approximate mean of 9 is
E09) = yo») + étriGuﬂﬂwl
where
Hint: Use the result
EPITY) = EUNYID) = tr(E(YIT))-
9.9 In this problem we determine the approximatevariance of an estimator to second
order or by using a second-order Taylor expansion approach. To do so ﬁrst expand
0 = g(T) about the mean of T or T = p, retaining the terms up to and including
the quadratic term. Next, use the results of Problem 9.8 and ﬁnally make the
assumption that T ~ A/(p, CT). You should be able to verify the following result:
var(0) - 6T
The r >< r G(p) is deﬁned in Problem 9.8. Hint: You will need the following result,
which is valid for a symmetric N >< N matrix A,
var(xTAx) = 2tr [(AC,)2]
if x ~ A/(QCI).
9.10 Using the results of Problems 9.8 and 9.9 ﬁnd the approximate mean and variance
to second order for the estimator of /\ discussed in Example 9.4. How do your
results compare to the ﬁrst-order result? Be sure to justify the approximate
Gaussian PDF of i, required to apply the variance expression. Also, compare the
results to those predicted from asymptotic MLE theory (recall that the estimator
is also the MLE).
9.11 Prove that for a sinusoidal signal
s[n] = Acos(21rfQn + (b)
where the phase qb is deterministic but unknown, that
————- s[n]s[n + 1] -+ — cos 21rfQ
as N —> oo. Assume that f0 is not near 0 or 1/2. Hence, comment on the use of
the method of moments estimator proposed in the signal processing example in
Section 9.6 for the deterministic phase sinusoid.
9.12 To extend the applicability of the frequency estimator proposed in the signal
processing example in Section 9.6 now assume that the signal amplitude A is
unknown. Why can’t the proposed estimator (9.20) be used in this case? Consider
the estimator
Show that at a high SNR and for large N , E(fg) = f0. Justify this estimator
as a. method of moments estimator based on the ACF. Would you expect this
estimator to work well at a lower SNR?
f0 = —— arccos
9.13 For the signal processing example in Section 9.6 assume that the parameters are
A = ﬂvfﬂ = 0.25, qb = 0, and that N is odd. Show that from (9.22) var(fo) = ()_
To understand this result note that
for i =.0, 1, .. . . , N — 1. What does this say about the ﬁrst-order Taylor expansion
approximation? How could you improve this situation?
Chapter 10
The Bayesian Philosophy
10.1 Introduction
We now depart from the classical approach to statistical estimation in which the pa-
rameter 0 of interest is assumed to be a deterministic but unknown constant. Instead,
we assume that 9 is a random variable whose particular realization we must estimate.
This is the Bayesian approach, so named because its implementation is based directly
on Bayes’ theorem. The motivation for doing so is twofold. First, if we have available
some prior knowledge about 0, we can incorporate it into our estimator. The mech-
anism for doing this requires us to assume that 0 is a random variable with a given
prior PDF. Classical estimation, on the other hand, ﬁnds it diﬁicult to make use of any
prior knowledge. The Bayesian approach, when applicable, can therefore improve the
estimation accuracy. Second, Bayesian estimation is useful in situations where an MVU
estimator cannot be found, as for example, when the variance of an unbiased estimator
may not be uniformly less than that of all other unbiased estimators. In this instance,
it may be true that for most values of the parameter an estimator can be found whose
mean square error may be less than that 0f all other estimators. By assigning a PDF
to 0 we can devise strategies to ﬁnd that estimator. The resultant estimator can then
be said to be optimal “on the average,” or with respect to the assumed prior PDF of
0. In this chapter we attempt to motivate the Bayesian approach and to discuss some
of the issues surrounding its use. The reader should be aware that this approach to
estimation has had a long and controversial history. For a more deﬁnitive account [Box
and Tiao 1973] is recommended.
10.2 Summary
The Bayesian MSE is deﬁned in (10.2) and is minimized by the estimator of (10.5),
which is the mean of the posterior PDF. The example of a DC level in WGN with a
Gaussian prior PDF is described in Section 10.4. The minimum MSE estimator for this
example is given by (10.11) and represents a weighting between the data knowledge and
prior knowledge. The corresponding minimum MSE is given by (10.14). The ability to
1 enTi/Tli“) pixie/i)
114(5) A) =  
(b) PDF of truncated sample mean
(a) PDF of sample mean
Figure 10 1 Improvement of estimator via prior knowledge
. . . . e 10.5
- - bl b ed on data is described in Section i
e the reahzatlon of a randliivneeliliailhi rZndaoSm variables. Theorem 10.2 SHmIHH-YIZeS
PDF yields a conditional PDF that is also Gaussian,
This is then applied t0 theiBayeslan
ior PDF as summarized in Theorem 10.3. As
_ . f the posterior PDF (10.28) is the minimum
will be shown in Chapter 11) the mean 0 _ _ . ters from
- t . Section 10.7 discusses nuisance parame _
b11831? esilmatwl fggiarlzefﬁgirl: $1132: r10 8 describes the potential difficulties of using a
a ayesian view , -
. - - ' ' tion roblem.
Bayesian estimator in a classical estima P
estimat
as being due to the correlation bet
the result that ajoilltly Gellsslall _ 25
having a mean (10.24) and a covariance (10. 
linear model of (10.26) t0 yield the Poster
10 3 Prior Knowledge and Estimation
1 1 f e timation theorv that the use of prior knowledge will lead to
ru e o s -
- ‘ trained to lie in a known
' For example. if a parameter 1S cons
a more accurate estimator.
" duce only estimates within that interval.
interval then any good estimator should pro _ _ h l mean i
l - he MVU estimator of A is t e samP e -
In Example 3.1 it was shown that t
. 1 ' the interval ~00 < A < oo.
However, this assumed that ‘A coulcintarlrtxirtlnrzglgozzkﬁz :1) assume that A can take on
Due to Physical constraints it rlnay4 < A < A To retain A z i as the best estimator
only values "(l1 the irlnte. mtegamgy yield valuelslloutside the known interval. As shown
would be un esira e since _ . r
_ . . ~ l ould expect to improve ou
in Figure 10.1a, this is due to noise effects. Certain y, we W
_ . . t'mator
estimation if we used the truncated sample mean es l
It is a fundamenta
A = i -AO g a 5 A0
- ‘ ld have
- ' ' known constraints. Such an estlmetol‘ W0“
which would be consistent with the
the PDF
p Ag; A) = Pr{i s —A@}<5(€ + A0)
+ PM; A)lu(€ + 140% "K - Aﬁll
+ Pie z Ame - A0) <1“)
where  is the unit step function. This is shown in Figure 10.1b. It is seen that A
is a biased estimator. However, if we compare the MSE of the two estimators, we note
that for any A in the interval —A0 § A g A0
msAA) = /OO(§-A)2PA(§;A)d5
= “Y: - AVPAK; A) d: +  (s - Afpiis. A) d:
+ [we - A)*pi<§.A)ds
> "ii-A. » Arise; A) d€ +  (s - Afpie. A) As
+ [OYAD — A)2P,i(§; A) d:
= mse(A).
Hence, A, the truncated sample mean estimator, is better than the sample mean esti-
mator in terms of MSE. Although A is still the MVU estimator, we have been able to
reduce the mean square error by allowing the estimator to be biased. In as much as
we have been able to produce a better estimator, the question arises as to whether an
optimal estimator exists for this problem. (The reader may recall that in the classical
case the MSE criterion of optimality usually led to unrealizable estimators. We shall
see that this is not a problem in the Bayesian approach.) We can answer affirmatively
but only after reformulating the data model. Knowing that A must lie in a known
interval, we suppose that the true value of A has been chosen from that interval. We
then model the process of choosing a value as a random event to which a PDF can
be assigned. With knowledge only of the interval and no inclination as to whether A
should be nearer any particular value, it makes sense to assign a Ll[—A0, A0] PDF to
the random variable A. The overall data model then appears as in Figure 10.2. As
shown there, the act of choosing A according to the given PDF represents the departure
of the Bayesian approach from the classical approach. The problem, as always, is to
estimate the value of A or the realization of the random variable. However, now we
can incorporate our knowledge of how A was chosen. For example, we might attempt
to ﬁnd an estimator A that would minimize the Bayesian MSE deﬁned as
Bmse(A) = E[(A - AV]. (10.2)
We choose to deﬁne the error as A —- A in contrast to the classical estimation error of
A - A. This deﬁnition will be useful later when we discuss a vector space interpretation
of the Bayesian estimator. In (10.2) we emphasize that since A is a random variable, the
expectation operator is with respect to the joint PDF p(x, A). This is a fundamentally
different MSE than in the classical case. We distinguish it by using the Bmse notation.
To appreciate the difference compare the classical MSE
mse(A) = [pi - A)2p(x; A) dx (10.3)
i Bayesian shiiésééil]
Figure 10.2 Bayesian approach to data modeling
to the Bayesian MSE
Bmse(A) = f / (A _ A)2P(X, A) dxdA. (10.4)
Even the underlying experiments are different, as is reﬂected in the averaging PDFs.
If we were to assess the MSE performance using a Monte Carlo computer simulation,
then for the classical approach we would choose a realization of wln] and add it to a
given A. This procedure would be repeated M times. Each time we would add a new
realization of wln] to the same A. In the Bayesian approach, for each realization we
would choose A according to its PDF U[—A0,A0] and then generate wln] (assuming
that w[n] is independent of A). We would then repeat this procedure M times. In
the classical case we would obtain a MSE for each assumed value of A, while in the
Bayesian case the single MSE ﬁgure obtained would be an average over the PDF of
A. Note that whereas the classical MSE will depend on A, and hence estimators that
attempt to minimize the MSE will usually depend on A (see Section 2.4), the Bayesian
MSE will not. In effect, we have integrated the parameter dependence away! It should
be clear that comparing classical and Bayesian estimators is like comparing “apples
and oranges.” The reader who is tempted to do so may become thoroughly confused.
Nonetheless, at times, the forms of the estimators will be identical (see Problem 10.1).
To complete our example we now derive the estimator that minimizes the Bayesian
MSE. First, we use Bayes’ theorem to write
MK. A) = P(AlX)P(X)
so that
Bmse(A) = f  — A)2p(A|x) dA] p(x) dx.
Now since p(x) 2 0 for all x, if the integral in brackets can be minimized for each x,
then the Bayesian MSE will be minimized. Hence, ﬁxing x so that A is a scalar variable
E(A|X)
(a) Prior PDF (b) Posterior PDF
Figure 10.3 Comparison of prior and posterior PDFs
(as opposed to a general function of x), we have
i [w - Arm/mi) dA = f 81AM - A)2p(A|x) dA
f —2(A - A)p(A|x) dA
= -2 /Ai>(A|X)dA HA/poitqdi
which when set equal to zero results in
A = f Ap(A|x)dA
or ﬁnally
A = E(A|X) (10.5)
since the conditional PDF must integrate to 1. It is seen that the optimal estimator in
erms of minimizing the Bayesian MSE is the mean of the posterior PDF p(A|x) (see
is; Probleilili 10.5 for an alternative derivation). The posterior PDF refers to the PDF
aftert e data have been observed. In contrast, p(A) or
Iigay be thought of as the prior PDF of A, indicating the PDF before the data are
ﬁhgﬁgzdénuge will henceforth term the estimator that minimizes the Bayesian MSE
data will be tmean square error (MMSE) estimator. I-ntuitively, the effect of observing
o concentrate the PDF of A as shown in Figure 10.3 (see also Problem
$613M $311511‘: Zectaliiissiaidlteiloglteadge of the data should reduce our uncertainty about A.
In determining the MMSE estimator we ﬁrst require the posterior PDF. We can use
Bayes’ rule to determine it as
P(X|A)1>(A)
P(XIA)P(A)
P(AIX) =
(10.0)
Note that the denominator is just a normalizing factor, independent of A, needed to
ensure that p(A]x) integrates to 1. If we continue our example, we recall that the prior
PDF p(A) is U[—A0,A0]. To specify the conditional PDF p(x]A) we need to further
assume that the choice of A via p(A) does not affect the PDF of the noise samples or
that w[n] is independent of A. Then, for n = 0, 1, . . . , N — 1
Pw($["l _ A)
= h exp [—-£—2(z[n] — AF]
and therefore N_l
100A) = —-‘—  Z (A1111 - A?) (m)
exp
(21ra2)% "=0
It is apparent that the PDF is identical in form to the usual classical PDF p(x; A). In
the Bayesian case, however, the PDF is a conditional PDF, hence the “I” separator,
while in the classical case, it represents an unconditional PDF, albeit parameterized
by A, hence the separator “;” (see also Problem 10.6). Using (10.6) and (10.7), the
posterior PDF becomes
p<Ar==> -  1 (-3. ” ‘ _ A 2 dA
a. 2An<z1wz>t exp 2"’ M )
= N(A—;i')2+ Zz2[n]—N;i-2
so that we have
2 exp [~ (A - if] (A: s A.
p(A|X) = c 211'"?
(10.0)
The factor c is determined by the requirement that p(A|x) integrate to 1, resulting in
The PDF is seen "to be a truncated Gaussian, as shown in Figure 10.3b. The MMSE
estimator, which is the mean of p(A|x), is
A = E(A|x)
= [OoAp(A]x)dA
A exp — H, (A — as) dA
[i0 1 exp[—1(A—."E)2:IdA'
Although this cannot be evaluated in closed form, we note that A will be a function of
i as well as of A0 and a2 (see Problem 10.7). The MMSE estimator will not be 1E due
to the truncation shown in Figure 10.3b unless A0 is so large that there is effectively no
truncation. This will occur if A0 >> y/Uz/N. Otherwise, the estimator will be “biased”
towards zero as opposed to being equal to i. This is because the prior knowledge
embodied in p(A) would in the absence of the data x produce the MMSE estimator
(see Problem 10.8)
(10.9)
A =E(A) =0.
The effect of the data is to position the posterior mean between A = 0 and A = :2 in a
compromise between the prior knowledge and that contributed by the data. To further
appreciate this weighting consider what happens as N becomes large so that the data
knowledge becomes more important. As shown in Figure 10.4, as N increases, we have
from (10.8) that the posterior PDF becomes more concentrated about :2 (since az/N
decreases). Hence, it becomes nearly Gaussian, and its mean becomes just i. The
MMSE estimator relies less and less on the prior knowledge and more on the data. It
is said that the data “swamps out” the prior knowledge.
P(A|X)
—Ao x A0 A
E-(AIX) i: z E(A]x)
(a) Short data record
(b) Large data record
Figure 10.4 Effect of increasing data record on posterior PDF
The results of this example are true in general and are now summarized. The
Bayesian approach to parameter estimation assumes that the parameter to be estimated
is a realization of the random variable a9. As such, we assign a prior PDF 12(9) to it. After
the data are observed, our state of knowledge about the parameter is summarized by
the posterior PDF p(a9|x). An optimal estimator is deﬁned to be the one that minimizes
the MSE when averaged over all realizations of 9 and x, the so-called Bayesian MSE.
This estimator is the mean of the posterior PDF or é = E(a9|x). The estimator is
determined explicitly as
é: Egan) = f 9p(0lx) d9. (10.10)
The MMSE estimator will in general depend on the prior knowledge as well as the
data. If the prior knowledge is weak relative to that of the data, then the estimator
will ignore the prior knowledge. Otherwise. the estimator will be “biased” towards the
prior mean. As expected, the use of prior information always improves the estimation
accuracy (see Example 10.1).
The choice of a prior PDF is critical in Bayesian estimation. The wrong choice will
result in a poor estimator, similar to the problems of a classical estimator designed with
an incorrect data model. Much of the controversy surrounding the use of Bayesian
estimators stems from the inability in practice to be able to justify the prior PDF.
Suffice it to say that unless the prior PDF can be based on the physical constraints of
the problem, then classical estimation is more appropriate.
10.4 Choosing a Prior PDF
As shown in the previous section, once a prior PDF has been chosen, the MMSE
estimator follows directly from (10.10). There is no question of existence as there is
with the MVU estimator in the classical approach. The only practical stumbling block
that remains, however, is whether or not E (0|x) can be determined in closed form. In
the introductory example the posterior PDF p(A|x) as given by (10.8) could not be
foundexplicitly due to.the need to normalize p(x|A)p(A) so that it integrates to 1.
Additionally, the posterior mean could not be found, as evidenced by (10.9). We would
have to resort to numerical integration to actually implement the MMSE estimator.
This problem is compounded considerably in the vector parameter case. There the
posterior PDF becomes
which requires a p-dimensional integration over 0. Additionally, the mean needs to
be evaluated (see Chapter 11), requiring further integration. For practical MMSE
estimators we need to be able to express them in closed form. The next example
illustrates an important case where this is possible.
Example 10.1 - DC Level in WGN - Gaussian Prior PDF
We now modify our prior knowledge for the introductory example. Instead of assuming
the uniform prior PDF
mm={2a “SA”
which led to an intractable integration, consider the Gaussian prior PDF
p( )  exp [ 26E‘  ILA) '
The two prior PDFs clearly express different prior knowledge about A, although with
p,‘ = 0 and 3a,; = A0 the Gaussian prior PDF could be thought of as incorporating
the knowledge that [AI g A0. Of course, values of A near zero are thought to be more
probable with the Gaussian prior PDF. Now if
1701i l (271172)? exp) 2H2 "=0 ) J
_ (gmrzy; exp —-§'—2 g a:  exp [—7(NA — ZNAz)
we have
P(X|A)P(A)
1 l 1 NA q 1] [ 1(NA’ zzvmm]
N ex —-——— IE TL EXP "--"- _‘
(21119)? 21mg p 202 "=0 202
/  exp[_i2 ¢Z[n]]eXP[-L2(NA2—ZNAE)]
_ea(21r02)? 2x63 2U "=0 U
EXP "mi ‘Mal
ex —E L(NA2——ZNA.i)+L(A“M/\)2)] 1
p 2 a2 "i;
z 0° 1 1 NA? 2NAi)+i(A— )2 dA
Loo exp —§ ;;( — 0i HA
Note however that the denominator does not depend on A, being a normalizing factor,
and the argument of the exponential 1s quadratic 1n A. Hence, p(A|x) must be a
Gaussian PDF whose mean and variance depend on x. Continuing, we have for Q(A)
‘ 1W2)“ “(UZHUQ 1v?
Let
mm “ <§ g>afm
Then, by completing the square we have
Q(A) - UEHAA —2HA|%A+#A|:) 62b3,,‘
= —(A-#|@)2— +-
so that
EXP [362 (A — llA|¢l2 EXP [-5  ~ 6;"
[on EXP —2Ui| (A-lhm) EXP ""5 E - Us“ 11A
= 2 ex — 2 (A — pmxf]
where the last step follows from the requirement that p(A[x) integrate to 1. The pos-
terior PDF is also Gaussian, as claimed. (This result could also have been obtained by
using Theorem 10.2 since A, x are jointly Gaussian.) In this form the MMSE estimator
is readily found as
A = E(A|x)
or ﬁnally, the MMSE estimator is
= ai+(1~a);4,4 (10.11)
where 2
(l: 26/102
Note that a is a weighting factor since 0 < a < 1. Using a Gaussian prior PDF, we
are able to determine the MMSE estimator explicitly. It is interesting to examine the
interplay between the prior lfnowledge and the data. When there is little data so that
U: << crz/N, a is small and A z pA, but as more data are observed so that 0f‘ >> crz/N,
a z 1 and A z i. The weighting factor a depends directly on our conﬁdence in the prior
knowledge or of, and the data knowledge or az/N. (The quantity crz/N is interpreted
as the conditional variance or E — A)2|A]). Alternatively, we may view this process
by examining the posterior PDF as N increases. Referring to Figure 10.5, as the data
record length N increases, the posterior PDF becomes narrower. This is because the
A Figure 10.5 Effect of increasing
data record length on posterior PDF
Aazi A2 A1%HA
posterior variance 1
vargqx) = U?“ = Nil (10.12)
will decrease. Also, the posterior mean (10.11) or A will also change with increasing
N. For small N it will be approximately a,‘ but will approach i for increasing N. In
fact as N —+ oo, we will have A —> i, which in turn approaches the true value of A
chosen. Observe that if there is no prior knowledge, which can be modeled by letting
0'2 —-> oo, then A —-> i for any data record length. The “classical” estimator is obtained.
Filnally it was originally claimed that by using prior knowledge we could improve the
estimation accuracy. To see why this is so recall that
Bmse(A) = E[(A — 
where we evaluate the expectation with respect to p(x, A). But
Bmse(A) = / f (A - A)2P(X, A) dxdA
f / (A - il)2p(A|x) dAp(x) dx.
Since A = E(A|x), we have
Bmse(A) = f f [A - E(A|x)]2p(A|x) dAp(x) dx
f varwwopoodx- (10-w)
We see that the Bayesian MSE is just the variance 0f the posterior PDF when averaged
over the PDF 0f x. As such, we have
Bmse(A) = /a§|,p(x)dx
a2 of,
since aim does not depend on x. This can be rewritten as
ma) N(U5+,; < >
so that finally we see that
Bmse(A) %
where az/N is the minimum MSE obtained when no prior knowledge is available (let
0i —> o0). Clearly, any prior knowledge when modeled in the Bayesian sense will
improve our Bayesian estimator. <>
Gaussian prior PDFs are quite useful in practice due to their mathematical tractability,
as illustrated by the previous example. The basic property that makes this so is the
reproducing property. If p(x, A) is Gaussian, then p(A), being the marginal PDF, is
Gaussian, as is the posterior PDF p(A|x). Hence, the form of the PDF remains the
same, as it is conditioned on x. Only the mean and variance change. Another example
of a PDF sharing this property is given in Problem 10.10. Furthermore, Gaussian prior
PDFs occur naturally in many practical problems. For the previous example we can
envision the problem of measuring the DC voltage of a power source by means of a
DC voltmeter. If we set the power source to 10 volts, for example, we would probably
be willing to assume that the true voltage is close to 10 volts. Our prior knowledge
then might be modeled as A ~ N (10, of‘), where of, would be small for a precision
power source and large for a less reliable one. Next, we could take N measurements
of the voltage. Our model for the measurements could be  = A + w[n], where the
voltmeter error wln] is modeled by WGN with variance 02. The value of 02 would
reﬂect our conﬁdence in the quality of the voltmeter. The MMSE estimator of the true
voltage would be given by (10.11). If we repeated the procedure for an ensemble of
power sources and voltmeters with the same error characteristics, then our estimator
would minimize the Bayesian MSE.
10.5 Properties of the Gaussian PDF
We now generalize the results of the previous section by examining the properties of the
Gaussian PDF. The results of this section will be needed for the derivations of Bayesian
estimators in the next chapter. The bivariate Gaussian PDF is ﬁrst investigated to
illustrate the important properties. Then, the corresponding results for the general
multivariate Gaussian PDF are described. The remarkable property that we shall
exploit is that the posterior PDF is also Gaussian, although with a different mean and
variance. Some physical interpretations are stressed.
Consider a jointly Gaussian random vector [z y]T whose PDF is
p(a' y)=i;exp —l[z—E(z) ]TC'1] (1015)
’ zwdetnc) 2 y ~ Ea) y —Ea> ' ‘
Figure 10.6 Contours of con-
stant density for bivariate Gaussian
$0 E(:::)
This is also termed the bivariate Gaussian PDF. The mean vector and covariance matrix
Note that the marginal PDFs p(.r) and p(y) are also Gaussian, as can be veriﬁed by
the integrations
pa) = figp<ty>dy=qriéexpl-Wakxyw-Ewnf)
The contours along which the PDF p(a:,y) is constant are those values of z and y for
which a: _E(r) TC—1[I _ Lxm]
l y — 13(1)) i’ — E(y)
is a constant. They are shown in Figure 10.6 as elliptical contours. Once 9:, say mo, is
observed, the conditional PDF of y becomes
(In, y) PM» y)
( | t) = p = <.e—»—
P Z1 I17 p(:E0) L p030’ y) dy
so that the conditional PDF of y is that of the cross section shown in Figure 10.6 when
suitably normalized to integrate to 1. It is readily seen that since p(z0,y) (where 0:0
is a ﬁxed number) has the Gaussian forrn in y (from (10.15) the exponential argument
is quadratic in y), the conditional PDF must also be Gaussian. Since p(y) is also
Gaussian, we may view this property as saying that if a: and y are jointly Gaussian,
the prior PDF p(y) and posterior PDF p(y|a:) are both Gaussian. In Appendix 10A we
derive the exact PDF as summarized in the following theorem.
Theorem 10.1 (Conditional PDF of Bivariate Gaussian) If z and y are dis-
tributed according to a bivariate Gaussian PDF with mean vector  and
covariance matriz
° = l e53)?» “$12? l
so that
1 1 a:—E(z) T _1 a:—E(z)
110w) = ——1—e><i> —5 C ,
2" d8" (C) y — E(y) y — E01)
then the conditional PDF p(y|z) is also Gaussian and
cov(z, y)
E0111) = E(y)+ vam) (iv-EM) (10-16)
var(y|z) = var(y)— . (10.17)
We can view this result in the following way. Before observing z, the random variable y
is distributed according to the prior PDF p(y) or y ~ N(E(y), va.r(y)). After observing
0:, the random variable y is distributed according to the posterior PDF p(y]z) given
in Theorem 10.1. Only the mean and variance have changed. Assuming that z and
y are not independent and hence cov(a:,y) 7E 0, the posterior PDF becomes more
concentrated since there is less uncertainty about y. To verify this, note from (10.17)
that
= var(y)(1— p2) (19-18)
where ( )
p — var(z)var(y) (1019)
is the correlation coeﬁicient satisfying |p| g 1. From our previous discussions we also
realize that  is the MMSE estimator of y after observing z, so that from (10.16)
cov(z, y)
(z - Ea». (10.20)
In normalized form (a random variable with zero mean and unity variance) this becomes
i~EW): cwaa> I-Ea)
y" = pzn. (10.21)
Figure 10.7
Contours of constant density of nor-
malized bivariate PDF
The correlation coefficient then acts to scale the normalized observation 03,, to obtain the
MMSE estimator of the normalized realization of the random variable y". If the random
variables are already normalized (E = E(y) = 0, var(.z') = var(y) = 1), the constant
PDF contours appear as in Figure 10.7. The locations of the peaks of p(.c,y)), when
considered as a function of y for each 0:, is the dashed line y = pa, and it is readily
shown that 3) =  = pa‘ (see Problem 10.12). The MMSE estimator therefore
exploits the correlation between the random variables to estimate the realization of one
based on the realization of the other.
The minimum MSE is, from (10.13) and (10.18),
Bmse(3)) = /var(yIa:)p(a')da:
= Vaﬂyli)
= var(y)(1—-p2) (10.22)
since the posterior variance does not depend on z (var(y) and p depend on the covariance
matrix only). Hence, the quality of our estimator also depends on the correlation
coefficient, which is a measure of the statistical dependence between z and y.
To generalize these results consider a jointly Gaussian vector [xT yTjT, where x is
k x 1 and y is l x 1. In other words, [xT yTjT is distributed according to a multivariate
Gaussian PDF. Then, the conditional PDF of y for a given x is also Gaussian. as
summarized in the following theorem (see Appendix 10A for proof).
Theorem 10.2 (Conditional PDF of Multivariate Gaussian) If x and y are
jointly Gaussian, where x is k x 1 andy is l x 1, with mean vector [E(x)T E(y)T]T and
partitioned covariance matrix
C=l yl=iz>><<k 1:1) (1023)
so that
then the conditional PDF p(y|x) is also Gaussian and
E611) = EU) + Cyﬂiibr — E00) (10-24)
Cull Cay _ CyICx-ICIU‘ (1025)
Note that the covariance matrix of the conditional PDF does not depend on x, although
this property is not generally true. This will be useful later. As in the bivariate case,
the prior PDF p(y) is Gaussian, as well as the posterior PDF p(y|x). The question may
arise as to when the jointly Gaussian assumption may be made. In the next section we
examine an important data model for which this holds, the Bayesian linear model.
10.6 Bayesian Linear Model
Recall that in Example 10.1 the data model was
where A ~ N(#A7UZ‘), and w[n] is WGN independent of A. In terms of vectors we
have the equivalent data model
This appears to be of the form of the linear model described in Chapter 4 except for
the assumption that A is a random variable. It should not then be surprising that a
Bayesian equivalent of the general linear model can be deﬁned. In particular let the
data be modeled as
x = H9 + w (10.26)
where x is an N x 1 data vector, H is a known N x p matrix, 9 is a p x 1 random
vector with prior PDF A/(pg, Cg), and w is an N x 1 noise vector with PDF A/(O, Cw)
and independent of 9. This data model is termed the Bayesian general linear model. It
differs from the classical general linear model in that 9 is modeled as a random variable
with a Gaussian prior PDF. It will be of interest in deriving Bayesian estimators to
have an explicit expression for the posterior PDF p(9|x). From Theorem 10.2 we know
that if x and 9 are jointly Gaussian, then the posterior PDF is also Gaussian. Hence,
it only remains to verify that this is indeed the case. Let z = [xT 9T]T, so that from
(10.26) we have
where the identity matrices are of dimension N x N (upper right) and p x p (lower
left), and 0 is an N x N matrix of zeros. Since 9 and w are independent of each other
and each one is Gaussian, they are jointly Gaussian. Furthermore, because z is a linear
transformation of a Gaussian vector, it too is Gaussian. Hence. Theorem 10.2 applies
directly, and we need only determine the mean and covariance of the posterior PDF.
We identify x as H9 + w and y as 9 to obtain the means
E(x) = E(H9 + w) = HE(9) = Hp),
E0’) = 5(9) = #0
and covariances
[(X — E(X))(X — E(X))T]
[(110 + w _ 110M110 + w - HWT]
[(H(9 — m) + W)(H(9 — 110+ WV]
= HE [(9 — #009 — 110T] HT + E(WWT)
ncenT + 0,,
recalling that 9 and w are independent. Also, the cross-covariance matrix is
Cy: = E l(Y-E(Y))(X-E(X))Tl
= E [(9 _ #00109 _ #0) + WV]
= E109 - ”'0)(H(0 - MDT]
We can now summarize our results for the Bayesian general linear model.
Theorem 10.3 (Posterior PDF for the Bayesian General Linear Model) If
the observed data x can be modeled as
x = n0 + w (10.27)
where x is an N >< 1 data vector, H is a known N >< p matrix, 9 is a p >< 1 random
vector with prior PDF A/(pg, C9), and w is an N >< 1 noise vector with PDF A/(O, Cw)
and independent of 9, then the posterior PDF p(9]x) is Gaussian with mean
E(9|x) = 11,, + CQHT(HCQHT + Cw)_1(x - 1111,) (10.28)
and covariance
09,, = 0., - C9HT(HCQHT + cwylnc, (10.29)
In contrast to the classical general linear model, H need not be full rank to ensure the
invertibility of HCQHT + Cw. We illustrate the use of these formulas by applying them
to Example 10.1.
Example 10.2 - DC Level in WGN - Gaussian Prior PDF (continued)
Since a:[n] = A + w[n] for n = 0,1,...,N -1 with A ~ N(iiA,af,) and w[n] is WGN
with variance a2 and independent of A, we have the Bayesian general linear model
According to Theorem 10.3, p(A|x) is Gaussian and
5041K) = 11.4 + HZITUUQT + J2I)’I(X - 11M)-
Using Woodbury’s identity (see Appendix 1)
(I+—"11’) a-"i, (10-30)
a
sothat
EAX = [l-A-FQIT I_ 2 (x_l/"A)
2 a
= MHZ-g 1T- 021T (x-lia)
Z HA+ U2 ("E—/"A)
= ,,A+”i»*ﬁ(@_,,,4)_ (10.31)
(Ti-FF
It is interesting to note that in this form the MMSE estimator resembles a “sequential”-
type estimator (see Section 8.7). The estimator with no data or A = 14A 1s corrected
by the error between the data estimator i and the “previous” estimate iiA. The “ga1n
factor” ai/(af, + az/N) depends on our conﬁdence in the previous estimate and the
current data. We will see later that a sequential MMSE estimator can be deﬁned and
will have just these properties. Finally, with some more algebra (10.11) can be obtained.
To ﬁnd the posterior variance we use (10.29), so that
var(A|x) = a: — ailTﬂailT + Uznqlafr
Using (10.30) once again, we obtain
var(A|x) = a2 — gglT I— 2 10f,
a a
which is just (10.12). O
In the succeeding chapters we will make extensive use of the Bayesian linear model.
For future reference we point out that the mean (10.28) and covariance (10.29) of the
posterior PDF can be expressed in alternative forms as (see Problem 10.13)
E(9|x) = ,1, + (c; + nTcjn)" HTc;1(x - B0,) (10.32)
and _1
09,, = (of + nTcjn) (10.33)
0;; = agl + HTCjH. (10.34)
The latter expression is particularly interesting. For the previous example we would
have
This form lends itself to the interpretation that the “information” or reciprocal of the
variance of the prior knowledge 1 / of, and the “information” of the data 1/(02 /N) add
to yield the information embodied in the posterior PDF.
10.7 Nuisance Parameters
Many estimation problems are characterized by a set of unknown parameters, of which
we are really interested only in a subset. The remaining parameters, which serve only
to complicate the problem, are referred to as nuisance parameters. Such would be the
case if for a DC level in WGN, we were interested in estimating a2 but A was unknown.
The DC level A would be the nuisance parameter. If we assume that the parameters
are deterministic, as in the classical estimation approach, then in general we have no
alternative but to estimate 02 and A. In the Bayesian approach we can rid ourselves of
nuisance parameters by “integrating them out.” Suppose the unknown parameters to
be estimated are 9 and some additional nuisance parameters a are present. Then, if
p(9, a|x) denotes the posterior PDF, we can determine the posterior PDF of 9 only as
p(9|X) = /p(9,a1x)da. (10.35)
We can also express this as
where
p(X)9) = /p(x10,a)p(a|0) da. (10.37)
If we furthermore assume that the nuisance parameters are independent of the desired
parameters, then (10.37) reduces to
p(X|9) = /p(x|0, a)p(a) da. (10.33)
We observe that the nuisance parameters are ﬁrst integrated out of the conditional
PDF p(x|9, a) and then the posterior PDF is found as usual by Bayes’ theorem. If a
MMSE estimator is desired, we need only determine the mean of the posterior PDF.
The nuisance parameters no longer enter into the problem. Of course, their presence
will affect the ﬁnal estimator since from (10.38) p(x|9) depends on p(a). From a
theoretical viewpoint, the Bayesian approach does not suffer from the problems of
classical estimators in which nuisance parameters may invalidate an estimator. We
now illustrate the approach with an example.
Example 10.3 - Scaled Covariance Matrix
Assume that we observe the N >< 1 data vector x whose conditional PDF p(x|9, 0'2) is
N (0, 02C(9)). (The reader should not confuse C(9), the scaled covariance matrix of x,
with Cg, the covariance matrix of 9.) The parameter 9 is to be estimated, and a2 is
to be regarded as a nuisance parameter. The covariance matrix depends on 9 in some
unspeciﬁed manner. We assign the prior PDF to a2 of
p(cr2) = U4 a > 0 (10.39)
0 a2 < 0
where /\ > 0, and assume a2 to be independent of 9. The prior PDF is a special case mse(A) lsayesia“)
of the inverted gamma PDF. Then, from (10.38) we have
p<x|v> = /p<»=|@.a2>p<a2>da* M” W”
0o A __i2 --------------- ~-
= f  8X1) [—lXT(U2C(9))_1X:|   (102
0 (21r)2 det’ [02C(9)] 2 U
1 /\exp  1m
f0 (21r)%a~<iew[c(0)] expi 262x (lx v4 a
Letting g = l/az, we have
Figure 10.8 Mean square error of MVU and Bayesian estimators for
deterministic DC level in WGN
p(x|9) _—_ m! f? exp [—— (A + —XTC_1(9)X>  df. A a?‘ _ az/N
(21r) det [C(9)] 0 A U: + Ug/NI + U?‘ + Ug/N/‘A
But = ai+(1—a);iA
o0 m-l __ —m
f0 Z eXM-ax) dz — a mm) and 0 < a < 1. If A is a deterministic parameter, we can evaluate the MSE using (2.6)
for a > 0 and m > 0, resulting from the properties of the gamma integral. Hence, the as
integral can be evaluated to yield mse(A) — val-(A) + b (A)
 — A is the bias. Then,
where b(A)
pme) _ Ar(%+i)
was det? [can] (A +%XTC"1(0)>=) = +‘ mse(A) ' a “f” * l“ + (1 “l” Al
The posterior PDF may be found by substituting this into (10.36) (at least in theory!) _ a N + (1 a) (A m‘) ' (1040)
It is seen that the use of the Bayesian estimator reduces the variance, since 0 < a < 1,
but may substantially increase the bias component of the MSE. As shown further in
Figure 10.8, the Bayesian estimator exhibits less MSE than the MVU estimator i only
if A is close to the prior mean iiA. Otherwise, it is a poorer estimator. It does not have
the desirable property of being uniformly less in MSE than all other estimators but
only “on the average” or in a Bayesian sense. Hence, only if A is random and mse(A)
can thus be considered to be the MSE conditioned on a known value of A, do we obtain
10.8 Bayesian Estimation for Deterministic
Parameters
Although strictly speaking the Bayesian approach can be applied only when 9 is ran-
dom, in practice it is often used for deterministic parameter estimation. By this we
mean that the Bayesian assumptions are made to obtain an estimator, the MMSE es-
Bmse(A) = EA[mse(A)]
timator for example, and then used as if 9 were nonrandom. Such might be the case = 02L + (1 — COZEAKA - I4A)2i
if no MVU estimator existed. For instance, we may not be able to ﬁnd an unbiased pg
estimator that is uniformly better in terms of variance than all others (see Example = Q21 + (1 _ (1)263
2.3). Within the Bayesian framework, however, the MMSE estimator always exists and N
thus provides an estimator which at least “on the average” (as different values of 9 are = f U21
chosen) works well. Of course, for a particular9 it may not perform well, and this is the N 5f‘ + "F:
risk we take by applying it for a deterministic parameter. To illustrate this potential U2
pitfall consider Example 10.1 for which (see (10.11)) < w = Bmse(;i-),
or that the Bayesian MSE is less. In effect, the Bayesian MMSE estimator trades off
bias for variance in an attempt to reduce the overall MSE. In doing so it beneﬁts from
the prior knowledge that A ~ A/(iiA, of‘). This allows it to adjust a so that the overall
MSE, on the average, is less. The large MSEs shown in Figure 10.8 for A not near it,‘
are of no consequence to the choice of a since they occur infrequently. Of course, the
classical estimation approach will not have this advantage, being required to produce
an estimator that has good performance for all A.
A second concern is the choice of the prior PDF. If no prior knowledge is available,
as we assume in classical estimation, then we do not want to apply a Bayesian estimator
based on a highly concentrated prior PDF. Again considering the same example, if A
is deterministic but we apply the Bayesian estimator anyway, we see that
of, + az/N 01+ a2/NuA
There is a bias and it is towards iiA. To alleviate this bias we would need to specify a
prior PDF that is nearly ﬁat, or for this example, by letting of, —> oo. Otherwise, as
already described, the MSE can be quite large for A not near iiA. (Also, note that as
of, —> o0, we have A —> i: and mse(A) —> mse(.i'), so that the two curves in Figure 10.8
become identical.) In general, such a prior PDF is termed a noninformative prior PDF.
The use of a Bayesian estimator for a deterministic parameter is often justiﬁed on the
basis that the noninformative prior PDF does not add any information to the problem.
How the prior PDF is chosen for a given problem is beyond the scope of our discussions
(see Problems 10.15-10.17 for one possible approach). Suffice it to say that there is a
good deal of controversy surrounding this idea. The interested reader should consult
[Box and Tiao 1973] for further details and philosophy.
E(A) =
References
Ash, R., Information Theory, J. Wiley, New York, 1965.
Box, G.E.P., G.C. Tiao, Bayesian Inference in Statistical Analysis, Addison-Wesley, Reading,
Mass“ 1973.
Gallager, R.G., Information Theory and Reliable Communication, J. Wiley, New York, 1968.
Zacks, 3., Parametric Statistical Inference, Pergamon, New York, 1981.
Problems
10.1 In this problem we attempt to apply the Bayesian approach to the estimation of
a deterministic parameter. Since the parameter is deterministic, we assign the
prior PDF 12(0) = 5(0 — 0g), where n90 is the true value. Find the MMSE estimator
for this prior PDF and explain your results.
10.2 Two random variables z and y are said to be conditionally independent of each
other if the joint conditional PDF factors as
where z is the conditioning random variable. We observe z[n] = A + wh] for
nH-f- 3,1, wghere A, w[0], and w[1] are random variables. If A,w[0],w[1] are
a 1n epen ent, prove that .r[0] and a'[1] are conditionally independent. The
conditioning random variable is A. Are a'[0] and a:[1] independent unconditionally
or is it true that
Mwlolwlll) = 17(~Tl0])p(.r[1])?
To answer this consider the case where A,w[0],w[1] are independent and each
random variable has the PDF N (0, 1).
conicfiitiztllzlgigpfor n O,1,...,N 1 are observed, each sample having the
9 z exp l-(wlnl - 0)] m] > 0
P(z[nll ) { 0 m] < 07
and conditioned on 9 the observations are independent. The prior PDF is
Finditihe MlVISE estimator of 0. Note: See Problem 10.2 for the deﬁnition of
conditional independence.
10.4 Repeat Problem 10.3 but with the conditional PDF
P(Il"l|9) = { 9
0 otherwise
and the uniform prior PDF 9 ~ Ll [0, B]. What happens if ﬂ is very large so that
there 1S little prior knowledge?
10.5 Rederive the MMSE estimator by letting
Bmse(a9)
EM [(0 - 0f]
E... { [(0 - meta) + (E(0|x) ~ m2}
and evaluating. Hint: Use the result E, 9() = E, [Email 
10.6 In Example 10.1 modify the data model as follows:
where w[n] is WGN_with variance oi if A Z 0 and of if A < 0. Find the PDF
;D(X|A). Compare this to p(x; A) for the classical case in which A is deterministic
and w[n] is WGN with variance 02 for
a. 0i = a2
b. 0i 7E a2;
10.7 Plot A as given by (10.9) as a function ofi if t/cr2/N = 1 for A0 = 3 and A0 = 10.
Compare your results to the estimator A = :2. Hint: Note that the numerator
can be evaluated in closed form and the denominator is related to the cumulative
distribution function of a Gaussian random variable.
10.8 A random variable 9 has the PDF p(9). It is desired to estimate a realization of
9 without the availability of any data. To do so a MMSE estimator is proposed
that minimizes E[(9 — 9)2], where the expectation is with respect to 12(9) only.
Prove that the MMSE estimator is 9 =  Apply your results to Example 10.1
to show that the minimum Bayesian MSE is reduced when data are incdrporated
into the estimator.
10.9 A quality assurance inspector has the job of monitoring the resistance values
of manufactured resistors. He does so by choosing a resistor from a batch and
measuring its resistance with an ohmmeter. He knows that the ohmmeter is of
poor quality and imparts an error to the measurement which he models as a
N (0, 1) random variable. Hence, he takes N independent measurements. Also,
he knows that the resistors should be 100 ohms. Due to manufacturing tolerances,
however, they generally are in error by c, where e ~ N (0, 0.011). If the inspector
chooses a resistor, how many ohmmeter measurements are necessary to ensure
that a MMSE estimator of the resistance R yields the correct resistance to 0.1
ohms “on the average” or as he continues to choose resistors throughout the day?
How many measurements would he need if he did not have any prior knowledge
about the manufacturing tolerances?
10.10 In this problem we discuss reproducing PDFs. Recall that
where the denominator does not depend on 9. If p(9) is chosen so that when
multiplied by p(x|9) we obtain the same form of PDF in 9, then the posterior
PDF p(9|x) will have the same form as p(9). Such was the case in Example 10.1
for the Gaussian PDF. Now assume that the PDF of  conditioned on 9 is the
exponential PDF
w (lb)
h (n)
5 6 5 6 (ft)
(a) Data for Planet Earth (b) Data for faraway planet
Figure 10.9 Height-weight data
where the z[n]’s are conditionally independent (see Problem 10.2). Next, assume
the gamma prior PDF
Au a-l
{ r(a)9 exp(—/\9) 9> O
where A > 0,o > 0, and ﬁnd the posterior PDF. Compare it to the prior PDF.
Such a PDF, 1n this case the gamma, is termed a conjugate prior PDF.
10.11 It is ‘desired to estimate a person’s weight based on his height. To see if this
1S feasible, data were taken for N = 100 people to generate the ordered pairs
(h, w), where h denotes the height and w the weight. The data that were ob-
tained appear as shown in Figure 10.9a. Explain how you might be able to guess
someonejs weight based on his height using a MMSE estimator. What modeling
assumptions would you have to make about the data? Next, the same experiment
was performed for people on a planet far away. The data obtained are shown in
Figure 10.9b. What would the MMSE estimator of weight be now?
10.12 If [IE 111T ~ N“), C), where
let 9(9) = P($o,y) for some z = x0. Prove that g(y) is maximized for y = pzg.
A150, ShOW that E(y|.z‘0) = pzg. Why are they the same? If p = 0, what is the
MMSE estimator of y based on w?
10.13 Verify (10.32) and (10.33) by using the matrix inversion lemma. Hint: Verify
(10.33) ﬁrst and use it to verify (10.32).
10.14 The data
where r is known, w[n] is WGN with variance 02, and A ~ N (0, of‘) independent
of wln] are observed. Find the MMSE estimator of A as well as the minimum
Bayesian MSE.
10.15 A measure of the randomness of a random variable 9 is its entropy deﬁned as
Hw) = E<- mpw» = - / 1I1P(9)P(9)d9-
If 9 ~ A/(pgmg), ﬁnd the entropy and relate it to the concentration of the PDF.
After observing the data, the entropy of the posterior PDF can be deﬁned as
Hum) = 12(- lnp(9]x)) = - / lnp(9|x) p(X,9) dxd9
and should be less than H Hence, a measure of the information gained by
observing the data is )
I = H(9) — H(9]x).
Prove that I 2 O. Under what conditions will I = O? Hint: Express H (9) as
11(9) =- / lnp(9)p(x,9)dxd9.
You will also need the inequality
[in p2(u)p1(u)du Z 0
for PDFs p1(u), p2(u). Equality holds if and only if p1(u) = p2(u). (This approach
is described more fully in [Zacks 1981] and relies upon the standard concept of
mutual information in information theory [Ash 1965, Gallagher 1968].)
10.16 For Example 10.1 show that the information gained by observing the data is
10.17 In choosing a prior PDF that is noninformative or does not assume any prior
knowledge, the argument is made that we should do so to gain maximum informa-
tion from the data. In this way the data are the principal contributor to our state
of knowledge about the unknown parameter. Using the results of Problem 10.15,
this approach may be implemented by choosing 12(9) for which I is maximum. For
the Gaussian prior PDF for Example 10.1 how should 14A and a3 be chosen so
that p(A) is noninformative?
any
Appendix 10A
Derivation of Conditional Gaussian
In this appendix we derive the conditional PDF of a multivariate Gaussian PDF
as summarized in Theorem 10.2. The reader will note that the result is the vector
extension of that given in Example 10.1. Also, the posterior mean and covariance
expressions are valid even if the covariance matrix C is singular. Such was the case in
Appendix 7C in which x was a linear function of y. The derivation in this appendix,
however, assumes an invertible covariance matrix.
Using the notation of Theorem 10.2, we have
_ wwﬁldetﬂc) 2 y-Ew) y~E(y)
 exp [ﬁe - Ewofrzax - Eom]
That p(x) can be written this way or that x is also multivariate Gaussian with the given
mean and covariance follows from the fact that x and y are jointly Gaussian. To verify
thislet
and apply the property of a Gaussian vector that a linear transformation also produces
a Gaussian vector. We next examine the determinant of the partitioned covariance
matrix. Since the determinant of a partitioned matrix may be evaluated as
dGt     = d€t(A11)d€t(A22 — AglAulAlg)
it follows that i T C_1 0 i
 I det(Cm)det(Cyy — CWCQQCW) = I: ~ C C_1~ ] l: 61 B-l i’ [ _ l j 
and thus d C _ (_y C yC_1_ TBA _ _1_ y C” CIIX
5.9% ___ deqcyy _ CyICI-Ilcxyy - Y y: 11x) (Y - Cyrcza: x)
e“ I’) or ﬁnally
We thus have that
mm = 1 exp (-162) Q = 1y - (E(y) + cwrzxx ~ Eoonf
(ml deti (cm, - cy,,c;;c,,,) [Cw - 0119550111“ [Y — (E(y) + CyJXﬂX — E(X)))] -
where glilaegsean of the posterior PDF is therefore given by (10.24), and the covariance by
x-E(x) T _1 x—E(x) T _1
Q = C ‘(X_E(X)) Crx(x_E(x))'
y — E(y) y — E(y)
To evaluate Q we use the matrix inversion formula for a partitioned symmetfic matrix
[ (An — A12A§21A21Y1 —A1_1IA12(A22 - A21A1_11A12)_1
“(A22 — A21AI1lA12T1A21Af11 (A22 - A21A1_11A12)_1
Using this form, the off-diagonal blocks are transposes of each other so that the inverse
matrix must be symmetric. This is because C is symmetric, and hence C“ is symmetric.
By using the matrix inversion lemma we have
(A11 * AQAEQAnY‘ = A11 + A1_11A12(A22 — A21A1_11A12)_1A21A1_11
so that
where
The inverse can be written in factored form as
C_,= 1 -c;;c,,, 0;; o 1 o
o 1 o B-l -c,,,c;; 1
so that upon letting i = x — E(x) and y = y — E(y) we have
Q _ i T 1 4:30“, 0;; 0 1 o
- 9 o 1 o B-l 4W0; 1
Chapter 11
General Bayesian Estimators
1 1. 1 Introduction
Having introduced the Bayesian approach to parameter estimation in the last chapter,
we now study more general Bayesian estimators and their properties. To do so, the
concept of the Bayesian risk function is discussed. Minimization of this criterion results
in a variety of estimators. The ones that we will concentrate on are the MMSE estimator
and the maximum a posteriori estimator. These two estimators are the principal ones
used in practice. Also, the performance of Bayesian estimators is discussed, leading
to the concept of error ellipses. The use of Bayesian estimators in signal processing
is illustrated by a deconvolution problem. As a special case we consider in detail the
noise ﬁltering problem, in which we use the MMSE estimator in conjunction with the
Bayesian linear model to yield the important Wiener ﬁlter. In the succeeding chapter
we will describe more fully the properties and extensions of the Wiener ﬁlter.
11.2 Summary
The Bayes risk is deﬁned in (11.1). For a quadratic cost function the mean of the
posterior PDF or the usual MMSE estimator minimizes the risk. A proportional cost
function (11.2) results in an optimal estimator which is the median of the posterior PDF.
For a “hit-or-miss” cost function (11.3) the mode or maximum location of the posterior
PDF is the optimal estimator. The latter is termed the maximum a posteriori (MAP)
estimator. The MMSE estimator for a vector parameter is given in (11.10), and the
corresponding minimum Bayesian MSE by (11.12). Some examples of the computation
of the MAP estimator are included in Section 11.5, followed by the deﬁnition for a
vector MAP estimator, (11.23) or (11.24). The vector MAP estimator is not a simple
extension of the scalar MAP estimator but minimizes a slightly different Bayes risk. It is
pointed out that the MMSE and MAP estimators commute over linear transformations,
but this property does not carry over to nonlinear ones. The performance of the MMSE
estimator is characterized by the PDF of its error (11.26). For the Bayesian linear model
this may be determined explicitly from (11.29) and (11.30) and leads to the concept of
9(6) = lil
(a) Quadratic error (b) Absolute error
(c) Hit-or- miss error
Figure 11.1 Examples of cost function
an error ellipse as discussed in Example 11.7. Finally, Theorem 11.1 summarizes the
MMSE estimator and its performance for the important Bayesian linear model.
11.3 Risk Functions
Previously, we had derived the MMSE estimator by minimizing E [(0 — 9f], where the
expectation is with respect to the PDF p(x, a9). If we let e = a9—6l denote the error of the
estimator for a particular realization of x and o9, and also let C(c) = e2, then the MSE
criterion minimizes E The deterministic function C(e) as shown in Figure 11.1a
is termed the cost function. It is noted that large errors are particularly costly. Also,
the average cost or E[C is termed the Bayes risk R or
1a = E[c(6)] (11.1)
and measures the performance of a given estimator. If C (c) = e2, then the cost function
is quadratic and the Bayes risk is just the MSE. Of course, there is no need to restrict
ourselves to quadratic cost functions, although from a mathematical tractibility stand-
point, they are highly desirable. Other possible cost functions are shown in Figures
11.1b and 11.1c. In Figure 11.1b we have
C(e) =  (11.2)
This cost function penalizes errors proportionally. In Figure 11.1c the “hit-or-miss”
cost function is displayed. It assigns no cost for small errors and a cost of 1 for all
errors in excess of a threshold error or
C(e)={ ‘l’   (11.3)
where 5 > 0. If 5 is small, we can think of this cost function as assigning the same
penalty for any error (a “miss”) and no penalty for no error (a “hit”). Note that in all
three cases the cost function is symmetric in e, reﬂecting the implicit assumption that
positive errors are just as bad as negative errors. Of course, in general this need not
be the case.
We already have seen that the Bayes risk is minimized for a quadratic cost function
by the MMSE estimator é = E (Olx). We now determine the optimal estimators for the
other cost functions. The Bayes risk 'R, is
1a = E[c(€)]
= //C(0—§)p(x,0)dxd0
= / [ f C(o9—1il)p(o9|x)dn9 p(x)dx. (11.4)
As we did for the MMSE case in Chapter 10, we will attempt to minimize the inner
integral for each x. By holding x ﬁxed 9 becomes a scalar variable. First, considering
the absolute error cost function, we have for the inner integral of (11.4)
m) f w- élmelxwe
f (0 - 9)p(o9|x) d9 +  (e - é)p(e|x) d0.
—oo 0
To differentiate with respect to 6i we make use of Leibnitz’s rule:
a fiwih<u~i>dv = fmiahfifﬂd" + d"”(“)h<u.¢.<u>>
Letting h(é,a9) =  — 9)p(9|X) for the ﬁrst integral, we have
hotter» = 149.9) = (9 - é>p<é|x> = 0
and d¢1(u)/du = 0 since the lower limit does not depend on u. Similarly, for the second
integral the corresponding terms are zero. Hence, we can differentiate the integrand
only to yield
95g)=Li°p(o|x)de-Lmp(o|x)do=o
[:9 p(o9|x)do9= wpuapqao.
By deﬁnition 6i is the median of the posterior PDF or the point for which Pr{9 g  =
For the “hit-or-miss” cost function we have C (c) = 1 for e > 5 and e < —6 or for
9 > 0 + 5 and 9 < 9 — 6, so that the inner integral in (11.4) is
g(@)=[é_61-p(a9]x)d9 + foo 1-p(9|X) d9.
f p(o9]x) d9 = 1,
yielding _
9w) = -  p(9|><)d9-
This is minimized by maximizing
[é p(t9lx)d9.
For 5 arbitrarily small this is maximized by choosing (i to correspond to the location of
the maximum of p(19|x). The estimator that minimizes the Bayes risk for the “hit-or-
miss” cost function is therefore the mode (location of the maximum) of the posterior
PDF. It is termed the maximum a posteriori (MAP) estimator and will be described
in more detail later.
In summary, the estimators that minimize the Bayes risk for the cost functions of
Figure 11.1 are the mean, median, and mode of the posterior PDF. This is illustrated in
Figure 112a. For some posterior PDFs these three estimators are identical. A notable
example is the Gaussian posterior PDF
PWIX) = —-— EXP l- (9 — a I)2] -
(l27TUglz 26g): 0’
The mean am is identical to the median (due to the symmetry) and the mode, as
illustrated in Figure 11.2b. (See also Problem 11.2.)
11.4 Minimum Mean Square Error Estimators
In Chapter 1O the MMSE estimator was determined to be E(a9|x) or the mean of the
posterior PDF. For this reason it is also commonly referred to as the conditional mean
3,11: ca) = s’
- ‘ JlL cm = |e|
Mode Mean Median
(a) General posterior PDF
P(9|X)
Mean = median = mode
(b) Gaussian posterior PDF
Figure 11.2 Estimators for different cost functions
estimator. We continue our discussion of this important estimator by ﬁrst extending it
to the vector parameter case and then studying some of its properties.
If 9 is a vector parameter of dimension p >< 1, then to estimate 91, for example, we
may view the remaining parameters as nuisance parameters (see Chapter 10). If p(x|9)
is the conditional PDF of the data and 10(9) the P1101‘ PDF of the Vector Para-meter» We
may obtain the posterior PDF for 191 as
memo = f  /p(9l1<)d92-~d9p (115)
where
_1l("ll’(_‘9l__ (11.6)
Then, by the same reasoning as in Chapter 10 W8 have
(>1 = Ewan
= faiPwllxl 4'91
or in general
91 = #1110111) 11a 1= 1,2, . . . ,p. (11.7)
This is the MMSE estimator that minimizes
EH91 - 902] = [(91 — 91)’P(X.91)d1<d91 (11.8)
or the squared error when averaged with respect to the marginal PDF p(x 19-) Thus
the MMSE estimator for a vector parameter does not entail anything newlbiit only a
need to determine the posterior PDF for each parameter. Alternatively we can express
the MMSE estimator for the ﬁrst parameter from (11.5) as l ’
a=~ﬁmmna
/0, [/---/p(9|x)d192...d9p] 1191
/91p(9|x) d9
or in general
éi=f9ip(9lx)d9 i=1,2,...,p.
In vector form we have
[91p(9|x)d9
é = /921>(9l1=)d9
feppwpqde
= [9P(9l1‘)d9 (11.9)
= EWIX) (11.10)
where the expectation is with respect to the posterior PDF of the vector parameter
0r p(9|x). Note that the vector MMSE estimator E(9|x) minimizes the MSE for each
component of the unknown vector parameter, or 9i = E 9 . ' ' ' E ._ ”. 2
This follows from the 11611111111011. [ 1 [ ( ‘m’ minimizes K0? a’) l‘
As discussed in Chapter 10, the minimum Bayesian MSE for a scalar parameter is
the posterior PDF variance when averaged over the PDF of x (see (10.13)). This is
because
Bmse(91) =  —é1)2l I  - 91)2p(x,91) d91dx
and since 91 = E(91|x), we have
f var(91|x)p(x) dx.
Bmse(91)
Now, however, the posterior PDF can be written as
p(91|x) = d92...d9p
so that
Bmse(91) = f U (a, —E(91|x))2p(9|x)d9 ((1)1111. (11.11)
The inner integral in (11.11) is the variance of 91 for the posterior PDF p(9|x). This
is just the [1,1] element of Cm, the covariance matrix of the posterior PDF. Hence, in
general we have that the minimum Bayesian MSE is
Bmse(9,-) = /[C9|,],-ip(x)dx i = 1,2,...,p (11.12)
where
09,, = E9), [(0 - E(9|x))(9 _ E(9|x))T]. (11.13)
An example follows.
Example 11.1 - Bayesian Fourier Analysis
We reconsider Example 4.2, but to simplify the calculations we let M = 1 so that our
data model becomes
z[n]=acos2irfgn+bsin2irfgn+wlnl n=O,1,...,N—~1
where f0 is a multiple of 1/N, excepting O or 1/2 (for which sin 211' fan is identically
zero), and wln] is WGN with variance a2. It is desired to estimate 9 = [a blT. We
depart from the classical model by assuming a, b are random variables with prior PDF
9 ~ A/(O, 0Z1)
and 9 is independent of  This type of model is referred to as a Rayleigh fad-
ing sinusoid [Van Trees 1968] and is frequently used to represent a sinusoid that has
propagated through a dispersive medium (see also Problem 11.6). To ﬁnd the MMSE
estimator we need to evaluate E(9 The data model is rewritten as
where
H z cos 21rf0 sin 21rf0
cos[21rf0(N — 1)] sin[21rf0(N — 1)]
which is recognized as the Bayesian linear model. From Theorem 10.3 we can obtain
the mean as well as the covariance of the posterior PDF. To do so we let
to obtain
é = E(9|x) = agnﬂnagrﬂ + 021)“):
Cm, = 0Z1 — agHqHagHT + U2I)_1H0’;.
A somewhat more convenient form is given by (10.32) and (10.33) as
é = E(9|x)=(-15I+HTi2H) nTlzx
06 U a
09,, — (EI+HTFH)
Now, because the columns of H are orthogonal (due to the choice of frequency), we
have (see Example 4.2)
and
(a)? + 202 ) a2
or the MMSE estimator is
a 1 [2 M [l 2 r
The results differ from the classical case only in the scale factor, and if 0g >> 202/N,
the two results are identical. This corresponds to little prior knowledge compared to
the data knowledge. The posterior covariance matrix is
which does not depend on x. Hence, from (11.12)
Bmse(a) = 1 1
Bmse(b) = 1 1
It is interesting to note that in the absence of prior knowledge in the Bayesian linear
model the MMSE estimator yields the same form as the MVU estimator for the classical
linear model. Many fortuitous circumstances enter into making this so, as described in
Problem 11.7. To verify this result note that from (10.32)
9 = Ewlx) = it + (C? + HTC.;‘H>"HTC;‘(=< — Hm)-
For no prior knowledge Cf —-> 0, and therefore,
é -> (nTo;1n)-1HTc;1x (11.14)
which is recognized as the MVU estimator for the general linear model (see Chapter 4).
The MMSE estimator has several useful properties that will be exploited in our study
of Kalman ﬁlters in Chapter 13 (see also Problems 11.8 and 11.9). First, it commutes
over linear (actually affine) transformations. Assume that we wish to estimate a for
a=A9+b (11.15)
where A is a known r >< p matrix and b is a known r >< 1 vector. Then, a is a random
vector for which the MMSE estimator is
d = E (alx).
Because of the linearity of the expectation operator
a = E(A9+b{x)
= AE(9|x)+b
= Aé+b. (11.16)
This holds regardless of the joint PDF p(x,6). A second important property focuses
on the MMSE estimator based on two data vectors xhxg. We assume that 9, x1, x; are
jointly Gaussian and the data vectors are independent. The MMSE estimator is
é I E(9|X1,X2).
Letting x = [xlT xglT, we have from Theorem 10.2
é = E(6|x) = 5(a) + 09.0; (x - Em). (11.11)
Since xhxg are independent,
Ix Cacgzl C1221;
and also
It follows from (11.17) that
E(6)+[ Co“ can 1 [c.7611 C91 ] [xl-Eﬁii) ]
I212 X2 — E(X2)
= E(9) + C@1.C;.‘..(X1 — E(X1)) + 001205.11. (X2 — E(X2))~
We may interpret the estimator as composed of the prior estimator E (6) as well as that
due to the independent data sets. The MMSE is seen to have an additivity property for
independent data sets. This result is useful in deriving the sequential MMSE estimator
(see Chapter 13).
Finally, in the jointly Gaussian case the MMSE is linear in the data, as can be seen
from (11.17). This will allow us to easily determine the PDF of the error, as described
in Section 11.6.
11.5 Maximum A Posteriori Estimators
In the MAP estimation approach we choose 9 to maximize the posterior PDF or
9 = arg mlaxpwlx).
This was shown to minimize the Bayes risk for a “hit-or-miss” cost function. In ﬁnding
the maximum of 12(91):) we observe that
_ P(XI9)P(9)
so an equivalent maximization is of p(xl0)p(0). This is reminiscent of the MLE except
for the presence of the prior PDF. Hence, the MAP estimator is
0 = arg mgxp(x|0)p(6) (11.18)
or, equivalently, A
0 = arg méax [lnp(x|6) + 1.. p(6)]. (11.19)
Before extending the MAP estimator to the vector case we give some examples.
Example 11.2 - Exponential PDF
Assume that 0 _ eexp(*ez[n]) mm] > 0
Penn)-{O ﬂm<0
where the 2:[n]’s are conditionally IID, or
P(Xl9) = II p(w["l|9)
and the prior PDF is
/\exp(--/\0) 6 > 0
9(9)={0 0<0.
Then, the MAP estimator is found by maximizing
9(9) = 111P(X|9) + 111119)
ln [ON exp <—0NZ_:1 
= Nln0—N0a':+ln/\—/\6
+ In [/\ exp(—/\0)]
for 0 > 0. Differentiating with respect to 6 produces
99(9) _ N _
and setting it equal to zero yields the MAP estimator
a=_Q.
(l; + w
p( ) Figure 11.3 Domination of prior
PDF by conditional PDF in MAP
estimator
Note that as N —+ 0o, d —+ 1/i. Also, recall that  = 1/0 (see Example 9.2),
so that 1
conﬁrming the reasonableness of the MAP estimator. Also, if /\ —> 0 so that, the prior
PDF is nearly uniform, we obtain the estimator 1 / i. In fact, this is the Bayesian MLE
(the estimator obtained by maximizing p(x|0)) since as /\ —> 0 we have the situation in
Figure 11. 3 in which the conditional PDF dominates the prior PDF. The maximum of
g is then unaffected by the prior PDF. <>
Example 11.3 - DC Level in WGN - Uniform Prior PDF
Recall the introductory example in Chapter 10. There we discussed the MMSE esti-
mator of A for a DC level in WGN with a uniform prior PDF. The MMSE estimator
as given by (10.9) could not be obtained in explicit forrn due to the need to evaluate
the integrals. The posterior PDF was given as
In determining the MAP estimator of A we ﬁrst note that the denominator or normal-
izing factor does not depend on A. This observation formed the basis for (11.18). The
posterior PDF is shown in Figure 11.4 for various values of i. The dashed curves indi-
cate the portion of the original Gaussian PDF that was truncated to yield the posterior
PDF. It should be evident from the ﬁgure that the maximum is i if  5 A0, A0 if
i > A0, and —A0 if i < —A0. Thus, the MAP estimator is explicitly given as
A = z —A0 g 2 5 A0 ' (11.20)
(C) i < —A|3
Figure 11.4 Determination of MAP estimator
The “classical estimator” i is used unless it falls outside the known constraint interval,
in which case we discard the data and rely only on the prior knowledge. Note that the
MAP estimator is usually easier to determine since it does not involve any integration
but only a maximization. Since p(6) is the prior PDF, which we must specify, and
p(x|6) is the PDF of the data, which we also assume known in formulating our data
model, we see that integration is not required. In the case of the MMSE estimator we
saw that integration was a necessary part of the estimator implementation. Of course,
the required maximization is not without its potential problems, as evidenced by the
difficulties encountered in determining the MLE (see Section 7.7). <>
To extend the MAP estimator to the vector parameter case in which the posterior PDF Pwzl“)
is now p(0 Ix), we again employ the result
1110,11) =/---/p(o1x)de,...de,,. (11.21)
Then, the MAP estimator is given by
91 = are HyXPUhIX)
or in general
0A, = argniiaxpﬂﬁlx) i: 1,2, . . . ,p. (11.22)
This estimator minimizes the average “hit-or-miss” cost function
n, = new,» - 9.)]
for each i, where the expectation is over p(x, 0,).
One of the advantages of the MAP estimator for a scalar parameter is that to
numerically determine it we need only maximize p(x|0)p(0). No integration is required.
This desirable property of the MAP estimator for a scalar parameter does not carry
over to the vector parameter case due to the need to obtain p(6,-|x) as per (11.21).
However, we might propose the following vector MAP estimator
0 = arg mgxp(6[x) (11.23)
in which the posterior PDF for the vector parameter 9 is maximized to ﬁnd the es-
timator. Now we no longer need to determine the marginal PDFs, eliminating the
integration steps, since, equivalently,
0 = arg mgxp(x|9)p(9) (11.24)
That this estimator is not in general the same as (11.22) is illustrated by the example
shown in Figure 11.5. Note that p(01,02|x) is constant and equal to 1 / 6 on the rectangu-
lar regions shown (single cross-hatched) and equal to 1/3 on the square region (double
cross-hatched). It is clear from (11.23) that the vector MAP estimator, although not
unique, is any value of 0 within the square so that 0 < (i; < 1. The MAP estimator of
02 is found as the value maximizing p(02|x). But
1102s) = j 1>(91,9zl1¢)d91
(milk? C071
(b) Posterior PDF p(9z|x)
(a) Posterior PDF p(91,9z|x)
Figure 11.5 Comparison of scalar and vector MAP estimators
and is shown in Figure 11.5b. Clearly, the MAP estimator is any value 1 < 02 < 2,
which diﬁers from the vector MAP estimator. It can be shown, however, that the vector
MAP estimator does indeed minimize a Bayes risk, although a diﬁerent one than the
“hit-or-miss” cost function (see Problem 11.11). We will henceforth refer to the vector
MAP estimator as just the MAP estimator.
Example 11.4 - DC Level in WGN - Unknown Variance
We observe
In contrast to the usual example in which only A is to be estimated, we now assume the
variance 02 of the WGN w[n] is also unknown. The vector parameter is 9 = [A 021T.
We assume the conditional PDF
110414.112) =  (T; _0 (1111 - Ar]
for the data, so that conditioned on knowing 6, we have a DC level in WGN. The prior
PDF is assumed to be
1104,02) = 11(Alv2)1>(v2)
1 1 g ’\exp(—ai2)
z ﬁexpi-QQUAA-‘mi <14 '
The prior PDF of A conditioned on a2 is the usual A/(pA, 0,24) PDF except that 0?, =
0102. The prior PDF for a2 is the same one used in Example 10.3 and is a special case
of the inverted gamma PDF. Note that A and a2 are not independent. This prior PDF
is the conjugate prior PDF (see Problem 10.10). To ﬁnd the MAP estimator we need
to maximize
904.112) = 11(X|A,v2)1>(14,v2)
" 1104A, U2)11(AI<Y2)P(U2)
over A and a2. To ﬁnd the value of A that maximizes g we can equivalently maximize Therefore, we must maximize over a2
just p(x|A,a2)p(A|a2). But in Example 10.1 we showed that 1 1 N_1 1 Aexpbﬁéi)
h(A) = POM, UZHKAIUQ) 9M1” l * (2,,,2)%./‘*2,Ta,2 exp l 202 "=0 I [n] 202 a4
1 1 N“ 2 1 c a
= (Qwdng 21rd; exp h? 1g) I [n] exp l—§Q(A)l _ (cﬁi? exp ( v2)
where 1 #124, #2 where c is a constant independent of g2 and
Q( ) Ui|x( IIAI) aiyz+ai a=%Z:I2[n]+5£+/\_
and n:
$12 _ + 5% Diﬁerentiating ln g produces A
llAlw = 1+3; 6lng(A,a2) _-1Y'2ﬂ +1
17%|; = N 1 ) and setting it to zero yields
Maximizing h(A) over A is equivalent to minimizing Q(A), so that U2 : LV-gﬁ a
o2 ‘l’ i}; 2  2 1
and since of, = 0402, we have —_- i- 2 z2[n] + IL — A2 (N + —) + 2/\
A a Ni + e N + 5  <1 a
_ _ 2 __ _ —_— i- —zz2[n]—A2 +iﬁ+m (/1?i~A2)+-iN+5-
Since this does not depend on 0 nit is the MAP estimator. We next ﬁnd the MAP N + 5 N "=0 ( o‘
estimator for a2 by maximizing g(A, a2) = h(A)p(02). First, note that Th MAP tmator is therefore
e es i
= a x ——— n ex —— ~
M) 1 e 1 {:11 1 l 1cm] A
(21ra2)% 2x01 p 2172 “L; p 2 9 = r2
a
where N5. + H_A
‘ I12  a
Lettingcr§=acr2,wehave N+5 ﬁézln] A +(N+5)a(’uA A)+N+5
Q( = iii _ A?  _|_  As expected, if N —> oo so that the conditional or data PDF dominates the prior PDF,
“U2 U2 “U2 we have
a2 a a U2 _, _ Z$2[n]_5-2=_ Z(:z[n]—:c)2
which is the Bayesian MLE. Recall that the Bayesian MLE is the value of 9 that
maximizes p(x|9). O
It is true in general that as N —> oo, the MAP estimator becomes the Bayesian
MLE. To ﬁnd the MAP estimator we must maximize p(x|9)p(9). If the prior p(9) is
uniform over the range of 9 for which p(x|9) is essentially nonzero (as will be the case
if N —> oo so that the data PDF dominates the prior PDF), then the maximization
will be equivalent to a maximization of p(x|9). If p(x|9) has the same form as the PDF
family p(x; 9) (as it does for the previous example), then the Bayesian MLE and the
classical MLE will have the same form. The reader is cautioned, however, that the
estimators are inherently diﬁerent due to the contrasting underlying experiments.
Some properties of the MAP estimator are of importance. First, if the posterior
PDF is Gaussian, as in the Bayesian linear model for example, the mode or peak
location is identical to the mean. Hence, the MAP estimator is identical to the MMSE
estimator if X and 9 are jointly Gaussian. Second, the invariance property encountered
in maximum likelihood theory does not hold for the MAP estimator. The next example
illustrates this. i
Example 11.5 - Exponential PDF
Assume in Example 11.2 that we wish to estimate a = 1/9. It might be supposed that
the MAP estimator is
where 9 is the MAP estimator of 9, so that
a-a: N. ( . )
We now show that this is not true. As before
P<IIn1w> ={ 3e""“"‘l"l) ZlZl Z3.
The conditional PDF based on observing a is
p(w[nll<1)= ( iexp  “l” > °
since knowing a is equivalent to knowing 9. The prior PDF
pm) ___{ 3exp(—}\9) 
cannot be transformed to p(a) by just letting 9 = 1 / a. This is because 9 is a random
variable, not a deterministic parameter. The PDF of a must account for the derivative
of the transformation or
P (901))
Pal“) = liza-
/\exp(—A/a) a > 0
0 a a < 0.
Otherwise, the prior PDF would not integrate to 1. Completing the problem
9(a) = lnzﬂxla) +1I1P(<1)
1 N 1”“ /\eXP(—/\/Q)
= ln  exp C-Egzhﬂ) +ln ——-—i-——a2
= _N1na—N£ +ln)\— — —2lna
a _ a
= —(N+2)lna — Nxa-i-A +1I1/\-
Diﬁerentiating with respect to a
da a a
and setting it equal to zero yields the MAP estimator
which is not the same as (11.25). Hence, it is seen that that MAP estimator does.not
commute over nonlinear transformations, although it does so for linear transformations
(see Problem 11.12).
(ﬁ:
11.6 Performance Description
In the classical estimation problem we were interested in the mean. and variance of an
estimator. Assuming the estimator was Gaussian, we could then immediately obtain
the PDF. If the PDF was concentrated about the true value of the parameter, then
we could say that the estimator performed well. In the case of a random parameter
the same approach cannot be used. Now the randomness of the parameter results in a
different PDF of the estimator for each realization of 9. We denote this PDF by p(9l9).
To perform well the estimate should be close to 9 for every possible value of 9 or the
error .
e = 9 — 9
p<é|e>
/ pane»
Figure 11.6 Dependence of estimation error on realization
of parameter
should be small. This is illustrated in Figure 11.6 where several conditional PDFs are
shown. Each PDF corresponds to that obtained for a given realization of 0. For 0
to be a good estimator p(0|0) should be concentrated about 0 for all possible 0. This
reasoning led to the MMSE estimator, which minimizes E,,@[(0 —  For am arbitrary
Bayesian estimator it thus makes sense to assess the performance by determining the
PDF of the error. This PDF, which now accounts for the randomness of 0, should be
concentrated about zero. For the MMSE estimator 0 = E(0|x), the error is
c = 0 — E(0|x)
and the mean of the error is
E,‘9(e) = EN; [0 — E(0|x)]
= E, [E(0|x) — E(0|x)] = 0
so that the error of the MMSE estimator is on the average (with respect to p(x, 0))
zero. The variance of the error for the MMSE estimator is
VHF) = 510(3)
since E(e) = 0, and therefore
var(e) = EM [(0 _ 9V]
which is just the minimum Bayesian MSE. Finally, if e is Gaussian, we have that
c ~ A/(O, Bmse(0)). (11.26)
Example 11.6 - DC Level in WGN - Gaussian Prior PDF
Referring to Example 10.1, we have that
and
Bmse(A) =
In this case the error is e = A —  Because A depends linearly on x, and x and A are
jointly Gaussian, e is Gaussian. As a result, we have from (11.26)
As N ——+ oo, the PDF collapses about zero and the estimator can be said to be consistent
in the Bayesian sense. Consistency means that for a large enough data record A will
always be close to the realization of A, regardless of the realization value (see also
Problem 11.10). <>
For a vector parameter 0 the error can be deﬁned as
e = 0 — 0A.
As before, if 0 is the MMSE estimator, then c will have a zero mean. Its covariance
matrix is
E,_,,(¢J) = EM [(0 - E(0|x)) (a - 12(a|x))’"] .
The expectation is with respect to the PDF p(x, 0 . Note that
iEielxllizEieiixl = jel-pwtode
and depends on x only. Hence, E(0|x) is a function of x, as it should be for a valid
estimator. As in the scalar parameter case, the diagonal elements of the covariance
matrix of the error represent the minimum Bayesian MSE since the [i,  element is
[E,_,,(eJ)],, = // [0, —E(0,~|x)]2p(x,0)dxd0.
Integrating with respect to 01, . . . ,0,<_1,0,+1, . . . , 0,, produces
[E,,,,(¢¢T)],, = // [0,- —E(0,-|x)]2p(x,0,-)dxd0,
which is the minimum Bayesian MSE for 0;. Hence, the diagonal elements of the
error covariance matrix E,_9(eeT) are just the minimum Bayesian MSEs. The error
covariance matrix is sometimes called the Bayesian mean square error matrix and is
termed M9. To determine the PDF of e we now derive the entire covariance matrix.
We specialize our discussion to the Bayesian linear model (see Theorem 10.3). Then,
= Em... [w - E<v|x>> <6 - Ewwof]
= Eﬂcop)
where Cm, is the covariance matrix of the posterior PDF p(0|x). If 6 and x are jointly
Gaussian, then from (10.25)
M, = C99 - 00.03019 (11.27)
since C9|,, does not depend on x. Specializing to the Bayesian linear model, we have
from (10.29) (shortening the notation from C99 to Cg)
M, = 0,, - C9HT(HC9HT + cwrlncg (11.28)
or from (10.33)
M§=(C,,'1+HTC;1H)_1. (11.29)
Finally, it should be observed that for the Bayesian linear model e = 9 —- é is Gaussian
since from (10.28)
e = 9-0
6 — p9 — C9HT(HC@HT + Cw)_1(x — Hpe)
and thus e is a linear transformation of x,9, which are themselves jointly Gaussian.
Hence, the error vector for the MMSE estimator of the parameters of a Bayesian linear
model is characterized by
e~./\/(0,Mé) (11.30)
where M9 is given by (11.28) or by (11.29). An example follows.
Example 11.7 - Bayesian Fourier Analysis (continued)
We now again consider Example 11.1. Recall that
Hence, from (11.29)
(a) Contours of constant probability density (b) Error “ellipses” for a3 = 1, dz/N : 1 / 2
Figure 11.7 Error ellipses for Bayesian Fourier analysis
M9 = (c;1+HTc;‘H)“
(0511102)
(034-202)
Therefore, the error e = [ca 65V has the PDF
6 ~ A/(Ov MBA)
where 1
$112- + ZaZI/N
M9‘ = a 1
The error components are seen to be independent and, furthermore,
Bmse(¢“z) = [Méhl
in agreement with the results of Example 11.1. Also, for this example the PDF of the
error vector has contours of constant probability density that are circular, as shown in
Figure 11.7a. One way to summarize the performance of the estimator is by the error
0r concentration ellipse (in this case it is a circle). It is in general an ellipse within
which the error vector will lie with probability P. Let
eTMglc = é’. (11.31)
Then, the probability P that e will lie within the ellipse described by (11.31) is
P = Pr{eTMé_le g c2}.
But u = eTMgle is a X3 random variable with PDF (see Problem 11.13)
—exp(——) u>0
so that 3
P = Pr{eTMg1egc2}
A 2exp( 2) u
= 1—exp 
eTMgle = 2ln <1 P)
describes the error ellipse for a probability P. The error vector will lie within this ellipse
with probability P. An example is shown in Figure 11.7b for 0g = 1, az/N = 1 / 2 so
that 1
Mela l
eTe=ln<1_P).
In general, the contours will be elliptical if the minimum MSE matrix is not a scaled
identity matrix (see also Problems 11.14 and 11.15). <>
and thus
We now summarize our results in a theorem.
Theorem 11.1 (Performance of the MMSE Estimator for the Bayesian Lin-
ear Model) If the observed data x can be modeled by the Bayesian linear model of
Theorem 10.3, the MMSE estimator is
é = ,1, + 0,1? (HCQHT + Cu’)_1 (x - 11s,) (11.32)
= u, +(c;1+ 11Tc;111)“HTc,;'(x - 11s,). (11.33)
The performance 0f the estimator is measured by the error e = 9 — 6, whose PDF is
Gaussian with mean zero and covariance matriz
C6 = E1,9(66T)
= Cg-CQHT (HC0HT+C,,,)—1HC@ (11.34)
= (c;1+HTc;1H)“. (11.35)
The error covariance matrix is also the minimum MSE matrix M5, whose diagonal
elements yield the minimum Bayesian MSE 0r
[Mélii : [Celii
= Bmse(0A,-). (11.36)
See Problems 11.16 and 11.17 for an application to line ﬁtting.
11.7 Signal Processing Example
There are many signal processing problems that ﬁt the Bayesian linear model. \(Ve
mention but a few. Consider a communication problem in which we transmit a signal
s(t) through a channel with impulse response h(t). At the channel output we observe a
noise corrupted waveform as shown in Figure 11.8. The problem is to estimate s(t) over
the interval 0 g t g T, Because the channel will distort and lengthen the signal, we
observe :c(t) over the longer interval 0 g t g T. Such a problem is sometimes referred
to as a deconvolution problem [Haykin 1991]. We wish to deconvolve s(t) from a noise
corrupted version of s,,(t) = s(t) *h(t), where ir denotes convolution. This problem also
arises in seismic processing in which s(t) represents a series of acoustic reﬂections of an
explosively generated signal due to rock inhomogeneities [Robinson and Treitel 1980].
The ﬁlter impulse response h(t) models the earth medium. In image processing the same
problem arises for two-dimensional signals in which the two-dimensional version of s(t)
represents an image, while the two-dimensional version of h(t) models a distortion due
to a poor lens, for example [Jain 1989]. To make any progress in this problem we
assume that h(t) is known. Otherwise, the problem is one of blind deconvolution, which
is much more difficult [Haykin 1991]. We further assume that s(t) is a realization of a
random process. This modeling is appropriate for speech, as an example, for which the
signal changes with speaker and content. Hence, the Bayesian assumption appears to
be a reasonable one. The observed continuous-time data are
:c(t) = 0 h(t — r)s('r) dr + w(t)
0 g t < T. (11.37)
It is assumed that s(t) is nonzero over the interval [0,T,], and h(t) is nonzero over the
interval [0, Th], so that the observation interval is chosen to be [0, T], where T = T, +T;,.
We observe the output signal s,,(t) embedded in noise w(t). In converting the problem
(a) System model
s(t)
(b) Typical signals
a:(t)
(c) Typical data waveform
Figure 11.8 Generic deconvolution problem
to discrete time we will assume that s(t) is essentially bandlimited to B Hz so that the
output signal so(t) is also bandlimited to B Hz. The continuous-time noise is assumed
to be a zero mean WSS Gaussian random process with PSD
Pww(F) =
We sample :c(t) at times t = nA, where A = 1/ (2B), to produce the equivalent discrete-
time data (see Appendix 11A)
where :c[n] = z(nA), hln] = Ah(nA), s[n] = s(nA), and w[n] = w(nA). The discrete-
time signal s[n] is nonzero over the interval [0,n, — 1], and h[n] is nonzero over the
interval [0, m, — l]. The largest integers less than or equal to Ts/A and Th/A are
denoted by n, — 1 and m, — 1, respectively. Therefore, N = n, + m, — 1. Finally, it
follows that w[n] is WGN with variance 02 = NOB. This is because the ACF of w(t) is
a sinc function and the samples of w(t) correspond to the zeros of the sinc function (see
Example 3.13). If we assume that s(t) is a Gaussian process, then s[n] is a discrete-time
Gaussian process. In vector-matrix notation we have
 l (11.38)
h[O] 0 0 $[0] w[0]
h[1] mo] o Sll] w[l]
hpvé 1] h[Na— 2] .1. h[N L 11,] stt; 1] wpv! 1]
Note that in H the elements h[n] = 0 for n > m, — 1. This is exactly in the form of
the Bayesian linear model with p = n5. Also, H is lower triangular due to the causal
nature of h(t) and thus h[n]. We need only specify the mean and covariance of 6 = s
to complete the Bayesian linear model description. If the signal is zero mean (as in
speech, for example), we can assume the prior PDF
s ~ N(0, Cs).
An explicit form for the covariance matrix can be found by assuming the signal is WSS,
at least over a short time interval (as in speech, for example). Then,
[Celia : Twli - 
where rss[k] is the ACF. If the PSD is known, then the ACF may be found and therefore
C, speciﬁed. As a result, the MMSE estimator of s is from (11.32)
s = C,HT (HC,HT + U21)“ x. (11.39)
An interesting special case occurs when H = I, where I is the n5 >< n, identity matrix.
In this case the channel impulse response is h[n] = 6h], so that the Bayesian linear
model becomes
In eﬁect the channel is transparent. Note that in the classical linear model we always
assumed that H was N >< p with N > p. This was important from a practical viewpoint
since for H = I the MVU estimator is
é = (HTH)_1HTX
In this case there is no averaging because we are estimating as many parameters as
observations. In the Bayesian case we can let N = p or even N < p since we have
prior knowledge about 9. If N < p, (11.39) still exists, while in classical estimation,
the MVU estimator does not. We now assume that H = I. The MMSE estimator of s
~ a
s = 04c, + 020-11 (11.40)
Note that the estimator may be written as s = Ax, where A is an n5 >< n8 matrix. The
A matrix is called the Wiener ﬁlter. We will have more to say about Wiener ﬁlters in
Chapter 12. As an example, for the scalar case in which we estimate s[0] based on z[0]
the Wiener ﬁlter becomes
a0] 1p]
where 1; = rss[0]/cr2 is the SNR. For a high SNR we have  z z[0], while for a low
SNR §[0] m: 0. A more interesting example can be obtained by assuming that s[n] is a
realization of an AR(1) process or
s[n] = —-a[l]s[n — 1] + u[n]
where u[n] is WGN with variance 0i and a[l] is the ﬁlter coefﬁcient. The ACF for this
process can be shown to be (see Appendix l)
rm =  (—a[1])"“‘
and the PSD is
For a[1] < 0 the PSD is that of a low-pass process, an example of which is shown in
Figure 11.9 for a[1] = —0.95 and 0E = l. For the same AR parameters a realization of
s[n] is shown in Figure ll.l0a as the dashed curve. When WGN is added to yield an
Power spectral density (dB)
s[n] (dashed)
s[n] (dashed)
Frequency
Figure 11.9 Power spectral density of AR( 1) process
(a) Signal and data
Sample number, n
8T (b) Signal and signal estimate
-1° a riv- 1—~i——r —1—: 1—-ri
0 5 10 15 2o 25 so 35 4o 4s so
Sample number, n
Figure 11.10 Wiener ﬁltering example
SNR of 5 dB, the data zln] become the solid curve shown in Figure 11.10a. The Wiener
ﬁlter given by (11.40) will smooth the noise ﬂuctuations, as shown in Figure 1l.10b by
the solid curve. However, the price paid is that the signal will suffer some smoothing
as well. This is a typical tradeoH. Finally, as might be expected, it can be shown that
the Wiener ﬁlter acts as a low-pass ﬁlter (see Problem 11.18).
References
Haykin, S., Adaptive Filter Theory, Prentice-Hall. Englewood Cliffs, N..l., 1991.
Jain, A.K., Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs, N..]., 1989.
Robinson, E.A, S. Treitel, Geophysical Signal Analysis. Prentice-Hall, Englewood Cliffs, N..]., 1980.
Van Trees, H.L., Detection, Estimation, and Modulation Theory, Part l, J. Wiley, New York, 1968.
Problems
11.1 The data :c[n], n = 0,1,. .. ,N — 1 are observed having the PDF
p(w[n]I/»)=  ~§<1in1w>2 -
The z[n]’s are independent when conditioned on p. The mean [1 has the prior
Find the MMSE and MAP estimators of p. What happens as a3 -—+ 0 and 0g —+
o0?
11.2 For the posterior PDF
p(9|:c) = é exp l-éw - If] + b}: exp l-éw + If] .
Plot the PDF for c = 1/2 and e = 3/4. Next, ﬁnd the MMSE and MAP estimators
for the same values of c.
11.3 For the posterior PDF
ﬁnd the MMSE and MAP estimators.
11.4 The data :c[n] = A + w[n] for n = 0,1,...,N — 1 are observed. The unknown
parameter A is assumed to have the prior PDF
MA) ={ 3exp(—/\A) i
where /\ > 0, and w[n] is WGN with variance a2 and is independent of A. Find
the MAP estimator of A.
11.5 If 9 and x are jointly Gaussian so that
l Z l ~/\/(#.C)
where
E(6)
ﬁnd the MMSE estimator of 9 based on x. Also, determine the minimum Bayesian
MSE for each component of 9. Explain what happens if C9, = 0.
11.6 Consider the data model in Example 11.1 for a single sinusoid in WGN. Rewrite
the model as
z[n] = Acos(21rf0n + (b) + w[n]
where
A = Va2+b2
¢ = arctan — .
a
If 9 = [a b]T ~ A/(O, 0Z1), show that the PDF of A is Rayleigh, the PDF of ¢ is
U{0, 211'], and that A and ¢ are independent.
11.7 In the classical general linear model the MVU estimator is also efficient and
therefore the maximum likelihood method produces it or the MVU estimator is
found by maximizing
iE-Li exp ——(x — H9)TC'1(x — H9) .
(21r) 1 det’ (C) 2
For the Bayesian linear model the MMSE estimator is identical to the MAP
estimator. Argue why in the absence of prior information the MMSE estimator
for the Bayesian linear model is identical in form to the MVU estimator for the
classical general linear model.
11.8 Consider a parameter 9 which changes with time according to the deterministic
relation
where A is a known p >< p invertible matrix and 9[0] is an unknown parameter
which is modeled as a random vector. Note that once 9[0] is speciﬁed, so is 9[n]
for n Z 1. Prove that the MMSE estimator of Hln] is
where  is the MMSE estimator of 9[O] or, equivalently, i 11.14 If c is a 2 >< 1 random vector and e ~ N (0, M6‘), plot the error ellipse for P = 0.9
- - for the following cases:
a M~ — 1 0
11.9 A vehicle starts at an unknown position (z[O],y[0]) and moves at a constant ' 9 _ 0 1
velocity according to 1 0
where 11,, Uy are the velocity components in the a: and y directions, respectively, 0 1 2
and are both unknown. It is desired to estimate the position‘ of the vehicle for 1115 As a special Case of the Bayesian linear model assume that
each time n, as well as the velocity. Although the initial position (1:[0],y[0]) and
velocity [11, Uy]T are unknown, they can be modeled by a random vector. Show x z g + w
that 0[n] =  y[n] u, vylT can be estimated by the MMSE estimator
61”] z Aném] where 6 is a 2 >< 1 random vector and
where 9[0] is the MMSE estimator of  y[0] u, 12y]? Also, determine A. w 2 0
a
11.10 Show that the MA? estimator in Example 11.2 is consistent in the Bayesian C9 z i ()1 U; i '
sense or as N —> oo, 0 —+ 6 for any realization 0 of the unknown parameter. Give
an argument why this may be true in general. Hint: See Example 9.4. Plot the error ellipse if of = a? and 0g > of. What happens if 0g >> of?
11-11 Consider the VBCtOI‘ MAP eﬁimatol" 01‘ 11.16 In ﬁtting a line through experimental data we assume the model
9=a‘gm5“1’(9lx)' z[n]-—-A+Bn+w[n] —1VI§n§M
Show that this estimator minimizes the Bayes risk for the cost function Where wln] is WGN with variance 02. If We have some prior knowledge of the
c _ 1  > 5 slope B and intercept A such as
l‘) - 0 ueu < 6
where¢=@-@.u¢||2= f=.@i’.and6—>0~ B M’ B,  0 0%]
11.12 Show that the MAP estimator of
ﬁnd the MMSE estimator of A and B as well as the minimum Bayesian MSE.
a = A6 Assume that A, B are independent of w[n]. Which parameter will beneﬁt most
is from the prior knowledge?
d z A9 11.17 If s[n] = A+Bn for n = —M,—(M-—1),...,M and 121,55 are the MMSE estima-
where a is a p X 1 vector and A is an invertible p X p matrix. In other words, the tors for A and B, respectively, ﬁnd the MMSE estimator é of 5 = [5[- 1L1 ] 5[-( M _
MAP estimator commutes over invertible linear transformations. 1)] _ , _ t-[M Next, ﬁnd the PDF of the error e = s _§ if the data model is that
11.13 Ifx is a 2 X 1 random vector with PDF given in Problem 11.16. How does the error vary with n and why?
x N N“) C) 11.18 In this problem we derive the noncausal Wiener ﬁlter for estimation of a signal
l i in WGN. We do so by extending the results in Section 11.7. There the MMSE
prove that the PDF of xTC‘1x is that of a X; random variable. Hint: Note that estimator of the signal was given by (11.40). First, show that
C“ may be factored as DTD, where D is a whitening transformation asldescribed
in Section 4.5. (C, + ﬁns = Csx.
Next. assume the signal is WSS, so that
[C5 + 0211i]- = nth — 11+ <126[i—a'] = mm — j]
[Cain Kali ~ Jl-
As a result, the equations to be solved for s can be written as
for i = 0,1,... ,N — 1. Next, extend this result to the case where s[n] is to be
estimated for |n| g M based on z[n] for |n| g M, and let 1V1 —+ oo to obtain
for —oo < i < oo. Using Fourier transforms, ﬁnd the frequency response of the
Wiener ﬁlter H(f) or A
5 (f)
H(f)=m-
(You may assume that the Fourier transforms of :c[n] and §[n] exist.) Explain
your results.
Appendix 11A
Conversion of
Continuous-Time System
to Discrete-Time System
A continuous-time signal s(t) is the input to a linear time invariant system with
impulse response h(t). The output s,,(t) is
s,,(t) = f” h(t—7‘)$(7‘) d1.
We assume that s(t) is bandlimited to B Hz. The output signal must therefore also
be bandlimited to B Hz and hence we can assume that the frequency response of the
system or .F{h(t)} is also bandlimited to B Hz. As a result, the original system shown in
Figure 11A.la may be replaced by the one shown in Figure 11A.1b, where the operator
shown in the dashed box is a sampler and low-pass ﬁlter. The sampling function is
p(t) = i: 6(t — nA)
and the low-pass ﬁlter has the frequency response
H‘P‘(F)=f 0 ||F|'>B.
For bandlimited signals with bandwidth B the system within the dashed box does
not change the signal. This is because the sampling operation creates replicas of the
spectrum of s(t), while the low-pass ﬁlter retains only the original spectrum scaled by
A to cancel the l/A factor introduced by sampling. Next, note that the ﬁrst low-pass
ﬁlter can be combined with the system since h(t) is also assumed to be bandlimited to
B Hz. The result is shown in Figure 11A.lc. To reconstruct s,,(t) we need only the
376 APPENDIX 11A. CONTINUOUS-TIME TO DISCRETE-TIME APPENDIX 11A CONTINUOUSJTIME To DISCRETETIME 377
samples s,,(nA). Referring to Figures 1l.11b and 11.1lc, we have
m) -—-> 500) s,,(t)
loo Ah(t — T) ﬁ s(mA)6(T — mA) dT
(a) Original continuous-time system : A ﬁ sUnA) loo h“ _ T)6(T _ mA) dT
2 s(nA)6(t—nA) Z Sa(TlA)6(f— VIA) z A i smmhu _mA)
so that the output samples are
so(nA) = i: Ah(nA — mA)s(mA)
or letting s,,[n] = so(nA), h[n] = Ah(nA), and s[n] = s(nA), this becomes
(b) Conversion to sampled data system 50h] = Z: hln _ 
Z Slnlm - "Al E 50mm: — "M The continuous-time output signal is obtained by passing the signal
Z s,,[n]6(t — nA)
through the low-pass ﬁlter as shown in Figure 11A.1c. In any practical system we
will sample the continuous-time signal s,,(t) to produce our data set s,,[n]. Hence, we
may replace the continuous-time data so(t) by the discrete-time data s,,[n], as shown
in Figure 1lA.1d, without a loss of information. Note ﬁnally that in reality s(t) cannot
be bandlimited since it is time-limited. We generally assume, and it is borne out in
practice, that the signal is approximately bandlimited. The loss of information is usually
negligible.
(c) Sampled data system
(d) Discrete-time system
Figure 11A.1 Conversion of continuousetime system to discrete-time system
Chapter 12
Linear Bayesian Estimators
* 12. 1 Introduction
The optimal Bayesian estimators discussed in the previous chapter are difficult to deter-
mine in closed form, and in practice too computationally intensive to implement. They
involve multidimensional integration for the MMSE estimator and multidimensional
maximization for the MAP estimator. Although under the jointly Gaussian assump-
tion these estimators are easily found, in general, they are not. When we are unable to
make the Gaussian assumption, another approach must be used. To ﬁll this gap we can
choose to retain the MMSE criterion but constrain the estimator to be linear. Then,
an explicit form for the estimator may be determined which depends only on the ﬁrst
two moments of the PDF. In many ways this approach is analogous to the BLUE in
classical estimation, and some parallels will become apparent. In practice, this class of
estimators, which are generically termed Wiener ﬁlters, are extensively utilized.
12.2 Summary
The linear estimator is deﬁned by (12.1), and the corresponding Bayesian MSE by
(12.2). Minimizing the Bayesian MSE results in the linear MMSE (LMMSE) estimator
of (12.6) and the minimum Bayesian MSE of (12.8). The estimator may also be derived
using a vector space viewpoint as described in Section 12.4. This approach leads to
the important orthogonality principle which says that the error of the linear MMSE
estimator must be uncorrelated with the data. The vector LMMSE estimator is given
by (12.20), and the minimum Bayesian MSE by (12.21) and (12.22). The estimator
commutes over linear transformations (12.23) and has an additivity property (12.24).
For a data vector having the Bayesian linear model form (the Bayesian linear model
without the Gaussian assumption), the LMMSE estimator and its performance are
summarized in Theorem 12.1. If desired, the estimator for the Bayesian linear model
form can be implemented sequentially in time using (12.47 )—(12.49).
a
a
12.3 Linear MMSE Estimation
We differentiate this with respect to a0 and a1 and set the results equal to zero to
We begin our discussion by assuming a scalar parameter 9 is to be estimated based produce
on the data set {z[0],a:[l], . . . ,:c[N — 1]} or in vector form x =  z[1] . ..z[N —1]]T. E [(9 _ aozioi _ aﬂzion z 0
The unknown parameter is modeled as the realization of a random variable. We do not E 6 _
assume any speciﬁc form for the joint PDF p(x, 9), but as we shall see shortly, only a ( _ aOIlO] _ a1) 0
knowledge of the ﬁrst two moments. That 9 may be estimated from x is due to the Q1‘
assumed statistical dependence of 9 on x as summarized by the joint PDF p(x, 9), and
in particular, for a linear estimator we rely on the correlation between 9 and x. We
now consider the class of all linear (actually affine) estimators of the form
aQE(z2[O]) + a1E(z[0]) = E(9z[0])
a0E(z[0]) + a1 = 
9 ¢ IVZ-Ilanzln] + a1v (12.1) But Ehlol) I 0 and Ewzlol) z E(z3[0]) = o, so that
"=0 (IQ = 0
and choose the weighting coefficients a"’s to minimize the Bayesian MSE a1 ( ) (I  a
Therefore, the LMMSE estimator is 9 = a2 and does not depend on the data. This is
Bmse(9) = E [(9 —  (12.2) because 9 and a:[0] are uncorrelated. The minimum MSE is
Bmse(9) = E [(9 — 
= E [(9 — 02m
= E(z4[O]) —— 202E(z2[0]) + a4
'~ 304 — 204 + a4
where the expectation is with respect to the PDF p(x,9). The resultant estimator is
termed the linear minimum mean square error (LMMSE) estimator. Note that we have
included the a N coefficient to allow for nonzero means of x and 9. If the means are
both zero, then this coefficient may be omitted, as will be shown later.
Before determining the LMMSE estimator we should keep in mind that the estima-
tor will be suboptimal unless the MMSE estimator happens to be linear. Such would
be the case, for example, if the Bayesian linear model applied (see Section 10.6). Other-
wise, better estimators will exist, although they will be nonlinear (see the introductory
example in Section 10.3). Since the LMMSE estimator relies on the correlation between as Opposed to a minimum MSE of zero for the nonlinear estimator 6 : 12in] Clearly
random variables, a parameter uncorrelated with the data cannot be linearly estimated. the LMMSE estimator is inappropriate for this problem Problem 12 1 ex iores now
Consequently, the proposed approach is not always feasible. This is illustrated by the to modify the LMMsE estimator to make it applicable I i p
following example. Consider a parameter 9 to be estimated based on the single data We now derive the O t-m i ~ ht- fﬁ - t f - 12 l . .
sample :c[0], where :c[0] ~ N (0, a2). If the parameter to be estimated is the power of (12.1) into (12.2) and diﬁpeirlenfialzliieilgg mg we men s or use m ( i )' Substmmng
the 1:[0] realization or 9 = z2[0], then a perfect estimator will be
(9 ~ Z anz[n] — a”) i = -2E [9 — Z anz[n] — aNi.
since the minimum Bayesian MSE will be zero. This estimator is clearly nonlinear. If,
however, we attempt to use a LMMSE estimator or Setting this equal to zero produces
9 = a0z[0] + a1, aN = E(9) _ 2 angkgini) (123)
then the optimal weighting coefficients a0 and a1 can be found by minimizing which as as rt d i. . .f tn "-0 C
se e ear ier is zero 1 e means are zero. ontinuing, we need to minimize
Bmse(9) = E [(9 —  _ A N_1 2
= E [(9 _ Geno] _ a1)2]_ Bmse(9) = E { [Z 11.41411] - E(a:[n])) - (a - men] }
over the remaining a,,’s, where a N has been replaced by (12.3). Letting
a = [a0 a1. ..aN_1]T, we have
3.1158(6) = E { [aT(x - E(x)) - (a _ 12(e))]’}
= E [aT(x — E(x))(x — E(x))Ta] — E [aT(x —— E(x))(0 — 
- E [<6 - Eunxx - Ecofa] + E [w — E(9))’l
= aTCma — aTCrg — Cara + C99 (12.4)
where C“ is the N >< N covariance matrix of x, and C9,, is the 1 >< N cross-covariance
vector having the property that 0,3", = C1,, and C99 is the variance of 0. Making use
of (4.3) we can minimize (12.4) by taking the gradient to yield
8Bmse(é)
6a = 20113 —- 2019
which when set to zero results in
a = 0,30,, (12.5)
Using (12.3) and (12.5) in (12.1) produces
é = aTx + aN
or ﬁnally the LMMSE estimator is
é= 15(6) + chem ~ Eur»- (116)
Note that it is identical in form to the MMSE estimator for jointly Gaussian x and 0, as
can be veriﬁed from (10.24). This is because in the Gaussian case the MMSE estimator
happens to be linear, and hence our constraint is automatically satisﬁed. If the means
of 6 and x are zero, then A
6 = CQICQQX. (12.7)
The minimum Bayesian MSE is obtained by substituting (12.5) into (12.4) to yield
Bmse(0A) = 01,0; 0,,,0;;0,, - 01,0; 0.,
or ﬁnally
Bmse(é) = C99 — C9¢C;:1C$9. (12.8)
Again this is identical to that obtained by substituting (10.25) into (11.12). Anexample
Example 12.1 - DC Level in WGN with Uniform Prior PDF
Consider the introductory example in Chapter 10. The data model is
where A ~ U[—AO, A0], w[n] is WGN with variance a2, and A and w[n] are independent.
We wish to estimate A. The MMSE estimator cannot be obtained in closed form due
to the integration required (see (10.9)). Applying the LMMSE estimator, we ﬁrst note
that E(A) = 0, and hence  = 0. Since E(x) = 0, the covariances are
C“ = E(xxT)
E [(A1 + w)(A1 + W)T]
E(A2)11T + 021
0,, = E(AxT)
= E [A(A1 + W)T]
= E(A’)1T
where 1 is an N X 1 vector of all ones. Hence, from (12.7)
— 0i1T(ai11T+02I)-1x
where we have let oi = E (A2). But the form of the estimator is identical to that
encountered in Example 10.2 if we let aA = 0, so that from (10.31)
Since 0?, = E(A2) = (2AQ)2/12 = Ag/3, the LMMSE estimator of A is
A= 3 :2. (12.9)
As opposed to the original MMSE estimator which required integration, we have ob-
tained the LMMSE estimator in closed form. Also, note that we did not really need to
know that A was uniformly distributed but only its mean and variance, or that w[n]
was Gaussian but only that it is white and its variance. Likewise, independence of
A and W was not required, only that they were uncorrelated. In general, all that is
required to determine the LMMSE estimator are the ﬁrst two moments of p(x, 6) or
Ew) [ Coo Co: ]
E (x) ’ C10 C2: i
However, we must realize that the LMMSE of (12.9) will be suboptimal since it has been
constrained to be linear. The optimal estimator for this problem is given by (10.9).
1-[1] Figure 12.1 Vector space inter-
Ilol pretation of random variables
12.4 Geometrical Interpretations
In Chapter 8 we discussed a geometrical interpretation of the LSE based on the concept
of a vector space. The LMMSE estimator admits a similar interpretation, although now
the “vectors” are random variables. (Actually, both vector spaces are special cases of the
more general Hilbert space [Luenberger 1969].) This alternative formulation assumes
that 6 and x are zero mean. If they are not, we can always deﬁne the zero mean random
wariables 0' = 9 — E(0) and x’ = x — E (x), and consider estimation of 0’ by a linear
function of x’ (see also Problem 12.5). Now we wish to ﬁnd the an’s so that
0 = 2 anz[n]
minimizes A A
Bmse(0) = E [(0 - or] .
Let us now think of the random variables 0,:1:[O],:c[1],...,:c[N — l] as elements in a
vector space as shown symbolically in Figure 12.1. The reader may wish to verify that
the properties of a vector space are satisﬁed, such as vector addition and multiplication
by a scalar, etc. Since, as is usually the case, 0 cannot be perfectly expressed as a linear
combination of the 1:[n]’s (if it could, then our estimator would be perfect), we picture
0 as only partially lying in the subspace spanned by the 2:[n]’s. We may deﬁne the
“length” of each vector a: as  = \/E(z2) or the square root of the variance. Longer
length vectors are those with larger variances. The zero length vector is the random
variable with zero variance or, therefore, the one that is identically zero (actually not
random at all). Finally, to complete our description we require the notion of an inner
product between two vectors. (Recall that if x,y are Euclidean vectors in R3, then
the inner product is (x,y) = xTy =   cosa, where a is the angle between the
vectors.) It can be shown that an appropriate deﬁnition, i.e., one that satisﬁes the
properties 0f an inner product between the vectors a: and y, is (see Problem 12.4)
(z, y) =  (12.10)
With this deﬁnition we have that
(1,1) = E(I2) = IIIIIZ, (12-11)
Figure 12.2 Orthogonal random
variables——y cannot be linearly es-
timated based on z
consistent with our earlier deﬁnition of the length of a vector. Also, we can now deﬁne
two vectors to be orthogonal if
(m1) = Bey) = 0- (1112)
Since the vectors are zero mean, this is equivalent to saying that two vectors are orthoy-
onal if and only if they are uncorrelated. (In R3 two Euclidean vectors are orthogonal if
the angle between them is a = 90°, so that (x, y) =   cosa = 0.) Recalling our
discussions from the previous section, this implies that if two vectors are orthogonal, we
cannot use one to estimate the other. As shown in Figure 12.2, since there is no com-
ponent of y along z, we cannot use a: to estimate y. Attempting to do so means that we
need to ﬁnd an a so that y = az minimizes the Bayesian MSE, Bmse(y) = E[(y — y)2].
To ﬁnd the optimal value of a
_ —2E(:1:y) + 2aE(z2)
which yields
a _ E(Iy) _ 0
E(z2) i
The LMMSE estimator of y is just y = 0. Of course, this is a special case of (12.7)
where N = 1,0 = y, z[O] = I, and C01 = 0-
With these ideas in mind we proceed to determine the LMMSE estimator using the
vector space viewpoint. This approach is usefulfor conceptualization of the LIMMSE
estimation process and will be used later to derive the sequential LMMSE estimator.
As before, we assume that
é = Z auxin]
where a N = 0 due to the zero mean assumption. We wish to estimate 6 as a linear
combination of :c[0], 1:[l], . . . , z[N —- l]. The weighting coefﬁcients an should be chosen
(a) (b)
Figure 12.3 Orthogonality principle for LMMSE estimation
to minimize the MSE ,
(0 — 2-: anz[n]) 5|
n0 — NZ? anzln] .
But this means that minimization of the MSE is equivalent to a minimization of the
squared length of the error vector e = 0 -—  The error vector is shown in Figure 12.3b
for several candidate estimates. Clearly, the length of the error vector is minimized
when e is orthogonal to the subspace spanned by {:c[0],z{1], . . . ,z[N —  Hence, we
require
E[(0 - of] E
c _L z[0],1:[1],... ,z[N - 1] (12.13)
or by using our deﬁnition of orthogonality
E[(0-é)z[n]] =0 n=0,1,...,N—1. (12.14)
This is the important orthogonality principle or projection theorem. It says that in
estimating the realization of a random variable by a linear combination of data samples,
the optimal estimator is obtained when the error is orthogonal to each data sample.
Using the orthogonality principle, the weighting coefficients are easily found as
E[<0—1§amz[m])z[n]]=0 n=0,1,...,N—1
if amE(:z:[m]a:[n]) = E(0z{n]) n = 0, 1, . . . , N — 1.
In matrix form this is
E(z2[O])   E(z[0]:c[N — 1]) a0
 E(:c2[1])  E(z[l]z[N — 1]) a,
Emu l 111401) Emu l 11141;) ... Ewpv - 1]) a1\i_1
= i ii l) (12.15)
E(e@[1v _ 1])
These are the normal equations. The matrix is recognized as C", and the right-hand
vector as C19. Therefore,
and
a = CQQCIQ. (12.17)
The LMMSE estimator of 0 is
0 aTx
CZeCI-mlx
or ﬁnally A
9 = 0610;; (12.18)
in agreement with (12.7). The minimum Bayesian MSE is the squared length of the
error vector or
Bmse(0)
for the a,,’s given by (12.17). But
Bmse(0) = E K0 — NZ-l anz[n]) (0 — lg am2:[m])]
= E [<0 — Z anz[n]) 0] —— E [(0 - Z anz[n]) Z- am:c[m]]
: E(02) — Z anE(z[n]0) — Z amE K0 — 2 a,,1:[n])  .
(a) (b)
Figure 12.4 Linear estimation by orthogonal vectors
he last term 1S zero due to the orthogonality principle, producing
Bmseﬂi) = C00 — aTCzg v
in agreement with (12.8).
in ahiaélgfoirllgpzrcigaéit results kCHJII be easily derived when we view the LMMSE estimator
p amewor . n Section 12.6 we examine its application to sequential
estimation. As a simple example of its utilit ' ' ' ' -
_ y in conceptualizin th t t -
we provide the following illustration. g e es lma Ion problem’
ExamPle 12-2 - Estimation by Orthogonal Vectors
thtlylgl; eazghzrglsceﬁlliglltlege?iglliinlgrld ulrlicotirrelatedhwith each other. However,
- . a i us rates t is case. The LMMSE esti.
2.12.2112; ..;::.:‘;;?;;1‘2J. I11 1S   of   of 1 on Iii  1w as
( M14011! Hwlﬂlll +( 111.411“) Flum-
ac component ‘S the length of the 1110196151011 (9,  times the unit vector
in the d‘ t' . ' ' _ .
13in] 119C 1°11 Equlvalently» 511196 llllnlll — \/ V&r(z[n]) 1S a constant, this may
be written
(eixloi) z (0,111))
(zwizwn m] WERE“
By the deﬁnition of the inner product (12.10) we have
(9 E(9Il0l)
Ewan)
[E(0¢[0]) E(6¢[1])l W”) Eglm, 
Clearly, the ease with which this result was obtained is due to the orthogonality of
z[0], :c[1] or, equivalently, the diagonal nature of C“. For nonorthogonal data samples
the same approach can be used if we ﬁrst orthogonalize the samples or replace the data
by uncorrelated samples that span the same subspace. We will have more to say about
this approach in Section 12.6. <>
12.5 The Vector LMMSE Estimator
The vector LMMSE estimator is a straightforward extension of the scalar one. Now we
wish to ﬁnd the linear estimator that minimizes the Bayesian MSE for each element.
We assume that
0A1; z Z amzln] ‘l’ aiN 
for i = 1, 2, . . . , p and choose the weighting coefﬁcients to minimize
Bmse(é,-) = E [(0,- _ éif] i = 1, 2, . . . ,p
where the expectation is with respect to p(x,6,-). Since we are actually determining p
separate estimators, the scalar solution can be applied, and we obtain from (12.6)
0i = E(0,~) + C9,,C;;(x — E(x)) i = 1,2,. . . ,p
and the minimum Bayesian MSE is from (12.8)
Bmse(é,~) = Cgg-g‘. — CgﬂCr-IICIQ, i = 1,2,... ,p.
The scalar LMMSE estimators can be combined into a vector estimator as
E091) 9014751101 - E (11))
» E092) 902x951‘ (X — E (11))
E(9p) CIHICL: (x " 
E(61) C9,,
E(01) C92,,
= 5(9) + CQZCZHX — 5(1)) (1220)
where now C9, is a p >< N matrix. By a similar approach we ﬁnd that the Bayesian
MSE matrix is (see Problem 12.7)
M); = 5 [(0 - 0)(0 - 0)T
= C99 ~ C91C;I1C1Q (12.21)
where C99 is the p >< p covariance matrix. Consequently, the minimum Bayesian MSE
is (see Problem 12.7)
Bmse(6,~) = [Mé],,. (12.22)
Of course, these results are identical to those for the MMSE estimator in the Gaussian
case, for which the estimator is linear. Note that to determine the LMMSE estimator
we require only the ﬁrst two moments of the PDF.
Two properties of the LMMSE estimator are particularly useful. The ﬁrst one states
that the LMMSE estimator commutes over linear (actually aﬁine) transformations.
This is to say that if
a = A6 + b
then the LMMSE estimator of a is
a = A6 + b (12.23)
with 6 given by (12.20). The second property states that the LMMSE estimator of a
sum of unknown parameters is the sum of the individual estimators. Speciﬁcally, if we
wish to estimate a = 61 + 62, then
a = 0, + 02 (12.24)
where
Q1 = Ewi) + 09.10;.‘ (x — Eon)
92 =  + CQWC; (X - 
The proof of these properties is left to the reader as an exercise (see Problem 12.8).
In analogy with the BLUE there is a corresponding Gauss-Markov theorem for
the Bayesian case. It asserts that for data having the Bayesian linear model form,
i.e., the Bayesian linear model without the Gaussian assumption, an optimal linear
estimator exists. Optimality is measured by the Bayesian MSE. The theorem is just the
application of our LMMSE estimator t0 the Bayesian linear model. More speciﬁcally,
the data are assumed to be
where 6 is a random vector to be estimated and has mean E(6) and covariance C99, H
is a known observation matrix, w is a random vector with zero mean and covariance Cw,
and 6 and w are uncorrelated. Then, the LMMSE estimator of 6 is given by (12.20),
where
5(1) = 115(0)
(See Section 10.6 for details.) This is summarized by the Bayesian Gauss-Markov
theorem.
Theorem 12.1 (Bayesian Gauss-Markov Theorem) If the data are described by
the Bayesian linear model form
x = H6 + W (12.25)
where x is an N >< 1 data vector, H is a known N >< p observation matrix, 6 is a p >< 1
random vector of parameters whose realization is to be estimated and has mean E(6)
and covariance matrix C99, and w is an N >< 1 random vector with zero mean and
covariance matrix C“, and is uncorrelated with 6 (the joint PDF p(w, 6) is otherwise
arbitrary), then the LMMSE estimator of 6 is
0 = 5(0) + c).,11T(11c,@11T + cw)—1(x _ 115(0)) (12.20)
5(0) + (Cg; + 11Tc;‘11)-111Tc;1(x - 115(0)). (12.27)
The performance of the estimator is measured by the error e = 6 — 6 whose mean is
zero and whose covariance matrix is
C. = E,_.,9(eeT)
= C69 - c,,,11T(11c,,,,11T + cwylncg, (12.28)
= (Cg; + 11Tc;111)-1. (12.29)
The error covariance matrix is also the minimum MSE matrix M9 whose diagonal
elements yield the minimum Bayesian MSE
lMéln = lceln" A
= Bmse(6,~). (12.30)
These results are identical to those in Theorem 11.1 for the Bayesian linear model except
that the error vector is not necessarily Gaussian. An example of the determination of
this estimator and its minimum Bayesian MSE has already been given in Section 10.6.
The Bayesian Gauss-Markov theorem states that within the class of linear estimators
the one that minimizes the Bayesian MSE for each element of 6 is given by (12.26)
or (12.27). It will not be optimal unless the conditional expectation E(6|x) happens
to be linear. Such was the case for the jointly Gaussian PDF. Although suboptimal,
the LMMSE estimator is in practice quite useful, being available in closed form and
depending only on the means and covariances.
12.6 Sequential LMMSE Estimation
In Chapter 8 we discussed the sequential LS procedure as the process of updating in
time the LSE estimator as new data become available. An analogous procedure can
be used for LMMSE estimators. That this is possible follows from the vector space
viewpoint. In Example 12.2 the LMMSE estimator was obtained by adding to the
old estimate 0A0, that based on ar]0], the estimate 61, that based on the new datum
 This was possible because z[0] and z[1] were orthogonal. When they are not, the
algebra becomes more tedious, although the approach is similar. Before stating the
general results we will illustrate the basic operations involved by considering the DC
level in white noise, which has the Bayesian linear model form. The derivation will be
purely algebraic. Then we will repeat the derivation but appeal to the vector space
approach. The reason for doing so will be to “lay the groundwork” for the Kalman
ﬁlter in Chapter 13, which will be derived using the same approach. We will assume a
zero mean for the DC level A, so that the vector space approach is applicable.
To begin the algebraic derivation we have from Example 10.1 with 11A t 0 (see
(10.11))
where A]N — 1] denotes the LMMSE estimator based on {:r]0], z[1], . . . , z[N —  This
is because the LMMSE estimator is identical in form to the MMSE estimator for the
Gaussian case. Also, we note that from (10.14)
Bmse(A]N —- 1]) = Tag-xii. (12.31)
To update our estimator as z[N] becomes available
M] = e3 01$. 71%? L7)”
_ (N  +02% ("Zzozhﬂ +z[]\(l)
z (N  + v2 0321f: AW _ 11+ (N 1353+ 02mm]
= (Nliafiéai 02A[N _ 1+ (N + $33, +e2zlNl
= AlN 11+ i(N]j-Uf);,ia2 _1)A[N _ 11+ (N + 133E, +02z[N]
= Aw - 11+ "3 (@1111 - Aw - 11> <12 32>
Similar to the sequential LS estimator we correct the old estimate A[N — 1] by a scaled
version of the prediction error z[N] — A]N — 1]. The scaling or gain factor is from
(12.32) and (12.31)
(N + 1)0§', + 02
Bmse(A[N — 1])
and decreases to zero as N —> oo, reﬂecting the increased conﬁdence in the old estimator.
We can also update the minimum Bayesian MSE since from (12.31)
(N +1)0f,+ 02
(N +1)0f,+ 02 N03, + 02
= (1 — K[N])Bmse(A[N — 
Bmse(A[N
Summarizing our results we have the following sequential LMMSE estimator.
Estimator Update:
Am = Apv - 1] + K[N](z[N] - Auv - 1]) (12.34)
where
K[N] =  . (12.35)
Bmse(A[N - 1]) + 02
Minimum MSE Update:
Bmse(A[N]) = (1 _ K[N])Bmse(A[N - 1]). (12.36)
We now use the vector space viewpoint to derive the same results. (The general sequen-
tial LMMSE estimator is derived in Appendix 12A using the vector space approach. It
is a straightforward extension of the following derivation.) Assume that the LMMSE
estimator  is to be found, which is based on the data {a:[0],  as shown in Fig-
ure 12.5a. Because w[0] and z[1] are not orthogonal, we cannot simply add the estimate
based on z[0] to the estimate based on  If we did, we would be adding in an extra
component along  However, we can form  as the sum of  and a component
orthogonal to  as shown in Figure 12.5b. That component is AA[1]. To ﬁnd a vector
in the direction of this component recall that the LMMSE estimator has the property
that the error is orthogonal to the data. Hence, if we ﬁnd the LMMSE estimator of
z[1] based on z[0], call it :2[1]0], the error z[1] — £[1]0] will be orthogonal to  This
Figure 12.5 Sequential estimation using vector space approach
is shown in Figure 12.5c. Since z[0] and z[1] — a":[1|0] are orthogonal, we can project A
onto each vector separately and add the results, so that
To ﬁnd the correction term AA[1] ﬁrst note that the LMMSE estimator of a random
variable y based on w, where both are zero mean, is from (12.7)
g: Elzylw. (12.37)
Thus, we have that
and the error vector :E[1] = z[1] — :E[110] represents the new information that z[1]
contributes to the estimation of A. As such, it is called the innovation. The projection
of A along this error vector is the desired correction
E(Ai[1])a":[1]
If we let K[1] = E(A:i:[1])/E(i2[1]), we have for the LMMSE estimator
A]1] = A]0] + K]1](@]1] - @]1|o]).
To evaluate a“:[1|O] we note that z[1] = A + w[l]. Hence, from the additive property
(12.24) i[1|0] = A[0] +}D[1|0]. Since w[1] is uncorrelated with w[O], we have 1I2[1|0] = O.
Finally, then a":[1|O] = A[0], and A[0] is found as
All] = AlOl + K l1l(wl1l — AlOD-
It remains only to determine the gain K [l] for the correction. Since
= w]1]-U3A”A*_*U2w]o]
the gain becomes
which agrees with (12.33) for N = 1. We can continue this procedure to ﬁnd Al2], A[3],
 In general, we
1. ﬁnd the LMMSE estimator of A based on z[0], yielding 
2. ﬁnd the LMMSE estimator of z[1] based on z[0], yielding .w‘:[1|0]
3. determine the innovation of the new datum z[1], yielding z[1] — i[1|0]
4. add to  the LMMSE estimator of A based on the innovation, yielding 
5. continue the process.
In essence we are generating a set of uncorrelated or orthogonal random variables,
namely, the innovations {z[O], z[1] —ai[1|0], z[2]—i‘[2]O, 1], . .  This procedure is termed
a Gram-Schmidt orthogonalization (see Problem 12.10). To ﬁnd the LMMSE estimator
of A based 0n {z[0], r[1], . . . ,z]N — 1]} we simply add the individual estimators to yield
Aw - 1] = Z K[n] (z[n] - @]n]o, 1, . . .,n - 1]) (12.39)
where each gain factor is
KM z E [(1411] - a140, 1, . . . ,1) - 11f]
, (12.40)
This simple form is due to the uncorrelated property of the innovations. In sequential
form (12.39) becomes
Aw] = Aw - 1] + K[N](z[N] _ 11w]0, 1, . . . ,N -1]). (12.41)
To complete the derivation we must ﬁnd .i[N]O, 1, . . . , N — 1] and the gain K[N]. We
ﬁrst use the additive property (12.24). Since A and w[N] are uncorrelated and zero
mean, the LMMSE estimator of z[N] = A + w[N] is
But A[N]O, 1, . . . , N — 1] is by deﬁnition the LMMSE estimator of the realization of A
at time N based on {z[0],z[1], . . . ,z[N — 1]} or A[N — 1] (recalling that A does not
change over the observation interval). Also, 1D[N|0, 1, . . . , N — 1] is zero since w[N] is
uncorrelated with the previous data samples since A and w[n] are uncorrelated and
w[n] is white noise. Thus,
aw]0, 1, . . . , N - 1] = Aw - 1]. (12.42)
To ﬁnd the gain we use (12.40) and (12.42):
E ]A(z[N] - Aw - 1])]
E](w[N1 - 411v - 1112]
E]A(.~1.-]1v] - Aw - 1])] = E ](A - Aw - 1])(@]1v] - Aw - 1])] A (12.43)
since z[N] — A[N — 1] is the innovation of z[N] which is orthogonal to {z[0], r[1], . . . ,
z[N — 1]} and hence to A[N — 1] (a linear combination of these data samples). Also,
E [w[N ](A — A[N — 1])] = 0 as explained previously, so that
E ]A(x[N] - Aw - 1])] = E ](A - Aw - 1])’]
= Bmse(A[N - 1]). (12.44)
E ](w[N] ~ Aw - 11V] E ](w[N] + A - Aw - 1])2]
E(w2[N]) + E [(21 - Aw -1])2]
o2 + Bmse(A[N - 1] ) (12.45)
so that ﬁnally
KW] =  . (12.46)
The minimum MSE update can be obtained as
Bmse(A[N]) = E](A—A[N])’]
E ](A - Aw - 1] - Kw](@]1v] - Aw -1]))2]
E ](A - Aw -1])2] - 2K[N]E [(11 ~ Aw —1])(z[N]— Aw - 1])]
+ K’w]E [ow] - Aw - 1])2]
Bmse(A[N - 1]) - 2K[N]Bmse(A[N - 1])
+ K2[N] (o? + Bmse(A[N - 1]))
where we have used (12.43)—(12.45). Using (12.46), we have
Bmse(A[N]) = (1 — K[N])Bmse(A[N — 
We have now‘ derived the sequential LMMSE estimator equations based on the vector
space viewpoint.
_ In Appendix 12A we generalize the preceding vector space derivation for the sequen-
tial vector LMMSE estimator. To do so we must assume the data has the Bayesian
linear model form (see Theorem 12.1) and that Cw is a diagonal matrix, so that the
w[n] ’s are uncorrelated with variance E(w2 = 03,. The latter is a critical assumption
and was used in the previous example. Fortuitously, the set of equations obtained are
valid for the case of nonzero means as well (see Appendix 12A). Hence, the equations
to follow are the sequential implementation of the vector LMMSE estimator of (12.26)
a3,,1.[n],M[n - 1]
Figure 12.6 Sequential linear minimum mean square estimator
or (12.27) when the noise covariance matrix Cw is diagonal. To deﬁne the equations
we let 6[n] be the LMMSE estimator based on {:n[0],z[1], . . . ,z[n]}, and M[n] be the
corresponding minimum MSE matrix (just the sequential version of (12.28) or (12.29))
Mp1] = E ](o - émmo - émf].
Also, we partition the (n + 1) x p observation matrix as
Then, the sequential LMMSE estimator becomes (see Appendix 12A)
Estimator Update:
6[n] = 6[n —— 1] +  — hT[n]6]n — 1]) (12.47)
where
(12.48)
Minimum MSE Matrix Update:
M[n] = (I — K[n]hT[n])M]n — 1]. (12.49)
The gain factor K]n] is a p X 1 vector, and the minimum MSE matrix has dimension
p >< p. The entire estimator is summarized in Figure 12.6, where the thick arrows
indicate vector processing. To start the recursion we need to specify initial values for
6[n — 1] and M[n — 1], so that K[n] can be determined from (12.48) and then  from
(12.47). To do so we specify 6[—1] and M[—1] so that the recursion can begin at n = 0.
Since no data have been observed at n = —1, then from (12.19) our estimator becomes
the constant 6,- = am. It is easily shown that the LMMSE estimator is just the mean
of 6,- (see also (12.3)) or
6[—1] = E(6).
As a result, the minimum MSE matrix is
Several observations are in order.
1. For no prior knowledge about 6 we can let C99 —> oo. Then, we have the same form as
the sequential LSE (see Section 8.7 ), although the approaches are fundamentally
different.
2. No matrix inversions are required.
3. The gain factor K[n] depends on our conﬁdence in the new data sample as measured
by 0i versus that in the previous data as summarized by M]n — 1].
An example follows.
Example 12.3 - Bayesian Fourier Analysis
We now use sequential LS to compute the LMMSE estimator in Example 11.1. The
data model is
z[n]=acos21rf0n+bsin21rf0n+w[n] n=0,1,...,N—1
where f0 is any frequency (in Example 11.1 we assumed f0 to be a multiple of 1 /N ),
excepting 0 and 1 / 2 for which sin 21rfon is identically zero, and w[n] is white noise with
variance 02. It is desired to estimate 6 = [a b]T sequentially. We further assume that
the Bayesian linear model form applies and that 6 has mean E (6) and covariance 0Z1.
The sequential estimator is initialized by
é[-1] = 5(a)
Then, from (12.47) A A A
9l0l = 9l-1l + KlOlWlOl * hTlolfil-lll-
The hT[0] vector is the ﬁrst row of the observation matrix or
Subsequent rows are
hT[n] = ] cos 21rf0n sin21rf0n ] .
The 2 x 1 gain vector is, from (12.48),
where M[— 1] = 031. Once the gain vector has been found,  can be computed.
Finally, the MSE matrix update is, from (12.49),
MlOl = (I - KlQlhTlODMl-ll
and the procedure continues in like manner for n Z 1. <>
12.7 Signal Processing Examples - Wiener Filtering
We now examine in detail some of the important applications of the LMMSE estimator.
In doing so we will assume that the data {z[0],z]1], . . . ,:r]N — 1]} is WSS with zero
mean. As such, the N >< N covariance matrix C“ takes the symmetric Toeplitz form
mm - 1] w - 2]  mo]
= R“ (12.50)
where ruUc] is the ACF of the z[n] process and R“ denotes the autocorrelation matrix.
Furthermore, the parameter 6 to be estimated is also assumed to be zero mean. The
group of applications that we will describe are generically called Wiener ﬁlters.
There are three main problems that we will study. They are (see Figure 12.7)
1. Filtering, where 6 = s[n] is to be estimated based on z]m] = s[m] + w[m] for
m = O, 1, . . . , n. The sequences s]n] and w]n] represent signal and noise processes.
The problem is to ﬁlter the signal from the noise. Note that the signal sample
is estimated based on the present and past data only, so that as n increases, we
view the estimation process as the application of a causal ﬁlter to the data.
2. Smoothing, where 6 = s[n] is to be estimated for n = 0,1,... ,N — 1 based on the
data set {z]0], z[1], . . . ,:1:[N—1]}, where z[n] = s[n] +w[n]. In contrast to ﬁltering
we are allowed to use future data. For instance, to estimate s[1] we can use the
entire data set {z]0],z]1], . . . ,x]N — 1]}, while in ﬁltering we are constrained to
use only {z]0],  Clearly, in smoothing an estimate cannot be obtained until
all the data has been collected.
3. Prediction, where 6 = z[N — 1 + l] for l a positive integer is to be estimated based
on {.r:]0], a:[1], . . . ,a:[N —  This is referred to as the l-step prediction problem.
i“ __.. so]
#131 as]
(a) Filtering (b) Smoothing
a5] 1p]
(c) Prediction (d)
Interpolation
Figure 12.7 Wiener ﬁltering problem deﬁnitions
A related problem that is explored in Problem 12.14 is to estimate z[n] based on
{-Tl0l1--- Jl" * 1], 1‘[T1 + 1],. . ., z[N — 1]} and is termed the interpolation problem. To
solve all three problems we use (12.20) with E(6) = E(x) = 0 or
9 = C6ICQI1X (12.51)
and the minimum Bayesian MSE matrix given by (12.21)
Mé = C60 - C6zC;11Cz6' (12.52)
Consider ﬁrst the smoothing problem. We wish to estimate 6 = s = [s[0] s[1] . . . s[N —
1]]T based on x =  . ..x[N — 1]]T. We will make the reasonable assumption
that the signal and noise processes are uncorrelated. Hence,
Then, we have
Also,
C01 = E(SXT) = E(S(S + W)T) = R13-
Therefore, the Wiener estimator of the signal is, from (12.51),
g: 11,411,, + Rwwylx. (12.53)
The N x N matrix
w 2 Rss(Rss + Rww)_1 
is referred to as the Wiener smoothing matrix. The corresponding minimum l\ISE
matrix is, from (12.52),
Mai Z Rss _' Rss(Rss "i" Rww)-lRss
= (I — W)R,,s. (12.55)
As an example, if N = 1 we would wish to estimate s[0] based on z[0] = s[0] + 10(0)
Then, the Wiener smoother W is just a scalar W given by
138(0) + rm. [0] T] + 1 (
where n = r,,[0]/rww[0] is the SNR. For a high SNR so that W —> 1, we have  —> x[0],
while for a low SNR so that W —> 0, we have §[O] —> O. The corresponding minimum
M5 = (1 _ Wyﬁiﬂ] = (1- ni1)r..]0]
which for these two extremes is either 0 for a high SNR or 138(0) for a low SNR. A
numerical example of a Wiener smoother has been given in Section 11.7. See also
Problem 12.15.
We next consider the ﬁltering problem in which we wish to estimate 6 = s[nl based
on x = [a1 [0] 1(1) . . . x[n]]T. The problem is repeated for each value of n until the entire
signal s[n] for n = 0, 1, . . . , N — 1 has been estimated. As before, z[n] = s[n] + w(n],
where s[n] and w[n] are signal and noise processes that are uncorrelated with each
other. Thus,
where R8,, R4,", are (n + 1) X (n + 1) autocorrelation matrices. Also,
Co. = E8111] l IlOl will  11"] l)
~ E (s[n][ s[0] s[1]  s[n] 
= (r,,[n] rss[n —— 1] . . . r,,[0]].
Letting the latter row vector be denoted as ti, we have from (12.51)
.§[n] = r2: (RH + RwwYl x. (12.57)
The (n + 1) >< 1 vector of weights is seen to be
a = (11,, + 11W)“ 1;,
recalling that
§[n] = aTx
where a = [a0 a] . . . an]T as in our original formulation of the scalar LMMSE estimator
of (12.1). We may interpret the process of forming the estimator as n increases as a
ﬁltering operation if we deﬁne a time varying impulse response h(")[k]. Speciﬁcally, we
let h(")[k] be the response of a ﬁlter at time n to an impulse applied k samples before.
To make the ﬁltering correspondence we let
Note for future reference that the vector h = [hl"l[0] hl") [1] . . . h(">[n]]T is just the vector
a when flipped upside down. Then,
§[n] = i akaﬂk]
= i h(")[n — 
.§[n] = Z": h(")[k]a:[n — k] (12.58)
which is recognized as a time varying FIR ﬁlter. To explicitly ﬁnd the impulse response
h we note that since
(Rss + Rww) a = 1"“
it follows that
(Rss ‘i’ Rww) h = rss
where r5, = [r,,[0] r,,[1] . ..r8s[n]]T. This result depends on the symmetric Toeplitz
nature of Rs, + RM, and the fact that h is just a when flipped upside down. Written
out, the set of linear equations becomes
Tzvceill rza   - Tax: — ll h I : rsiill 
where r,,[k] = Tgsik] + rwwUv]. These are the Wiener-Hopf ﬁltering equations. It
appears that they must be solved for each value of n. However, a computationally
efﬁcient algorithm for doing so is the Levinson recursion, which solves the equations
recursively to avoid resolving them for each value of n [Marple 1987]. For large enough
n i can e s own that the ﬁlter becomes time invariant so that only a single solution
is necessary. In this case an anal tical sol t’ b ' -
ﬁltering equations can be written i: u 1on may e possible. The Wiener-Hopf
w ere we ave used the property rm[ k] - rx-Ilk]. As n —> oo, we have upon replacing
the “me varying Impulse response hinllkl by lts time invariant version hlkl
i: h[kln1[l— kl = mu] z= 0,1,... . (12.60)
The same set of equations result if we attempt to estimate s[n] based on the present
and ' ﬁ 't t b - - . . ,
To slate w:  igrso avizdlelfzn wlml for m S n‘ Thls is termed the mﬁnlte WWW?‘ ﬁlicr.
and use the orthogonality principle (12.13). Then, we have
Sln] ——  .L ...,z[n ——1],z(n]
or by using the deﬁnition of orthogonality
El(sl"l — ﬁlnllrl" — 11)]
Hence,
E h[k]z[n — k]z[n — 1]) = E(5[n]¢[n __ 1])
and therefo , th ' - - - .
are I8 8 equations to be solved for the inﬁnite Wiener ﬁlter impulse response
grgllﬁféiltg€zililrtrlztlilllGTE/tinge tge reader tfhat the solutions must be identical since the
“Si the present andginﬁ ‘tase on  or 0 g m 5 n as n —> oo is really just that of
of 3i ﬁlter makes the Sgiuiigililstiggeesgrlllglate tlée cgrrlent sample. The time invariance
independent of” The Solution of (lziéo) etnl‘ o w ic sample ‘IS to be estimated or
in Problem 12.16. At ﬁrst glance it m-i ht u 1 izes Zpectral factorization and is explored
Fourier transform techniques since theg lefztuhpea; Hat (y-GO) Coukl be.solved by using
of two Sequences‘ This is not the case ho - an ‘Si e o the equation is a convolution
If, indeed, they were Valid for l < 0 , ﬁvezier, since the equations hold only for l Z 0.
be viable. Such a set of - as We ’. en the Fourier transform approach would
equations arises in the smoothing problem in which s[n] is
to be estimated based on  . .,z[—1],z[()],a:[1], . .  or z[k] for all k. In this case the
smoothing estimator takes the form
§[n] = Z akzUc]
and by letting h[k] = an-k we have the convolution sum
where h[k] is the impulse response of an inﬁnite two-sided time invariant ﬁlter. The
Wiener-Hopf equations become
— oo < l < oo (12.61)
(see Problem 12.17). The difference from the ﬁltering case is that now the equations
must be satisﬁed for all l, and there is no constraint that h[k] must be causal. Hence,
we can use Fourier transform techniques to solve for the impulse response since (12.61)
becomes
h[n] +< rnhi] = rah] (12.62)
where * denotes convolution. Letting H ( f) be the frequency response of the inﬁnite
Wiener smoother, We have upon taking the Fourier transform of (12.62):
Pss(f)
Pas )
H(f)
Pss(f)
(f + Pwwlf)
The frequency response is a real even function of frequency, so that the impulse response
must also be real and even. This means that the ﬁlter is not causal. Of course, this is
to be expected since we sought to estimate s[n] using future as well as present and past
data. Before leaving this topic it is worthwhile to point out that the Wiener smoother
emphasizes portions of the frequency spectrum of the data where the SNR is high and
attenuates those where it is low. This is evident if we deﬁne the “local SNR” as the
SNR in a narrow band of frequencies centered about f as
"(fl ‘ P....<f)‘
Then, the optimal ﬁlter frequency response becomes
n(f) + 1'
H(f) =
Clearly, the ﬁlter response satisﬁes 0 < H (f) < 1, and thus the Wiener smoother
response is H(f) z 0 when 1](f) w 0 (low local SNR) and H(f) z 1 when n(f) —> oo
(high local SNR). The reader may wish to compare these results to those of the Wiener
ﬁlter given by (12.56). See also Problem 12.18 for further results on the inﬁnite Wiener
smoother.
Finally, we examine the prediction problem in which we wish to estimate 6 = z[N —
1 + l] for l _>_ 1 based on x =  z[1] . . . z[N — 1]]T. The resulting estimator is termed
the l-step linear predictor. We use (12.51) for which
where RI, is of dimension N X N and
Let the latter vector be denoted by rg. Then, »
Recalling that
a = 1136;, (12.63)
we have
i'[N — 1 + l] = akxﬂc]
If we let h[N — k] = a), to allow a “filtering” interpretation, then
161ml - 16] (12.64)
and it is observed that the predicted sample is the output of a ﬁlter with impulse
response h[n]. The equations to be solved are from (12.63) (noting once again that h
is just a when ﬂipped upside down)
where r" = [rmﬂ] r,,[l + 1] . . . r_.,,]N —- 1 + l]]T. In explicit form they become
rxzlo] rzzu] TzavlN —  
6,411} - 1] mp1} - 2]  64(1)] 66v]
= i (12.65)
These are the Wiener-Hopf prediction equations for the l-step linear predictor based
on N past samples. A computationally efficient method for solving these equations is
the Levinson recursion [Marple 1987]. For the speciﬁc case where l = 1, the one-step
linear predictor, the values of —h[n] are termed the linear prediction coefficients which
are used extensively in speech modeling [Makhoul 1975]. Also, for l = 1 the resulting
equations are identical to the Yule-Walker equations used to solve for the AR ﬁlter
parameters of an AR(N) process (see Example 7.18 and Appendix 1).
The minimum MSE for the l-step linear predictor is, from (12.52),
or, equivalently,
M, = rum] -r;’,a
r,,]0] — Z akr,,]N — 1+ l — k]
rum] -— i]: h[k]rn[k + (l —  (12.66)
As an example, assume that z[n] is an AR(1) process with ACF (see Appendix 1)
1 —- a2[1]
and we wish to ﬁnd the one-step predictor .i[N] as (see (12.64))
rrwlkl : (_al1l)]k]
To solve for the h[k]’s we use (12.65) for l = 1, which is
Substituting the ACF, we have
It is easily veriﬁed that these equations are solved for
hikl=i (Tam 
Hence, the one-step linear predictor is
5:]N] = —a]1].r[N -1]
and depends only 0n the previous sample. This is not surprising in light of the fact
that the AR(1) process satisﬁes
z[n] = —a[1]z]n — 1] + u]n]
Where u[n] is white noise, so that the sample t0 be predicted satisﬁes
.r[N] = —a[1]z]N — 1] + 
The predictor cannot predict u]N] since it is uncorrelated with the past data samples
(recall that since the AR(1) ﬁlter is causal. z[n] is a linear combination of {u[n], u]n —
1], . .  and thus z[n] is uncorrelated with all future samples of  The prediction
error is z[N] — a:[N] = u]N], and the minimum MSE is just the driving noise power of].
To verify this we have from (12.66)
= 1',,[O] + a:1l]r,,[1]
= ljjql] +a[1]1_ 3m (will)
We can extend these results to the l-step predictor by solving
Substituting the ACF for an AR(1) process this becomes
i: hlkl (—ai1l)'""k] = (—aill)lm+""-
The solution, which can be easily veriﬁed, is
hut] : { (“dilly
a: [n] (solid)
ﬂu] (dashed)
-5—] w r 1 v l i r 1 a
0 s 10 15 20 25 a0 as 40 45 so
Sample number, n
Figure 12.8 Linear prediction for realization of AR(1) process
and therefore the l-step predictor is
aEKN - 1) +1] = (—a[1])iz[N _ 1]. (12.67)
The minimum MSE is, from (12.66),
z liizlll _<_a[1])l1-U;2[1] (_ [m]
= ,j’+,,]<1-a”u1>-
It is interesting to note that the predictor decays to zero with l (since |a[1]] < 1). This
is reasonable since the correlation between z](N — 1) +1], the sample to be predicted,
and z[N -1], the data sample on which the prediction is based, is rm As l increases,
THU] decays to zero and thus so does i[(N — 1) + l]. This is also reﬂected in the
minimum MSE, which is smallest for l = 1 and increases for larger l.
A numerical example illustrates the behavior of the l-step predictor. If a[1] = —0.95
and of: = 0.1, so that the process has a low-pass PSD, then for a given realization of
z[n] we obtain the results shown in Figure 12.8. The true data are displayed as a solid
line, while the predicted data for n Z 11 is shown as a dashed line. The predictions
are given by (12.67), where N = 11 and l = 1,2,... ,40, and thus decay to zero with
increasing l. As can be observed, the predictions are generally poor except for small l.
See also Problems 12.19 and 12.20.
References
Kay, S., “Some Results in Linear Interpolation Theory," IEEE Trans. Acoust, Speech. Signal
Process., Vol. 31, pp. 746-749, June 1983.
Luenberger, D.G., Optimization by Vector Space Methods, J. Wiley, New York, 1969.
Makhoul, J., “Linear Prediction: A Tutorial Review,” Proc. IEEE, Vol. 63, pp. 561-580, April
Marple, S.L., Jr., Digital Spectral Analysis, Prentice-Hall, Englewood Cliffs, N.J., 1987.
Orfanidis, S.J., Optimum Signal Processing, Macmillan, New York, 1985.
Problems
12.1 Consider the quadratic estimator
é: az2[0] + bzl0] + c
of a scalar parameter 6 based on the single data sample :r[0]. Find the coeiﬁcients
a, b, c that minimize the Bayesian MSE. If z[0] ~ Z1[—l l], ﬁnd the LMMSE
estimator and the quadratic MMSE estimator if 6 = cos2 Also, compare
the minimum MSEs.
12.2 Consider the data
where A is a parameter to be estimated, r is a known constant, and w[n] is zero
mean white noise with variance 02. The parameter A is modeled as a random
variable with mean pA and variance of, and is independent of w[n]. Find the
LMMSE estimator of A and the minimum Bayesian MSE.
12.3 A Gaussian random vector x = [x1 zﬂT has zero mean and covariance matrix C“.
If w; is to be linearly estimated based on x1, ﬁnd the estimator that minimizes
the Bayesian MSE. Also, ﬁnd the minimum MSE and prove that it is zero if and
only if C“ is singular. Extend your results to show that if the covariance matrix
of an N X 1 zero mean Gaussian random vector is not positive deﬁnite, then any
random variable may be perfectly estimated by a linear combination of the others.
Hint: Note that
E  ma) = TCna.
12.4 An inner product (w, y) between two vectors z and y of a vector space must satisfy
the following properties
a. (ma) Z 0 and (ma) = 0 if and only if z = O.
b’ (may) = (y:
c- (CW1 ‘i’ 0212,11): C1($1,!/) ‘i’ Czlxmil)‘
Prove that the deﬁnition (w, y) = E(zy) for z and y zero mean random variables
satisﬁes these properties.
12.5 If we assume nonzero mean random variables, then a reasonable approach is
to deﬁne the inner product between z and y as (any) = cov(z,y). With this
deﬁnition z and y are orthogonal if and only if they are uncorrelated. For this
“inner product” which of the properties given in Problem 12.4 is violated?
12.6 We observe the data 1(a) = s[n] + w[n] for n = 0,1,...,N —— 1, where s[n] and
w[n] are zero mean WSS random processes which are uncorrelated with each other.
The ACFs are
rwwUc] = a’6[k].
Determine the LMMSE estimator of s = [s[0] s[1] . ..s[N — 1]]T based on x =
[z[0] z[1] . . . z[N — 1]]T and the corresponding minimum MSE matrix.
12.7 Derive the Bayesian MSE matrix for the vector LMMSE estimator as given by
(12.21), as well as the minimum Bayesian MSE of (12.22). Keep in mind the
averaging PDFs implied by the expectation operator.
12.8 Prove the commutative and additive properties of the LMMSE estimator as given
by (12.23) and (12.24).
12.9 Derive a sequential LMMSE estimator analogous to (12.34)-—(12.36) but for the
case where pA ¢ 0. Hint: Use an algebraic approach based on the results in
Example 10.1.
12.10 Given a set of vectors {zhzgpumn}, the Gram-Schmidt orthogonalization
procedure ﬁnds a new set of vectors {e1,e2, . . .,e,,} which are orthonormal (or-
thogonal and having unit length) or (e,, eJ) = 6,). The procedure is
a e‘ 111.1»
b z2-z2 — (z2,e1)e1
C’ nan
c. and so on,
or in general for n Z 2
1.. = w" — Z(I..,@.)e.
Give a geometrical interpretation of the procedure. For the Euclidean vectors
ﬁnd three orthonormal vectors using the Gram-Schmidt procedure. The inner
product is deﬁned as (x,y) = xTy for x and y Euclidean vectors.
12.11 Let x denote the vector composed of three zero mean random variables with a
covariance matrix
If y = Ax, determine the 3 X 3 matrix A so that the covariance matrix of y is I or,
equivalently, so that the random variables {y1,y2,y3} are uncorrelated and have
unit variance. Use the Gram-Schmidt orthogonalization procedure in Problem
12.10 to do so. What is interesting about A? Finally, relate C; to A. Note that
A may be viewed as a whitening transformation (see Section 4.5).
12.12 For the sequential LMMSE estimator of a scalar parameter explain what would
happen if of, —> 0 for some n. Do you obtain the same results for the vector case?
12.13 Find the sequential LMMSE estimator for the problem described in Example
12.1 . Verify that the sequential estimator is identical to the batch estimator (12.9).
Hint: Solve for the 1/K[n] sequence.
12.14 In this problem we examine the interpolation of a data sample. We assume that
the data set {z[n — 1VI],...,z[n — 1], z[n + 1], . . . , z[n +  is available and that
we wish to estimate or interpolate  The data and z[n] are assumed to be a
realization of a zero mean WSS random process. Let the LMMSE estimator of
a:[n] be
Find the set of linear equations to be solved for the weighting coefficients by using
the orthogonality principle. Next, prove that a_k = a1. and explain why this must
be true. See also [Kay 1983] for a further discussion of interpolation.
12.15 Consider the Wiener smoother for a single data sample as given by (12.56).
Rewrite W and M 5 as a function of the correlation coefficient
¢°v(§l0l, I101)
” = weivnvawioi)
between s[0] and  Explain your results.
12.16 In this problem we explore the solution of the Wiener-Hopf ﬁlter equations based
on the present and inﬁnite past or the solution of
Z hikirmil - k1 = Mm 1 z o
by the use of z transforms. First we deﬁne the one-sided z transform as
[a 
[(111)].
This is seen to be the usual z transform but with the positive powers of z omitted
Next, we write the Wiener-Hopf equation as
hlnl * ruin] — min] = 0 n z 0 (12.68)
where h[n] is constrained to be causal. The z transform (two-sided) of the left-
hand side is
’H(Z)'PII(Z) _ pss(z)
so that to satisfy (12.68) we must have
l’H(Z)'PII(Z) _ Pss(z)l+ z 
By the spectral factorization theorem [Orfanidis 1985], if P,,,(z) has no zeros on
the unit circle, then it may be factored as
7’@1(Z)= 509501")
where  is the z transform of a causal sequence and B (z‘1) is the z transform
of an anticausal sequence. Hence,
[7-L(z)l5'(z)l5'(z'1) — P,,(z)]+ = 0.
Let §(Z) = 7-i(z)B(z), so that
[Biz-w (at?) — Qjiiln + = 0.
Noting that Q’ (z) is the z transform of a causal sequence, show that
For anexample of the computations involved in determining the Wiener ﬁlter, see
[Orfanidis 1985].
12.17 Rederive the inﬁnite Wiener smoother (12.61) by ﬁrst assuming that
kI-oo
and then using the orthogonality principle.
12.18 For the inﬁnite Wiener smoother show that the minimum Bayesian MSE is
Then, using Fourier transform techniques, show that this can be rewritten as
where
P33(f)
l” Par) + Rum
Evaluate the Wiener smoother and minimum MSE if
Pas - O 4
and explain your results.
12.19 Assume that z]n] is a zero mean WSS random process. We wish to predict z[n]
based on {zln- 1], z[n~2] . . . , zln -N]}. Use the orthogonality principle to derive
the LMMSE estimator or predictor. Explain why the equations to be solved are
the _same as (12.65) for l = 1, i.e., they are independent of n. Also, rederive the
minimum MSE (12.66) by again invoking the orthogonality principle.
12.20 Consider an AR(N) process
r1:[n] = - E a[k]z[n — k] + u[n]
where u]n] is white noise with variance 0,2,. Prove that the optimal one-step linear
predictor of zln] is
i‘[n] = — Z a[k]z[n — 
Also, ﬁnd the minimum MSE. Hint: Compare the equations to be solved to the
Yule-Walker equations (see Appendix 1).
Appendix 12A
Derivation of Sequential LMMSE
Estimator for the Bayesian Linear
Model
We will use a vector space approach to update the ith component of 6[n — 1] as
follows. A A
6,-[n] = 6,-[n — 1] + K,~[n]  — i[n]n — 1]) (12A.1)
where a":[n]n — 1] is the LMMSE estimator of z[n] based on {z[0],z[1], ...,z[n — 
We now shorten the notation from that used in Section 12.6 where we denoted the
same LMMSE estimator as a":[n|O, 1,. . . , n — 1]. The motivation for (12A.1) follows by
the same rationale as depicted in Figure 12.5 (see also (12.41)). Also, we let i[n] =
z[n] — a":[n]n - 1], which is the innovation sequence. We can then form the update for
the entire parameter as
Before beginning the derivation we state some properties which are needed. The reader
should recall that all random variables‘ are assumed to be zero mean.
1. The LMMSE estimator of A6 is A6, the commutative property of (12.23).
2. The LMMSE estimator of 61 + 62 is 61 + 62, the additive property of (12.24).
3. Since 6,-[n - 1] is a linear combination of the samples {z[0], z[1], . . . , zln — 1]}, and
the innovation zln] — i:[n]n — 1] is uncorrelated with the past data samples, it
follows that A
4. Since 6 and w[n] are uncorrelated as assumed in the Bayesian linear model and
6[n — 1] and w[n] are also uncorrelated, it follows that
E [(0 - 6m - 1]) 1.41.1] = o.
1o see why 6[n — 1] and w]n] are uncorrelated note ﬁrst that 6]n — 1] depends i
linearly on the past data samples or on 6 and {w]0], w]1], . . . , w[n —  But wln]
1S uncorrelated with 6 and by the assumption that Cw is diagonal or that w[n] is
(12A.2)
a sequence of uncorrelated random variables, w[n] is uncorrelated with the past
noise samples.
With these results the derivation simpliﬁes. To begin we note that
Using properties 1 and 2 we have that
But iiz]n]n — 1] = 0 since wln] is uncorrelated with the past data samples, as explained
in property 4. Thus,
We next find the gain factor K,- Using (12.37), we have
El6i(-'l3]7'l] — i]n|n —1])]
Evaluating the denominator
E ](.r[n] - filnhl - 11f]
a
= hT[n]M]n — 1]h[n] + of,
E in-(nini - ninin - 11)] = E [an — tin —1l)(wln1 - ninin ~ 11)]
= E [an - tin - 11)(hTln]6 + nini - hTlnléln ~ 11)]
= E [(0, - 6,]n —1])(hT]n](6 - 9m -1]))]
= E [(0, - 9,111 ~1])(o - 9m - 11V] m] (12113)
so that
hT]n]M[n — 1]h]n] + a:
and therefore Mm _ 11m ]
To determine the update for the MSE matrix
= E ](6—6]n— 1] —K]n](.r]n] —.i[n]n— m)
(a - 6]n -1] - K]n](z]n] - i]TL]TL - 1]))T]
a
But from (12A.2) and (12A.4)
K[n]E  — a3]n]n — 1])2] = M]n —1]h]n]
and from property 3 and (12A.3)
E ](9 - 911i -1ii<nini ~ ninin - 11)] = E ivinini ~ ninin - 1i>i
so that
= (I — K[n]hT]n]) M[n — 1].
We next show that the same equations result if 6 and z[n] are not zero mean. Since
the sequential implementation of the minimum MSE matrix (12.49) must be identical
to (12.28), which does not depend on the means, M]n] likewise must be independent of
the means. Also, since M]n] depends on K]n], the gain vector must also be independent
of the means. Finally, the estimator update equation (12.47) is valid for zero means, as
we have already shown in this appendix. If the means are not zero, we may still apply
(12.47) to the data set z[n] —  and view the estimate as that of 6 —  By
the commutative property, however, the LMMSE estimator of 0 + b for b a constant
vector is 6 + b (see (12.23)). Thus, (12.47) becomes
6]n] — E(6) = 6[n -1] — E(6) + K[n]  ——  —— hT[n](6]n —— 1] — 
where  is the LMMSE estimator for nonzero means. Rearranging and canceling
terms we have
6[n] = 6[n — 1] + K[n]  —- hT[n]6[n —— 1] ——  — hT]n]E(6))]
and since
Emu» = E(11T[~l6+w[nl)
twine)
we arrive at the identical equation for the estimator update
Chapter 13
Kalman Filters
13.1 Introduction
We now discuss an important generalization of the Wiener ﬁlter. The signiﬁcance of the
extension is in its ability to accommodate vector signals and noises which additionally
may be nonstationary. This is in contrast to the Wiener ﬁlter, which is restricted to
stationary scalar signals and noises. This generalization is termed the Kalman ﬁlter. It
may be thought of as a sequential MMSE estimator of a signal embedded in noise, where
the signal is characterized by a dynamical or state model. It generalizes the sequential
MMSE estimator in Section 12.6, to allow the unknown parameters to evolve in time
according to a dynamical model. If the signal and noise are jointly Gaussian, then the
Kalman ﬁlter is an optimal MMSE estimator, and if not, it is the optimal LMMSE
estimator.
13.2 Summary
The scalar Gauss-Markov signal model is given in recursive form by (13.1) and explicitly
by (13.2). Its mean, covariance, and variance are given by (13.4), (13.5), and (13.6),
respectively. Generalizing the model to a vector signal results in (13.12) with the
statistical assumptions summarized just below. Also, the explicit representation is given
in (13.13). The corresponding mean and covariances are (13.14), (13.15), and (13.16) or
in recursive form by (13.17) and (13.18). A summary of the vector Gauss-Markov signal
model is given in Theorem 13.1. The sequential MMSE estimator or Kalman ﬁlter for
a scalar signal and scalar observations is given by (13.38)—(13.42). If the ﬁlter attains
steady-state, then it becomes the inﬁnite length Wiener ﬁlter as described in Section
13.5. Two generalizations of the Kalman ﬁlter are to a vector signal, whose equations
are (13.50)—(13.54), and to a vector signal and vector observations, whose assumptions
and implementation are described in Theorem 13.2. When the signal model and/ or
observation model is nonlinear, the preceding Kalman ﬁlter cannot be applied directly.
However, using a linearization approach, one obtains the suboptimal extended Kalman
ﬁlter, whose equations are (13.67)—(13.71).
13.3 Dynamical Signal Models
We begin our discussion of signal modeling by recalling our usual example of a DC level
in WGN or
where A is the parameter to be estimated and w[n] is WGN with variance a2. The
signal model A might represent the voltage output of a DC power supply, and the
noise w]n] might model the error introduced by an inaccurate voltmeter as successive
measurements are taken. Hence, z[n] represents the noise corrupted observations of the
power supply output. Now, even though the power supply should generate a constant
voltage of A, in practice the true voltage will vary slightly as time progresses. This is
due to the effects of temperature, component aging, etc., on the circuitry. Hence, a
more accurate measurement model would be
where A[n] is the true voltage at time n. However, with this model our estimation
problem becomes considerably more complicated since we will need to estimate A[n]
for n = 0,1,. ..,N — 1 instead of just the single parameter A. To underscore the
difficulty assume that We model the voltage A[n] as a sequence of unknown deterministic
parameters. Then, the MVU estimator of A[n] is easily shown to be
The estimates will be inaccurate due to a lack of averaging, and in fact the variability
will be identical to that of the noise or var(A[n]) = a2. This estimator is undesirable
in that it allows estimates such as those shown in Figure 13.1. We can expect that if
the power supply is set for A = 10 volts, then the true voltage will be near this and the
variation over time will be slow (otherwise it’s time to buy a new power supplyl). An
example of the true voltage is given in Figure 13.1 and is seen to vary about 10 volts.
Successive samples of A[n] will not be too different, leading us to conclude that they
display a high degree of “correlation.” This reasoning naturally leads us to consider A[n]
as a realization of a random process with a mean of 10 and some correlation between
samples. The imposition of a correlation constraint will prevent the estimate of A[n]
from ﬂuctuating too wildly in time. Thus, we will consider A]n] to be a realization of a
random process to be estimated, for which Bayesian approaches are appropriate. This
type of modeling was used in Chapter 12 in our discussion of Wiener ﬁltering. There
the signal to be estimated was termed s[n], and it was assumed to be zero mean. In
keeping with this standard notation, we now adopt s[n] as our notation as opposed to
0[n]. Also, because of the zero mean assumption, s]n] will represent the signal model for
A[n] — 10. Once we specify the signal model for zero mean s[n], it is easily modiﬁed to
accommodate nonzero mean processes by adding E(s[n]) to it. We will always assume
that the mean is known.
Estimated and true voltage
Sample number, n
Figure 13.1 True voltage and MVU estimator
A simple model for s[n] which allows us to specify the correlation between samples
is the ﬁrst-order Gauss-Markov process
s[n] = as[n — 1] + Min] T1 Z 0 (131)
where u[n] is WGN with variance a5, s[—1] ~ A/(undfli and si-ll is indePenflent of
u[n] for all n Z 0. (The reader should not confuse the Gauss-Markov process with the
model considered in the Gauss-Markov theorem since they are different.) The noise
u[n] is termed the driving or excitation noise since s[n] may be viewed as the output
of a linear time invariant system driven by  In the control literature the system
is referred to as the plant, and u[n] is termed the plant noise [Jaswinski 1970]. The
model of (13.1) is also called the dynamical or state model. The current output s[n]
depends only on the state of the system at the previous time, or s[n— 1], and the current.
input  The state of a system at time no is generally defined to be the amount o
information, which together with the input for n Z no determines the output for  Z no
[Chen 1970]. Clearly, the state is s]n— 1], and it summarizes the effect of all past inputs
to the system. We will henceforth refer to (13.1) as the Gauss-Markov model, where it
is understood to be first order. _ l
The signal model of (13.1) resembles an AR(1) process except that the signa starts
at n = 0, and hence may not be WSS. We shall see shortly that as n —> oo, so that
the effect of the initial condition is negligible, the process is actually WSS and may be
regarded as an AR(1) process with ﬁlter parameter a[1] = —a. A typical realization of
s[n] is shown in Figure 13.2 for a = 0.98, 0,2, = 0.1, it, = 5, U? = 1- Note that the mean
starts off at about 5 but quickly decreases to zero. This behavior is Just the transient
response of the system to the large initial sample s[—1] z y, = 5_ A150, the Samples
are heavily correlated. We may quantify these results by determining the mean and
covariance. of s[n]. Then, a complete statistical description will have been speciﬁed
since s[n] 1s a Gaussian process, as we will now show.
First, we express s[n] as a function of the initial condition, and the inputs as
S10] = as[—1l+ “[0]
s[1] = as[0] + u[1]
= a2s[—1]+ au[O] + u[1]
etc.
In general, we have
Sh] = Wsi-n + 211w» ~ k1 (13.2;
and we see that s[n] is a linear function of the initial condition and the driving noise
inputs from the initial time to the present time. Since these random variables are all
independent and Gaussian, s[n] is Gaussian, and it may further be shown that s[n] is
a Gaussian random process (see Problem 13.1). Now, the mean follows from (13.2) as
Ehlnl) = a"*‘E(sl~1]) (13.3)
= amus- (13.4)
The covariance between samples s[m] and s[n] is, from (13.2) and (13.4),
Cslmﬂll = E Kslml - E($lml))($l"l - E($l"l))l
= E Kam+1(s[—1]— m) + Z aku[m — kl)
' <¢1"+1($l“1l~ #5) + ﬁaluh — 
= amwwzof + i i ak+'E(u[m —— k]u[n — 
E(u[m — k]u[n - 1]) = 05W — (n —- m + k)]
EM“ — km" ‘ ll) ={ iiu otheriwisl: + k
cshn, n] : am+n+2a2 + 0,2 aZk-Q-n-m
am+n+2og +U12Lam—n Z a2k 
0-1 1 1 w a r 1 1* I “T 1
0 10 20 a0 40 so e0 70 so 9o 100
Sample number, n
Figure 13.2 Typical realization of ﬁrst order Gauss-Markov process
and of course cs[m, n] = c_,[n,m] for m < n. Note that the variance is
112M205 + a: Z 112'“. (13.6)
var(s[n])
Clearly, s[n] is not WSS since the mean depends on n and the covariance depends on
m and n, not the difference. However, as n —> oo, we have from (13.4) and (13.5)
E(s[n]) —> 0
l——a2
since lal < 1, which is necessary for stability of the process. Otherwise, the mean and
variance would increase exponentially with n. Thus, as n —> oo, the mean is zero and
the covariance becomes an ACF with
which is recognized as the ACF of an AR(1) process. It is even possible that s[n] will
be WSS for n Z 0 if the initial conditions are suitably chosen (see Problem 13.2). In
any event it is seen from (13.5) that by properly choosing a the process can be made
heavily correlated (]a| —> 1) or not correlated (|a| —> 0). This is also apparent from
(13.1). As an illustration, for the process described earlier we plot the mean, variance,
and steady-state covariance or ACF in Figure 13.3.
Because of the special form of the Gauss-Markov process, the mean and variance can
also be expressed recursively. This is useful for conceptualization purposes as well as
for extending the results to the vector Gauss-Markov process. The mean and variance
are obtained directly from (13.1) as
or  = aE(s[n — 1]) (13.7)
and
vaﬂslnl) = E [lslnl — E(Sl"l))2l
= E ](as[n — 1] + u]n] — aE(s[n ——1]))2]
= a2var(s[n —1])+ oi (13.8)
where we have used E(u[n]s[n — 1]) = 0. This follows from (13.2) because s[n —- 1]
depends only on {s[—1],u[0], . . . ,u[n— 1]} and they are independent of u[n] by assump-
tion. The equations (13.7) and (13.8) are termed the mean and variance propagation
equations. A covariance propagation equation is explored in Problem 13.4. Note from
(13.8) that- the variance is decreased due to the effect of as[n — 1] but increased due to
 In steady-state or as n —> oo, these effects balance each other to yield the variance
aZ/(l — a2) (just let var(s[n — 1]) = var(s[n]) in (13.8) and solve).
The ease with which we were able to obtain the mean and variance propagation
equations resulted from the dependence of s]n] 0n only the previous sample s]n — 1]
and the input  Consider now a pth-order Gauss-Markov process expressed in a
notation reminiscent of an AR(p) process or
s[n] = — Z a[k]s[n — k] + 
(13.9)
Because s[n] now depends on the p previous samples, the mean and variance propa-
gation equations become more complicated. To extend our previous results we ﬁrst
note that the state of the system at time n is {s[n — 1], s[n -— 2], . . . , s[n — p]} since the
previous p samples together with u[n] determine the output. We thus deﬁne the state
vector as
s[n — 1] = 2p (13.10)
w-munwnnwvw-w-mwwnpv-t-u-qmayn»w-w" ma.
E($l"l)
var(s[n])
(a) Mean
Sample number, n
(b) Variance
Sample number, n
(c) Steady-state covariance (ACF)
0.0-] r- | a 1 —|— 1 1 n" "-1- i
0 10 20 30 40 so so 70 so 90 100
Lag number, k
Figure 13.3 Statistics of ﬁrst-order Gauss-Markov process
With this deﬁnition we can rewrite (13.9) in the form
—a]p] —a]p —- 1] —a]p — 2]  —a]1] s[n —— 1]
where the additional (p — 1) equations are just identities. Hence, using the deﬁnition
of the state vector, we have
s[n] = As[n — 1] + Bu[n] (13.11)
where A is a p >< p nonsingular matrix (termed the state transition matrix) and B is a
p X 1 vector. Now we have the desired form (compare this to (13.1)) in which the vector
signal s[n] is easily computed based on its value at the previous time instant, the state
vector, and the input. This is termed the vector Gauss-Markov model. A ﬁnal level of
generality allows the input u[n] to be an r >< 1 vector so that, as shown in Figure 13.4,
we have a model for a vector signal as the output of a linear time invariant system (A
and B are constant matrices) excited by a vector input. In Figure 13.4a 71(2) is the
p X r matrix system function. Summarizing, our general vector Gauss-Markov model
takes the form
s[n] = As[n — 1] + Bu[n] n 2 0 (13.12)
where A, B are constant matrices with dimension p X p and p X r, respectively, s[n] is
the p X 1 signal vector, and u[n] is the r X 1 driving noise vector. We will from time to
time refer to (13.12) as the state model. The statistical assumptions are that
1. The input u[n] is a vector WGN sequence, i.e., u[n] is a sequence of uncorrelated
jointly Gaussian vectors with E(u[n]) = 0. As a result, we have that
E(u[m]uT[n]) = 0 m ¢ n
and the covariance of u[n] is
where Q is an r X r positive deﬁnite matrix. Note that the vector samples are
independent due to the jointly Gaussian assumption.
(b) Equivalent vector model
Figure 13.4 Vector Gauss-Markov signal system model
2. The initial state or s]—1] is a random vector with
Si-l] ~ Nil-lave!)
and is independent of u[n] for all n Z 0.
We now illustrate with an example.
Example 13.1 - Two DC Power Supplies
Recalling the introductory example, consider now the model for the outputs of two D-C
power supplies that vary with time. If we assume that the outputs are independent (in
a functional sense) of each other, then a reasonable model would be the scalar model
of (13.1) for each output or
silo] 11ml" * 11+ "ilnl
where s,[_1] ~ A/‘(psnggl)’ s2[_1] ~ A/(ywofz), u1[.n] is WGN with variance 0,2,1,
u2]n] is WGN with variance 0Z2, and all random variables are independent of each
omen Considering Sh] = [§1[n] s2[n]]T as the vector parameter to be estimated, we
have the model
51in] [a1 O]]S1l"_1l +]1 O]]u1[n]]
szlnl _ ° a2 szln- ll ° 1 uzlnl
where u[n] is vector WGN with zero mean and covariance
so that 2 0
‘Fl 6" val
and
SH] z l $21-11 l
If, on the other hand, the two outputs were generated from the same source (maybe
some of the circuitry was shared), then they would undoubtedly be correlated. As
we will see shortly, we could model this by letting any of the matrices A, B, or Q be
nondiagonal (see (13.26)). O
We now complete our discussion of the vector Gauss-Markov model by deriving its
statistical properties. The computations are simple extensions 0f those for the scalar
model. First, we determine an explicit expression for s[n]. From (13.12) we have
etc.
In general, we have
Sm] = A"+1s[-1] + Z A’°Bu[n - k] (13.13)
where A0 = I. It is seen that s[n] is a linear function of the initial condition and the
driving noise inputs. As a result, s[n] is a Gaussian random process. It remains only
to determine the mean and covariance. From (13.13)
EMHI) = A"*‘E(s[—1])
= Amp, (13.14)
The covariance is
Cslminl = E [(51111]-E(slml)l(sinl—E(S["]))T]
E [(A"'+1(s[-1] - us) + i: A'=Bu[m - k1)
- <A"+1(s[—-1] — 11,) + S3 AlBu[n — 1]) 1
A’"*‘C,A"+‘T + Z Z AkBE(u[m - 111%) - l])BTA'T.
E(u[m~k]uT[n—-l]) = Q6[l—(n—m+k)]
0 otherwise.
C,[m, n] = A'"+1c.A"+1’ + Z AkBQBTA"""*'“T (13.15)
and for m < n
The covariance matrix for s[n] is
A"+1c,A"+1’ + i‘ AkBQBTA“. (13.16)
Also, note that the mean and covariance propagation equations can be written as
E(s[n]) AE(s[n — 1]) (13.17)
cm] = Act) - 11H + BQBT n g 0 (13.18)
which follow from (13.12) (see Problem 13.5). As in the scalar case, the covariance
matrix decreases due to the AC[n - 1]AT term but increases due to the BQBT term.
(It can be shown that for a stable process the eigenvalues of A must be less than 1 in
magnitude. See Problem 13.6.) Steady-state properties similar to those of the scalar
Gauss-Markov model are evident from (13.14) and (13.16). As n —> 0o, the mean will
converge to zero or
E(s[n]) = A"+1ps -> 0
since the eigenvalues of A are all less than 1 in magnitude. Also, from (13.16) it can
be shown that as n —+ oo (see Problem 13.7)
A"+1c,A"+1’ -> o
so that w
C]n] -> c = Z AkBqBTAkT. (13.19)
It is interesting to note that the steady-state covariance is also the solution of (13.18)
if we set C[n ~ 1] = C[n] = C in (13.18). Then, the steady-state covariance satisﬁes
c = ACAT + BQBT (13.20)
and (13.19) is the solution, as can be veriﬁed by direct substitution. This is known as
the Lyapunov equation.
Although in our deﬁnition of the Gauss-Markov model we assumed that the)matrices
A, B, and Q did not depend on n, it is perfectly permissible to do so, and in some cases
quite useful. Similar expressions for the mean and covariance can be developed. One
major difference though is that the process may not attain a statistical steady-state.
We now summarize the model and its properties.
Theorem 13.1 (Vector Gauss-Markov Model) The Gauss-Markov model for a
p >< 1 vector signal s[n] is
s[n] = As[n — 1] + Bu[n] n Z 0. (13.21)
The A, B are known matrices having dimensions p >< p and p >< r, respectively, and it
is assumed that the eigenvalues 0f A are less than 1 in magnitude. The driving noise
vector u[n] has dimension r >< 1 and is vector WGN or u[n] ~ N (0, Q) with the u[n] ’s
independent. The initial condition s[—1] is a p >< 1 random vector distributed according
to s[—-1] ~ A/(uUCS) and is independent of the u[n] ’s. Then, the signal process is
Gaussian with mean
 = AWHuS (13.22)
and covariance for m Z n
Cdmm] = E Kslml - E(§lml))(§l"l — E(Sl"l))Tl
= A’"+1C,A"*‘T + Z A'°BQBTA""”'*'°T (13.23)
and for m < n
and covariance matria:
CM = 03(7), n] = A"+*c,A"+1’ + Z A'°BQBTA'°T. , (13.24)
The mean and covariance propagation equations are
E(s[n]) = AE(s[n _1]) (13.25)
C]n] Ac)” - 111T + BQBT. (13.26)
13.4 Scalar Kalman Filter
The scalar Gauss-Markov signal model discussed in the previous section had the form
s[n] = as[n — 1] + u[n] n Z 0.
We now describe a sequential MMSE estimator which will allow us to estimate s[n]
based on the data {:c[O],:c[1], . . . ,:c[n]} as n increases. Such an operation is referred to
as ﬁltering. The approach computes the estimator s[n] based on the estimator for the
previous time sample s[n — 1] and so is recursive in nature. This is the so-called K alman
ﬁlter. As explained in the introduction, the versatility of the Kalman ﬁlter accounts
for its widespread use. It can be applied to estimation of a scalar Gauss-Markov signal
as well as to its vector extension. Furthermore, the data, which previously in all our
discussions consisted of a scalar sequence such as {ac[0], 2:[1], . . . , w[n]}, can be extended
to vector observations or {x[0],x[1], . . . ,x[n]}. A common example occurs in array
processing in which at each time instant we sample the outputs of a group of sensors.
If we have M sensors, then each data sample x[n] or observation will be a M >< 1 vector.
Three different levels of generality are now summarized in hierarchical order.
1. scalar state — scalar observation (s[n — 1], 
2. vector state — scalar observation (s[n — 1],:c[n])
3. vector state — vector observation (s[n — 1],x[n]).
In this section we discuss the ﬁrst case, leaving the remaining ones to Section 13.6.
Consider the scalar state equation and the scalar observation equation
s[n] = as[n — 1] + u[n]
w[n] = s[n] + w[n] (13.27)
where u[n] is zero mean Gaussian noise with independent samples and E = of),
w[n] is zero mean Gaussian noise with independent samples and E(w2 = 0,2,. We
further assume that s[—1], u[n], and w[n] are all independent. Finally, we assume
that s[—1] ~ A/(umof). The noise process w[n] differs from WGN only in that its
variance is allowed to change with time. To simplify the derivation we will assume
that u, = O, so that according to (13.4)  = 0 for n Z O. Later we will account
for a nonzero initial signal mean. We wish to estimate s[n] based on the observations
{:c[0], :c[1], . . . w[n]} or to ﬁlter w[n] to produce  More generally, the estimator of
s[n] based on the observations {z[O],:c[1], . . . ,:c[m]} will be denoted by §[n|m]. Our
criterion of optimality will be the minimum Bayesian MSE or
E [(s[n] — ﬂnlnhz]
where the expectation is with respect to p(a:[0],z[1], . . . ,:c[n], s[n]), But the MMSE
estimator is just the mean of the posterior PDF or
§lnlnl = E(s["lIw[0].w[1].-- - .w[n]). (13.28)
Using Theorem 10.2 with zero means this becomes
élnln] = 09.0.2.5: (13.29)
since 6 =  and. x_=  z[1]  . :c[n]]T are jointly Gaussian. Because we are assum-
1ng Gaussian statistics for the signal and noise, the MMSE estimator is linear and is
identical in algebraic form to the LMMSE estimator. The algebraic properties allow us
to utilize the vector space approach to ﬁnd the estimator. The implicit linear constraint
does not detract from the generality since we already know that the optimal estima-
tor is linear. Furthermore, if the Gaussian assumption is not valid, then the resulting
estimator is still valid but can only be said to be the optimal LMMSE estimator. Re-
turning to the sequential computation of (13.29), we note that if :c[n] is uncorrelated
with {:c[O], a:[1], . . . , a:[n — 1]}, then from (13.28) and the orthogonality principle we will
have (see Example 12.2)
§[n]n] E(s[n]|:c[O],:c[1], . . . ,:c[n —1])+ 
which ha.s the desired sequential form. Unfortunately, the z[n]’s are correlated due
to their dependence on s[n], which is correlated from sample to sample. From our
discussions in Chapter 12 of the sequential LMMSE estimator, we can use our vector
space interpretation to determine the correction of the old estimator §[n]n — 1] due
to the observation of  Before doing so we will summarize some properties of the
MMSE estimator that will be used.
1. The MMSE estimator of 6 based on two uncorrelated data vectors, assuming jointly
Gaussian statistics, is (see Section 11.4)
6 = E(6]x1,x2)
E(9IX1)+ E(9|Xz)
if 6 is zero mean.
2. The MMSE estimator is additive in that if 6 = 61 + 62, then
6 = E(6|x)
E(61 +62|X)
= E(61|x)+E(62]x).
With thTese properties we begin the derivation of (13.38)—(13.42). Let X[n] =  :c[1]
. . .  (we now use X as our notation to avoid confusion with our previous notation
x[n] which will subsequently be used for vector observations) and :E[n] denote the in-
novation. Recall that the innovation is the part of ac[n] that is uncorrelated with the
previous samples {a:[0], z[1], . . . ,ac[n — 1]} or
.i[n] = :c[n] — §t[n]n -1]. (13.30)
This is because by the orthogonality principle i[n]n — 1] is the MMSE estimator of
:c[n] based on the data {ac[0],a:[1],...,:c[n — 1]}, the error or 5:[n] being orthogonal
(uncorrelated) with the data. The data X[n — 1],:E[n] are equivalent to the original
data set since :c[n] may be recovered from
= 5414+ Z akrc[k]
1==o
where the ak’s are the optimal weighting coefﬁcients of the MMSE estimator of z[n]
based on {:c[O],:c[1], . . . ,a:[n —  Now we can rewrite (13.28) a.s
and because X[n —— 1] and ﬂn] are uncorrelated, we have from property 1 that
Silnlnl = E($l"l|X[" —1l)+ Eﬁlnllilnll-
But E(s[n]]X[n— 1]) is the prediction of s[n] based on the previous data, and we denote
it by §[n|n — 1]. Explicitly the prediction is, from (13.1) and property 2,
.§[n|n -1] = E(s[n]]X[n — 1])
— E(as[n —- 1] + u[n]|X[n -— 1])
aE(s[n —1]|X[n —- 1])
a§[n —- 1|n — 1]
since E(u[n]|X[n -— 1]) = O. This is because
since u[n] is independent of {:c[0], :c[1], . . . , z[n —  This follows by noting that u[n] is
independent of all w[n], and from (13.2) s[O],s[1],. .. , s[n — 1] are linear combinations
of the random variables {u[O], u[1], . . . ,u[n — 1], s[—1]} and these are also independent
of  We now have that
§[n|n] = §[n|n — 1] + E(s[n]|i[n]) (13.31)
where
§[n]n — 1] = a.§[n -1]n — 1].
To determine E we note that it is the MMSE estimator of s[n] based on 
As such, it is linear, and because of the zero mean assumption of s[n], it takes the form
E($1"]]i1"]) = K1"]i1"]
= K1"](r1"]—1“11"]"—1])
where
E(S1"]i1"])
This follows from the general MMSE estimator for jointly Gaussian 6 and z
~ _ E(6:c)
But z[n] = s[n] + w[n], so that by property 2
since ziﬂnln — 1] = O due to w[n] being independent of {:c]O], z[1], . . . ,z[n —  Thus,
E(S1"]Ii1"]) = K1"](I1"] - §1"]" -1])
and from (13.31) we now have
.§[n|n] = §]n]n — 1] +  — §[n|n — 1]) (13.33)
where
.§[n]n — 1] = a.§]n —1]n — 1]. (13.34)
It remains only to determine the gain factor K From (13.32) the gain factor is
E s[n](.r[n — §]n|n -
To evaluate this we need the results
E1$1"](11"] - §1"]" '11)] = E1($1"] - §1"]" — 1])(111"]- §1"]" -1])]
E [w[n](s[n] -— §]n|n —  = 0.
The ﬁrst result is a consequence of the fact that the innovation
rc[n] — .§[n|n - 1] (13.35)
is uncorrelated with the past data and hence with §[n|n — 1], which is a linear com-
bination of a:[0], z[1], . . . ,ac[n — 1]. The second result follows from s[n] and w[n] being
uncorrelated and w[n] being uncorrelated with the past data (since w[n] is an uncorre-
lated process). Using these properties, the gain becomes
E1($1"] - §1"]" -1])(111"]— §1"]" —1])]
E1($1"] - §1"]" — 11+ w1"])”]
E1($1"] — §1"]" —1])’]
<13. + E1($1"] — §1"]" -1])’]'
K111] (13.36)
But the numerator is just the minimum MSE incurred when s[n] is estimated based on
the previous data or the minimum one-step prediction error. We will denote this by
M[n|n -1], so that
a§+M]n|n—1]'
To evaluate the gain we need an expression for the minimum prediction error. Using
(13.34)
Km] = (13.37)
E[(as]n — 1] +u[n] — .§[n]n —1])2]
E](a(s[n -1] — .§[n — 1]n — 1]) + 
We note that
E](s[n — 1] — §]n — 1|n —  = O
since s[n — 1] depends on {u[O], u[1], . . . , u[n — 1], s[—1]}, which are independent of u[n],
and §[n — IIn — 1] depends on past data samples or {s[O] + w[O], s[1] + w[1], . . . , s[n —
1] + w[n — 1]}, which also are independent of  Thus,
lVI[n]n -1] = a2M[n ~1]n — 1] + oi.
Finally, we require a recursion for M [nln]. Using (13.33), we have
1($1"] - §1"]"])2]
1151"] - §1"]" - 1] — K1"](111"] - §1"]" ~ 1]))2]
= 5191"] - $71111" r 1])2] - 2K1"]E1($1"] - §1"]" "1])(11"] — 5111]" - 11)]
+ K2[n]E](:r]n] — .§[n|n — 1])2].
But the ﬁrst term is M [nIn — 1], the second expectation is the numerator of K [n], and
the last expectation is the denominator of K [n], as seen from (13.36). Hence, from
(13.37)
M[n]n] = M]n]n -1] — 2K2]n](M]n|n — 1] + 03,) + K[n]M[n]n -1]
(1 — K[n])M[n|n -1].
This completes the derivation of the scalar state—scalar observation Kalman ﬁlter. Al-
though tedious, the ﬁnal equations are actually quite simple and intuitive. We summa-
rize them below. For n Z O
Prediction:
§[n|n — 1] = aé[n — 1]n — 1]. (13.38)
Minimum Prediction MSE:
M[n|n—1] =a2M[n—1|n—1]+0:. (13.39)
Kalman Gain: [ ' 1
K = a. .
[n] 0121+ M[n|n -1] (13 40)
Correction: ’
§[n|n] = .§[n|n — 1] +K[n](:r[n] — §[n|n —  (13.41)
M[n]n] = (1 — K[n])M[n|n -1]. (13.42)
Although derived for p, = 0 so that E(s[n]) = O, the same equations result for p, 76 0
(see Appendix 13A). Hence, to initialize the equations We use §[—1|—1] = E(s[—1]) = p,
and M [—1|— 1] = a? since this amounts to the estimation of s[—1] without any data. A
block diagram of the Kalman ﬁlter is given in Figure 13.5. It is interesting to note that
the dynamical model for the signal is an integral part of the estimator. Furthermore,
we may view the output of the gain block as an estimator i:[n] of  From (13.38)
and (13.41) the signal estimate is
.§[n]n] = a.§[n — 1]n —- 1] + 11[n]
where 12[n] = K — .§[n|n —  To the extent that this estimate is approximately
u[n], we will have §[n|n] z s[n], as desired. We now consider an example to illustrate
the flow of the Kalman ﬁlter.
Example 13.2 - Scalar State—Scalar Observation Kalman Filter
The signal model is a ﬁrst-order Gauss-Markov process
where s[- 1] ~ A/(O, 1) and 0i = 2. We assume that the signal is observed in noise
w[n], so that the data are z[n] = s[n] + w[n], where w[n] is zero mean Gaussian noise
Dynamical model
(b) Kalman ﬁlter
Figure 13.5 Scalar state-scalar observation Kalman ﬁlter and rela-
tionship to dynamic model
with independent samples, a variance of of, = (1/2)", and independent of 5l_1l and
u[n] for n 2 O. We initialize the ﬁlter with
§[-1| -1] E(s[—1])= 0
M[—1| - 1] E[(s[—1]— §[-1|-1])’]
= E(s2[-1])=1.
According to (13.38) and (13.39), we ﬁrst predict the s[O] sample to obtain
§[o|-1] = a§[—1]—1]
= §(o)=0
M[0|—1] = a2M[-1|-1]+a§~j
= §(1)+2=%.
Next, as 2:[O] is observed, we correct our predicted estimate by using (13.40) and (13.41)
t » t .. . “m. M“ t an...  -.-n--‘-»n_-¢¢~"-»W.
to yield A
Figure 13.6 Kalman ﬁlter as time varying ﬁlter
2 0 +  _ 0) :  case with h[n] = 1 and 6[n] replaced by s[n], those equations become
Next, the minimum MSE is updated using (13.42). an] z H" _ l] + KlnMzhl] _ ﬁn _ 1]) (1343)
M10101 = (1 - K101>M10| ~11 KM = Jﬁi (13.44)
_ (1_£)2_9 , aZ-i-Mhi-l]
' 13 4 _ 13‘ Min] = (1 ~K["l)Ml"— ll- (13-45)
For 1t = 1 we Obtain the results The Kalman ﬁlter will reduce to these equations when the parameter to be es-
timated does not evolve in time. This follows by assuming the driving noise to
émo] z  be zero or 0i = O and also a = 1. Then, the state equation becomes from (13.1)
26 s[n] = s[n — 1] or explicitly s[n] = s]—1] = 0. The parameter to be estimated is a
Mmo] = L13 constant, which is modeled as the realization of a random variable. Then, the pre-
52 diction is s[n|n— 1] = §[n—1]n— 1] with minimum MSE M[n|n—1] = M[n—1|n—1],
Km _ L13 so that we can omit the prediction stage of the Kalman ﬁlter. This says that the
_ 129 prediction is just the last estimate of s[n]. The correction stage reduces to (13.43)—
§[1|1] = %e[g] + E:;(Z[1]_ yiﬂm) gﬂnlrg-Sllllfivgteﬂciii f]. slnln] slnl slnln l sln l [nln] [n] an
Mull] = E . No matrix inversions are required. This should be compared to the batch method
1677 of estimating 6 = s[n] as
which the reader should verify. O 6 = 091C323‘
where x =  :c[1] . . . ac[n]]T. The use of this formula requires us to invert C"
for each sample of s[n] to be estimated. And in fact, the dimension of the matrix
is (n + 1) x (n + 1), becoming larger with n.
We should note that the same set of equations result if E (s[— 1]) 75 O. In this case the
mean of s[n] will be nonzero since E(s[n]) = a"+1E(s[-—1]) (see (13.3)). This extension
is explored in Problem 13.13. Recall that the reason for the zero mean assumption
was to allow us to use the orthogonality principle. We now discuss some important 3. The Kalman ﬁlter is a time varying linear ﬁlter, Note from (13.38) and (13.41)
properties of the Kalman ﬁlter. They are: that
1. The Kalman ﬁlter extends the sequential MMSE estimator in Chapter 12 to the ghﬂn] = a§[n _ 1|" _. 1] +  _ a_§[n _ 1|” _ 1])
case where the unknown parameter evolves in time according to the dynamical ]
model. In Chapter 12 we derived the equations for the sequential LMMSE esti- = an _ Khlnﬂn _ 1|” _ l] + Khlﬂn]
mator (see (12.47)—(12.49)), which are identical in form to those for the sequential
_ _ _ _ _ _ This is a ﬁrst-order recursive ﬁlter with time varying coefﬁcients as shown in
MMSE estimator WhICh assumes Gaussian statistics. In particular, for the scalar
Figure 13.6.
M]n]n] (x) and lll]n|n — l] (o)
90+ i a 1 1 2
Sample number, n
Figure 13.7 Prediction and correction minimum MSE
4. The Kalman ﬁlter provides its own performance measure. From (13.42) the min-
imurn Bayesian MSE is computed as an integral part of the estimator. Also. the
error measure may be computed off-line. i.e., before any data are collected. This
is because AI[n]n] depends only on (13.39) and (13.40), which are independent
of the data. We will see later that for the extended Kalman ﬁlter the minimum
MSE sequence must be computed on-line (see Section 13.7).
. The prediction stage increases the error, while the correction stage decreases it.
There is an interesting interplay between the minimum prediction MSE and the
minimum MSE. If as n —> oo a steady-state condition is achieved, then 1U
and 1iI]n|n - 1] each become constant with Jlﬂnln — 1] > M[n — 1]n — 1] (see
Problem 13.14). Hence, the error will increase after the prediction stage. When
the new data sample is obtained, we correct the estimate, which decreases the
MSE according to (13.42) (since K]n] < 1). An example of this is shown in
Figure 13.7, in which a = 0.99, 0i = 0.1, of, = 0.9"“, and M[—1] — 1] = 1. We
will say more about this in the next section when we discuss the relationship of
the Kalman ﬁlter to the Wiener ﬁlter.
. Prediction is an integral part of the Kalman ﬁlter. It is seen from (13.38) that to
determine the best ﬁltered estimate of s[n] we employ predictions. We can ﬁnd
the best one-step prediction of s[n] based on {ac[0], :c[1], . . . , ac[n— 1]} from (13.38).
If we desire the best two-step prediction, we can obtain it easily by noting that
this is the best estimate of s[n +1] based on {ac[O], :c[1], . . . ,:c[n —  To ﬁnd this
we let of, —> oo, implying that z[n] is so noisy that the Kalman ﬁlter will not use
it. Then, §[n + 1|n — 1] is just the optimal two-step prediction. To evaluate this
we have from (13.38)
§]n + 1|n] = a§[n|n]
Figure 13.8 Innovation-driven Kalman ﬁlter
Figure 13.9 Orthogonality (un-
correlated property) of innovation
Span of m]0], a:[1] sequence 
and since K[n] —> O, we have from (13.41) that §[n]n] = §[n|n -1], where .§]n|n —
1] = a,§]n — 1|n — 1]. Thus, the optimal two-step prediction is
a.§[n|n]
a.§[n|n — 1]
= a2§]n — 1|n -1].
This can be generalized to the l-step predictor, as shown in Problem 13.15.
. The Kalman ﬁlter is driven by the uncorrelated innovation sequence and in steady-
state can also be viewed as a whitening ﬁlter. Note from (13.38) and (13.41) that
§]n]n] = a.§]n — 1|n — 1] +K[n](l‘i"l — 591i" _ ll)
so that the input to the Kalman ﬁlter is the innovation sequence  =  -
.§]n|n - 1] (see (13.35)) as shown in Figure 13.8. \¢Ve know from our discussion of
the vector space approach that .'i[n] is uncorrelated with {:13[0], 11311], - - - » 931" _ ll},
which translates into a sequence of uncorrelated random variables as shown 1n
Figure 13.9. Alternatively, if we view :i[n] as the Kalman ﬁlter output and 1f the
ﬁlter attains steady-state, then it becomes a linear time invariant whitening ﬁlter
as shown in Figure 13.10. This is discussed in detail in the next section.
(a) Kalman ﬁlter
(b) System model
Figure 13.10 Whitening ﬁlter interpretation of Kalman ﬁlter
8. The Kalman ﬁlter is optimal in that it minimizes the Bayesian MSE for each
estimator .§[n]. If the Gaussian assumption is not valid, then it is still the optimal
linear MMSE estimator as described in Chapter 12.
All these properties of the Kalman ﬁlter carry over to the vector state case, except for
property 2, if the observations are also vectors.
13.5 Kalman Versus Wiener Filters
The causal inﬁnite length Wiener ﬁlter described in Chapter 12 produced an estimate
of s[n] as the output of a linear time invariant ﬁlter or
The estimator of s[n] is based on the present data sample and the inﬁnite past. To
determine the ﬁlter impulse response h[k] analytically we needed to assume that the
signal s[n] and noise w[n] were WSS processes, so that the Wiener-Hopf equations could
be solved (see Problem 12.16). In the Kalman ﬁlter formulation the signal and noise
need not be WSS. The variance of w[n] may change with n, and furthermore, s[n] will
only be WSS as n -> oo. Additionally, the Kalman ﬁlter produces estimates based
on only the data samples from 0 to n, not the inﬁnite past as assumed by the inﬁnite
length Wiener ﬁlter. The two ﬁlters will, however, be the same as n —> oo if 0f, = a2.
This is because s[n] will approach statistical steady-state as shown in Section 13.3, i.e.,
it will become an AR(1) process, and the estimator will then be based on the present
and the inﬁnite past. Since the Kalman ﬁlter will approach a linear time invariant
ﬁlter, we can let K[n] —> Klool, Mlnlnl _* MlOOl» and Mini" _ ll _* Mplooli Where
M,,[oo] is the steady-state one-step prediction error. To ﬁnd the steady-state Kalman
ﬁlter we need to ﬁrst ﬁnd  From (13.39), (13.40), and (13.42) we have
Aﬂoo]
(13.46)
which must be solved for M The resulting equation is termed the steady-state
Ricatti equation and is seen to be quadratic. Once M [o0] has been found, M,,[oo] can
be determined and ﬁnally K Then, the steady-state Kalman ﬁlter takes the form of
the ﬁrst-order recursive ﬁlter shown in Figure 13.6 with K [n] replaced by the constant
K Note that a simple way of solving the Ricatti equation numerically 1s to run
the Kalman ﬁlter until it converges. This will produce the desired time invariant ﬁlter.
The steady-state ﬁlter will have the form
§[n|n] = a§[n — 1|n — 1] + K[oo](:c[n] -— a§[n — lln — 1])
= a(1 - K[oo])§[n - 111. - 1] +K[oolwlnl
so that its steady-state transfer function is
K [o0]
= a. (13.47)
1 — a(1— K[oo])z“1
As an example, jfa = ()9, gf, = 1, a2 = 1, we will ﬁnd from (13.46) that M[oo] = 0.5974
(the other solution is negative). Hence, from (13.39) M,,[o0] = 1.4839, and from (13-40)
K [o0] = 0.5974. The steady-state frequency response 1s
H600‘) = Hm(e><1>(J'21rf))
1- 0.3623 exp(—j21rf)
whose magnitude is shown in Figure 13.11 as a solid line versus the PSD of the steady-
state signal
I1 - aeXP(-J'21Tf)|2
|1 — 0.9exp(—j21rf)|2
shown as a dashed line. The same results would be obtained if the Wiener-Hopf equation
had been solved for the causal inﬁnite length Wiener ﬁlter. Hence, the steady-state
PS5(-f)
PSD and ﬁlter magnitude response (dB)
PSD and ﬁlter magnitude response (dB
Frequency Frequency
Figure 13-11 5151131 PSD and steadYsiate Kalman ﬁlm‘ magnitude Figure 13.12 Whitening ﬁlter property of steady-state Kalman ﬁlter
_ _ _ shown as a dashed line. Note that the PSD of z[n] is
Kalman ﬁlter is equivalent to the causal inﬁnite length Wiener ﬁlter if the signal and
the noise become WSS as n —> oo. _ ‘ ‘ _ _ PHU) : PM“) + U2
Finally, the whitening ﬁlter property discussed in property 7 in the previous section U2
may be veriﬁed for this example. The innovation is _   + g‘
ﬁn] = ﬁn] -§[n|n— 1] : a§+a2|1—aexp(—j21rf)|2
:c[n]-—a§[n—-1|n—1]. (13.48) ]1—-aexp(—j21rf)|2
But in steady-state §[n|n] is the output of the ﬁlter with system function 7130(2) driven which f0!‘ this example 1S
by  Thus, the system function relating the input z[n] to the output :c[n] is, from _ 2
(13.48) and (13.47), pug) = ﬂﬂi‘EB__(‘1Z11l_
l ) [1—O.9exp(—j21rf)|2
"Hw z = 1—az" Hw(z
( ) _1 The PSD at the output of the whitening ﬁlter is therefore
_ 1 az K [o0]
1;a(1 ilKiwllzql ‘H (‘OPP (f)_1+|1—0.9exp(——j21rf)|2
* 11 - 0.3623 exp(—j21rf)|2
1- a(1— K[oo])z‘1'
which can be veriﬁed to be the constant PSD Pﬁ( f ) = 2.48. In general, we have
For this example we have the whitening ﬁlter frequency response
11 (f) = _____-_1- °-9°"P('1""f l P”) z 111111112
w 1 — 0.3623 exp(—j21rf)
_ _ _ where 0g is the variance of the innovation. Thus, as shown in Figure 13.13, the output
whose magnitude is plotted in Figure 13.12 as a solid line along with the PSD of ﬁn], PSD of a ﬁlter with frequency response Hm“) is a ﬁat PSD with height U;
Figure 13.13 Input and output PSDs of steady-state Kalman whiten-
ing ﬁlter
13.6 Vector Kalman Filter
The scalar state—scalar observation Kalman ﬁlter is easily generalized. The two general-
izations are to replace s]n] by s[n], where s[n] obeys the Gauss-Markov model described
in Theorem 13.1, and to replace the scalar observation ac[n] by the vector observation
x]n]. The first generalization will produce the vector state—scalar observation Kalman
ﬁlter, while the second leads to the most general form, the vector state—vector obser-
vation Kalman ﬁlter. In either case the state model is, from Theorem 13.1,
where A, B are known p X p and p >< r matrices, u]n] is vector WGN with u]n] ~ N (0, Q),
S[—1] ~ A/(pMCS), and s[—1] is independent of the u[n]’s. The vector state—scalar
observation Kalman ﬁlter assumes that the observations follow the Bayesian linear
model (see Section 10.6) with the added assumption that the noise covariance matrix
is diagonal. Thus, for the nth data sample we have
z[n] = hT[n]s[n] + w[n] (1349)
where h[n] is a known p >< 1 vector and w]n] is zero mean Gaussian noise with uncor-
related samples, with variance oi, and also independent of s[—1] and u[n]. The data
model of (13.49) is called the observation or measurement equation. An example is
given in Section 13.8, where we wish to track the coefficients of a random time varying
FIR ﬁlter. For that example, the state is comprised of the impulse response values.
The Kalman ﬁlter for this setup is derived in exactly the same manner as for the scalar
state case. The derivation is included in Appendix 13A. We now summarize the results.
The reader should note the similarity to (13.38)—(13.42).
Prediction:
§[n|n — 1] = As]n — 1|n — 1]. (13.50)
Minimum Prediction MSE Matrix (p >< p):
M[n]n - 1] = AM[n ~1|n ~ 1]AT + BQBT. (13.51)
Kalman Gain Vector (p >< 1):
Correction:
s[n]n] = s[n|n — 1] +  — hT]n]§[n]n —-  (13.53)
Minimum MSE Matrix (p >< p):
M[n]n] = (I — K[n]hT[n])M]n]n — 1] (13.54)
where the mean square error matrices are deﬁned as
Mlnln] = E [($111] ~ élnlnlﬂsln] ~§I1LIRDTI (13-55)
M[n|n -1} = E [($94 - s]n|n - 1])(s]n] - §[n]n -1])T] . (13.56)
To initialize the equations we use §[—1| — 1] = E(s[-—1]) = p, and M[—1] -1] = C5. The
order of the recursion is identical to the scalar state—scalar observation case. Also, as
before, no matrix inversions are required, but this is no longer true when we consider
the vector observation case. If A = I and B = 0, then the equations are identical to the
sequential LMMSE estimator (see (12.47)—(12.49)). This is because the signal models
are identical, the signal being constant in time. An application example is given in
Section 13.8.
Finally, we generalize the Kalman ﬁlter to the case of vector observations, which
is quite common in practice. An example is in array processing, where at each time
instant we sample the output of an array of M sensors. Then, the observations are
x[n] = [m1 [n] z2]n] . ...'z:M]n]]T, where a:,~]n] is the output of the ith sensor at time n.
The observations are modeled using the Bayesian linear model
where each vector observation has this form. Indexing the quantities by n and replacing
0 by s, we have as our observation model
x[n] = H[n]s]n] + w[n] (13.57)
where H[n] is a known M >< p matrix, x[n] is an M >< 1 observation vector, and w[n]
is a M >< 1 observation noise sequence. The w[n]’s are independent of each other
and of u[n] and s]—1], and W[n] ~ A/(0,C[n]). Except for the dependence of the
covariance matrix on n, w[n] can be thought of as vector WGN. For the array processing
problem s[n] represents a vector of p transmitted signals, which are modeled as random,
and H[n] models the linear transformation due to the medium. The medium may
be modeled as time varying since H[n] depends -on n. Hence, H[n]s[n] is the signal
output at the h! sensors. Also, the sensor outputs are corrupted by noise w[n]. The
statistical assumptions on w[n] indicate the noise is correlated from sensor to sensor
at the same time instant with covariance C[n], and this correlation varies with time.
From time instant to time instant, however, the noise samples are independent since
 = 0 for i 75 j. With this data model we can derive the vector state—vector
observation Kalman ﬁlter, the most general estimator. Because of the large number of
assumptions required, we summarize the results as a theorem.
Theorem 13.2 (Vector Kalman Filter) The p X 1 signal vector s[n] evolves in time
according to the Gauss-Markov model
where A, B are known matrices of dimension p >< p and p X r, respectively. The driving
noise vector u[n] has the PDF u[n] ~ N (0, Q) and is independent from sample to
sample, so that E(u[m]uT[n]) = 0 for m 96 n (u[n] is vector WGN). The initial state
vector s[—1] has the PDF s[—1] ~ A/(uwCs) and is independent of u[n].
The M >< 1 observation vectors x[n] are modeled by the Bayesian linear model
where H[n] is a known .M X p observation matrix (which may be time varying) and w[n]
is an M >< 1 observation noise vector with PDF w[n] ~ N (0, C[n]) and is independent
from sample to sample, so that E(w[m]wT = 0 for m 76 n. (If C[n] did not depend
on n, then w[n] would be vector WGN.)
The MMSE estimator of s[n] based on {x[O],x[1], . . . ,x[n]} or
can be computed sequentially in time using the following recursion:
Prediction:
§[n|n -1] = A§]n — 1]n — 1]. (13.58)
Minimum Prediction MSE Matrix (p >< p):
M[n|n - 1] = AM]?! - 1|n - 11M + BQBT. (13.59)
Kalman Gain Matrix (p >< M):
K]n] = M[n|n — 1]HT[n] (C]n] + H]n]M[n]n — 1]HT[n])_1. (13.60)
Correction: .
§[n]n] = §[n|n — 1] + K[n](x[n] -— H[n]§[n|n —  (13.61)
Minimum MSE Matria: (p >< p):
M[n|n] = (I — K[n]H[n])M[n|n -1]. (13.62)
The recursion is initialized by §[—1| — 1] = us, and M[—1| — 1] = Cs.
All the comments of the scalar state—scalar observation Kalman ﬁlter apply here as well,
with the exception of the need for matrix inversions. We now require the inversion of
an M X M matrix to ﬁnd the Kalman gain. If the dimension of the state vector p is less
than the dimension of the observation vector M, a more efﬁcient implementation of the
Kalman ﬁlter can be obtained. Referred to as the information form, this alternative
Kalman ﬁlter is described in [Anderson and Moore 1979].
Before concluding the discussion of linear Kalman ﬁlters, it is worthwhile to note
that the Kalman ﬁlter summarized in the previous theorem is still not the most general
one. It is, however, adequate for many practical problems. Extensions can be made
by letting the matrices A, B, and Q be time varying. Fortuitously, the equations that
result are identical to those of the previous theorem when we replace A by A[n], B by
B[n], and Q by Q[n]. Also, it is possible to extend the results to colored observation
noise and to signal models with deterministic inputs (in addition to the driving noise).
Finally, smoothing equations have also been derived based on the Kalman philosophy.
These extensions are described in [Anderson and Moore 1979, Gelb 1974, Mendel 1987].
13.7 Extended Kalman Filter
In practice we are often faced with a state equation and/or an observation equation
which is nonlinear. The previous approaches then are no longer valid. A simple example
that will be explored in some detail in Section 13.8 is vehicle tracking. For this problem
the observations or measurements are range estimates  and bearing estimates 
If the vehicle state is the position (TI’Ty) in Cartesian coordinates (it is assumed to
travel in the ac-y plane), then the noiseless measurements are related to the unknown
parameters by
Mn] = arctan 
Due to measurement errors, however, we obtain the estimates  and 6 [n], which are
assumed to be the true range and bearing plus measurement noise. Hence, we have for
our measurements
Mn] + w@[n] (13.63)
arctan + w),
Clearly, we cannot express these in the linear model form or as
Where gin] = [Fri/l] ry[n]]T. The observation equation is nonlinear.
An example of a nonlinear state equation occurs if we assume that the vehicle is
traveling in a given direction at a known ﬁxed speed and we choose polar coordinates
. _ _ _ a
range and bearing, ‘to describe the state. (Note that this choice would render the
measurement equation linear as described by (13.63)). Then, ignoring the driving
noise, the state equation becomes
ruin] = vynA+Tyiol (13.64)
where (111,113)) is the known. velocity, A is the time interval between samples, and
(T,]0], Ty is the initial position. This can be expressed alternatively as
so that the range becomes
rﬂn — 1] + rﬂn — 1] + 2v,Ar,[n —~ 1] + 2vyAry[n — 1] + (vi +UZ)A2
= [12]]; — 1] + 2R[n — 1]A(v, cos ﬂ[n — 1] + vy sinﬂ[n — 1]) + (vi + U§)A2_
This is ‘clearly very nonlinear in range and bearing. In general, we may be faced with
sequential state estimation where the state and / or observation equations are nonlinear.
Then, instead of our linear Kalman ﬁlter models
we would have
Si"! = “Si” — 11) + Buin] (13.65)
Xlnl = hﬁinl) + Win] (13.66)
where i; i; a p-dimensional function and h is an M-dimensional function. Thedimen-
sions o t e remaining matrices and vectors are the same as before. Now a(s[n — 1])
represents the true physical model for the evolution of the state, while u[n] accounts for
the modeling errors, unforeseen inputs, etc. Likewise, h(s]n]) represents the transfor-
mation from the state variables to the ideal observations (without noise). For this case
the MMSE estimator is intractable. The only hope is an approximate solution based
on linearizing a and h, much the same as was done for nonlinear LS, where the data
were nonlinearly related to the unknown parameters. The result of this linearization
and the subsequent application of the linear Kalman ﬁlter of (13.58)—(13.62) results in
the extended Kalman ﬁlter. It has no optimality properties, and its performance will
depend on the accuracy of the linearization. Being a dynamic linearization there is no
way to determine its performance beforehand.
Proceeding with the derivation, we linearize a(s[n — 1]) about the estimate of s[n— 1]
or about §]n — 1]n -1]. Likewise, we linearize h(s[n]) about the estimate of s[n] based on
the previous data or s[n|n —— 1] since from (13.61) we will need the linearized observation
equation to determine §[n]n]. Hence, a ﬁrst-order Taylor expansion yields
a(s[n — 1]) z a(§[n —- 1|n — 1])
6a A
+ 65in _  s[n—1]=§]n—1|n—1](s[n ‘l  _ siin _1]n __ 
mini) ~ htiinnn ~ 11> + a“ (SM - int» - 11>-
asinl S[Tl]=§]7l]'ﬂ.-l]
We let the Jacobians be denoted by
3a
so that the linearized state and observation equations become from (13.65) and (13.66)
s]n] = A[n —1]s[n — 1] +Bu]n] + (a(§[n — 1|n —1])— A[n — 1]§[n -— 1|n — 1])
Kin] = Hinlslnl + Wlnl + (h(§l"|" ~11) — Hlnlﬁiinl" — 1l)~
The equations differ from our standard ones in that A is now time varying and both
equations have known terms added to them. It is shown in Appendix 13B that the
linear Kalman ﬁlter for this model, which is the extended Kalman ﬁlter, is
Prediction:
int. - 1] = a(§]n - 1]n - 1]). (13.67)
Minimum Prediction MSE Matrix (p >< p):
M[n|n - 1] = Apt —-1]M[n - 1]n - 1]AT[n - 11+ BQBT. (13.68)
Kalman Gain Matrix (p >< M):
K[n] = MW. - 1]HT[n] (C[n] + H[n]M[n|n - i]iiT]n])“. (13.69)
Correction:
s[n|n] = s]n]n — 1] + K[n](x[n] — h(s[n|n ——  (13.70)
Minimum MSE Matrix (p >< p): Input u p“
M[n|n] = (I — K[n]H[n])M[n|n -1] (13.71)
where
6a .
1 asln _  s[n—1]=§[n—1|n—l] - t t
35h] shllziinhhl] (a) Multipath channel
Note that in contrast to the linear Kalman ﬁlter the gain and MSE matrices must be Input 0MP“
computed on-line, as they depend upon the state estimates via A]n — 1] and  Also,
the use of the term MSE matrix is itself a misnomer since the MMSE estimator‘ has not
been implemented but only an approximation to it. In the next section we will apply
the extended Kalman ﬁlter to vehicle tracking. t t
13.8 Signal Processing Examples
We now examine some common signal processing applications of the linear and extended
b Fad h 1
Kalman ﬁlters. ( ) mg C anne
Exmnple 133 ' Time Varying Channel Estimation Figure 13.14 Input-output waveforms for fading and multipath channels
Many transmission channels can be characterized as being linear but not time invariant.
These are referred to by various names such as fading dispersive channels or fading
multipath channels. They arise in communication problems in which the troposphere "M
is used as a. medium or in sonar in which the ocean is used [Kennedy 1969]. In either
case, the medium acts as a linear ﬁlter, causing an impulse at the input to appear as
a continuous waveform at the output (the dispersive or multipath nature), as shown
in Figure 1314a. This effect is the result of a continuum of propagation paths, i.e.,
multipath, each of which delays and attenuates the input signal. Additionally, however,
a sinusoid at the input will appear as a narrowband signal at the output or one whose
amplitude is modulated (the fading nature), as shown in Figure 13.14b. This effect
is due to the changing character of the medium, for example, the movement of the
scatterers. A little thought will convince the reader that the channel is acting as a
linear time varying ﬁlter. If we sample the output of the channel, then it can be shown
that a good model is the low-pass tapped delay line model as shown in Figure 13.15
[Van Trees 1971]. The input-output description of this system is
Figure 13.15 Tapped delay line channel model
y[n] = Z h,,[lc]v[n — k]. (13.72)
This is really nothing more than an FIR ﬁlter with time-varying coefficients. To design
effective communication or sonar systems it is necessary to have knowledge of these
coefficients. Hence, the problem becomes one of estimating hnﬂc] based on the noise
corrupted output of the channel
z[n] = p53 hn]k]v]n — k] + w[n] (13.73)
where w[n] is observation noise. A similar problem was addressed in Example 4.3,
except that there we assumed the ﬁlter coefficients to be time invariant. Consequently,
the linear model could be applied to estimate the deterministic parameters. It is not
possible to extend that approach to our current problem since there are too many
parameters to estimate. To see why this is so we let p = 2 and assume that v[n] = 0
for n < 0. The observations are, from (13.73),
r10] = hololvlol + holllvbll + wlol = ho[0]v[0] + w[0]
etc.
It is seen that for n Z 1 we have two new parameters for each new data sample. Even
without corrupting noise we cannot determine the tapped delay line weights. A way
out of this problem is to realize that the weights will not change rapidly from sample
to sample, as for example, in a slow-fading channel. For example, in Figure 1314b
this would correspond to an amplitude modulation (caused by the time variation of
the weights) which is slow. Statistically, we may interpret the slow variation as a
high degree of correlation between samples of the same tap weight. This observation
naturally leads us to model the tap weights as random variables whose time variation
is described by a Gauss-Markov model. The use of such a signal model allows us to ﬁx
the correlation between the successive values of a given tap weight in time. Hence. we
suppose that the state vector is
where h[.n] = [h,,[0] h,,[1] . . . hnlp — 1]]T, A is a known p >< p matrix, and u[n] is vector
WGN with covariance matrix Q. (The reader should note that h[n] no longer refers to
the observation vector as in (13.49) but is now the signal s[n].) A standard assumption
that is made to simplify the modeling is that of uncorrelated scattering [Van Trees
1971]. It assumes that the tap weights are uncorrelated with each other and hence
independent due to the jointly Gaussian assumption. As a result, we can let A, Q, and
Ch, the covariance matrix of h[—1], be diagonal matrices. The vector Gauss-Markov
Erlicgdébthen becomes p independent scalar models. The measurement model is, from
Zhl] z [ UPI] U]7L —  'U[1L—p+  ] + 19(7),]
where w[n] is assumed to be WGN with variance a2 and the v[n] sequence is assumed
known (since we provide the input to the channel). We can now form the MMSE
estimator for the tapped delay line weights recursively in time using the Kalman ﬁlter
equations for a vector state and scalar observations. With obvious changes in notation
we have from (13.50)~(13.54)
KM] — a2 + vT[n]M[n|n — 1]v[n] A
M]n|n] = (I — K[n]vT[n])M[n|n — 1]
and is initialized by I1]—1] - 1] = ph, M[—1| -— 1] -—- Ch. As an example, we now
implement the Kalman ﬁlter estimator for a tapped delay line having p = 2 weights.
We assume a state model with
A particular realization is shown in Figure 13.16, in which h,,[0] is decaying to zero while
h,,[1] is fairly constant. This is because the mean of the weights will be zero in steady-
state (see (13.4)). Due to the smaller value of [A]11, h" [0] will decay more rapidly. Also,
note that the eigenvalues of A are just the diagonal elements and they are less than 1 in
magnitude. For this tap weight realization and the input shown in Figure 13.17a, the
output is shown in Figure 13.17b as determined from (13.72). When observation noise
is added with a2 = 0.1, we have the channel output shown in Figure 13.17c. We next
apply the Kalman ﬁlter with lA1[—1| — 1] = 0 and M[—1] - 1] = 1001. which were chosen
to reflect little knowledge about the initial state. In the theoretical development of the
Kalman ﬁlter the initial state estimate is given by the mean of s[—1]. In practice this
is seldom known, so that we usually just choose an arbitrary initial state estimate with
a large initial MSE matrix to avoid “biasing” the Kalman ﬁlter towards that assumed
state. The estimated tap weights are shown in Figure 13.18. After an initial transient
the Kalman ﬁlter “locks on” to the true weights and tracks them closely. The Kalman
ﬁlter gains are shown in Figure 13.19. They appear to attain a periodic steady-state,
although this behavior is different than the usual steady-state discussed previously since
v[n] varies with time and so true steady-state is never attained. Also, at times the gain
is zero, as for example in [K]1 = K1 [n] for O f n 5 4. This is because at these times v[n]
is zero due to the zero input and thus the observations contain only noise. The Kalman
ﬁlter ignores these data samples by forcing the gain to be zero. Finally, the minimum
MSEs are shown in Figure 13.20 and are seen to decrease monotonically, although this
generally will not be the case for a Kalman ﬁlter. O
Tap weight, hnlﬂ]
0 1o 20 so 40 so e0 70 s0 90 100
Sample number, n
0 10 2o 30 40 50 e0 10 so 90 100
Tap weight, h" [l]
Sample number, n
Figure 13.16 Realization of TDL coefficients
Example 13.4 - Vehicle Tracking
In this example we use an extended Kalman ﬁlter to track the position and velocity of a
vehicle moving in a nominal given direction and at a nominal speed. The measurements
are noisy versions of the range and bearing. Such a track is shown in Figure 13.21. In
arriving at a model for the dynamics of the vehicle We assume a constant velocity,
perturbed only by wind gusts, slight speed corrections, etc., as might occur in an
aircraft. We model these perturbations as noise inputs, so that the velocity components
in the :11: and y directions at time n are
val" " 1l+ uylnl-
(13.74)
Channel input, v[n]
Channel output,  Noiseless channel output, y[n]
0 10 20 30 40 s0 e0 10 s0 90 100
Sample number, n
+ (b)
0 10 2o 3o 40 50 e0 70 so 90 100
Sample number, n
(¢)
0 10 2o 30 40 s0 so 10 so 9o 100
Sample number, n
Figure 13.17 Input-output waveforms of channel
continuous behavior. Now, we choose the signal vector as consisting of the position and SM A 5[,,_1] uh]
a l w 0.0
$ l E -0.2-i
___ Sample number, n g Sample number, n
é l a
U a
50-0 l——v #—r—1—r -r “T” r-a 41-4-1 1-1 i r r—r—1—-1 v 1
Sample number, n Sample number, n
Figure 13.18 Kalman ﬁlter estimate a Figure 13.19 Kalman ﬁlter gains
_ _ _ _ _ ' velocity components or
Without the noise perturbations u,[n], uyln] the velocities would be constant, and hence [n]
the vehicle would be modeled as traveling in a straight line as indicated by the dashed  aw ]
line in Figure 13.21. From the equations of motion the position at time n is | s[n] = U”
ryh] ___ n/[n _ 1]+vy[n _ 11A (1375) an rom ( ) an ( )1 1s seen o sa 1s y
where A is the time interval between samples. In this discretized model of the equations l Tylnl = 0 1 0 A 7'11 l" _ ll + 0 (1315)
of motion the vehicle is modeled as moving at the velocity of the previous time instant n v1 [n] 0 0 1 O Ulln _ l] urln]
and then changing abruptly at the next time instant, an approximation to the true l v11 [n] 0 O O 1 vl/ln ' l] “ylnl
0.0a
_ 0.0a
Sample number, n
0 20 a0 40 s0 00 10 s0 90100
Sample number, n
Figure 13.20 Kalman ﬁlter minimum MSE
"v l"l """""" ' ' Vehicle track
Figure 13.21 Typical track of vehicle moving in given direction at
constant speed
The measurements are noisy observations of the range and bearing
ﬂ[n] = arctan 
3m] = 0011+ w5[n]. (13.77)
In general terms the observation equation of (13.77) is
where h is the function
Unfortunately, the measurement vector is nonlinear in the signal parameters. To esti-
mate the signal vector we will need to apply an extended Kalman ﬁlter (see (13.67)—
(13.71)). Since the state equation of (13.76) is linear, we need only determine
arctan
because A[n] is just A as given in (13.76). Differentiating the observation equation, we
have the Jacobian
Finally, we need to specify the covariances of the driving noise and observation noise.
If we assume that the Wind gusts, speed corrections, etc., are just as likely to occur
in any direction and with the same magnitude, then it seems reasonable to assign the
same variances to u,[n] and uy[n] and to assume that they are independent. Call the
common variance of. Then, we have
The exact value to use for a5 should depend on the possible change in the velocity
component from sample to sample since u,[n] = v, [n] — v, [n — 1]. This is just the
acceleration times A and should be derivable from the physics of the vehicle. In speci-
fying the variances of the measurement noise we note that the measurement error can
be thought of as the estimation error of  and  as seen from (13.77). We usu-
ally assume the estimation errors wﬂn], w5[n] to be zero mean. Then, the variance of
wﬂn], for example, is E(w%[n]) = E [(R[n] — R[n])2]. This variance is sometimes deriv-
able but in most instances is not. One possibility is to assume that E (wﬁnl) does not
depend on the PDF of R[n], so that E(w§,[n]) = E[(R[n] — R[n])2|R[n]]. Equivalently,
we could regard R[n] as a deterministic parameter so that the variance of w R[n] is just
the classical estimator variance. As such, if  were the MLE, then assuming long
data records and/or high SNRs, we could assume that the variance attains the CRLB.
Using this approach, we could then make use of the CRLB for range and bearing such
as was derived in Examples 3.13 and 3.15 to set the variances. For simplicity we usually
assume the estimation errors to be independent and the variances to be time invariant
(although this is not always valid). Hence, we have
In summary, the extended Kalman ﬁlter equations for this problem are, from
(13.67)—(13.71),
rqn] = MW. - 111131.] (c + H[n]M[n|n - 1]HT[n])_1
§[n|n] = §{n|n — l] + K[n](x[n] — h(§[n|n ~ 
Mlnlnl = (FKlnlﬂlnllMlnln-ll
where
Q = 0 0 a; 0
0 0 0 of,
“Shh = arctan Pym]
Final
position
True track
Initial
position
a
a
i Ideal track
True and straight line ry[n]
4s -10 -5 0 s 1o 15
True and straight line r,
Figure 13.22 Realization of vehicle track
HM _ Jem+em J¢w+em
¢m+ew
and the initial conditions are §{-1| — 1] = us, M[—1| — 1] = Cg- AS 811 example,
consider the ideal straight line trajectory shown in Figure 13.22 as a dashed line. The
coordinates are given by
for n = 0, 1,. . . , 100, where we have assumed A = 1 for convenience. From (13.75) this
trajectory assumes v, = —0.2, vy = 0.2. To accommodate a more realistic vehicle track
we introduce driving or plant noise, so that the vehicle state is described by (13.76)
with a driving noise variance of 0i = 0.0001. With an initial state of
(13.78)
which is identical to that of the initial state of the straight line trajectory, a realization
of the vehicle position [r1 [n] Ty [nHT is shown in Figure 13.22 as the solid curve. The
state equation of (13.76) has been used to generate the realization. Note that as time
increases, the true trajectory gradually deviates from the straight line. It can be shown
that the variances of v,[n] and vy[n] will eventually increase to inﬁnity (see Problem
13.22), causing TIM] and ry[n] to quickly become unbounded. Thus, this modeling
is valid for only a portion of the trajectory. The true range and bearing are shown
in Figure 13.23. We assume the measurement noise variances to be ail = 0.1 and
0g = 0.01, where ﬂ is measured in radians. In Figure 13.24 we compare the true
trajectory with the noise corrupted one as obtained from
$1M] =  cos [3 [n]
To employ an extended Kalman ﬁlter we must specify an initial state estimate. In
practice, it is unlikely that we will have knowledge of the position and speed. Thus, for
the sake of illustration we choose an initial state that is quite far from the true one or
and so as not to “bias” the extended Kalman ﬁlter we assume a large initial MSE or
M[—-1| — 1] = 100I. The results of an extended Kalman ﬁlter are shown in Figure 13.25
as the solid curve. Initially, because of the poor state estimate, the error is large. This
is also reﬂected in the MSE curves in Figure 13.26 (actually these are only estimates
based on our linearization). However, after about 20 samples the extended Kalman
ﬁlter attains the track. Also, it is interesting to note that the minimum MSE does
not monotonically decrease as it did in the previous example. On the contrary, it
increases for part of the time. This is explained by contrasting the Kalman ﬁlter with
the sequential LMMSE estimator in Chapter 12. For the latter we estimated the same
parameter as we received more and more data. Consequently, the minimum MSE
decreased or at worst remained constant. Here, however, each time we receive a new
data sample we are estimating a. new parameter. The increased uncertainty of the new
parameter due to the influence of the driving noise input may be large enough to offset
the knowledge gained by observing a new data sample, causing the minimum MSE
to increase (see Problem 13.23). As a ﬁnal remark, in this simulation the extended
Kalman ﬁlter appeared to be quite tolerant of linearization errors due to a poor initial
state estimate. In general, however, we cannot expect to be so fortunate. O
(a) Range
0 10 2o so 40 50 e0 10 so 90 10o
Sample number, n
(b) Bearing
ma] (degrees)
0 10 20 30 40 s0 e0 70 so 90 100
Sample number, n
Figure 13.23 Range and bearing of true vehicle track
Oj’ True track
Noise corrupted and true Ty [n]
Observed track
Noise corrupted and true r,[n]
Figure 13.24 True and observed vehicle tracks
Minimum MSE for 1-y[n] Minimum MSE for 1'1 [n] Kalman estimate and true ry[n]
5R /True track
_5_) Extended Kalman/
ﬁlter estimate
Kalman estimate and true r,
Figure 13.25 True and extended Kalman ﬁlter estimate
Sample number, n
Sample number, n
Figure 13.26 “Minimum” MSEs for r, and ryln]
References
Anderson, B.D.O., .l.B. Moore, Optimal Filtering, Prentice-Hall, Englewood Cliffs, N.J., 1979.
Chen, C.T., Introduction to Linear System Theory, Holt, Rinehart, and Winston, New York, 1970.
Gelb, A., Applied Optimal Estimation, l\i.I.T. Press, Cambridge, Mass, 1974.
Jazwinski, A.H., Stochastic Processes and Filtering Theory, Academic Press, New York, 1970.
Kennedy, 11.5., Fading Dispersive Communication Channels, J. Wiley, New York, 1969.
Mendel. .l.M., Lessons in Digital Estimation Theory, Prentice-Hall, Englewood Cliffs, N.J., 1987.
Van Trees, H.L., Detection, Estimation, and Modulation Theory III, J. Wiley, New York, 1971.
Problems
13.1 A random process is Gaussian if for arbitrary samples {s[n1], s[n2], . . . , s[n;,]} and
for any k the random vector s = [s[n1] s[n2] . . . s[nk]]T is distributed according to
a multivariate Gaussian PDF. If s[n] is given by (13.2), prove that it is a Gaussian
random process.
13.2 Consider a scalar Gauss-Markov process. Show that if u, = O and of = a: / ( 1 —
a2), then the process will be WSS for n Z O and explain why this is so.
13.3 Plot the mean, variance, and steady-state covariance of a scalar Gauss-Markov
process if a = 0.98, 0,2, = 0.1, u, = 5, and of = 1. What is the PSD of the
steady-state process?
13.4 For a scalar Gauss-Markov process derive a covariance propagation equation, i.e.,
a formula relating c,[m, n] to c,[n, n] for m Z n. To do so ﬁrst show that
c,[m, n] = am‘ var(s[n])
13.5 Verify (13.17) and (13.18) for the covariance matrix propagation of a vector Gauss-
Markov process.
13.6 Show that the mean of a vector Gauss-Markov process in general will grow with
n if any eigenvalue of A is greater than 1 in magnitude. What happens to the
steady-state mean if all the eigenvalues are less than 1 in magnitude? To simplify
matters assume that A is symmetric, so that it can be written as A = EL, /\,-v,-v,~T,
where v,- is the ith eigenvector of A and /\,- is the corresponding real eigenvalue.
13.7 Show that
as n —> oo if all the eigenvalues of A are less than 1 in magnitude. As in Problem
13.6, assume that A 1s symmetric. Hint: Examine eZA"C,A" ej = [A"C,A" ],-j,
where e,- is a vector of all zeros except for a one in the ith element.
Jrwvvw "Howe Wm- zmwawmm*
a,
13.8 Consider the recursive difference equation (for q < p)
Tin] = " i (1[/C]T'[n — [c] + u]n] +
Me
where u[n] is WGN with variance a5. (In steady-state this would be an ARMA
process.) Let the state vector be deﬁned as
Wlfele elfll ~ ‘A/‘(psﬁc-ﬁ) and is independent of  Also, deﬁne the vector
driving noise sequence as
at]
Rewrite the process as in (13.11). Explain why this is not a vector Gauss-Markov
model by examining the assumptions on u[n].
13.9 For Problem 13.8 show that the process may alternatively be expressed by the
diﬁerence equations
slnl = _ 2 al/‘Jlsln — k] + u[n]
ﬁolrtn Z 0].) Assume that s[—1] = [s[—p]s[—p +.1]._.s]—-1]]T ~ A/(MHCS) and
a we ° Serve elnl = Tin] + wln], where w[n] is WGN with variance 0f, , and
sl- ll» “[11], and w[n] are independent. Show how to set up the vector state-scalar
observation Kalman ﬁlter.
13.10 ‘Assume that we observe z[n] = A+w[n] for n = O, 1, _ _ _, where A is the realiza.
tion of a random variable with PDF N (0, of.) and w[n] is WGN with variance 02.
UfSEgb the dscalar state-scalar observation Kalman ﬁlter ﬁnd a sequential estimator
O _ a-Se on  fllll, - - ~ Jlnl} or A[n]. Solve explicitly for A[n], the Kalman
gain, and the minimum MSE.
13.11 In this problem we implement a scalar state-scalar observation Kalman ﬁlter
(See (13-33)—(13.42)). A computer solution is advised. If a = 0.9, a2 = 1 p = 0
0f = 1, ﬁnd the Kalman gain and minimum MSE if u i- a 7
a. 0?, = (0.9)"
c. of, = (1.1)".
Explain your results. Using a Monte Carlo computer simulation generate a real-
ization of the signal and noise and apply your Kalman ﬁlter to estimation of the
signal for all three cases. Plot the signal as well as the Kalman ﬁlter estimate.
13.12 For the scalar state-scalar observation Kalman ﬁlter assume that 0,2, = 0 for all
n so that we observe s[n] directly. Find the innovation sequence. Is it white?
13.13 In this problem we show that the same set of equations result for the scalar
state-scalar observation Kalman ﬁlter even if E(s]—1]) 96 0. To do so let s’ [n] =
s[n] — E(s]n]) and ac’]n] = ac]n] — E(z]n]), so that equations (13.38)—(13.42) apply
for s’ Now determine the corresponding equations for s[n]. Recall that the
MMSE estimator of 6 + c for c a constant is 0+ c, where é is the MMSE of 9.
13.14 Prove that for the scalar state-scalar observation Kalman ﬁlter
for large enough n or in steady-state. Why is this reasonable?
13.15 Prove that the optimal l-step predictor for a scalar Gauss-Markov process s[n]
§[n + lln] = a'.§[n|n]
where §[n]n] and .§]n + lln] are based on {ac[0],:c[1], . . .,ac[n]}.
13.16 Find the transfer function of the steady-state, scalar state-scalar observation
Kalman ﬁlter if a = 0.8, 0,5] = 1, and a2 = 1. Give its time domain form as a
recursive difference equation.
13.17 For the scalar state-scalar observation Kalman ﬁlter let a = 0.9, a: = 1, a2 = 1
and ﬁnd the steady-state gain and minimum MSE by running the ﬁlter until
convergence, i.e., compute equations (13.39), (13.40), and (13.42). Compare your
results to those given in Section 13.5.
13.18 Assume we observe the data
for k = 0,1,...,n, where A is the realization of a random variable with PDF
A/(ihhai), 0 < r < 1, and the w[lc]’s are samples of WGN with variance a2.
Furthermore, assume that A is independent of the w[lc]’s. Find the sequential
MMSE estimator of A based on {z[0],:c[1], . . . ,w[n]}.
13.19 Consider the vector state—vector observation Kalman ﬁlter for which H[n] is
assumed to be invertible. If a particular observation is noiseless, so that C[n] = 0,
ﬁnd §[n]n] and explain your results. What happens if C[n] —> o0?
13.20 Prove that the optimal l-step predictor for a vector Gauss-Markov process s]n]
Where glnlnl and 5i" i‘ llnl are baﬁed on {x[0],x]1],...,x[n]}. Use the vector
state—vector observation Kalman ﬁlter.
13.21 In this problem we set up an extended Kalman ﬁlter for the frequency tracking
application. Speciﬁcally, we wish to track the frequency of a sinusoid in noise.
The frequency is assumed to follow the model
folnl =af0]"—1]+u[n] n>0
Where  is  with variance a5 and f0]—1] is distributed according to
N (ll 1010M) and ls lndependent of  The observed data are
17in] = ¢<>S(21rf@[n])+ w[n] n Z 0
where w[n] is WGN with variance a2 and is independent of u[n] and fo[—1)]. Write
down the extended Kalman ﬁlter equations for this problem.
13.22 For the vehicle position model in Example 13.4 we had
Darin] = v,[n — 1] + u,[n]
where u,[n] is WGN with variance a5. Find the variance of v, [n] to Show that
1t increases with n. What might be a more suitable model for v, [n]? (See alsg
[Anderson and Moore 1979] for a further discussion of the modeling issue.)
13.23 For the scalar state-scalar observation Kalman ﬁlter ﬁnd an expression relating
M[n|n] to M[n - 1|n -1]. Now, let a _—. 0,9, g5 -_- 1, and U2 = n + 1_ If
M [-1] — 1] = 1, determine M [n|n] for n 2 0. Explain your results.
Appendix 13A
Vector Kalman Filter Derivation
We derive the vector state—vector observation Kalman ﬁlter with the vector state-
scalar observation being a special case. Theorem 13.2 contains the modeling assump-
tions. In our derivation we will assume us = 0, so that all random variables are zero
mean. With this assumption the vector space viewpoint is applicable. For p8 76 0 it
can be shown that the same equations result. The reason for this is a straightforward
generalization of the comments made at the end of Appendix 12A (see also Problem
The properties of MMSE estimators that we will use are
1. The MMSE estimator of 9 based on two uncorrelated data samples x1 and x2,
assuming jointly Gaussian statistics (see Section 11.4), is
é = E(0]x1,x2)
= E(9lx1)+E(@|x2)
if 0,x1,x2 are zero mean.
2. The MMSE estimator is linear in that if 6 = A101 + A292, then
é E(6|x)
E(A101 + A292|x)
A1E(01]X) + AgE(62]X)
The derivation follows that for the scalar state-scalar observation Kalman ﬁlter with
obvious adjustments for vector quantities. We assume p8 = 0 so that all random vectors
are zero mean. The MMSE estimator of s[n] based on {x[0],x[1], . . . ,x[n]} is the mean
of the posterior PDF
s[n]n] = E(s[n]|x[0],x[1], . . . ,x[n]). (13A.1)
But s[n],x[0], . . . ,x[n] are jointly Gaussian since from (13.13) s[n] depends linearly on
{s[—1],u[0], . . . ,u[n]} and x['n] depends linearly on s[n],w[n]. Hence, we have a linear
dependence on the set of random vectors S = {s[—1],u[0],...,u[n],w]0],...,w[n]},
where each random vector is independent of the others. As a result, the vectors in
S are jointly Gaussian and any linear transformation also produces jointly Gaussian
random vectors. Now, from (10.24) with zero means we have
s[n|n] = CQICQQX (13A.2)
where 0 = s[n] and x = [xT [0] xT[1] . . .xT[n]]T and is seen to be linear in x. To determine
a recursive in time algorithm we appeal to the vector space approach. Let
and let ﬂn] = x[n] — i[n|n — 1] be the innovation or the error incurred by linearly
estimating x[n] based on X[n — 1]. Now we can replace (13A.l) by
élnlnl = E(§l"l|Xl" —1l,il"])
since x[n] is recoverable from X[n -1] and  (see comments in Section 13.4), Because
X[n —- 1] and i[n] are uncorrelated, we have from property 1
51'4"] = E(Sl"l|Xl" * 1l)+ Eﬁlnllilnl)
= s[n]n -— 1] + E(s[n]]i[n]).
The predicted sample is found from (13.12) as
s[n]n -1] = E(As[n — 1] + Bu[n]|X[n — 1])
= As[n — 1|n — 1] + BE(u]n]|X[n — 1])
since the second term is zero. To see why, note that
since from (13.13) X[n— 1] depends on {s]—1],u[0],. . . ,u[n—1],w[0],w[1],. . . ,w]n—1]}
which are all independent of u]n]. We now have
s[n]n] = s[n]n — 1] +  (13A.3)
where
s]n]n -1] = A§[n — 1|n — 1]. (13A.4)
To determine E (s[n]]i[n]) we recall that it is the MMSE estimator of s[n] based on 
Since s[n] and i[n] are jointly Gaussian, we have from (10.24)
E(s[n]|i[n]) = CﬁCg-lihz].
Letting K[n] = CMECQQ be the gain matrix, this is rewritten as
so that from property 2
= H[n]s[n]n — 1] (13A-5l
since w[n|n — 1] = 0 due to the independence of w[n] with {x[0],x[1], - - - Ail" - 
E(s[n]]i[n]) = Khzlbrln] - Hlnlﬁlnln — 1])
and from (13A.3) we now have
s[n]n] = s[n]n —— 1] +  - Hlnlélnl" _ ll) (HA6)
where A
The gain matrix K[n] is
= E(s[n]iT[n])E(i]n]iT]n])—l.
To evaluate it we will need the results
The ﬁrst result follows from the innovation i[n] = x[n] —i[n|ﬂ -1l = Xlnl “Hlnlélnlfl- 1]
being uncorrelated with the past data and hence with s[n|n—1]. The S€COI1d result 15 due
o e sum tion that s n — s n n —— 1 is a linear combination of {s[—1],ll]0], - - - , lllnl»
  w[np-— 1]}, which is iniieLenddnt of w]n]. We ﬁrst examine C“; us1ng results 1
and 2 and also (13A.5).
= E [s[n](X[n] - Hlnlélnl" - lllTl
= E [W] - s[n|n - 1])(H[n]s[n] - H[n]§[nln — 11+ WIHDT]
and using result 2 and (13A.5) again
Cﬁ = E l(Xl"] - i]"]" - 1])(X]"] - i]"]" - 111T]
= E ](X]"] - H]"]§]"]" -1])(Xl"] - H]"]§]"]" - lllT]
= E [(H[n]s[n] - H[n]§[n|n — 1] + w[n])
- (H[n]s[n] — H[n]§[n|n — 1] + w[n])T]
Thus, the Kalman gain matrix is
K[n] = M[n|n —-1]HT[n](C[n]+ H[n]M[n]n — 1]HT[n])_1,
To evaluate this we require M[n|n — l], so that by using (13A.4)
M]"]" - 1] '—' E l($l"] - §]"|" -1])($]"]-§]"|" - 111T]
= E ](As[n — 1] + Bu[n] — A§[n — 1|n —- 1])
- (As[n — 1] + Bu[n] - A§[n — 1|n — l])T]
E [(A(s[n — 1] — s[n - l]n — 1]) + Bu[n])
- (A(s[n — 1] —— s[n — 1 ] n — 1]) + Bu[n])T]
and using the result
E [(s[n - 1] - §[n - l|n —1])uT[n]] = 0 (13A.7)
we have
The equation (13A.7) is true because s[n— 1] —s[n— 1]n— 1] depends on {s[——1],u[0], . . . ,
u[n — 1],w[0], . . . ,w[n — 1]}, which are independent of u[n]. Finally, to determine the
recursion for M[n|n] we have from (l3A.6)
E [(5["] — §["|"])(5["] — 5]"]"])T]
E — s[n]n — 1] — K[n](x[n] — H[n]s[n|n — 
- (s[n] — s[n]n — 1] — K[n](x[n] — H[n]§[n]n — 1]))T]
E l(S["] — 51"!" - 1])($["] - §]"]" —1])T]
— E ](S[n] — §[n|n —  — H[n]§[n]n -— 1])T] KT[n]
- K]"]E ](Xl"] - Hl"]§]"|" - 1])($]"] — §]"|" - IDT]
+ K]"]E ](X]"] - Hl"]§l"]" - 1])(Xl"] " Hl"]§l"]" - 1])T] KT]"]-
The ﬁrst term is M[n|n — 1], the second expectation is C“; using result 1, the third
expectation is CZ}, and the last expectation is Cﬁ. From the derivation for K[n] we
have that
where
Hence,
which completes the derivation.
M[n]n — 1] — CsiKTln] _ Klvllcfi '1' C-‘JiCi-iaicgi
(I — K["]H["])M["l" — 1]
~ - - --»-,- 4- -.-_-»= wwwyw‘ ~nmwwvwwwwmx~vwawwwnurnmnugwwwwnm we’:
Appendix 13B
Extended Kalman Filter Derivation
To derive the extended Kalman ﬁlter equations we need to ﬁrst determine the equa
tions for a modiﬁed state model that has a known deterministic input or
Sh] = Ash» —11+B11[nl+v[n] n 2 o (135.1)
w ere Vlnl {S kmlwn- The Presence 0f v[n] at the input Wlll produce a deterministic
component in the output, so that the mean of s[n] will no longer be zero We assum
th t - = ~ _ ' e
a   Oﬂso that 1f vlnl — o» then E(S]n]) = 0. Hence, the effect of the
deterministic input is to produce a nonzero mean signal vector
SW = 5'1"] + Ehlnl) (133.2)
Where s lnl 15 the Value 0f S[n] when v[n] = 0. Then, we can write a state equation for
the zero mean signal vector s’ [n] = Sh] _ E(s[n]) as
Note that the mean satisﬁes
Eﬁlnl) = ABM" —1l)+ vlnl (133.3)
which follows from (13B.1). Likewise, the observation equation is
H[n]s'[n] + w]n] + H[n]E(s[n])
or letting
we have the usual observation equation
The Kalman ﬁlter for s’ [n] can be found using (13.58)—(13.62) with x[n] replaced by
x’ Then, the MMSE estimator of s[n] can easily be found by the relations
s'[n]n — 1] = s[n]n — 1] — E(s[n])
s’]n —1|'n -1] = §]n — 1]n — 1] — E(s[n — 
Using these in (13.58), we have for the prediction equation
or  - 1] -— E(s[n]) = A — 1]n — 1] — E(s]n — 
and from (13B.3) this reduces to
For the correction we have from (13.61)
or slum - Eeni)
= 5111]" — ll — Eﬁlnl) + Klnl [Xlﬂl — ﬁlnlElslnll — 3111161111" — ll - E(S["l))l-
This reduces to the usual equation
 = s[n|n — 1] + K]n](x[n] —- H[n]s[n]n — 
The Kalman gain as well as the MSE matrices remain the same since the MMSE
estimator will not incur any additional error due to known constants. Hence, the only
revision is in the prediction equation.
Returning to the extended Kalman ﬁlter, we also have the modiﬁed observation
equation
where z[n] is known. With x’ [n] = x[n] — z]n] we have the usual observation equation.
Hence, our two equations become, upon replacing A by A[n],
s[n|n] = s[n]n — 1] + K]n](x[n] — z[n] —~ H[n]s[n]n -— 
v[n] = a(s[n — 1|n — 1]) — A]n — l]s[n — 1]n — 1]
z['n] = h(§[n]n —- 1]) — H]n]s]n]n -1]
so that ﬁnally
§[n]n —- 1] = a(§[n — l]n — 1])
s[n]n] = s[n]n - 1] + K[n](x]n] — h(§[n]n — 
Chapter 14
Summary of Estimators
14.1 Introduction
The choice of an estimator that will perform well for a particular application depends
upon many considerations. Of primary concern is the selection of a good data model.
It should be complex enough to describe the principal features of the data, but at the
same time simple enough to allow an estimator that is optimal and easily implemented.
We have seen that at times we were unable to determine the existence of an optimal
estimator, an example being the search for the MVU estimator in classical estimation.
In other instances, even though the optimal estimator could easily be found, it could
not be implemented, an example being the MMSE estimator in Bayesian estimation.
For a particular problem we are neither assured of ﬁnding an optimal estimator or,
even if we are fortunate enough to do so, of being able to implement it. Therefore, it
becomes critical to have at one’s disposal a knowledge of the estimators that are opti-
mal and easily implemented, and furthermore, to understand under what conditions we
may justify their use. To this end we now summarize the approaches, assumptions, and
for the linear data model, the explicit estimators obtained. Then, we will illustrate the
decision making process that one must go through in order to choose a good estima-
tor. Also, in our discussions we will highlight some relationships between the various
estimators.
14.2 Estimation Approaches
We ﬁrst summarize the classical approaches to estimation in which the unknown p x 1
parameter vector 0 is assumed to be a deterministic constant, followed by the Bayesian
approaches in which 0 is assumed to be the realization of a random vector. In the
classical approach the data information is summarized by the probability density func-
tion (PDF) p(x; 0), where the PDF is functionally dependent on 0. In contrast to this
modeling, the Bayesian approach augments the data information with a prior PDF
11(0) which describes our knowledge about 9 (before any data are observed). This is
.~ wm-ww-nvrzwrsuns-w-nsgwi» Hairs,“
summarized by the joint PDF p(x, 0) or, equivalently, by the conditional PDF p(x|0)
(data information) and the prior PDF p(0) (prior information).
1. Cramer-Rao Lower Bound (CRLB)
a. Data Model/Assumptions
PDF p(x; 0) is known.
b. Estimator
If the equality condition for the CRLB
is satisﬁed, then the estimator is
where 1(9) is a p X p matrix dependent only on 0 and g(x) is a p-dimensional
function of the data x.
c. Optimality/Error Criterion
0 achieves the CRLB, the lower bound on the variance for any unbiased
estimator (and hence is said to be efficient), and is therefore the mimmum
variance unbiased (MVU) estimator. The MVU estimator is the one whose
variance for each component is minimum among all unbiased estimators.
d. Performance
ft is unbiased or
and has minimum variance
var(9.~)=[I“(6)].. i=1,2,...,p
where
8 ln p(x; 9) 3 ln p(x; 9)
ll(9)lij = E aei 861
e. Comments
An efficient estimator may not exist, and hence this approach may fail
f. Reference
Chapter 3
a. Data Model/Assumptions
PDF p(x;9) is known.
b. Estimator
i. Find a sufficient statistic T(x) by factoring PDF as
MK; 9) = 9(T(X). 9W0‘)
where T(x) is a p-dimensional function of x, g is a function depending
only on T and 0, and h depends only on x.
ii. If E[T(x)] = 0, then é = T(x). If not, we must ﬁnd a p-dimensional
function g so that E[g(T)] = 0, and then 0 = g(T).
c. Optimality/Error Criterion
0 is the MVU estimator.
d. Performance
6, for i = 1,2,...,p is unbiased. The variance depends on the PDF—- no
general formula is available.
e. Comments
Also, “completeness” of sufficient statistic must be checked. A p-dimensional
sufficient statistic may not exist, so that this method may fail.
f. Reference
Chapter 5
3. Best Linear Unbiased Estimator (BLUE)
a. Data Model/A ssamptions
E(x) = no
where H is an N X p (N > p) known matrix and C, the covariance matrix of
x, is known. Equivalently, we have
where E(w) = 0 and Cw = C.
b. Estimator
é =(nTc-1ii)-1HTc-1x.
c. Optimality/Error Criterion
9,» for i = 1,2,. . . ,p has the minimum variance of all unbiased estimators
that are linear in x.
d. Performance
6, for i = 1, 2, . . . ,p is unbiased. The variance is
var(0A,-)=[(HTC'1H)“‘]ii i= l,2,...,p.
e- Comments A ' c. Optimality/Error Criterion
If w is a Gaussian random vector so that w ~ N (0, C), then 9 is also the None in general.
MVU estimator (for all linear or nonlinear functions of x). d Performance
f‘ Reference Depends on PDF of w—no general formula is available.
Chapter 6 '
e. Comments
4- Ma-Ximllm LikeIihOOd Estimﬂml‘ (MLE) ' The fact that we are minimizing a LS error criterion does not in general
  translate into minimizing the estimatign error, Also, if w is a Gaussian
a’ Data Model/Assumptions random vector with w ~ N (0,021), then the LSE is equivalent to the MLE.
PDF p(x; 9) is known.
b. Estimator l
9 is the value of 9 maximizing p(x;9), where x is replaced by the observed
data samples‘ 6. Method of Moments
c. Optimality/Error Criterion _
Not optimal in general. Under certain conditions on the PDF, however, the a- Data Model/Assumptions
MLE is efficient for large data records or as N —-> oo (asymptotically). Hence, There are p moments ii,- = E for i = 1, 2, . . . , p, which depend on 9 in
asymptotically it is the MVU estimatim > a known way. The entire PDF need not be known.
f. Reference
Chapter 8
d. Performance b. Estimator _ _
For ﬁnite N depends on PDF-no general formula is available. Asymptoti- q If H = hm), Where h is an invertible Pdimensmnal functmn 0f 9 and I1 =
cally, under certain conditions [a1 it; . . . up]? then
0&A/(e,1-1(a)). 0”” i“)
e. Comments where i Ezvjoirin]
If an MVU estimator exists, the maximum likelihood procedure will produce if E1331 rzini
f. Reference 3
Chapter 7 7b- EL-Ol rP[n]
5‘   l c. Optimality/Error Criterion
a. Data Model/Assumptions None in general‘
d. Performance
.r[n] = s[n; 9] + w[n] n = 0,1,... ,N — 1 For ﬁnite N it depends on the PDF of x. However, for large data records
where the signal s[n; 9] depends explicitly on the unknown parameters. Equiv- (asympmtleanyli if 6" = gim)’ then
alently, the model is A
x = 5(9) +w E( i) = a0»)
where s is a known N -dimensional function of 9 and the noise or perturbation V3119‘) :  T C . 
w has zero mean. 1 8ft ‘hi,’ l‘ 3ft p=p,
b. EsAtimator
9 is the value of 9 that minimizes
l fori=1,2...,p.
 ___ (x _ s(6))T(x _  li e. COTFLTVLBTZtS
Usually very easy to implement.
-_-  - s[n;9])2. f. Reference
n=0 Chapter 9
a. Data Model/Assumptions
The 101m PDF 0f L9 0T P(X,9) is known, where 9 is now considered to be
a. random vector. Usually p(x|9) is speciﬁed as the data model and
. * 11(9)
the prior PDF for 9, so that p(x, 0) = p(x|g)p(g)_ as
b. Estimator
é = awn)
where the expectation is with respect to the p0sterigr PDF
Mx) _ r>(1<|9)r>(@)
‘ fr>(=<|9)r>(@)d9 »
If x,9 are jointly Gaussian,
é = w) + eta-xx - Em)- (14.1)
c. Optimality/Error Criterion
9,- minimizes the Bayesian MSE
Bmse(9,-) = E [(0, ~ 9,92] ,» , 1,1,,” (14.2)
where the expectation is with respect to p(x, 9i).
d. Performance
The erml‘ 5i = 6i —  has zero mean and variance
are» = Bmse<e1>= / [Ce11]¢.-p(X)dx (14.3)
where C is th ' ' - - .
9|, e covariance matrix of 9 conditloned on x, or of the posterior
PDF Pwlxl- If X, 9 are jointly Gaussian, then the error is Gaussian with zero
mean and variance
valiei) = Bmseiéi) = [C00 — C9IC;I1CIQ].. .
e. Comments
In the non-Gaussian case, this will be difficult to implement
f. Reference
Chapters 10 and ll
8. Maximum A Posteriori (MAP) Estimator
a. Data Model/Assumptions
Same as for the MMSE estimator.
b. Estimator
9 is the value of 9 that maximizes p(9|x) or, equivalently, the value that
maximizes p(x|9)p(9). If x,9 are jointly Gaussian, then 9 is given by (14.1).
c. Optimality/Error Criterion
Minimizes the “hit-or-miss” cost function.
d. Performance
Depends on PDF—no general formula is available. If x,9 are jointly Gaus-
sian, then the performance is identical to that of the MMSE estimator.
e. Comments
For PDFs whose mean and mode (the location of the maximum) are the
same, the MMSE and MAP estimators will be identical, i.e., the Gaussian
PDF, for example.
f. Reference: Chapter 11
9. Linear Minimum Mean Square Error (LMMSE) Estimator
a. Data Model/Assumptions
The ﬁrst two moments of the joint PDF p(x,9) are known or the mean and
covariance
é = 12(0) + 09,0; (x - Em).
c. Optimality/Error Criterion
9,- has the minimum Bayesian MSE (see (14.2)) of all estimators that are
linear functions of x.
b. Estimator
d. Performance
The error e,- = 9, — 9,- has zero mean and variance
var(e,-) = Bmse(9,») = [C99 — CQIC; C16] it. .
e. Comments
If x, 9 are jointly Gaussian, this is identical to the MMSE and MAP estima-
f. Reference: Chapter 12
The reader may observe that we have omitted a summary of the Kalman ﬁlter. This is
because it is a particular implementation of the MMSE estimator and so is contained
within that discussion.
14.3 Linear Model
lVhen the linear model can be used to describe the data, the various estimation ap-
proaches yield closed form estimators. In fact, by assuming the linear model we are able
to determine the optimal estimator as well as its performance for both the classical and
Bayesian approaches. We ﬁrst consider the classical approach. The classical general
linear model assumes the data to be described by
where x is an N X 1 vector of observations, H is a known N >< p observation matrix
(N > p) of rank p, 9 is a p X 1 vector of parameters to be estimated, and w is an N >< 1
noise vector with PDF A/(O, C). The PDF of x is
p(x;9) =   exp —§(x — H9)TC'1(x — H9) . j (14.4)
1. Cramer-Rao Lower Bound
  = (HTC“H)(9 _ a)
where A
0 =(HTc~1H)-1HTc-‘x (14.5)
so that 9 is the MVU estimator (and also efficient) and has minimum variance
given by the diagonal elements of the covariance matrix
c, = I-lw) = (HTC"H)"1.
2. Rao-Blackwell-Lehmann-Scheffe
A factorization of the PDF will yield
p(x;9) =   exp {—é1-[(9 — 6A)THTC~1ﬂ(9 — 
9(T(X)»9)
- exp {—é1-[(x — H9)TC"1(x —- 
h(x)
where A
9 = (HTC“H)'1HTC‘1x.
The sufficient statistic is T(x) = 9, which can be shown to be unbiased as well as
complete. Thus, it is the MVU estimator.
3. Best Linear Unbiased Estimator
Now we will have the identical estimator as in the previous two cases since 9 is
already linear in x. However, if w were not Gaussian. 9 would still be the BLUE
but not the MVU estimator. Note that the data modeling assumption for the
BLUE is satisﬁed by the general linear model.
4. Maximum Likelihood Estimator
To ﬁnd the MLE we maximize p(x;9) as given in (14.4) or, equivalently, we
minimize
(x - H9)TC"‘(x - H6).
This leads to A
a = (rlTdlnylnTc-‘x
which we know to be the MVU estimator. Thus, as expected, since an efficient
estimator exists (satisﬁes the CRLB), then the maximum likelihood procedure
produces it.
5. Least Squares Estimator
Viewing H9 as the signal vector s(9), we must minimize
1(9) = (X — S(9))T(X — 5(9))
= (x — H9)T(x — H9)
which is identical to the maximum likelihood procedure if C = 021. The LSE
is 9 = (HTIU-IHTX, which is also the MVU estimator if C = 021. If C 76 021,
then 9 will not be the MVU estimator. However, if we minimize the weighted LS
criterion
ma) = (x _ H9)TW(x - H0)
where the weighting matrix is C“, then the resultant estimator will be the MVU
estimator of (14.5). Finally, if w is not Gaussian, the weighted LSE would still be
9 as given by (14.5), but it would only be the BLUE.
6. Method of Moments
This approach is omitted since the optimal estimator is known.
The results are summarized in Table 14.1.
Proceeding to the Bayesian linear model, we assume that
where x is an N >< 1 vector of observations, H is a known N >< p observation matrix
(with N g p possibly), 9 is a p >< 1 random vector with PDF N(I.l6, Cg), and w is an
N >< 1 noise vector independent of 9 and having PDF N (0, Cw). The conditional PDF
ofx is 1 1
-_— a _-- - H6 T *1 -H0
pew) (m, d“, (Cwexp 2<x > c. (x >
TABLE 14.1 Properties of é for Classical General
Linear Model
Model: x = H0 + w
Assumptions: E(w) = 0
Estimator: é = (nTc-lnylnTc-‘x
w ~ Gaussian
Properties (linear model) w ~ Non-Gaussian
Efficient *
Sufficient Statistic *
WLS (W = C“) * *
* Property holds.
and the prior PDF of 0 is
(27r)§det§(C9)eXP i He) Ce (9 Ila)
10(9) = 2
The posterior PDF p(9)x) is again Gaussian with mean and covariance
E(@|x) = n, + c@HT(nc,nT + c.,)-1(x - Hm) (14.6)
= n, + (C? + HTC;1H)‘1HTC;1(x - Hm) (14.1)
00.. = Co ~ CQHTHICQHT + Cw)—1I-ICQ (14.8)
= (cgl + HTCQH)“. (14.9)
7. Minimum Mean Square Error Estimator
The MMSE estimator is just the mean of the posterior PDF given by (14.6) or
(14.7). The minimum Bayesian MSE or E((6,- — 6A,»)2), is from (14.3),
Bmse(éi) = [Coplu
since Cg); does not depend on x (see (14.8)).
Because the location of the peak (or mode) of the Gaussian PDF is equal to the
mean, the MAP estimator is identical to the MMSE estimator.
9. Linear Minimum Mean Square Error Estimator
Since the MMSE estimator is linear in x, the LMMSE estimator is just given by
(14.6) or (14.7).
Hence, for the Bayesian linear model the MMSE estimator, MAP estimator, and the
LMMSE estimator are identical. A last comment concerns the form of the estimator
when there is no prior information. This may be modeled by letting C51 —> 0. Then,
from (14.7) we have
é = (HTC§,1H)'1HTC;1x
which is recognized as having the identical form as the MVU estimator for the classical
general linear model. Of course, the estimators cannot really be compared since they
have been derived under different data modeling assumptions (see Problem 11.7). How-
ever, this apparent equivalence has often been identiﬁed by asserting that the Bayesian
approach with no prior information is equivalent to the classical approach. When viewed
in its proper statistical context, this assertion is incorrect.
14.4 Choosing an Estimator
We now illustrate the decision making process involved in choosing an estimator. In
doing so our goal is always to ﬁnd the optimal estimator for a given data model. If this
is not possible, we consider suboptimal estimation approaches. We will consider our
old friend the data set
where the unknown parameters are {A[0],A[1], . . .,A[N —  We have allowed the
parameter A to change with time, as most parameters normally change to some extent
in real world problems. Depending on our assumptions on A[n] and w[n], the data may
have the form of the classical or Bayesian linear model. If this is the case, the optimal
estimator is easily found as explained previously. However, even if the estimator is
optimal for the assumed data model, its performance may not be adequate. Thus, the
data model may need to be modiﬁed, as we now discuss. We will refer to the flowchart
in Figure 14.1 as we describe the considerations in the selection of an estimator. Since
we are attempting to estimate as many parameters as data points, we can expect poor
estimation performance due to a lack of averaging (see Figure 14.1a). With prior
knowledge such as the PDF of 9 = [A[0] A[1], . . . A[N — 1]]T, we could use a Bayesian
approach as detailed in Figure 14.1b. Based on the PDF p(x,9) we can in theory ﬁnd
the MMSE estimator. This will involve a multidimensional integration and may not in
practice be possible. Failing to determine the MMSE estimator we could attempt to
maximize the posterior PDF to produce the MAP estimator, either analytically or at
least numerically. As a last resort, if the ﬁrst two joint moments of x and 0 are available,
we could determine the LMMSE estimator in explicit form. Even if dimensionality is
not a problem, as for example if A[n] = A, the use of prior knowledge as embodied
by the prior PDF will improve the estimation accuracy in the Bayesian sense. That
is to say, the Bayesian MSE will be reduced. If no prior knowledge is available, we
will be forced to reevaluate our data model or else obtain more data. For example, we
might suppose that A[n] = A or even A[n] = A + Bn, reducing the dimensionality of
the problem. This may result in bias errors due to modeling inaccuracies, but at least
Signal processing F_
Yes LMMSE
estimator
' Dimensionality Yes
Bayesian
a problem
approach
Prior
knowledge
Compute mean
posterior PDF
Maximize
posterior PDF
MMSE estimator
Prior
, Bayesian
i knowledge
approach New data model No
take more data
MAP estimator
Classical
(b) Bayesian approach
(a) Classical versus Bayesian
Figure 14.1 Decision-making process in estimator selection
Signal
in noise
Yes
the variability of any resultant estimator would be reduced. Then, we could resort
to a classical approach (see Figure 14.1c). If the PDF is known, we ﬁrst compute
the equality condition for the CRLB, and if satisﬁed, an efficient and hence MVU C3161; MVU estimam, N° L51;
estimator will be found. If not, we could attempt to ﬁnd a sufficient statistic, make it sans
unbiased, and if complete, this would produce the MVU estimator. If these approaches
fail, a maximum likelihood approach could be tried if the likelihood function (PDF
with x replaced by the observed data values) can be maximized analytically or at least Complete Make
numerically. Finally, the moments could be found and a method of moments estimator 51:51???‘ unblgsed MVU eslimam‘
tried. Note that the entire PDF need not be known for a method of moments estimator.
If the PDF is unknown but the problem is one of a signal in noise, then either a BLUE
or LS approach could be tried. If the signal is linear in 9 and the ﬁrst two moments of
the noise are known, a BLUE can be found. Otherwise, a LS and a possibly nonlinear
LS estimator must be employed. '
In general, the choice of an appropriate estimator for a signal processing problem
should begin with the search for an optimal estimator that is computationally feasible.
If the search proves to be futile, then suboptimal estimators should be investigated.
exist
First two
noise moments
Evaluate Yes
Evaluate
method of Moments
moments estimator
estimator
(c) Classical approach
Chapter 15
Extensions for Complex Data and
Parameters
15.1 Introduction
For many signal processing applications the data samples are more conveniently mod-
eled as being complex—typically the concatenation of two time series of real data into
a single time series of complex data. Also, the same technique can be useful in rep-
resenting a real parameter vector of dimension 2 X 1 by a single complex parameter.
In doing so it is found that the representation is considerably more intuitive, is ana-
lytically more tractable, and lends itself to easier manipulation by a digital computer.
An analogous situation arises in the more convenient use of a complex Fourier series
composed of complex exponentials for a real data signal as opposed to a real Fourier
series of sines and cosines. Furthermore, once the complex Fourier series representation
has been accepted, its extension to complex data is trivial, requiring only the Hermi-
tian symmetry property of the Fourier coefficients to be relaxed. In this chapter we
reformulate much of our previous theory to accommodate complex data and complex
parameters. In doing so we do not present any new theory but only an algebra for
manipulating complex data and parameters.
15.2 Summary
The need for complex data models with complex parameters is discussed in Section
15.3. A particularly important model is the complex envelope representation of a real
bandpass signal as given by (15.2). Next, the tedious process of minimizing a function
with respect to the real and imaginary parts of a complex parameter is illustrated in
Example 15.2. It is shown how the introduction of a complex derivative greatly simpli-
ﬁes the algebra. Complex random variables are deﬁned and their properties described
in Section 15.4. The important complex Gaussian PDF for a complex random variable
is given by (15.16), while for a complex random vector the corresponding PDF is given
by (15.22). These PDFs assume that the real covariance matrix has the special form
sw) §(F)
F (Hz)
F0 m? -% a "’
(a) Fourier transform of real bandpass signal (b) Fourier transform of complex envelope
Figure 15.1 Deﬁnition of complex envelope
of (15.19) or, equivalently, the covariances satisfy (15.20) for the case of two random
variables. Properties of complex Gaussian random variables are summarized i1? that
section as well. If a complex WSS random process has an autocorrelation and a cross-
correlation that satisfy (15.33), then it is said to be a complex Gaussian WSS random
process. An example is the complex envelope of a real bandpass Gaussian random
process, as described in Example 15.5. The complex derivative of a real function with
respect to a complex variable is formally deﬁned in (15.40). The associated complex
gradient can be used to minimize Hermitian functions by employing (15.44)—(15.46).
If the Hermitian function to be minimized has a linear constraint on its parameters,
then the solution is given by (15.51). Classical estimation based on complex Gaussian
data with real parameters employs the CRLB of (15.52) and the equality condition of
(15.53). When the Fisher information matrix has a special form, however, the equality
condition of (15.54) can be used. This involves a complex Fisher information matrix.
One important example is the complex linear model in Example 15.9, in which (15.58)
i_s the efficient estimator and (15.59) is its corresponding covariance. Bayesian estima-
tion for the complex Bayesian linear model is discussed in Section 15.8. The MMSE
estimator is given by (15.64) or (15.65), while the minimum Bayesian MSE is given by
(15.66) or (15.67). For large data records an approximate PDF for a complex Gaussian
WSS random process is (15.68) and an approximate CRLB can be computed based on
(15.69). These forms are frequently easier to use in deriving estimators.
15.3 Complex Data and Parameters
The most cornmon example of the use of complex data in signal processing occurs in
radar/ sonar in which a bandpass signal is of interest. As shown in Figure 15.1a, if the
Fourier transform S (F) of a continuous-time real signal s(t) is nonzero over a band (F0-
B / 2, F0 + B / 2 ), then the essential information is contained in the complex envelope §(t)
whose Fourier transform 5' (F ) is shown in Figure 15.1b. (We will generally use a tilde
to denote a complex quantity when necessary to avoid confusion. Otherwise, whether
the quantity is real or complex will be evident from the context of the discussion.) The
3U) cos 21rF0t
(in phase) 501A) = gin]
(quadrature)
Within digital computer
Figure 15.2 Extraction of discrete-time complex envelope from bandpass signal
relationship between the Fourier transforms of the real bandpass signal and its complex
envelope is
S(F) =.§‘(F—F0)+S“(—(F+F0)). (15.1)
To obtain S (F ) we shift the complex envelope spectrum up to F = F0 and also, after
ﬁrst “ﬂipping it around” in F and conjugating it, down to F = —F0 . Taking the
inverse Fourier transform of (15.1) produces
s(t) = §(t) exp(j21rF0t) + [§(t) exp(j21rF0t)]"
s(t) = 2Re  exp(j21rF0t)]. (15.2)
Alternatively, if we let §(t) = sR(t) + js1(t), where R and I refer to the real and
imaginary parts of the complex envelope, we have
s(t) = 2sR(t) cos 21rF0t — 2s1(t) sin 21rF0t. (15.3)
This is also referred to as the “narrowband” representation, although it is actually
valid if F0 > B/2. Note that sR(t),s1(t) are bandlimited to B/2 Hz, as is evident
from Figure 15.1b. Hence, the bandpass signal is completely described by its complex
envelope. In processing s(t) or a noise corrupted version we need only obtain the
complex envelope. From a practical viewpoint the use of the complex envelope, which
is a low-pass signal, allows us to sample at the lower rate of B complex samples/ sec
as opposed to the higher rate of 2(F0 + B /2) real samples/sec for the real bandpass
signal. This results in the widely adopted means of processing a real bandpass signal
shown in Figure 15.2. The aim is to produce the samples of the complex envelope
in a digital computer. Note from (15.3) that the output of the low-pass ﬁlter for the
“in-phase” channel is
[2sR(t) cos2 21rF0t — 2s;(t) sin 21rF11t cos 21rF0t] LPF
= [sR(t) + s11(t) cos 41rF11t —— s1(t) sin 41rF11t]LPF
since the other signals have spectra centered about i2F0. The “LPF” designation
means the output of the low-pass ﬁlter in Figure 15.2. Similarly, the output of the
low-pass ﬁlter in the “quadrature” channel is s1(t). Hence, the discrete signal at the
output of the processor in Figure 15.2 is sR(nA) +js1(nA) = §(nA). It is seen that
complex data naturally arise in bandpass systems. An important example follows.
Example 15.1 - Complex Envelope for Sinusoids
It frequently occurs that the bandpass signal of interest is sinusoidal, as for example,
s(t) = i A,» cos(21rF,»t + (b1)
where it is known that F11 — B/2 f F,- f F0 + B/2 for all i. The complex envelope for
this signal is easily found by noting that it may be written as
s(t) = Re [i A,- exp [j(21rF,-t + 
= 2 Re l Y’ exp [j(21r(F, — F0)t + ¢,~)] exp[j21rF0t]
so that from (15.2)
w) = 2  eXp(J¢1-)e><r>li2r(F1 - Fem-
Note that the complex envelope is composed of complex: sinusoids with frequencies
F1 — F0, which may be positive or negative, and complex: amplitudes 421 exp(j¢,-). If we
sample at the Nyquist rate of F, = 1 / A = B, we have as our signal data
501A) = 2  exput) expu21r<F1 - Form.
Furthermore, we can let §[n] = §(nA), so that
§[n] = f A1exp(j21rf,»n)
where A, = Q21 exp(j¢,-) is the complex amplitude and f,- = (F, —- F0)A is the frequency
of the ith discrete sinusoid. For the purposes of designing a signal processor we may
assume the data model
m] = Z A1e><p<121f1n) + win]
where 1D[n] is a complex noise sequence. This is a commonly used model. O
A complex signal can also arise when the analytic signal §,,(t) is used to represent a
real low-pass signal. It is formed as
5.10) = 8U) +J7il8(i)]
where 7i denotes the Hilbert transform [Papoulis 1965]. The effect of forming this
complex signal is to remove the redundant negative frequency components of the Fourier
transform. Hence, if S (F ) is bandlimited to B Hz, then S,1(F) will have a Fourier
transform that is zero for F < 0 and so can be sampled at B complex samples / sec. As
an example, if
s(t) = Z A,- cos(21rP‘,-t + $1)
then it can be shown that
§1<t>=  exp [J'(21rF1~t+¢1-)]-
Now that the use of complex data has been shown to be a natural outgrowth of
bandpass signal processing, how do complex parameters come about? Example 15.1
illustrates this possibility. Suppose we wanted to estimate the amplitudes and phases
of the p sinusoids. Then, a possible parameter set would be {A1, $1, A2, $2, . . . , A1,, $1,},
which consists of 2p real parameters. But, equivalently, we could estimate the complex
parameters {A1 exp(j¢1), A2 exp(j¢2), . . . , A1, exp(j¢,,)}, which is only a p-dimensional
but complex parameter set. The equivalence of the two parameter sets is evident from
the transformation
A = AeXP(J'¢)
and inverse transformation
(b — arctan i
Another common example where complex parameters occur is in spectral modeling of
a nonsymmetric PSD. For example, an AR model of the PSD of a complex process i[n]
would assume the form [Kay 1988]
If the PSD is nonsymmetric about f = O, corresponding to a complex ACF and thus a Setting this equal t0 Zem and SQb/ing Pmduces
complex random process [Papoulis 1965], then the AR ﬁlter parameters {a[1], a[2], . . . ,
a[p]} will be complex. Otherwise, for real a]k]’s we would have Pii(— f) = Piﬂf It
seems, therefore, natural to let the ﬁlter parameters be complex to allow for all possible
spectra.
In dealing with complex data and / or complex parameters we can always just decom-
pose them into their real and imaginary parts and proceed as we normally would for
real data vectors and/or real parameter vectors. That there is a distinct disadvantage
in doing so is illustrated by the following example.
A = (sfsl + sgsg)_l(sfx1g + sgx1)
$15511 + s]"s1 0 _1 sjlixR + six!
Example 15.2 - Least Squares Estimation of Amplitude SRXI S’ xR
Suppose we w1sh to m1n1m1ze the LS error SRSR + SI SI
J(A) = Z 121111 - 14511111’ * SESR + sfsl
"=0 which is the minimizing solution. However, if we rewrite A in complex form as A1; +
over 1i, where i[n],1‘i, §]n] are all complex. A straightforward approach would decom- j/i1 = Al, we have
pose all complex quantities into their real and imaginary parts to yield
N-l $15511 + s]"s1
JI(AR1AI) = lizzlnl ‘l’ jlTIlnl - (AR +J941)($Rl"l +i81l"l)l2 _ (XR .1. jxI)T(sR _ jsl)
= (x1;[n] — Ags11[n]+ A1s1[n])2 + (r1]n] — Ags1[n] — A1s1;[n])2. Eihdgwn]
This is a standard quadratic form in the real variables AR and A1. Hence, we can let N '1
1R =11R1o1111111...1,,11v - 111T, x1 = 11,1o11,111...1,11v -111T, SR = 1511101511111... Z lﬂnll’
s1=1[N — l]]T, s1 = [s1[0] s1[1] . . . s1[N — l]]T, so that "=9
a result analogous to the real case. We can simplify the minimization process using
complex variables as follows. First, deﬁne a complex derivative as (see Section 15.6 for
further discussions) [Brandwood 1983]
3] _ 1 ( 3.] . 3J >
JI(AR1AI) = (KR - ARSR + AISI)T(XR - ARSR + A151)
‘i’ (X1 — ARS] —- A1SR)T(X1 — ARS] —— A1513)
or letting s1 = [s11 —- s1], s2 = [s1 s11], and A =[A1;A1]T
J'(A) = (x1; — s1A)T(xR — 51A) + (x1 —- s2A)T(x1 —— s-1A)
and note that aJ aJ
8A 0 1f and on y 1f 8A 0
Then, we must establish that (see Section 15.6 for the deﬁnition of a derivative of a
Taking the gradients and using (4.3) yields wmplex fHTICtiQII)
8T‘ _—. -2s]"x11 + 2sTs1A — 2s§x1 + ZsZWsQA. ] 3A - 1 (15-4)
8A _ 0 (15.5)
8A 8A + 8A (15 6)
as we shall do in Section 15.6. Now, we can minimize J using the complex derivative
N-l a _ _ _ _
= 2 5 (@1511? - 1110114115] - 145151010] + 1414x5101?)
(o - o ~ 511110-10] + 141511111) .
Setting this equal to zero and solving produces the same results. Thus, we see that
with the use of some easily established identities the algebra required for minimization
becomes quite simple. <>
15.4 Complex Random Variables and PDFs
As we have seen, it is quite natural to encounter complex signal models. To formulate
complex noise models as well we now extend many of our standard deﬁnitions and
results for real random variables to complex ones. A complex random variable i is
deﬁned to be i = u+ jv, where 11,12, the real and imaginary parts of i, are real random
variables. The tilde denotes a complex random variable since we will have need to
distinguish it from a real random variable. We will assume that the real random vector
[u v]T possesses a joint PDF, allowing us to deﬁne moments of the complex random
variable. The mean of i is deﬁned as
12(5) = 12(0) + 112(0) (15.1)
where the expectation is with respect to the marginal PDFs or p(u) and p(v), and the
second moment is deﬁned as
E(|i|2) = E(u2) + 12(1)’). (15.8)
The variance is deﬁned as
WU?) = E (If — 13(5)?) (15-9)
which can easily be shown to reduce to
var(i) = E(|i|2) - |E(5:)|2. (15.10)
If we now have two complex random variables i1 and i; and a joint PDF for the real
random vector [111 u; v1 vq]T, then we can deﬁne a cross-moment as
Ehiiiiz) = E [W1 -J'U1)(“2 +1.11?“
= [E("1U2)+ E(v1v2)] +1 iE(u1U2) _ EWzi/Ql
which is seen to involve all the possible real cross-moments. The covariance between
i1 and i2 is deﬁned as
c0v(i1.i2)= E[(5r1— E(5I1))*(i2 - 5952B] (15-11)
and can be shown to reduce to
COV(51,52)=  ‘ E*(5"1)E(52)- (15-12)
Just as in the real case, if i1 is independent of i2, which is to say [a1 v1]T is independent
of [n2 112V, then cov(i1, i2) = 0 (see Problem 15.1). Note that the covariance reduces
to the variance if i1 = i2. We can easily extend these deﬁnitions to complex random
vectors. For example, if i = [i1 i2 . . . in]T, then the mean of i is deﬁned as
12(1) = I
and the covariance of i is deﬁned as
Cr = El(i"E(i))(i—E(i))Hl
—_— z [i;—E*(i1) i;—E"(i2)  iI,—E*(i,,)]
a, - ma.)
var(i1) cov(i1,i2) cov(i1,i,,) ‘
: cov(i2,i1) var(i2)  cov(i2,i,,) (15.13)
eevega) cov(i,,,i2)  Vere.)
where H denotes the conjugate transpose of a matrix. Note that the covariance matrix is
Hermitian or the diagonal elements are real and the off-diagonal elements are complex
conjugates of each other so that Cg = C5,. Also, C; can be shown to be positive
semideﬁnite (see Problem 15.2).
We frequently need to compute the moments of linear transformations of random
vectors. For example, if y = A)": + b, where i is a complex n X 1 random vector, A is
a. complex m >< n matrix, b is a complex m X 1 vector, so that y is a complex m X 1
random vector, then
12g) AE(i)+b
The ﬁrst result follows from considering
so that
and expressing the results in matrix form. The second result uses the ﬁrst to yield
Cg = E [w ~ Eons - E(5'))”l *
= E [A(i - E(i))(i _ E(i))HAH].
[Cﬁlij = E { [m - Earner - E<i>>"A”1.,.}
= E  n lAlikKi * E(i))(i * EﬁﬁllHlkllAHlu}
and expressing this in matrix form yields the desired result. It is also of interest to
determine the ﬁrst two moments of a positive deﬁnite Hermitian form or of
where i is a complex n X 1 random vector and A is an n X n positive deﬁnite hermitian
(AH = A) matrix. Note that Q is real since
c? = Q” = (»'=”Ai>”
Also, since A is assumed to be positive deﬁnite, we have Q > 0 for all i 76 0. To ﬁnd
the moments of Q we assume E(i) = 0. (If this is not the case, we can easily modify
the results by replacing i by y = i —- E and then evaluating the expressions.) Thus,
E (Q) EbiHAil
E(tr(AiiH))
since 1H5» = i491" ). But then
E(Q)
= tr(ACi). (15.14)
To ﬁnd the second moment we need to evaluate
E(Q=’) = E(iHAiiHAi) (15.15)
which will require fourth-order moments of i (see Example 15.4). Recall that for a
real Gaussian PDF the fourth-order moments were functions of second-order moments,
considerably simplifying the evaluation. We will show next that we can deﬁne a complex:
Gaussian PDF for complex random variables and that it will have many of the same
properties as the real one. Then, we will be able to evaluate complicated expressions
such as (15.15). Furthermore, the complex Gaussian PDF arises naturally from a
consideration of the distribution of the complex envelope of a bandpass process.
We begin our discussion by deﬁning the complex Gaussian PDF of a scalar complex
random variable i. Since i = u + jv, any complete statistical description will involve
the joint PDF of u and v. The complex Gaussian PDF assumes u to be independent
of v. Furthermore, it assumes that the real and imaginary parts are distributed as
A/(nmoz/Z) and A/(nwaz/Z), respectively. Hence, the joint PDF of the real random
variables becomes
p01’: U) _ 1 exp [-2 1 (u _ llulzi
i exp [—— ((11 — at)’ + (v — MW)-
But letting [i =  = n“ +111”, we have in more succinct form
pm = F exp i-Fli - 111*]. (ls-w)
Since the joint PDF depends on u and v only through 5:, we can view the PDF to be that
of the scalar random variable ti, as our notation suggests. This is called the complex:
Gaussian PDF for a scalar complex random variable and is denoted by CA/(ﬁ, a2). Note
the similarity to the usual real Gaussian PDF. We would expect p(:i') to have many of
the same algebraic properties as the real Gaussian PDF, and indeed it does. To extend
these results we next consider a complex random vector i = [i1 rig . .. in]? Assume
the components of i are each distributed as CA/(fihaf) and are also independent. By
independence we mean that the real random vectors [n1 v1]T, [n2 112V, . . . , [an vn]T are
independent. Then, the multivariate complex Gaussian PDF is just the product of the
marginal PDFs or
which follows from the usual property of PDFs for real independent random variables.
From (15.16) this can be written as
But noting that for independent complex random variables the covariances are zero,
we have the covariance matrix for i
i = diag(af,a§,...,a,2,)
so that the PDF becomes )
P“) =  “P H" r "W1 ‘e — m1 (ls-m
This is the multivariate complex Gaussian PDF. and it is denoted by CA/(ﬁ, C1). Again
note the similarity to the usual real multivariate Gaussian PDF. But (15.17) is more
general than we have assumed in our derivation. It is actually valid for complex covari-
ance matrices other than just diagonal ones. To deﬁne the general complex Gaussian
PDF we need to restrict the form of the underlying real covariance matrix, as we now
show. Recall that for a scalar complex Gaussian random variable i = v + jv the real
covariance matrix of (v v]T is
a?
For a 2 >< 1_complex random vector i = [i1 iﬂT = (v1 +jv1 v2 + jvﬂT with i, 111118-
pendent of i=1, as we have assumed, the real covariance matrix of [v1 v1 v; v2]T is
l 0 of 0 0
2 0 0 a; 0
0 0 0 a;
If we rearrange the real random vector as x = [v1 v2 v1 v2]T, then the real cgvariance
matrix becomes
of 0 | 0 0
1 O a5 | 0 0
CH5 -- ~— I —— ~— . (15.18)
0 O | of 0
0 O [ 0 a;
This is a special case of the more general form of the real 4 X 4 covariance matrix
C,=§[B A] (15.19)
where A and B are each 2 X 2 matrices. This form of C, allows us to deﬁne a complex
Gaussian PDF for complex Gaussian random variables that are correlated. By letting
u = [v1 v2]T and v = [v1 v2]T we see that the submatrices of C, are
§B = Euv - E<v>><u - Emma = —El(u - E<u>><v ~ Evm
Thus A is symmetric and B is skew-symmetric or BT = —B. Explicitly, the real
covariance matrix is
var(v1) cov(v1, v2) I cov(v1, v1) cov(v1, v2)
cov(v;, v1) var(v2) | cov(v2, v1) cov(v2, v2)
| var(v1) cov(v1, v2)
| cov(v2, v1) var(v2)
cov(v1, v1) cov(v1, v2)
cov(v2, v1) cov(v2,v2)
For C, to have the form of (15.19) we require (in addition to variances being equal or
 = var(v,-) for i = 1, 2, and the covariance between the real and imaginary parts
being zero or cov(v,-,v1) = 0 for i = 1,2, as we have assumed for the scalar complex
Gaussian random variable) the covariances to satisfy
cov(v1,vg) = cov(v1,v2)
cov(v1,v2) —cov(v2, v1). (15.20)
The covariance between the real parts is identical to that between the imaginary parts,
and the covariance between the real part of i1 and the imaginary part of i2 is the
negative of the covariance between the real part of i; and the imaginary part of i1.
With this form of the real covariance matrix, as in (15.19), we can prove the following
(see Appendix 15A) for i = [i1 i; . . . in] = [v1 + jv1 v2 + jv;  u" + jvn]T and for
A and B both having dimensions n X n.
1. The complex covariance matrix of i is Ci = A + jB. As an example, for n = 2
and independent i1, i2, C, is given by (15.18) and thus
2. The quadratic form in the exponent of the real multivariate Gaussian PDF may
be expressed as
(x — Wcﬂx — H) = 2b? — nﬂci-‘(i — i»)
where [1, = E(i) = nu + jpv. As an example, for n = 2 and independent 51,52,
we have from (15.18) and (15.19)
sothat
(X — u)TC;1(X — u)
2[(u — #i)TA“(u — m] + Kv - wTA-‘(v — u.»
2l(u — m.) —1(v — uv)]TCE‘l(u— u.) +1(v - w]
201 — n)”<1;‘(i — n) ’
since C; = A, where A is real.
3. The determinant of C, is
Cl€1§(Cx) = ﬁdetzwi).
Note that det(C,-,) is real and positive since C; is Hermitian and positive deﬁnite.
As an example, for n = 2 and independent 51,52 we have C; = A, and from
(15.18) and (15.19)
so that
det(C,) = det2 
With these results we can rewrite the real multivariate Gaussian PDF of
x= [a1  unvl  vnlT
p“) = (21r)2T" det
2,01) exp [gt - “fem - m]
;v(X)
  EXP l*(i “' IUHCQIG‘ _ ﬁll
where [i is the mean and C5, is the covariance matrix of i. Whereas the real Gaussian
PDF involves 2n X 1 real vectors and 2n X 2n real matrices, the complex Gaussian PDF
involves n X 1 complex vectors and n X n complex matrices. We now summarize our
results in a theorem.
Theorem 15.1 (Complex Multivariate Gaussian PDF) If a real random vector
x of dimension 2n X 1 can be partitioned as
where u, v are real random vectors and x has the PDF
and Cm, = Cm, and Cm, = —C,,u or
cov(u,~, uj) = cov(v,-, vj)
cov(u,~,vj) = —cov(v,-,uj) (15.21)
then deﬁning the n X 1 complex random vector i = u+jv, i: has the complex multivariate
Gaussian PDF
where
Ci : 2(Cuu+jCvu)
or more explicitly
mi) = 1,0 fxp H2 — WC? (2 - m] - (15.22)
1r" det i
It is important to realize that the complex Gaussian PDF is actually just a different
algebraic form of the real Gaussian PDF. What we have essentially done is to replace
2 X 1 vectors by complex numbers or [u vlT —> i = u+ jv. This will simplify subsequent
estimator calculations, but no new theory should be expected. The deeper reason that
allowed us to perform this trick is the isomorphism that exists between the vector
spaces M 2 (the space of all special 2 X 2 real matrices) and C1 (the space of all complex
~ sswixmmwws-wm-ww-w-mnwm-Pmweawn- . -
numbers). We can transform matrices from M 2 to complex numbers in C1, perform
calculations in C l, and after we’re done transform back to M 2. As an example, if we
wish to multiply the matrices
[a —b e —f _ ae—bf —af—be
b a f e T a f + be ae — bf
we can, equivalently, multiply the complex numbers
(a +Jb)(@ +11‘) = (a6 — bf) +J'(<1f + be)
and then transform back to M 2. We will explore this further in Problem 15.5.
There are many properties of the complex Gaussian PDF that mirror those of the
real one. We now summarize the important properties with the proofs given in Ap-
pendix 15B.
1. Any subvector of a complex Gaussian random vector is also complex Gaussian.
In particular, the marginals are complex Gaussian.
2. If i = [531 i2 . . . zinlT is complex Gaussian and {i1,:i2, . . . in} are uncorrelated,
then they are also independent.
3. If {?1’£27|llT‘,i"} are independent, each one being complex Gaussian, then i =
[r1 12 . . ma] 1s complex Gaussian.
4. Linear (actually affine) transformations of complex Gaussian random vectors are
again complex Gaussian. More speciﬁcally, if y = Ai + b, where A is a complex
m X n rnatrix with m g n and full rank (so that Cg is invertible), b is a complex
m X 1 vector, and if i ~ CA/(ﬂ, Ci), then
y ~ CN(Aﬂ+b,AC,-,A”). (15.23)
5. The sum of independent complex Gaussian random variables is also complex Gaus-
sian.
Etiiiziéin = E(i=§i2)E(i§i4) + E(i{a':4)E(i2i§). (15.24)
It is interesting ‘to note that Eﬁfig) = E(j2j4) = () (as we will Show shortly)’ so
that this result 1s analogous to the real case, with the last product being equal to
7. If [iT yTF is a complex Gaussian random vector, where i is k X 1 and y is l X 1
and _ ~
lZl~“(l§§§§llEii Ezzl)
then the conditional PDF p(y|i) is also complex Gaussian with
= E0‘) + 65505110? — 1501)) (1525)
cg, - 0,750,310..) (15.26)
E(5'|i
This is identical in form to the real case (see (10.24) and (10.25)).
These properties appear reasonable, and the reader will be inclined to accept them.
The only one that seems strange is property 6, which seems to imply that for a zero
mean complex Gaussian random vector we have E(i'15§2) = 0. To better appreciate
this conjecture we determine this cross-moment as follows:
Eiiliz) = El("1+jv1)("2+j"2)l
_ E("1U2)_ E(U1U2) iJllEiulW) + E(U1"2)l
= cov(u1, U2) — cov(v1, v2) + j[cov(u1, v2) + cov(u2, 111)]
which from (15.20) is zero. In fact, we have actually deﬁned E (£1522) = 0 in arriving
at the form of the covariance matrix in (15.19) (see Problem 15.9). It is this constraint
that leads to the special form assumed for the real covariance matrix. We will now
illustrate some of the foregoing properties by applying them in an important example.
Example 15.3 - PDF of Discrete Fourier Transform of WGN
Consider the real data {.r[0], r[1], . . . ,.r[N — 1]} which are samples of a WGN process
with zero mean and variance a2 / 2. We take the discrete Fourier transform (DFT) of
the samples to produce
Xm) = Z r[n1e><p<—121rf.n)
for f), = k/N and k = 0, 1, . . . , N — 1. We inquire as to the PDF of the complex DFT
outputs for k = 1,2, . . .,N/2— 1, neglecting those for k = N/2+ 1,N/2+2, . . . ,N —- 1
since these are related to the chosen outputs by X ( fk) = X *( fN_;,). We also neglect
X (f0) and X ( fN/Z) since these are purely real and are usually of little interest anyway,
occurring at DC and at the Nyquist frequency. We will show that the PDF is
X(f1)
x(f) = g ~ cN (0, M21) . (15.21)
To verify this we ﬁrst note that the real random vector
[Revurm mound» 1T
Re(X(f§-1)) Imvﬂfm
is a real Gaussian random vector since it is a linear transformation of x. Thus, we need
only show that the special form of the real covariance matrix of X is satisﬁed. First,
we show that the real and imaginary parts of X ( fk) are uncorrelated. Let X ( fk) =
U(fk) wl-jl/(fk), where U and V are both real. Then
U(fk) = 2z[n]cos21rfkn
l/(fk) = —~ Z :c[n] sin 21rfkn
and therefore
E(U(fk)) = Z E(:c[n])cos21rfkn = 0
and similarly E(V(fk)) = 0. Now ,
¢0v(U(fk),U(fz)) = E(U(fk)U(fl))
= E Z :c[m]:c[n] cos 21rfkm cos 21rf1n
= Z —6[m — n] cos 21rfkm cos 21rf1n
a N—1
= 3- cos 21rfkn cos 21rfln
a2 N-l 1
= 3 Z 5[cos21r(fk +f;)n +cos21r(fk — fl)n]
= I cos21r(fk —f,)n
due to the DFT relation in Appendix 1. Now if fk 7E fl, COV(U(fk),U(fl)) _—. () by
the same DFT relation. If fk = fl, we have cov(U(fk),U(fk)) —_- Ng2/4_ A Similar
calculation shows that
COv(V(fk)1V(fl)):{ L? k f,
and so
as required. To verify that CUV = _CVU we show that these are both zero.
Col/(lxfklvl/(fll) = EiUUkll/(fzl)
E  Z  cos 21rfkmsin 21rf1n)
a2 N-l
Y Z cos Zvrfkn sin 21rfln
= o _forallk,l
by the DFT relation in Appendix 1. Similarly, cov(V(fk), U(f;)) = 0 for all k, l. Hence,
CUV = —CvU = 0. From Theorem 15.1 the complex covariance matrix becomes
Cx = 2(Cuu wl-jcvu)
and the assertion is proved. Observe that the DFT coefficients are independent and
identically distributed. As an application of this result we may wish to estimate a2
a2 = warm?
This is frequently required for normalization purposes in determining a threshold for a
detector [Knight, Pridham, and Kay 1981]. For the estimator to be unbiased we need
to choose an appropriate value for c. But
Bu?) = E(lX(fk)l2)-
Since X(fk) ~ CA/(O, Naz/Z), we have that
varunm) = E(lX(fk)l2) = ,1’
and
Hence, c should be chosen to be N(N/2 — 1)/2. O
Example 15.4 - Variance of Hermitian Form
As a second application, we now derive the variance of the Hermitian form
where i ~ CA/(O, C5) and A-is Hermitian. The mean has already been given in (15.14)
as tr(ACi). We ﬁrst determine the second moment
12(4)’) = 12911511121111)
i Zn: i: Zn§[A].-j[A1k1E(-'i.*iji;i1)-
Using the fourth-order moment property (15.24), we have
E(iZ-'ijwkwl) = E(i;i,-)E(2;2,)+E(@;@,)E(@;z,)
13(4):’)
+ii ( ")A1.-.1<>£1k.) (imticzt). (15.28)
tr(BTA) = tr g [ a1  an ]
where a1», b,- are the ith columns of A and B, respectively. More explicitly this becomes
tr(BTA)
so that the ﬁrst term in (15.28) is just tr2(CiA) 0r tr2(ACi), the squared mean. Let
D = AC5. Then, the second term is
tr(DD) = tr(ACiACi)
so that ﬁnally we have for i ~ CA/(O, Ci) and A Hermitian
E(iHAi) = MACE) (15.29)
var(iHAi) = tI‘(AC5-ACi). (15.30)
An application of this result is given in Problem 15.10. <>
15.5 Complex WSS Random Processes
We are now in a position to discuss the statistical modeling of the complex random
process  =  + jv[n]. It will be assumed that  is WSS, which, as for real
processes, implies that the mean does not depend on n and the covariance between 
and i[n + k] depends only on the lag k. The mean is deﬁned as
Eﬁflnl) = 501W) +JE(v["])
and will be assumed to be zero for all n. Since  is WSS, we can deﬁne an ACF as
riﬂk] =  + k]) (15.31)
and as usual the PSD is the Fourier transform of riﬂk]. If the random vector formed
from any subset of samples, say {i‘[n1],i[n2], . . . ,.z':[nk]}, has a multivariate complex
Gaussian PDF, then we say that  is a WSS complex Gaussian random process. Its
omp.e e s a 1s ica description 1s then summarized by the ACF, which determmes the
<r3:a‘/A'l8-I.':1:‘I,1;Ieiarg1ir1x,tor, equivalently, by the PSD (since the mean is zero). Since the
Constraint on Tug] r1; 1s constrained to be 0t the form (15.19), this translates into a
I1: . o interpret this constraint recall that we require
for an u = l“ln1l"-ul"kllT and V = [v[n1]...v[nk]]T. This means that
EWlnilulnjll = E(v[n.-]v[n,-])
Eiulnilvlnjll = —E(v[n.-]u[n,-])
for all i JI- B)’ deﬁning the ACF of  it is implicit that u
.’ . [n],v[n] are both WSS, as
well as Jointly WSS. If we deﬁne the cross-correlation function (CCF) as
then we must have
Tuvlkl = ‘Hulkl (15.33)
for all k, Where We have let k = n]- —— ni. Alternatively, we require
Pmf) P....(f)
m!) = —Pm.(f) (15.34)
gleeifoss-éggsﬂlséilljlrﬁgiliﬁlgilgaiiggfandhPUti/Tgf)1gilﬂ-(f) ﬁr; the cross-PSDs. Since
_ _ eac o er, e secon condition is e uivalent
a Complex Gaussiai randos 1 zero Imlean and PSDs satisfying (15.34),  will be
those of uh] d fm PTOCBSS- Tthermofe, the PSD of  can be related to
an 12in] as ollows. From (15.31) and (15.33)
But from (15.33) this reduces to
and thus
Pa”) =2(Puu(fl+jPu,,(f)). (1536)
The reader at this point may reco ' ' -
gnize that the same relationships (15 33) and (15 34
r or a process and its Hilbert transform or the real and imaginary parts of a
bandpass noise process. Thus, the complex envelope or the analytic signal of a WSS
Gaussian random process is a complex WSS Gaussian random process (see Problem
15.12). Since these results are standard fare in many textbooks [Papoulis 1965], we
only brieﬂy illustrate them. We consider a bandpass noise process using the complex
envelope representation. What we will show is that by concatenating the in-phase
and quadrature processes together into a single complex envelope process, a complex
WSS Gaussian random process results if the bandpass process is a real WSS Gaussian
Example 15.5 - Bandpass Gaussian Noise Process
Consider a real continuous bandpass WSS Gaussian process :c(t) with zero mean. It
can be represented by its complex envelope as (using an argument similar to that in
Section 15.3 but replacing Fourier transforms by PSDs)
:c(t) = 2Re  exp(j21rF0t)] (15.37)
where :Z'(t) = u(t) +jv(t) and
u(t) = [:c(t) cos 21rF0t]LPF
v(t) =  sin 21rF0t]LPF .
Then, as shown in Section 15.3, after sampling we have the discrete process
where  = u(nA),v[n] = v(nA), and A = 1/B. But u(t),v(t) are jointly Gaussian
WSS processes because they are obtained by linear transformations of  Hence,
u[n], v[n] are jointly Gaussian also. Furthermore, from (15.37)
:c(t) = 2u(t) cos 21rFot — 2v(t) sin 21rF0t
so that because  = 0 for all t, it follows that  = E(v(t)) = 0 or  =
 = 0 for all n. To show that uln], v[n] are jointly WSS we determine the ACF
and the CCF. The ACF of :c(t) is
r,,(1') = E[:c(t):c(t + 1)]
— 4E  cos 21rFqt — v(t) sin 21rF0t)
- (u(t + T) cos 21rF0(t + T) — v(t + T) sin 21rF0(t + 
4 [(1'W(T) cos 21rF0t cos 21rF0(t + T) + rm,(r) sin 21rF0t sin 21rF0(t + T))
— (rm,(1') cos 21rF0t sin 21rF0(t + T) + rm,(T) sin 21rF0t cos 21rF0(t + 
2 [(rm,(1') + rm,(r)) cos 21rF0T + (ru,,(r) —— rm,(r)) cos 21rF@(2t + T)
— (ru,,(r) — r,m(r)) sin 21rF0T — (r,“,(1')+ rm,(r)) sin 21rF0(2t + T)].
Since this must not depend on t  was assumed to be WSS), we must have
r,m(1') = rm,(1')
rm,(r) = —r,,,,(T). (15.38)
F (Hz) Figure 15.3 Bandpass “white”
_F° F0 T g F0 F‘) + g Gaussian noise
Then
r,,(1') = 4(r.m(r) cos 21rF0T + rW(T) sin 21rF0T) (15.39)
and since
= E(:c1(nA).1:;(nA + kA))
= T1112 (kA)
we have ﬁnally
as required. Thus,  is a complex Gaussian WSS random process.
As an example, if :c(t) is the result of ﬁltering a continuous-time WGN process with
a PSD of N0/2 over a band of B Hz as shown in Figure 15.2, then
r,,(1') = NOB 822T?‘ cos 21rF0T.
By comparison with (15.39)
T ( ) _ NOB sin1rBT
r,m(1') = 0.
If we sample the complex envelope i‘(t) = u(t) + jv(t) at the Nyquist rate of F, =
l/A = B, we have
NoBsinvrk
Hence,
and ﬁnally,  is complex white Gaussian noise (CWGN) with (see (15.35) and (15.36))
rﬁut] = NgBapt]
Note that because the samples i'[n] are complex Gaussian and uncorrelated, they are
also independent. In summary, for CWGN we have
i'[n] ~ CA/(Opz)
where all the samples of the process are independent. <>
Before concluding this section let us point out that a real Gaussian random process
is not a special case of a complex Gaussian random process. As a counterexample,
consider the second-order moment  which is known to be zero for all m
and n. If  is a real Gaussian random process, then the second moment need not be
identically zero.
15.6 Derivatives, Gradients, and Optimization
In order to determine estimators such as the LSE or the MLE we must optimize a
function over a complex parameter space. We explored one such problem in Example
15.2. We now extend that approach so that a general method can be used for easily
ﬁnding estimators. The complex derivative of a real scalar function J with respect to
a complex parameter 0 was deﬁned to be
3J 1 (3J BJ)
w = 5 — - (15.40)
3a J3?
where 0 = a + jﬂ with a, ﬂ the real and imaginary parts of 0, respectively. In making
this deﬁnition we note that by the properties of complex numbers
60 lfan onyrf 6a 8g
Hence, in optimizing J over all a and ﬂ we can, equivalently, ﬁnd the 9 that satiﬁes
31/39 = 0. A typical real scalar function J is the LS error criterion in Example 15.2.
The reader should note that other deﬁnitions of the complex derivative are possible, as
for example [Monzingo and Miller 1980]
§£_a_J+-a_J
30-301 Jag
and would accomplish the same goal. Our choice results in differentiation formulas that
are analogous to some commonly encountered in real calculus but unfortunately others
that are somewhat strange, as we will see shortly (see (15.46)). The reason for having
to deﬁne the complex derivative is that a real function of complex variables is not an
analytic function. As an example, if J = 10]’, then
J z a2 + [62
and using standard real calculus techniques and our deﬁnition of the complex derivative
3J 1 (3J OJ)
ﬁ = 5 3_a_j%
= §(2<1—J2ﬂ)
But it is well known that the complex function (actually a real function) I912 is not an
analytic function and so cannot be differentiated. Yet by our deﬁnition a stationary
point of the real function can be found by solving 3J/30 = 0, producing 0 = 0 or
a = ﬂ = O as expected. Note that if we were t0 rewrite J as 90‘, then thé same
result would have been obtained by considering 0‘ to be a constant in the partial
differentiation. Specifically, consider J to be a function of the two independent complex
variables 0 and 0‘, so that we denote it by J (9, 0*). It is easily shown for this example
that J is an analytic function with respect to 9, holding 0‘ constant, and also with
respect to 0‘, holding 0 constant. Hence, applying the chain rule, we have
3J(0,0") 30 _ 30*
a0 ‘ a0” + 0 a0 '
To evaluate 39/30 we use the same deﬁnition of 3/ 30 to yield
% = §(§;-1§ﬂ-)<a+jﬂ)
1(30z .33 .30 33)
5 aa+’$"5b_+a_a
and ﬁnally
which is consistent with known differentiation results for analytic functions. Similarly,
it is easily shown that (see Problem 15.13)
aw _
a0 ‘
which again is also consistent, assuming that 0 and 0* are independent complex vari-
ables. Finally,
1 (15.41)
0 (15.42)
61(1), 0*)
a0 1 9 +
consistent with our earlier results. In summary, with the deﬁnition of the complex
derivative given by (15.40) stationary points of J (0, 0") can be found as follows. We
consider J (0, 0*) as a function of the two independent complex variables 0 and 0‘. We
then evaluate the partial derivative of J with respect to 0, where 0' is a constant,
by applying the usual differentation rules for analytic functions. Setting 3J/39 = 0
produces the stationary points. Equivalently, we could have considered 3J/30* in
obtaining a stationary point. In this case 0 is held constant, so that
1(a_J_. a1 )
2 3a Jays)
l aiQ-Q)
2(3a Jag
= (g3; .15....)
and setting 3J/30‘ = 0 will produce the same solutions as 3J/30 = 0.
The complex gradient of a real function J with respect to the complex vector pa-
rameter 9 is deﬁned as aJ
where each element is deﬁned by (15.40). Again note that the complex gradient is zero
if and only if each element is zero or 3J/36, = 0 for i = 1,2,. . . ,p, and hence if and
only if 3J/3a, = 3J/3ﬂ, = 0 for i = 1,2,. . . ,p. Stationary points of J may be found
by setting the complex gradient equal to zero and solving.
For the most part we will be interested in differentiating linear and Hermitian forms
such as 9H b and BHAO (where A is Hermitian), respectively. We now derive the
complex gradient of these functions. First, consider 1(0) = bHO, where we note that l
is complex. Then
1(0) = Zpjbgo.
and using linearity as well as (15.41), we have
T. _ bkao, ‘ bk
so that H
ab 9 = b‘. (15.44)
We let the reader show that (see Problem 15.14)
High = 0. (15.45)
Next, consider the Hermitian form J = 9H A9. Since AH = A, J must be real, which
follows from JH = 9HAH9 = 9HA9 = J. Now
J = ail-Aliﬁj
and using (15.41) and (15.42)
5'0; Z Z (91 lAlIJETQI ‘l’ 60k lAlw0J>
a Pr E
and therefore
- = 1T0‘ = (A0)". (15.46)
As alluded to earlier, this result appears somewhat strange since if 9 were real, we
would have had 8J/89 = 2A9 (see (4.3)). The real case then is not a special case.
Because we will wish to differentiate the likelihood function corresponding to the
complex Gaussian PDF to determine the CRLB as well as the MLE, the following
formulas are quite useful. If the covariance matrix C; depends upon a number of real
parameters {§1,§;,.. .,§p}, then denoting the covariance matrix as C5(£), it can be
shown that
  = tr (c;1(£)a%2l£)) (15.47)
  _~”c;1(g)a(g2l§)c;1(g)i. (15.4s)
These are derived in Appendix 3C for a real covariance matrix. For a complex co-
variance matrix the derivations are easily extended. The deﬁnitions and formulas are
summarized in Table 15.1. We now illustrate their use with an example.
TABLE 15.1 Useful Formulas
Deﬁnitions
9: Complex scalar parameter (9 = a +19)
J: Real scalar function of 9
as ‘ 2 a0. J an
9: Complex vector parameter
a1
a0,
Formulas
ﬁ ‘l as ‘O
ab”0 , 80%
aaaoAa = (A9)‘, where AH = A
31I1<1e¢(Ci(€))_ —1 3Ci(€) _
8£i -tr Ci  agi , f, real
@i”¢;‘(s>i a. -1 was) _. -
T—— Ca.- (S) a§i Ci (QX
Example 15.6 - Minimization of Hermitian Functions
Suppose we wish to minimize the LS error
J = (i ~ H9)HC'1(i — H9)
where i is a complex N x 1 vector, H is a complex N X p matrix with N > p and full
rank, C is a complex N X N covariance matrix, and 9 is a complex p X 1 parameter
vector. We ﬁrst note that J is a real function since J H = J (recall that CH = C). To
ﬁnd the value of 9 that minimizes J we expand the function and then make use of our
formulas.
6-9 0 -(H”c-1i)‘ - 0 + (HHC-‘Hef
_ [H”C"(i - Hen‘ (15.49)
using (15.44), (15.45), and (15.46). We can ﬁnd the minimizing solution by setting the
complex gradient equal to zero, yielding the LS solution
é = (H”c-1H)-1H”c~1i. (15.50)
To conﬁrm that é produces the global minimum the reader should verify that
(0 - é)”H"c-1H(e - é) + (i - HéWc-lbz - H9)
3 (i - H9)”C“(i - Hé)
with equality if and only if 9 =  As an example, using (15.50), the solution of the LS
problem in Example 15.2 is easily found by letting 0 = A, H =  5[1] . . . .§[N — 1]]T,
and C = I, so that from (15.50)
Z (H”H)-1H"i
A ﬁnal useful result concerns the minimization of a Hermitian form subject to linear
constraints. Consider the real function
9(a) = aHWa
where a is a complex n >< 1 vector and W is a complex n x n positive deﬁnite (and
Hermitian) matrix. To avoid the trivial solution a = 0 we assume that a satisﬁes the
linear constraint Ba = b, where B is a complex r X p matrix with r < p and full rank and
b is a complex rx 1 vector. A solution to this constrained minimization problem requires
the use of Lagrangian multipliers. To extend the use of real Lagrangian multipliers to
the complex case we let B = BR + 1B1, a = a); +ja1, and b = b); +jb1. Then the
constraint equation Ba = b is equivalent to
(BR + 131K312 +181) = bR + 1'51
BRaR — BIaI = b);
B1612 + BRaI = b1-
Since there are two sets of r constraints, we can let the Lagrangian be
J(a) = aHWa + AQBRaR - ma, - bR)
+ ARE,“ + Baa; - b,)
aHWa + Aﬁaeuaa - b) + ATIm(Ba - b)
where A R, A I are each real r X 1 Lagrangian multiplier vectors. But letting A = A R + j A I
be a complex r X 1 Lagrangian multiplier vector, the constraint equation is simpliﬁed
to yield
J(a) = aHWa + Re [(>.R _ jA;)T(Ba - b)]
= aHWa-l- %AH(Ba - b) + %AT(B‘a* - b‘).
We can now perform the constrained minimization. Using (15.44), (15.45), and (15.46),
we have
aJ BHA *
8a ( a) + ( 2 )
and setting it equal to zero produces
ﬂop; Z —W_1BH5.
Imposing the constraint Baopt = b, we have
Ba-Opg = ‘BW-IBHE.
Since B is assumed to be full rank and W to be positive deﬁnite, BW_1BH is invertible
and
5 = -(Bw-1B”)-1b
so that ﬁnally
aop, = W"1BH(BW"1BH)'1b. (15.51)
To show that this is indeed the global minimizing solution we need only verify the
identity
aH Wa
a
a
2 aH Waopta
which holds with equality if and only if a = aopt. We illustrate this constrained mini-
mization with an example.
Example 15.7 - BLUE of Mean of Complex Colored Random Process
Assume that we observe
where A is a complex parameter to be estimated and 1I:[n] is complex noise with zero
mean and covariance matrix C. The complex equivalent of the BLUE is
=aHi
$0
where A is an unbiased estimator and has minimum variance among all linear estima-
tors. To be unbiased we require
15(11): aHE(i) = aHAl = A
or aHl = 1. This constraint can also be written as lTa = 1,Awhich can be veriﬁed by
replacing a by its real and imaginary parts. The variance of A is
var(A) = E [)2 - 12(2)?)
E [IaHi — aHA1|2l
= E [aHGr — A1)(i — A1)Hal
= aH Ca.
We now wish to minimize the variance subject to the linear constraint on a. Letting
B = 1T, b = 1, and W = C, we have from (15.51) that
a... = c—11<1Tc—*1)-11
The BLUE of A is then
aomx
which is identical in form to theAreal case (see Example 6.2). A subtle difference,
however, is that A minimizes var(A), or if A = AR +1141,
var(;i) = E ()2 - 15(2)?)
= E (Ki. - E<AR)) +1141 - Etimn’)
= var(AR) + varUiz)
which is actually the sum of the variances for each component of the estimator. <>
15.7 Classical Estimation with Complex Data
We will restrict our discussion to complex data vectors that have the complex Gaussian
p(i; 9) =   eXP l-(i _ I](9))HC;1(9)(3~{ "  '
The parameter vector 9 is to be estimated based on the complex data i =  it[1] . . .
i‘[N — 1]]T and may have components that are real and/or complex. Many of the
results encountered for real data/ real parameter estimation theory can be extended t0
the complex case. A full account, however, would require another book—hence we will
mention the results we believe to be most useful. The reader will undoubtedly wish to
extend other real estimators and is certainly encouraged to do so.
We ﬁrst consider the MVU estimator of 9. Because 9 may have complex and real
components, the most general approach assumes that 9 is a real vector. To avoid
confusion with the case when 9 is purely complex we will denote the vector of real
parameters as 5. For example, if we wish to estimate the complex amplitude A and the
frequency f0 of a sinusoid, we let £ = [AR A; f0]? Then, the MVU estimator has its
usual meaning——it is the unbiased estimator having the minimum variance or
var(§A,-) is minimum i: 1, 2, . . . ,p.
A good starting point is the CRLB, which for a complex Gaussian PDF results in a
Fisher information matrix (see Appendix 15C)
_ _ 8cm) _ awe)
11(0)). - tr [(1.1s) 6,, Cm) a5], l
317(5) _ 611(5)
+2Re[ agi egg) 8% (15.52)
for i, j = 1,2,...,p. The derivatives are defined, as usual, as the matrix or vector
of partial derivatives (see Section 3.9), where we differentiate each complex element
y = 9R +191 as
The equality condition for the CRLB to be attained is, from (3.25),
 = now) - s) 05-53)
where é = g(i) is the efficient estimator of 5. Note that p(i; 5) is the same as p(i; 9)
since the former is the complex Gaussian PDF parameterized using real parameters.
The example to follow illustrates the computation of the CRLB for the estimation of
a real parameter based on complex data. This situation frequently occurs when the
covariance matrbc depends on an unknown real parameter.
Example 15.8 - Random Sinusoid in CWGN
Assume that
i‘[n]=Aexp(j2'/rf0n)+ﬂ:[n] n=O,1,...,N—-1
where  is CWGN with variance 02, A ~ CA/(O, 0i), and A is indePendent of 
In matrix form we have ~
i = Ae + w
where e = [1 exp(j21rf0) , _ . exp(_j21rfo(N — 1))]T. Since sums oi independent complex
Gaussian random variables are also complex Gaussian (see Section 15.4), we have
i ~ CA/(O, Ci).
To ﬁnd the covariance matrix
= E ((Ae + w)(Ae + wy’)
E(|A|2)eeH + 021
= 0iee +0I ,
since A and w are independent. We now examine the problem of estimating 0f‘, the
variance of the complex amplitude, assuming f0 and 02 are known. The PDF 1s
To see if the CRLB is attained, we compute
8lnp(i;0i) _ 3lndet(Ci(0i)) _ 3iHC;1(0§)i
Using formulas (15.47) and (15.48)
_n- (CT1(¢7E\)eeH) + iHC;1(0f\)eeHC§1(0?,)i
-@”@;%a:w+-w”c;%@iﬁP-
_ 1 1 02eeH
C—1(U2)_Z _i' A 3
sothat
Ci (U06 _ 026 04+N0§02e
e .
Hence
3lnp(i;0i) __ N + |iHe|2
30f, _ N03‘ + 02 (N03 + 02)2
N 2 |iHe|2 02 2
= a — — — 0 .
The CRLB is satisﬁed with
~2 |iHe|2 02
= N? Z:c[n]exp(—]21rf0n) —F
which is an efficient estimator and has minimum variance
var(0f,) = (01-11-  .
Note that the variance does not decrease to zero with N , and hence 03 is not consistent.
The reader may wish to review the results of Problem 3.14 in explaining why this is
When the parameter to be estimated is complex, we can sometimes determine if an
efﬁcient estimator exists more easily than by expressing the parameter as a real vector
parameter and using (15.53). We now examine this special case. First, consider the
complex parameter B, where 9 = a + jﬁ. Then, concatenate the real and imaginary
parts into the real parameter vector 5 = [aT [f]? If the Fisher information matrix
for 5 takes the form
ua=2[§'§]
then the equality condition of the CRLB may be simpliﬁed. From (15.53) we have
511114111) 6a
aﬁ
and using the deﬁnition of the complex gradient
9lnp(i; 9) 1 [9lnp(i; 9) ’ .9lnp(i;9)
a0 2 J
8a 9L5
(E-ﬂxd—w-xE-ﬁxﬁ-m
= 1*(a)(é - e)*
where we have deﬁned the complex Fisher information matrix as I(9) = E + jF. Or
ﬁnally, by using (15.43) this becomes
  = 1(e)(é ~ e). (15.54)
The latter form is easier to evaluate, as is shown in the next example. To test whether
the Fisher information matrix has the desired form we can partition the real Fisher
information matrix as
mw(%IM)
and note that we must have
9lnp(i;9)9lnp(i;9)T _
B) aw as, _0 (15.55)
for then
1 a1np(i;0) _9lnp(i;9)) (9lnp(i;9) .a1np(i;e))T
ii a5. ‘L’ as a5. ‘L’ an
1am, — I35) + Zjuug +1230» = 0
and thus
Iaa z 193:2};
When (15.54) is true, I_1(9) is the covariance matrix of the efficient estimator  (The
efficient estimator for 9 is deﬁned as 9 = d + 1'9, where [CIT ﬁTF is the efﬁcient
estimator of [aT ﬁTF.) To verify this we have from (15.54)
91m p(i; 9)
0 01(0) as,
and thus H
E 3lnp(i;9)9lnp(i; 9) rlw)
C‘? = 1'1”) ae- aa~
and
E (015145; e) 9lnp(i; a)”
aa- ae~
z 1E (3lnp(i;9) wwggg) (9lnp(i;9) _j9lnp(i;9))T]
4 a5. as a5. an
= 1(0)
and thus
9 = I_1(9). (15.56)
An important application of this result is the complex linear model.
Example 15.9 - Complex Classical Linear Model
Assume that we have the complex extension of the classical linear model or
where H is a known complex N X p matrix with N > p and full rank, 9 is a complex
p >< 1 parameter vector to be estimated, and w is a complex N X 1 random vector with
PDF w ~ CA/(O, C). Then, by the properties of the complex Gaussian PDF
i ~ CN(H9, C)
so that 1]. = H9 and C(9) = C (not dependent on 9). The PDF is
p(i; 0) = m?) exp [—(i _ He)”c-1(i - 110)] .
To check the equality condition we have
9lnp(i; 9) __ _9(i — H9)HC‘1(i — H9)
and from (15.49) this becomes (recall (15.43))
9 lnp(i; 9)
a0‘ C (x 9)
= HHC-‘H [(H”c"1H)-1H”c"‘i - 0]. (15.57)
Thus, the equality condition is satisﬁed, and
é = (H”c-1H)-1H”c-1i (15.5s)
is an efficient estimator, and hence the MVU estimator of B. Also, from (15.54), (15.56),
and (15.57)
c, = (H”C_‘H)_‘ (15.59)
is its covariance matrix. The covariance matrix of the real parameter vector 5 is
Cg = 17(5)
~ (2) 1 l)
because of the special form of 1(5) (see property 4 in Appendix 15A). But (A + jB)(E+
jF) =1, so that
A +112 (E +11)“
I“(9) 1
and hence
Re (HHC_1H)_1 —Im [(HHC_1H)_1]
C5 = 5 ) Im )(HHC_1H)_1) Re [(HHC"1H)‘1] )'
To check that the real Fisher information matrix has the special form we have from
(15.55) and (15.57)
E [3lnp(i;9)(9lnp(i;9)1l]
aw a9‘ H c E[(i H0)(i H0)]C H
= H”c-1E(1111T)c-1’11*.
But as shown in Section 15.4, E(wwT) = 0 for w ~ CA/(O, C). Hence, the condition
is satisﬁed. Also, é as given by (15.58) is also the MLE, as can be verified by setting
the gradient 0f the log-likelihood function (15.57) equal to zero. If W = C“ in the
weighted LSE, then é is also the weighted LSE (see Example 15.6). If 11 is not a complex
Gaussian random vector, then 0 can be shown to be the BLUE for this problem (see
Problem 15.20). <>
When the CRLB is not satisﬁed, we can always resort to an MLE. In practice, the
use of sufficient statistics for real or complex data appears to be of limited value. Hence,
our approach in Chapter 5, although it should be tried, frequently does not produce a
practical estimator. The MLE, on the other hand, being a “turn-the-crank” procedure,
is usually of more value. The general MLE equations for real parameters of a complex
Gaussian PDF can be expressed in closed form. For complex parameters there does
not seem to be any general simplification of these equations. Letting
——-1C—JE@ eip [- (i - 11(1))” Cm) (i ~ 11(9)]
1r” det(
we differentiate using the formulas (15.47) and (15.48). As shown in Appendix 15C
this leads to l
3lnp(i;£) _ __1 3Ci(§)
6a _ _tr (Ci   )
+ (i - 11(s))”<1;1(:)a‘§flc;*(:)(i - 11(1))
+ 1 Re [(i ~ 11(s))”c.-l(s)a—§,‘§) (15.11)
which when set equal to zero can sometimes be used to ﬁnd the MLE. An example
Example 15.10 - Phase of a Complex Sinusoid in CWGN
Consider the data
iln] = Aexp L1'(21rf@n+¢)] +1191] 11 =0,1,...,N - 1
where thereal amplitude A and freqigency f0 are known, the phase (b is to be estimated,
and w[n] 1s CWGN with variance a . Then,  is a complex Gaussian process (al-
though not WSS due to the nonstationary mean), so that to find the MLE of (b we can
use (15.60). But Ci = 021 does not depend on (b, and
A eXPlJ <15)
AeXplj(21rf1(N ~ 1) + (1)1
Differentiating, we have
1A explirbl
1A expli (21% + (1)1
jAeXPUQWfOZUV - 1) + 45)]
and from (15.60)
 — 3R11[(i~11(1))”a'~““”]
345 a2 845
= ; Re  (i111 ~ 111111-1111» + 111) 1111111111111 + 1))
= -F Im [ 5:‘ exp[j(21rf0n + 45)) * NA)
~ U2 m expmﬂgw ln]e><1>(121rf6n)-NA .
Setting this equal to zero, we have upon letting
X116) = Z i161 exm-jwrfon)
denote the Fourier transform of the data
1m l@XP(J'¢)X'(f6)l = 0
1m Kcosaﬁ +1‘ sin ¢)(Re(X (m) - jIm(X( 10)») = o
Sin¢Re(X(fo)) = ¢OS¢III1(X(f6))-
The MLE follows as )
4i = arctan [ ]
which may be compared to the results in Example 7.6. <>
15.8 Bayesian Estimation
glhe usuall Bayesian estimators, the MMSIE and MAP estimators, extend naturally to
i eSCOEIP BEC5CZSF- _W8 Wlll assume that 9 1s a complex p x 1 random vector. As we saw
n ec ion . , 1f x, 9 are jointly complex Gaussian, then the posterior PDF p(9|i) will
also be complex Gaussian. By “jointly complex Gaussian” we mean that if i = x R + jx 1
and Y = YR  JYI, then [XE yjT; X? yHT has a real multivariate Gaussian PDF, and
furthermore» 1f ll = [XE yTﬂT and v = [xf yHT, then the real covariance matrix of
[UT VT]T has the special form given in Theorem 15.1. The mean and covariance of the
posterior PDF are (see (15.25) and (15.26))
Ewli) = 5(9) + Ceicii‘ (i — E(i)) (15.61)
Cm = Cw — Cﬁ-‘Cgilcil? (15.62)
where the covariance matrices are deﬁned as
[ c” 0,. j z [ E [(9 — E(9))(9 — E(9))”] E [(0 - E(0))(i - E(i))”]
E Ki — E(i))(9 — E(9))”] E [(2 - E(i))(i - E(i))”]
Because of the form of the posterior PDF, the MAP and MMSE estimators are identical.
It can further be shown that the MMSE estimator based on i is just E (9,-|i) and that
it minimizes the Bayesian MSE
where the expectation is with respect to p(i, 9,). Ihirthermore, if 9,- = d,- wl-jlii, then d,
minimizes E[(a,- — 66f] and  minimizes E [(9, — 9J2] (see Problem 15.23).
An important example is the complex Bayesian linear model. It is deﬁned as
i = H0+w (15.63)
where H is a complex N X p matrix with N 5 p possibly, 9 is a complex p X 1 random
vector with 9 ~ CA/(pg, C99), and w is a complex N x 1 random vector with w ~
CA/(O, C5,) and is independent of 9. The MMSE estimator of 9 is given by E(9|i), so
that we need only ﬁnd the ﬁrst two moments of the PDF p(i, 9), according to (15.61)
and (15.62). The mean is
E(i) = HE(9)+E(w)
and the covariances are
C5; = E [(H9 + w —- Hp,)(H9 + w — HMQH]
= HE [(9 — 116KB — 119F111” + E0111”)
C05 = E [(9 * IMXHB ‘i’ ‘l’ - HﬂelHl
since 9 and w are independent and hence uncorrelated. Finally, we should verify that
11,9 are jointly complex Gaussian. Because 9, w are independent, they are also jointly
complex Gaussian. Due to the linear transformation
i, 9 are therefore jointly complex Gaussian. Finally, we have as the MMSE estimator
for the complex Bayesian linear model, from (15.61),
é = ,1, + CggHH (HCWHH + cw)“ (i - H119) (15.64)
= p9 +(c;,1 + iﬂcgln)“ H”c,;1(i - Hm) (15.65)
and the minimum Bayesian MSE can be shown to be, from (15.62),
BII1S8(9A,~) _—_. [C99 — C99HH (HCQQHH + Cu-J-l HG”) ii 
= ((g;,1+ Hlmglnfﬂii (15.67)
which is just the [i,  element of Cm. The reader will note that the results are identical
to those for the real case with the transposes replaced by conjugate transposes (see
Section 11.6). An example follows.
Example 15.11 - Random Amplitude Signal in CWGN
Consider the data set
where A ~ CN (0, of‘), §[n] is a known signal, and 1I1[n] is CWGN with variance a2
and is independent of A. This is a standard model for a slowly ﬂuctuating point target
[Van Trees 1971]. If  = exp(j21rf0n), we have the case examined in Example 15.8
in which we estimated 0,24. Now, however, suppose we wish to estimate the realization
of  Then, we have the complex Bayesian linear model with
Hence, frorn (15.65)
A z (JA) +5 031s S 
The minimum Bayesian MSE is, from (15.67),
Bmse(A) = ((0,24)_1 + iii) -
The results are analogous to the real case, and in fact, if § = 1 (the signal is a DC level)
,5 (E24 :
A - U2 + o, 2c
Bmse(A) = U2
where f: is the sample mean of the ir[n]’s. This is identical in form to the real case (see
(10.31 )). <>
15.9 Asymptotic Complex Gaussian PDF
Similar to the real case, if the data record is large and the data are a segment from a
complex WSS Gaussian random process with zero mean, then the PDF simplifies. In
essence the covariance matrix becomes an autocorrelation matrix which if large enough
in dimension is expressible in terms of the PSD. As we now show, the log-PDF becomes
approximately
lnp(i;£) = —-Nln7r — Nil [lnPi5(f) +  df (15.68)
where I ( f ) is the periodogram or
1m = g Z) an] exit-min)
This approximate form of the PDF then leads to the asymptotic CRLB, which is com-
puted from the elements of the Fisher information matrix
ialllpiiffi) alnpiilfié)
[not N  a5, 851 d; (15.69)
where the explicit dependence of the PDF on the unknown real parameters is shown.
These expressions simplify many CRLB and MLE calculations. To derive (15.68) we
could extend the derivation in Appendix 3D with obvious modiﬁcations for complex
data. However, a simpler and more intuitive proof will be given, which depends on the
properties of complex Gaussian PDFs. By doing so we will be able to illustrate some
useful properties of Fourier transforms of complex Gaussian random processes. For a
more rigorous proof and the conditions under which (15.68) applies, see [Dzhaparidze
1986]. In practice, the asymptotic results hold if the data record length N is much
greater than the “correlation time” (see Appendix 3D for a further discussion).
Consider the complex data set {it[0], 1':[1], . . . , i'[N — 1]}, where  is a complex zero
mean WSS Gaussian random process. The PSD is Pﬁ( f ) and need not be symmetric
about f = O. The discrete Fourier transform (DFT) of the data is
X(fk) = Z an1e><p<ﬂ21rtn>
where f), = k/N for k = 0,1,...,N — 1. We will ﬁrst find the PDF of the DFT
coefficients X (fk) and then transform back to  to yield the PDF p(i; 5). In matrix
form the DFT can be expressed as
where X = [X(f0) X(f1) . . .X(fN_1)]T and [Elkn = exp(——j21rfkn). If we normalize E
by letting U = E/JN, so that X = JNUi, then we note that U is a unitary matrix
or UH = U”. Next, since X is the result of a linear transformation of i, then X is
also complex Gaussian and has zero mean and covariance matrix N UCiUH . As an
example, if i is CWGN so that C5 = 021, then X will have the PDF
X ~ CA/(O, CX)
where
cX = NUa2IUH
The reader may wish to compare this to the results in Example 15.3 to determine the
reason for the difference in the covariance matrices. In general, to ﬁnd the covariance
matrix of the DFT coefficients we consider the [k7, l] element of CX or E [X ( fk)X ‘ ( f,)].
Because the DFT is periodic with period N, we can rewrite it as (for N even)
Xm) = Z ilnlexrrvawrkrn-
Then,
riﬂm — n] exp [-—j21r(fkm — fln)].
Letting i = m — n, we have
ElX(fk)X'(fl)l = Z Z Tiilil BXPI-JQWfIi) EXP l*J'21F(fk — filmi-
m=—% i=m—(—2M—1)
But as N ——> oo, the term in brackets approaches the PSD. or for large N
and by the orthogonality of the complex exponentials we have
EIXUIJXIUUl z NPi2(fk)5kz~
Thus we have asymptotically
fV1%5(jk) kI=l
E[X(f'°)X*(f')l = { O otherwise. (1570)
The DFT coefficients are approximately uncorrelated, and since they are jointly com-
plex Gaussian, they are also approximately independent. In summary, we have
X(f,,) »%c/\/(0,NPﬁ(f,,)) k=0,1,...,N—1 (15.71)
where “a” denotes the asymptotic PDF and the X ( fk)’s are asymptotically indepen-
dent. Recall that for CWGN these properties are exact, not requiring N —> oo. This
is due to the scaled identity matrix of i as well as to the unitary transformation of
the DFT. In general, however, C X is only approximately diagonal due to the prop-
erty that the columns of U or, equivalently, E are only approximately the eigenvec-
tors of C5, which is a Hermitian Toeplitz matrix [Grenander and Szego 1958]. In
other words, C X = U(NCi)UH is approximately an eigendecomposition of N C5. We
can now write the asymptotic PDF of X as X ~ CA/(QCX), where from (15.71)
CX = Ndiag(Pii(f@),Pi5(f1),...,Pii(fN_1)). To transform the PDF back to that
of i we note that X = Ei, and since U = E/x/N,
~ C/l/(O, TEEHCXE).
In explicit form this becomes
—Nln1r—lndet (WEHCXE) —xH (WEHCXE) x
—Nln1r - ln [det (NEHE) det (Ncxﬂ - HEHCxlEx
_Nln7r - Indet (%cx> - x”c;1x
since EHE/N = I and E71 = EH/N. Hence,
lnp(i;§) = ——Nln1r - In H Piigk) -
Since N is assumed to be large, we can further approximate this as
11117655) = _Nln"r_N 2 l1nPii(fk)l % “N 
z —Nln1r—NlollnPﬁ(f)df—Nlol df
In the last step we have used the fact that I ( f ) and Pii( f ) are periodic with period 1
to change the integration limits. To ﬁnd the Fisher information matrix we need only
compute the second partial derivatives of ln p(i; 5), noting that
Bum) = E(§|Xu>1’)
= ﬁvarurm)
z Pi-(f)
for large data records. An example of the utility of the asymptotic results follows.
Example 15.12 - Periodogram Spectral Estimation
The periodogram spectral estimator is deﬁned as
Z 1M exm-jwrrkn)
ﬁrear-
To determine the quality of the estimator we compute its mean and variance using the
results of (15.71). The mean has already been shown to be approximately Piﬂfk), so
that it is asymptotically unbiased. The variance is asymptotically
Vaﬂpiiifkl) = ElPZi(f1=)l-E2l15ii(fk)l
= ElP§i(fk)l“Pi2i(fk)-
$ElX<rk>1v<rk>1 EiXuaXwrkn
+  [Xukixwrkn E [X(fl)X‘(fk)l
= %var2(X(fk))
making use of the fourth-order moment properties of jointly complex Gaussian random
variables. Thus, we have that asymptotically
afipiﬂfkl) = Pgi-(fk)
and does not decrease with increasing N. Unfortunately, the periodogram is inconsis-
tent, an example of which is shown in Figure 15.4 for CWGN with 02 = 1. As can be
seen, the average value of the periodogram is the PSD value Pﬁ( f ) = a2 = 1. However,
the variance does not decrease as the data record length increases. The increasingly
“ragged” nature of the spectral estimate results from the larger number of points in
the PSD that we are estimating. As previously shown, these estimates are independent
(since X ( fk) is independent of X ( fl) for k 9E l), causing the more rapid ﬂuctuations
with increasing N. O
15.10 Signal Processing Examples
We now apply some of the previously developed theory to determine estimators based
on complex data. First we revisit the problem of sinusoidal parameter estimation in
Examples 3.14 and 7.16 to compare the use of real to complex data. Next, we show
how an adaptive beamformer results from classical estimation theory.
Example 15.13 - Sinusoidal Parameter Estimation
Consider a data model for a complex sinusoid in CWGN or
ir[n]=Aexp(j21rfon)+u7[n] n=0,1,...,N—1
where A is a complex amplitude, f0 is the frequency, and 1I1[n] is a CWGN process with
variance a2. We wish to estimate A and f0. Since the linear model does not apply
because of the unknown frequency, we resort to an MLE. To determine the asymptotic
performance of the MLE we also compute the CRLB. Recall from Chapter 7 that at
least at high SNRs and /or large data records the MLE is usually efficient. We observe
that one parameter, A, is complex and one, f0, is real. The simplest way to ﬁnd the
CRLB is to let A = Aexp(j¢) and ﬁnd it for the real parameter vector £ = [A f0 45F.
2°~ 2o
A loj 10
(Q v
‘go —5 in _5
“(H ~20
Frequency Frequency
(a) N=128 (c) N251;
Periodogram (dB)
Periodogram (dB)
$-
-0.5 -O.4 —0.3 -O.2 ~01 0.0 0.1 0.2 0.3 0.4 0.5 _0_5 _0_4 _O_3 A02 _()_1 Q0 ()1 ()3 Q3 Q4 Q5
Ffequemy Frequency
(b) N = 256 (d) 1v = 1024
Figure 15.4 Inconsistency of periodogram
where .§[n] = Aexp(j21rf0n) = Aexp[j(21rfon +  Now the partial derivatives are
Then, from (15.52) we have
l] a€i '72 a5: 85m] = j21rnAexpLj(21rfon + 45)]
_ 2 R N“ awn] 05m] aéffl]
5E e  as.- as. ' 645 = jAexpuwwron + w]
so that
2 O A2 Z(21rn)2 A2 221m
Upon inversion and using (3.22) we have
var(A) 3 5%
mm’) 2 (21r)2A2N(N2 - 1)
~ 02(2N — 1)
varw) 2 A2N(N +1) >
and in terms of the SNR, which is 17 = Ag/az, this reduces to
var(A) > %
varwl) _  (15.72)
The bounds for the complex case are one-half those for the real case for f}, and qAS, and
one-fourth that for A. (The reader should be warned, however, about drawing further
conclusions. We are actually comparing “apples to oranges” since the data models are
different.) To ﬁnd the MLE we must maximize
p(x;§) =  exp [—;5  — A8Xp(]27l'f()n)‘ ]
or, equivalently, we must minimize
1mm = Z liln] - Aexpij21rfo1lli
The_minimization can be done with respect to A and <15 or, equivalently, with respect
to A. The latter approach is easier since for f0 ﬁxed this is just a complex linear LS
problem. In matrix form we have
J(A, f0) = (i - eA)H(i _ eA)
where i :   . . .  -  and e =  exp(j21rf0) . . . exp(j21rfo(N —  But
we have already shown in (15.49) that
a, _—. - [eH(x—eA)l .
Setting this equal to zero and solving produces
1 eHi
= w Z exp(—j21rf0n).
Substituting back into J
J01, 10> = 11%": - e11)
_ _ iHeeHi
= x” x — eHe
= x” x — eHe .
To minimize J over f0 we need to maximize
|eHi|2 _ 1 ”“_ (_ .21”, n)
eHe - F 2:0 exp J 0
which is recognized as the periodogram. The MLE of A and 45 are found from
A = ‘A1 = 1V 1':[n]exp(—j21rf0n) (15.73)
. Im(A)
¢ = arc an 1
Re(A
Im exp(—j21rf0n))
— arctan :0 (15-74)
Re   exp(—j21rf0n))
Note that the MLE is exact and does not assume N is large, as was required for the
real case. The need for large N in the real case so that f0 1s not near 0 or 1/2 can be
explained by the inability of the periodogram to resolve complex sinusoids closer than
about 1 / N in frequency. For a real sinusoid
s[n] = Ac0s(21rf0n+¢)
= %eXPL7'(21Ffo1l + ¢>1 + geXpl-Ji21rfon + ¢>1
the peak of the periodogram (even in the absence of noise) will shift from the true 207
frequency if the frequency difference between the complex sinusoidal components or l (a) f° = 0'1
[f0 — (-—fO)| = 2f0 is not much greater than 1/N. As an example, in Figure 15.5 we
plot the periodogram for A = 1, <15 = 1r/2, f0 = 0.1,0.05,0.025, and N = 20. N0 noise
is present. The peak locations for f0 = 0.1 and f0 = 0.05 are slightly shifted towards
f = 0. However, for f = 0.025 the peak location is at f = 0. This problem does not
occur with a complex sinusoid since there is only one complex sinusoidal component
and therefore no interaction. See also Problem 15.21 for the extension to two complex
sinusoids. <>
Periodogram (dB)
Example 15.14 - Adaptive Beamforming i Y v
We continue the array processing problem in Example 3.15 but now assume that the Frequency
noise at the sensors is nonwhite. It is desired to take into account the coloration of the
noise in designing a “beamformer," which is an estimator of the transmitted signal. In 207 (b) f0 I (m5
many cases the nonwhite noise is the result of an intentional jammer. Recall that for 15-1
an emitted sinusoidal signal Acos(21rF0t + <15) the received signals at a line array are
(see Example 3.15)
s,,(t) =Acos[21rf,n+21rF0(t—tO)+45] n=0,1,...,M—1
where n is the sensor number, f5 = F0(d/c) cos ﬂ (d is the sensor spacing, c is the
propagation speed, and ﬂ is the arrival angle) is the spatial frequency, and to is the
propagation time to the zeroth sensor. If we choose to process the analytic signal of
the received data, then letting 45’ = —21rFOt0 + <15 (<15 and to are generally unknown), we
have for the signal component
Periodogram (dB)
§n(t) _—_ Aexptjqﬂ’) eXpLjZ/rfsn) eXpUQﬂFOt) —O.5 —0.4 —0.3 A02 —0.1 0.0 0.1 0.2 0.3 0:4 0.5
= Aexp(j21rfsn)exp(j21rF@t). Frequency
At each instant of time we have a signal vector “snapshot” at the M sensors g 207 (c) f0 z 0025
at) = [ 500) 5.0)  §M_1(t) f iii
= Aexp(j21rF0t)e  Q 5i
where e = [1 exp(j21rf,) . . .exp[j(21rfs(M — 1)]]T. The data model is assumed to be 5 0i
x(t) = Aexp(]21rF0t)e + w(t) é 40:)
where wu) is a zero mean noise vector with a covariance matrix C. The mean and E ‘l5
covariance are assumed not to vary with t. We wish to design a beamformer that ‘m?
combines the sensor outputs linearly as —25—T
at) = 2 azirxt) —0.5 -0.4 -0.3 412 40.1 00 0 1 02 03
=0 Frequency
aHi(t)
Figure 15.5 Periodograrn for single real sinusoid
so that the signal is passed undistorted but the noise at the output is minimized. By
undistorted we mean that if i(t) = §(t) = Aexp(j21rF0t)e, then at the beamformer
output we should have
3}(t) = A exp(j21rF0t).
This requires that
aH§(t) = Aexp(j21rF0t)
aHe=l
0r ﬁnally the constraint
e a = 1.
The noise at the beamformer output has variance
E]|aHw(t)|2] = E]aHw(t)wH(t)a] *
= aHCa
which is also the variance of  Hence, the optimal beamformer weights are given
as the solution t0 the following problem: Minimize a” Ca subject to the constraint
eH a = 1. As a result, this is sometimes called a minimum variance distortionless
response (MVDR) beamformer [Owsley 1985]. The reader will recognize that we have
already solved this problem in connection with the BLUE. In fact, from (15.51) with
W = C, B = eH, and b = 1 we have the optimum solution
_ C_1e
aw“ — eHC-‘ei
The beamformer output is given as
rw eHC"1i(t)
As can be veriﬁed, ﬂ(t) is just the BLUE of A exp(j21rF0t) for a given t. In Example 15.7
we derived this result for F 0 = 0, and hence for e = 1. The name “adaptive beamformer"
comes from the practical implementation of (15.75) in which the covariance matrix of
the noise is usually unknown and hence must be estimated before a signal is present.
The beamformer is then said to “adapt” to the noise ﬁeld present. Of course, when
an estimated covariance matrix is used, there is no optimality associated with the
beamformer. The performance may be poor if a signal is present when the covariance
is estimated [Cox 1973]. Note that if C = 021, so that the noise at the sensors is
uncorrelated and of equal variance (“spatially white”), then (15.75) reduces to
§(t) = M E i‘,,(t) exp(—j21rf,n).
The beamformer ﬁrst phases the signals at the various sensors to align _them in time,
because of the varying propagation delays, and then averages them. This is the so-called
conventional beamformer [Knight, Pridham, and Kay 1981]. _ I I
To illustrate the effect of a nonwhite spatial noise ﬁeld assume that in addition to
white noise we have an interfering plane wave at the same temporal frequency but a
different spatial frequency f,- or arrival angle. Then, the model for the received data is
i(t) = A exp(j21rF@t)e + E exp(j21rF0t)i + ﬁ(t) (15.76)
where i = [1 exp(j21rf,») . . .exp[j(21rf,(M—1)]]T_and2ﬁ(t) is a spatial white noise process
or ﬁ(t) has zero mean and a covariance matrix a I for a ﬁxed t. If we model the
complex amplitude of the interference B as a complex random variable with zero mean
and covariance P and independent of ﬁ(t), then the interference plus noise can be
represented as _ _
w) = Bexp(j21rF@t)i + 11C)
where E(i!(t)) = 0 and
C = E(i'v(t)i'”(t))
= Pii”+a2I.
Then, using Woodbury’s identity (see Appendix 1),
and thus
PiHe
_ e MP+02
3GP‘ H PIeHMZ
e e _ MP+U2
= Cle-Cgl
where
C‘ ‘ M P|eHi|2
MP-l-az
PiHe
M(MP + 02) - P]e”i]2'
We see that the beamformer attempts to subtract out the interference, with the amount
depending upon the interference-to-noise ratio (P/az) as well as the separation 1n arrival
angles between the signal and the interference (eH i). Note that if e” i = 0, we Wlll have
a conventional beamformer because then 210p, = e/ M . This i5 beCallse the lnterferelme
Interference cancellation (dB)
4o so so 10o 120 14o
Interference arrival angle (degrees)
Figure 15.6 Response of adaptive beamformer to interference ’
fzrfectlzonullified since agni = 0. As an example, if the input is given by (15.76),
a1, [A exp(j21rF0t)e + E exp(j21rF0t)i + ﬁ(t))
= Aexp(j21rF0t) + E exp(j21rFOt)(c1eHi —~ Meg) + agnﬂﬂ),
The-signal is passed undistorted, while the interference power at the output divided by
the interference power at the input is
IQQHi-MCEIZ- (15.77)
As an example, we assume a line array with half-wavelength spacing so that the spatial
frequency is
F0  cos ﬂ
5 cos ﬂ.
Then, for P = 10, a2 = 1, M = 10, and a signal at ﬂ, = 90° so that f, = (J, we plot
(15.77) in dB versus the arrival angle of the interference in Figure 15.6. Note that the
response is unity when ﬂ, = 90° or when the interference arrival angle is the same as
thatwplf the signal. However, it quickly drops to ‘about —60 dB when the difference in
atrlrl . angles is about 6 . Recall that the adaptive beamformer is constrained t0 pass
e signal undistorted, so that when ﬂ, = m, the interference cannot be attenuated.
References
Brandwood, D.H., “A Complex Gradient Operator and Its Application in Adaptive Array Theory,”
IEE Proc., Vol. 130, pp. l1—16, Feb. 1983.
Cox, H., “Resolving Power and Sensitivity to Mismatch of Optimum Array Processors,” J. Acoust.
Soc. Am., Vol. 54, pp. 771-785, 1973.
Dzhaparidze, K., Parameter Estimation and Hypothesis Testing in Spectral Analysis and Stationary
Time Series, Springer-Verlag, New York, 1986.
Grenander, U., G. Szego, Toeplitz Forms and Their Applications, University of California Press,
Berkeley, 1958.
Kay, S.M., Modern Spectral Estimation: Theory and Application, Prentice-Hall, Englewood Cliﬁs,
Knight, W.S., R.G. Pridham, S.M. Kay, “Digital Signal Processing for Sonar,” Proc. IEEE, Vol.
69, pp. 1451-1506, Nov. 1981.
Miller, K.S., Complex Stochastic Processes, Addison~Wesley, Reading, Mass, 1974, available from
University Microﬁlms International, Ann Arbor, Mich.
Monzingo, R.A., T.W. Miller, Introduction to Adaptive Arrays, J. Wiley, New York, 1980.
Owsley, N.L., “Sonar Array Processing,” in Array Signal Processing, S. Haykin, ed., Prentice-Hall,
Englewood Cliﬁs, N..I., 1985.
Papoulis, A., Probability, Random Variables, and Stochastic Processes, McGraw-Hill, New York,
Ran, C.R., Linear Statistical Inference and Its Applications, J. Wiley, New York, 1973.
Van Trees, I-I.L., Detection, Estimation, and Modulation Theory, Part III, J. Wiley, New York,
Williams, J.R., G.G. Ricker, “Signal Detectability of Optimum Fourier Receivers,” IEEE Trans.
Audio Electroacoust, Vol. 20, pp. 264—270, Oct. 1972.
Problems
15.1 Two complex random variables i, = ul + jvl and i‘; = u; + jUg are said to
be independent if the real random vectors [n1 vllT and [a2 v;]T are independent.
Prove that if two complex random variables are independent, then their covariance
is zero.
15.2 Prove that the complex covariance matrix in (15.13) is positive semidefinite. Un-
der what conditions will it be positive deﬁnite?
15.3 The real random vector x = [ul u; v1 v2]T has the PDF x ~ A/(O, CI), where
Can we define a complex Gaussian random vector, and if so, what is its covariance
matrix?
15.4 For the real covariance matrix in Problem 15.3 verify directly the following:
a. xTCfx = ZiHCQIi, where x = [uT vT]T and i = u + jv
b. det(C,) = det2(C5)/16
15.5 Let M 2 denote the vector space of real 2 >< 2 matrices of the form
a ——b
b a
and C1 the vector space of scalar complex numbers. Deﬁne addition and multi-
plication in the vector spaces in the usual way. Then, let m1, m2 be two vectors
in M2 and consider the operations
a. aml, where a is a real scalar
Show that these operations can be performed as follows. >
a. Transform m1, m; to C 1 using the transformation
a —b .
b a -+ a + 3b.
b. Carry out the equivalent operation in C l.
c. Transform the result back to M2 using the inverse transformation
e+jf—>[j, 2f].
Because operations in M2 are equivalent to those in C‘, we say that M2 is iso-
morphic to C‘.
15.6 Using the ideas in Problem 15.5, compute the matrix product ATA, where
a1 -b1
b1 a1
_ a2 -b2
A _ b2 (12
41a -bs
b3 (13
by manipulating complex numbers.
15.7 In this problem we prove that all third-order momentsaof a complex Gaussian PDF
are zero. The third-order moments are E(i:2),E(5:* ), E(i:*i‘2), E(i:i*z). Prove
that these are all zero by using the characteristic function (see Appendix 15B)
¢>.<~v> = exp (-;1,-@*|~v|’)-
15.8 Consider the zero mean complex random variable 5:. If we assume that E(i2) = O,
what does this say about the variances and covariances of the real and imaginary
parts of i?
15.9 Show that the assumption  — fr)(i — [QT] = 0, where x = u+jv leads to the
special form of the real covariance matrix for [uT vTlT given by (15.19).
15.10 A complex Gaussian random vector i has the PDF i ~ CA/(O, 02B), where B is a
known complex covariance matrix. It it desired to estimate 02 using the estimator
02 = ~HAi, where A is a Hermitian matrix. Find A so that 02 is unbiased and
has minimum variance. If B = I, what is 02? Hint: Use (15.29) and (15.30) and
the fact that tr(D'°) =  A1“, where the /\,-’s are the eigenvalues of the N >< N
matrix D and k is a positive integer.
15.11 If 52hr] is CWGN with variance 02, ﬁnd the mean and variance of 25:11 
Propose an estimator for 02. Compare its variance to that of 02 = (1 / N ) if;
for the real WGN case. Explain your results.
15.12 In this problem we show that the random analytic signal is a complex Gaussian
process. First consider the real zero mean WSS Gaussian random process 
Assume that uln] is input to a Hilbert transformer, which is a linear time-invariant
system with frequency response
and denote the output by uh]. Show that v[n] is also a real zero mean WSS
Gaussian process. Then. show that ﬁn] = uln] +jv[n], termed the analytic signal,
is a complex WSS Gaussian random process by verifying that ru,,[k] = rwlk] and
rm, = —r,,.,[k]. What is the PDF of ﬁn]?
15.13 Prove that 39*/39 = O. What would 39/30 and 36*/30 be if we used the
alternative deﬁnition
a0 ‘ 3a 2 a5
for the complex derivative?
15.14 Prove that
where b and 9 are complex.
15.15 Determine the complex gradient of BHAB with respect to complex 6 for an
arbitrary complex p X p matrix A, not necessarily Hermitian. Note that in this
case BHAO is not necessarily real.
15.16 If we observe the complex data
where A is a complex deterministic amplitude, "y is a known complex constant,
and 1D['n] is CWGN with variance a2, ﬁnd the LSE and also the MLE of A. Also,
ﬁnd its mean and variance. Explain what happens as N —~> oo if |'y| < 1, lry| = 1,
and |"y| > 1.
15.17 We observe the complex data  for n = O, 1, . . . , N — 1. It is known that the
i[n]’s are uncorrelated and have equal variance a2. The mean is E = A,
where A is real. Find the BLUE of A and compare it to the case when A is
complex (see Example 15.7). Explain your results.
15.18 If we observe the complex data iln] = §[n;6] + iI1[n], where the deterministic
signal is known to within a real parameter 9 and iI/[n] is CWGN with variance
a2, ﬁnd the general CRLB for 0. Compare it to the case of real data (see Section
3.5) and explain the difference.
15.19 If  is CWGN with variance a2, ﬁnd the CRLB for a2 based on N complex
samples. Can the bound be attained, and if so what is the efficient estimator?
15.20 In this problem we study the complex equivalent of the Gauss-Markov theorem
(see Chapter 6). The data model is
where H is a known complex N X p matrix with N > p and full rank, 6 is a
complex p X 1 parameter vector to be estimated, and w is a complex N >< 1 noise
vector with zero mean and covariance matrix C. Show that the BLUE of 6 is
given by (15.58). Hint: Let 9i = afii and use (15.51).
15.21 The MLE for the frequencies of two complex sinusoids in CWGN is explored in
this problem, extending the results in Example 15.13. The data model is
an] = A1exp(j21rf1n) + A. exp(j21rf2n) + 1141.} n = 0, 1, . . . , N - 1
where Al, A2 are complex deterministic amplitudes that are unknown, f1, f; are
unknown frequencies which are of primary interest to us, and iIiln] is CWGN with
variance a2. We wish to estimate the frequencies, but since the amplitudes are
also unknown, we will also need to estimate them as well. Show that to ﬁnd the
MLE of the frequencies we will need to maximize the function
J(f1, f2) = '”E(E”E)_‘E”i
where E = [e182] and e, = [1 exp(j21rf,-) . . . exp(j21rf,-(N—1))]T fori = 1, 2. To do
so ﬁrst note that for known frequencies the data model is linear in the amplitudes
and so the PDF can be maximized easily over Al, A2. Finally, show that under
the constraint {f1 — f2] >> 1/N, the function J decouples into the sum ,of two
periodograms. Now determine the MLE. Hint: Show that EH E is approximately
diagonal when the frequencies are spaced far apart.
15.22 A signal frequently used in radar/ sonar is a chirp whose discrete-time equivalent
§[n] = Aexp [j21r(f@n + $019)]
where the parameter a is real. The instantaneous frequency of the chirp may be
deﬁned as the difference of the phase for successive samples or
f,-[n] = lfon + $01112] - [fom - 1) + $0411 n1?)
= (f0 - %a)+0m.
The parameter a is thus the frequency sweep mte. Assume the chirp signal is
embedded in CWGN with variance a2 and N samples are observed. Show that
the function to be maximized to yield the MLE of f0 and a is
Assume that A is a unknown deterministic constant. Show how the function can
be computed efﬁciently using an FFT for each assumed sweep rate.
15.23 Consider the complex random parameter 0 = a + jB, which is to be estimated
based on the complex data vector x = u+ jv. The real Bayesian MMSE estimator
is to be used for each real parameter. Hence, we wish to ﬁnd the estimators that
minimize the Bayesian MSEs
Bmse(d) = E [(0 — 6J2]
where the expectations are with respect to the PDFs p(u,v,a) and p(u,v, B),
respectively. Show that
Bmse(
a = a +15
= E (Qlu, v)
= E (9|x)
is the MMSE estimator. Comment on the PDF used to implement the expectation
operator of 
15.24 Assume that i111] is a zero mean complex Gaussian WSS random process whose
PSI) is given as Pia f ) = POQ( f), where P0 is a real deterministic parameter to
be estimated. The function Q( f ) satiﬁes
i Q(f)df=1
so that P0 is the total power in  Find the CRLB and the MLE for PO by
using the exact results as well as the asymptotic results (see Section 15.9) and
compare.
15.25 In this problem we examine the “processing gain” of a DFT in detecting a
complex sinusoid in CWGN. Assume that we observe
Appendix 15A
53in] = Aexp(j2vrfcn) + 11in] n = 0,1,. ..,N - 1
where A is a complex deterministic amplitude and 111M] is CWGN with variance
a2. If we take the DFT of 52hr] and if fc = l/N for some integer l, ﬁnd the PDF
of the DFT coefficients X(fk) for k = O. 1. . . . , N — 1. Note that
Z exp (ﬂvrﬁn) = O
Derivation of Properties
of Complex Covariance Matrices
We assume that the 2n X 2n real covariance matrix C, has the form of (15.19),
where A is a real and symmetric n x n matrix, B is a real and skew-symmetric n >< n
matrix, and C, is positive deﬁnite. We deﬁne a complex matrix as Ci = A+jB. Then,
1. C; is Hermitian.
|E(i[ﬂl)|2
R(mput) U2
to the output SNR of the DFT at the frequency of the sinusoid or H
C? = (A +13)
:lE(X(f@))|2 = AT-JBT
SNmOutput) var(X(f¢)) = A+jB
and discuss. The improvement in the SNR is called the processing gain [Williams
and Ricker 1972]. If the frequency was unknown, how could you detect its pres-
2. C; is positive deﬁnite.
ence?
We ﬁrst prove that if i = u + jv, then
where x = [uT vT]T.
for k = O, 1, . . . , N — 1. Then, compare the input SNR or
(“T - JVTXA +J'B)(11 +1")
Thus, if C, is positive deﬁnite or xTCmx > O for all x 9E 0, it follows that iHCii >
l Since B is skew-symmetric, uTBu = vTBv = O, and since A is symmetric, we have
I Ofor a1lx9é0sincex=0 ifand only ifi=0.
3. C5 is the covariance matrix of i = u + jv.
By deﬁnition C; = A + jB, Where
1A = EH11 - E(\1))(\1 — E(\1))Tl = EKV — E(v))(v — E(v))Tl
$13 = El(v—E(v))(u—E(u))Tl-
E{[(u— Eon) +j<v- E<v>>1 uu- Em» -j<v —E<v>>1’}
4. The inverse of C, has the special form
so that the inverse of the complex covariance matrix may be found directly from
the inverse of the real covariance matrix as C? = E + jF.
To show this we use the partitioned matrix formula (see Appendix l) to yield
2 (A+BA-‘B)—1
—(A+BA“B)“BA“1
Now, to show that E + j F is the inverse of C;
(A+BA"B)“BA'1
(A+BA-‘B)—1
and thus
Using these relations, we have
Cecg‘ (A + 11am: +11")
AE — BF +j(BE + AF)
0-10,, = (ogci-l”)
= (CeC;‘)”
5- (x - “fem - ll) = 2e - nWCe-lor - m.
Let y = x — p. and y = i — f», so that we wish to show
But from property 4 we know that C? has the special form, and from property
2 we have already shown the equivalence of a quadratic and Hermitian form.
Accounting for the factor of 2 because we now have an inverse covariance matrix,
this property follows.
6. det(C,) = det2(C,-,)/(22“)
We ﬁrst note that det(Ci) is real. This is because Ci is Hermitian, resulting in
real eigenvalues, and the relationship det(Ci) = Hfzl /\,~. Also, because C; is
positive deﬁnite, we have /\,- > O, and thus det(Ci) > O. The determinant is real
and positive. Now since
we can use the determinant formula for partitioned matrices (see Appendix 1) to
yield
det(C,,) = (éyndetﬂg ‘ABD
= (1)211 det(A) det(A + BA"B).
But A + BA“1B = (A — jB)A_‘(A + jB), so that
det(A — jB) det(A +jB)
det(A)
det(C;) det(Ci)
det(A) i
Since det(C§) = det"(Ci) = det(Ci), we have ﬁnally
det(A + BA'1B) =
det(C,) = 2%det2(Ci).
Appendix 15B
Derivation 0f Properties
Complex Gaussian PDF
Any subset of a complex Gaussian random vector is also complex Gaussian.
This property follows from property 4 to be proven. If we let y be a subset of the
elements of i by setting y = Ai, where A is an appropriate linear transformation,
then by property 4 y is also complex Gaussian. For example, if i = [5:1:ij]T is a
complex Gaussian random vector, then
will also be complex Gaussian.
If i1 and i; are jointly complex Gaussian and uncorrelated, they are also inde-
pendent.
By inserting the covariance matrix
_ a2 O
e-loWal
into (15.22), it is easy to see that the PDF factors as p(i) = p(5:1)p(.i2). Then,
we note that p(i1) is equivalent to p(u1,111) and p(ip) is equivalent to p(up, U2).
Hence, [u1 v1[T is independent of [U2 up]? Of course, this property extends to any
number of uncorrelated random variables.
If 5:1 and i; are each complex Gaussian and also independent, then i = [521 52F
is complex Gaussian.
This follows by forming p(5:1)p(.ig) and noting that it is equal to p(i) of (15.22).
Again this extends to any number of independent complex Gaussian random vari-
ables.
4. A linear transformation of a complex Gaussian random vector is also complex
Gaussian.
We ﬁrst determine the characteristic function. It is well known that for a real
Ilrgiélijtivariate Gaussian PDF or x ~ N (p, CI) the characteristic function is [Rao
¢11(w) = E[exp(jwTx)[
= exp [jwTu—gwTCew[
Where w = [W1 W2 - - J-vznlT- If X = [UT VT]T and C, has the special form, then
letting i = “+1111 = lip +11%, and w = lwiiwilli 5° that <5’ = wR +jw;, we
have
= Reuvﬂl)
and ~2I§AJ~TCIw = QHCiQ from Appendix 15A. Likewise, we have that wTx =
Re(w x). Hence we can deﬁne the characteristic function of i as
exp) = E [expu Rewern] (15131)
and for the complex Gaussian PDF this becomes
exw = exp [jReavw - i ~ HCe-@[ . (15112)
Now, if y = Ai + b, we have
p101») = E [exp (j Re<w”§>)[
E [exp (j Re(¢DHAi + ea”b))[
exp [j Re(¢DHb)[ E [exp [j Re((AH¢D)Hi)]]
exp [j Re(¢DHb)[ ¢,-1(AH¢D)
exp [j Re(@”b)[ exp [j Re(¢l»”Ap)[ exp [-i " ”AC,1A”Q[
= @xp[1 Re(w”(Au + b)) - Z ”AciA”ez»[ .
By identifying the characteristic function with that for the complex Gaussian PDF
we have
y ~ CA/(Aﬂ + b,ACiAH).
5. The sum of two independent complex Gaussian random variables is also complex
Gaussian.
Using the characteristic function, we have
¢5I1+52  z E lexp(j Re(@*(:i1 + 
E leXP(j 314551)) EXPU Reﬂiﬁizdll
E leXP(J'Re(@*i1))l E [EXPU 3145750)]
$11 (‘TIME (51)
exp )1 Re(w P1) — llwlzﬂf) exP )1 Re(w m) — Zlwlzﬂg)
= exp [1 Raw (#1 +112» - 4-1|w|’<@f+ 02>)-
Hence, we have i1 + :32 ~ CA/(ﬂl + [12, of + 0g). Of course, this property extends
to any number of independent complex Gaussian random variables. >
6. The fourth moment of a complex Gaussian random vector is given by (15.24).
Consider the characteristic function for x = (5215:; 5:3 EAT, where x is complex
Gaussian with zero mean. Then,
¢i<w> = ElexP(iRe(@”x)))
exp [—4l1 ' HCiLD) .
We ﬁrst show that
_ = gEozyiaigir). (153.3)
(4):!)
34¢i(5’)
Differentiating, we obtain
 —  {exp gQDHi-kiffdaﬂ}
E1 (‘II
since, using (15.41) and (15.42),
Likewise,
19¢ (Q _ '
82* ) = {é-agexp  (~Hx+ iHcbm}
since
at»; _
(similar to the results of (15.41) and (15.42)). By repeated application (15B.3)
follows. We now need to evaluate the fourth partial derivative of
¢i(51)
Ea
“a
8&1 —- "'4-  LUZ  EXP (—Z HCILIJ)
awgo 1 4 q 1
012E012»; ‘ (‘Z 2% [C1111 “Z Zlczlzji] exr>(~1d»”C,d=)
- Zlcrlzi exp (—%GJHC,-,LD>
+ T6[Cil21 gull, [Ciliii EXP (—ZWHCI"W> -
Only the last two terms will be nonzero after differentation with respect to d);
and setting c5 = 0. Hence,
34¢i(@)
5x51525314) = E(i4il)E(i2i§) + 35253593453)-
. The conditional PDF of a complex Gaussian PDF is also complex Gaussian with
mean and covariance given by (15.61) and (15.62).
The easiest way to verify the form of the conditional PDF is to consider the
complex random vector 2 = f! — CgiCa-jgi. The PDF of 2 is complex Gaussian,
being a. linear transformation of jointly complex random vectors, with mean
5(5) = 5(5) - 9@i95§E(i)
and covariance
9H = BUY-EU?)—9i;i95§(i—E(i)))
~ (y - Em — C§i9§§(i — E<x>>)”]
+ Cgioggoiiof-‘Cf’
= 95m " cai9E§9w
But conditioned on i we have f! = i +CgiC§i1i, where i is just a constant. Then,
p(§'|i) must be a complex Gaussian PDF since i is complex Gaussian and i is a
constant. The mean and covariance are found as
E(ifli) = E( )+ Cﬂicgili
— E< )— CﬁiC;i1E(i)+ 0145;»?
5(9) + 0,7595%? — EGO)
and
Appendix 15C
Derivation of
CRLB and MLE Formulas
We ﬁrst prove (15.60) using (15.47).
<91I1P(i; 5)
_<?1ndet(C@(5)) _ aci - am)” one)
at
= -95‘(5)—— —
The last step follows from 050;‘ = I, so that
_ tr (Cfl  
) - (i- nus»
(5)
951(5)
3951(5)
3Ci(5)
311(5)
(i — ﬁ(5))
[(151001 — 11(0)]
(i—ﬁ(5))
3Ci(5)
(5) (i—ﬁ(5))
951(5) (i—ﬁ(5))~
+ a”3:f‘)<>;‘<:> (E - m»
from which (15.60) follows. Next we prove (15.52). To do so we need the following
lemma. If i ~ CN (0, C), then for A and B Hermitian matrices [Miller 1974]
E(i”Aii”Bi) = tr(AC)tr(BC) + tr(ACBC).
By deﬁnition of the Fisher information matrix and (15.60)
E { l-E (E;1<E*’°E<E>) +01 - Em)” Ens)
We note that all ﬁrst- and third-order moments of 3? = i — [1, are zero. Also, all second-
order moments of the form Eﬁ/f/T) and therefore E "“ ) = [E(§/)"T)]‘ are zero (see
Problem 15.7), so that the expectation becomes
= E <C;1(£)8Ci(£)) E (E;‘<:>a°*“))
-1 3912(5) -1 39145)
-tr(ci (s) 33 )E(E. (s) 33 )
t(<1.<:> 33 )t (C. (a) 33 )
+ E li”(35‘(£)a(332(i£)C;‘(£)ii”<3;‘(£)6(32££)C;‘(£)i)
+ E (E — E<:>>"E;‘<s>9-’3‘§a‘33‘9 C;1(£)(i - 11(0))
+ E W321i‘) Emmi - ﬂ(£))(i - E<:>>”E;‘<:>a3‘3‘f)]
where we have used
E li"<1;‘<:>8‘3*3(f) Cgwi] = E [E (C;‘(£)a(3“2€£)(3;‘(€)5'5'”))
Evie) -. --
EI(<E‘<E“2ZE“)-
= tr(<1;‘<s>
Simplifying, we have
[I(£)].-,~ = -E (E;‘<:>6°E‘°) tr(<1;1<:>8°*@)
+ E [S/"Aii/“Bil + 8 33l£)<1;‘<:><1@<:><1;‘(:>@3—§-Q
EE”(:> -1 _ Ems)
where
A = E;‘<:>8‘3§(_9¢;1<s>
Note that the last two terms are complex conjugates of each other. Now, using the
lemma,
-1 805K) 3. Erma)
+E(E. (a) 33 )E(E. (s) 33 )
-1 39519-1 3CE(€)
+E(E3 (s) 33 E3 (a) 33 )
6ﬁ”(£) _1 Ems
lI(£)l.-,-
+2Re
a
EE”(:> _1 E
33 C3 (a)
+2Re[ aéj
Appendix 1
Review of Important Concepts
A1.1 Linear and Matrix Algebra
Important results from linear and matrix algebra theory are reviewed in this section.
ln the discussions to follow it is assumed that the reader already has some familiarity
with these topics. The speciﬁc concepts to be described are used heavily throughout
the book. For a more comprehensive treatment the reader is referred to the books
[Noble and Daniel 1977] and [Graybill 1969]. All matrices and vectors are assumed to
be real.
A1.1.1 Deﬁnitions ’
Consider an m X n matrix A with elements a”, i = 1,2,...,m;j = 1,2,...,n. A
shorthand notation for describing A is
[Alia = “u-
The transpose of A; which is denoted by AT, is deﬁned as the n X m matrix with
elements a], or
A square matrix is one for which m = n. A s uare matrix is s metiiiil: if AT = A.
The ran? of a matrix is the number of linearl independent rows or columns,
whichever is less. The inverse of a sguare n X n matrix is t e square n X n matrix
A“ for which g y
where I is the n X n identity matrix. The inverse will exist if and onl if the rank of A
is n. Il tEe inverse does not exist then A is sin 141.1‘?
e e eHninantTBf a square n X n matrix is denoted by det(A). It is computed as
where _
C” = (-1)'+’M,-J-‘f’
is the determinant of the submatrix of A obtained by deletin the ith row and jth
4 -. Note that any choice
In deﬁning the quadratic form it is assumed that aJ-i = aij. This entails no loss in
generality since any quadratic function may be expressed in this manner. Q mav also
be expressed as
where x = lzl 222N412" T and A is a square n X n matrix with a]; = a” or A is a
symmetric matrix. v
A square n X n matrix A is positive semideﬁniteif A is symmetric and
for all x 96 0. If the quadratic form is strictl ositive. then A is positive deﬁnitej When
referring to a matrix as positive deﬁnite or positive semideﬁnite, it is always assumed
that the matrix is symmetric.
The tracéiof a square n X n matrix is the sum of its dia onal elements or
 = tan?)
A partitioned m X n matrix A? is one that is expressed in terms of its submatrices.
An example is the 2 X 2 partitioning
Each “element” Al-Z- is a submatrix of A. The dimensions of the partitions are given as
[ kXl k><(n—l) ]
(m—k)Xl (m-k)><(n—l) '
A1.1.2 Special Matrices 
A diagonal matrix is a square n X n matrix with a; = 0 for 2' ' or all elements off
the principal diagonal are zero. iagona matrix appears as
(111 0 . . . O
0 (122 0
0  am,
A dia onal matrix will sometimes be denoted by dia an, an, . . . , 111m). The inverse of
a diagonal matrix is found by simply inverting each element on the principal iagonal.
generalization of the diagonal matrix is the square n X n block diagonal matrix’
in which all submatrices A” are square and the other submatrices are identicall zero.
lhe dimensions of the submatrices need not be identical. For instance, if k = 2, A11
might have dimension 2 X 2 while A22 might be a scalar. If all A“ are nonsin ular then
the inverse is easily found as
Also, the determinant is
det(A) = HdeqAﬁ). ,2
A square n X n matrix is orihogonanf
For a matrix to be orthogonal the columns (and rows) must be orthonormal or if
 A=[a1 a;  an]
where a,~ denotes the ith column, the conditions
“iraﬁii 
must be satisﬁed. An important example of an orthogonal matrix arises in modeling of
data by a sum of harmonically related sinusoids or by a discrete Fourier series. As an
example, for n even
% COSALJ Lcoslzi; “£11  Sm "< X" >
is an orthogonal matrix. This follows from the orthogonality relationships for i, j =
Ecos n cos n = 5 2.: =1,2, .,;—1
and for i,j= 1,2,...,n/2—1
and ﬁnally f0ri=O,1,...,n/2;j =1,2,...,n/2—1
Z cos Lrm sin fr,” = O.
These orthogonality relationships may be proven by expressing the sines and cosines in
terms of complex exponentials and using the result
Z exp J-—I_cl = n5“,
for l = 0,1, . . . ,n — 1 [Oppenheim and Schafer 1975].
An idempotent matrix is a square n X n matrix which satisﬁes
This condition implies that A’ = A for l Z 1. An example is the projection matrix '
A = H(HTH)"HT
where H is an m X n full rank matrix with m > n.
A square n x n Toeplitgxnatrix is deﬁned as
[Alia = “w 
(IQ (1_1 0._2 . . . 0._(n__1)
G1 G0 0._1 . . . G._(n_g)
A = . . . . . . (A1.1)
an-l an-Z an-S - ~ - a0
Each element along a northwest-southeast diagonal is the same. If in addition, a_k =
ak, then A is symmetric Toeplitz.
._.._. . ......_.....-....-_a
A1.1.3 Matrix Manipulation and Formulas
Some useful formulas for the algebraic manipulation of matrices are summarized in this
section. For n X n matrices A and B the following relationships are useful.
(AB)T = BTAT
(ATl-l = (A-WT
(AB)-‘ = B-1A-1
det(AT) = det(A)
det(cA) = c"det(A) (cascalar)
det(AB) = det(A) det(B)
det(A ) = deﬂA)
) tr(BA)
Also, for vectors x and y we have
yTx = tr(xyT).
It is frequently necessary to determine the inverse of a matrix anal ticall . To do so
one can make use of the following formula. liie inverse of a square n X n matrix is
A ‘ det(A)
where C is the square n X n matrix of cofactors of A. The cofactor matrix is deﬁned by
lclu = (-1)'+"Mij
where M,-- is the minor of ai- d 'th column of A.
Another formula which is quite useful is the matrix inversion lemma
(A + BCD)“ = A" - A"B(DA“B + c-lylniv"
where it is assumed that A is n X n, B is n X m, C is m X m and D is m X n and that
t e indicate inverses exis . special case known as W00dbury’s identityﬁresults for B
an n X 1 column vector u, C a scalar of unit , and D a 1 X n row vector uT. Then,
( +uu ) 1+uTA-1u
Partitioned matrices may be manipulated according to the usual rules of matrix
algebra, considering each submatrix as an element. For multilication of partitioned
matrices the submatrices which are multiplied together must be conformable. As an
illustration, for 2 X 2 partitioned matrices
$A B = \ldots $
The transposition of a artitioned matrix is formed b trans osin the submatrices of
the matrix an applying T to each submatrix. For a 2 X 2 partitioned matrix
The extension of these properties to arbitrary partitioning is straightforward. Pi-
termiriation of the inverses and determinants of partitioned matrices is facilitated by
emp oying t e o owing ormu as. et A e a square n X n matrix partitioned as
A: i All Al: l = i (nfihkxk (niz)(z(—nklk) 
where the inverses of A11 and A2; are assumed to exist.
A1.1.4 Theorems
Some important theorems used throughout the text are summarized in this section.
1. A square n X n matrix A is invertible nonsin ular if and only if its columns (or
rows are inearly inde endent or, equivalently, if its determinant is nonzero. In
such a case, A is full mnk. Otherwise it is sin ular.
2. A sguare n X n matrix A is positive deﬁnite if and only if
a. it can be written as
A = cc’ (A1.2)
where C is also n X n and is full rank and hence invertible, or
M“- _ Q-“a-
b. the princi al minors are all positive. (The ith principal minor is the determi-
nant of the submatrix formed 5y deleting all rows and columns with an index
greater than  If A can be written as in (A12), hut C is not full rank or
the principal minors are only nonne ative then A is ositive semideﬁnite.
3. If A is positive deﬁnite, then the inverse exists and may be found from (A1.2) as
A“ =(C_1) (C‘ 
4. Let A be ositive deﬁnite. If B is an m X n matrix of full rank with m 5 n, then
BABT is also positive deﬁnite.
5. If A is positive deﬁnite positive semideﬁnite), then
a. the diagonal elements are positive Qnonnegativg)
b. the determinant of A, which is a principal minor, is positive (nonnegative).
A1.1.5 Eigendecompostion of Matrices
Ari eigenvector of a square n X n matrix A is an n X 1 vector v satisfying
Av = Av (ALB)
 . A is the eigenvalue (if A corresponding
to t e eigenvector v. It is assumed that the eigenvector is normalized to have unit
length or VIV = 1. If A is symmetric, then one can always ﬁnd n linearly indepen-
dent eigenvectors, althoug they wi not in general be unique. An example is the
identity matrix for which any vector is an eigenvector with eigenvalue 1. If A is sym-
metric then the ei envectors corresponding to distinct ei envalues are orthonormal or
vTvj = 51,-’ and the eigenv ues are real. If, furthermore, the matrix is positive deﬁnite
(positive semideﬁnite), then the eigenvalues are ositive (nonnegative . For a positive
semi e ni e matrix e ran is e ual to the number of nonzero ei envalues.
The deﬁning relation of (A1.3) can also be written as
AV = VA (A1.4)
where
A = diag(A1, A2, . . . , A");
If A is symmetric so that the eigenvectors corres onding to distinct eigenvalues are
orthonormal and the remainin ei envectors are chosen to yield an orthonormal ei -
vector set, then is an orthogonal matrix. As such, its inverse is V , so that (A14)
becomes
Also, the inverse is easily determined as
A ﬁnal useful relationship follows from (A14) as
det(A) deav) det(A) det(V_ 1 f’
= det(A) "
I a
A1.2 Probability, Random Processes, and Time Series
Models i
An assumption is made that the reader already has some familiarity with probability
theory and basic random process theory. This chapter serves as a review of these topics.
For those readers needing a more extensive treatment the text by Papoulis [1965] on
probability and random processes is recommended. For a discussion of time series
modeling see [Kay 1988].
A1.2.1 Useful Probability Density Functions
A probability density function lPDF) which is freguently used to model the statistical
behavior o a. ran om varia e is the Gaussian distribution. A random variable x with
mean p, and variance 0i is distributed according to a Gaussian or normal distribution
if the PDF is given by
pa) = ma exp [Ego - ps2]
The shorthand notation a: ~ A/(amai) is often used, where ~ means “is distributed
according to.” x ~ 0,0, , then t e moments of z are
— oo < a: < oo. (A1.5)
k _ 1-3---(k—1)a§ keven
E(’”)_{o kodd.
The extension to a set of random variables or a random vector x = [x1 x2 . . . :c,,]T with
mean
E(I) = us
and covariance matrix
E [(X — mXX — AMT] = C1.-
is the multivariate Gaussian PDF
(21r)')¢! det%(Cx)
pa) = exp [-5. - umcis - us] . (ALB)
Note that C; is an n X n symmetric matrix with [GIL]- = E{[rc,- — E(x,<)][:cj —  =
c0v(:c,-,:z,-) and is assumed to be positive deﬁnite 5Q that C, is invertible. If C, is a
diagonal matrix, then the random variables are uncorrelated. In this case {A115} factors
 S 0f the form 0f (A15), 12d hence the
random variables are also independent. If x is zero mean, then the higher order joint
moments are easi y computed. In particular the fourth-order moment is
Eﬁcizjzkzl) = E(a:,-.1:,-)E(:ckz;) + E(x,-zk)E(zJ-z1) + E(z,-.1:1)E(xjmk).
If x is linearly transformed as
where A is m X n and b is m >< 1 with m g n and A full rank (so that Cy is nonsingular),
then y is also distributed according to a multivariate Gaussian distribution with
E(y) = “y = A”: +h
and
Another useful PDF is the 2 distribution, which is derived from the Gaussian dis-
tribution. if x is composed of independent and identically distributed random variables
 » then
where Xi denotes a X2 random variable with n de rees of freedom. The PDF is given
1 y?" eXiK-éy) for y 2 0
my) {O a fory<0
where l“(u) is the gamma integral. The mean and variance of y are
5(9) = n
var(y) = 2n.
A1.2.2 Random Process Characterization
A discrete random process zln] is a. seguence of random variables deﬁned for every
integer n. If the discrete random process is wide sense stationary (WSS l, then it has a
mean
E($l"l) = #1
which does not depend on n, and an autocorrelation function lACFl
Tzzik] = E(z[n]z[n + k]) (A17)
which de ends onl on the la k between the two sam les and not their absolute
positions. Also, the autocovariance function is deﬁned as
Cal/v] = E Kwlnl — uxﬂwln + kl — m] = Tzzlkl - u?
In a similar manner, two jointly WSS random processes zln] and y n have a cross-
correlation function (CCF
and a cross-co variance function
Some useful properties of the ACF and CCF are
Twrlol 2 lrrrlkll
Note that rml0 is positive, which follows from (A1.7).
e z transforms of the ACF and CCF deﬁned as
lead to the deﬁnition of the s ectral density PSD . When evaluated on the
un1t circle, Pr,(z) and P,y(z) become the auto-PSD, P“(f) = P,,(exp[_7'21rf]), and
5717334951), Pndf) = P¢y(8Xp[j21rf]), or explicitly
P1¢(f)= 2 rn[klexp(—j21rfk) (A1.8)
P,y(f) = Z rwpt] exp(—j21rfk). (A1.9)
It also follows from the deﬁnition of the cross-P the r0 erty ry, = rxyl-k]
t at
The aut D describes the distribution in fre uenc of the ower of z n and as such
The magnitu e o t e cross- describes whether frequency components in z n are
associated with lar e or sma amplitudes at the same fre uenc in y n , and the hase
of the cross-PSD indicates t e ase la or lead of z n with respect to n for a iven
frequency component. Note that both spectral densities are periodic with period 1.
The frequency interval -1/2 5 f g 1/2 will be considered the fundamental period.
When there is no confusion, Pm( f) will simply be referred to as the ower s ect
density.
A process which will frequently be encountered is discrete white noise. It is deﬁned
8S 8. ZETO IHGB-Il TOCGSS havin 8J1
where ﬁle] is the discrete impulse function. This says that each sample is uncorrelated
with all the others. Using (A18), the PSD becomes
and is observed to be completely ﬁat with fre uenc . Alternatively, white noise is
composed of egui-power contributions from all freguencies.
For a linear shift invariant LSI system with im ulse response hln and with a
WSS random process in ut, various relationships between the correlations and spectral
density functions of the input process zln] and output rocess n hold. The correlation
relationships are
where ~k denotes convolution. Denoti the s stem function b H z) = Zfzwo hlnlz”,
the followin re ' nshi s f r the PSDs follow from these correlation proper 1es.
Py,(z) H(1/z)'P,,,(z)
Pw(z) = H(z)'I-l(1/z)P“(z).
In particular, letting H ( f ) = H[exp( j 21r f )] be the frequency response of the LSI system
results in
mo) = H(f)Pm(f)
PUI(f) = 
PMf) = lH(f)i2P:.-z(f)'
For the special case of a white noise input process the output PSD becomes
Pym = lH(f)|”v’ (A110)
since P“( f ) = a2. This will form the basis of the time series models to be described
A1.2.3 Gaussian Random Process
A Gaussian random process is one for which an sam les :0 no z n1], ..
_]0ll‘lt y istributed accor mg to a multivariate Gaussian PDF. If the samp es are a en
at successive times to generate the vector x =  z]1] . . . z]N — l]]T, then assuming a
zero mean WSS random process, the covariance matrix takes on the form
TzxiN _ ll TrriN _  ~ - ~ Txwiol
The covariance matrix, or more appropriately the autocorrelation matrix, has the spe-
cial symmetric Ioeplitz structure of (All) with ak = a_k.
An important Gaussian random process is the white process. As discussed previ-
ously, the ACF for a white process is a discrete delta function. In light of the deﬁnition
of a Gaussian random process a white Gaussian random process a:[n] with mean zero
and variance a2 is one for which
x[n]~N(0,a2) -—oo<n<oo
r,,[m — n] = E(:c[n]:c[m]) = 0 m gé n.
Because of the Gaussian assumption, all the samples are statistically independent.
A1.2.4 Time Series Models
A useful class of time series models consists of the rational transfer function models.
The time series .1:[n] is modeled as the output of a LSI ﬁlter with frequency response
H( excited at the input by white noise u[n] with variance 0,2,. As such, its PSD
follows from (A1.10) as
PxAf) = lH(f)|2<1§-
The ﬁrst time series model is termed an autoregressive (AR) process, which has the
time domain representation
z]n] = — i: a[k]x[n — k] + 
It is said to be an AR process of order p and is denoted by  The AR ‘parameters
consist of the ﬁlter coefﬁcients {a]1], a]2], . . . , a[p]} and the dflvlng Whlte 11°15‘? Varlance
oi. Since the frequency response is
H(f) = p 1
1 + 2a]k] exp(—j21rfk)
the AR PSD is a2
Pm:(f) z p u 2'
1 + pa] expenwfk)
It can be shown that the ACF satisﬁes the recursive difference equation
]_l]7'za:]k ~ l] k 3 1
_2a
In matrix form this becomes for k = 1, 2, . . . ,p
Tzxioi Trzill rIIip _   Txzill
Trail] r1401 aw MP1
 - 1]  - 21  11,10] air] rmilpl
and also
a; = H40] + Z a[k]rrx]k].
Given the ACF samples 1",,]k] for k  O, 1, . . . ,p, the AR parameters may be djtervréiigﬁed
by solving the set of p linear equations. These equations are termed the Ya e- a er
equations. As an example, for an AR(1) process
rmﬂc] = —a]1]r,,[k — 1] k Z 1
which can be solved to yield
rmlk] = raw] (will? k 2 0
and from
oi = rmlOl + a[1]ru[1]
we can solve for rmlO] to produce the ACF of an AR(1) process
rmlk] =   (-a[1])lkl.
The corresponding PSD is
an
' 11 +<1[11e><p<—j2wr>|"
The AR(l) PSD will have most of its power at lower frequencies if a[1] < O, and at
higher frequencies if all] > O. The system function for the AR(1) process is
1 + a[1]z_1
and has a pole at z = —a[1]. Hence, for a stable process we must have |a[1]l < 1.
While the AR process is generated as the output of a ﬁlter having only poles, the
moving average (MA) process is formed by passing white noise through a ﬁlter whose
system function has only zeros. The time domain representation of a MA(q) process is
P1I(f )
71(2)
Since the ﬁlter frequency response is
H(f) = 1 + ib[k]exp(—j21rfk)
P,,(f) = 1 + Z b[k] exp(—j21rfk) a5.
The MA(q) process has the ACF
The system function is
a
'H(z) = 1 + Z b[k]z_'°
and is seen to consist of zeros only. Although not required for stability, it is usually
assumed that the zeros satisfy lzil < 1. This is because the zeros z,- and l/zf can
both result in the same PSD, resulting in a problem of identiﬁability of the process
parameters.
References
Graybill, F.A., Introduction to Matrices with Application in Statistics, Wadsworth, Belmont, Calif,
Kay S Modern Spectral Estimation: Theory and Application, Prentice-Hall, Englewood Cliffs,
Noble, B., J.W. Daniel, Applied Linear Algebra, Prentice-Hall, Englewood Cliffs, N.J., 1977.
Oppenheim A V. R W. Schafer Digital Signal Processing, Prentice-Hall, Englewood Cliffs, N.J.,
Papoulis, A., Probability, Random Variables, and Stochastic Processes, McGraw-Hill. New York,
Appendix 2
Glossary of Symbols and
Abbreviations
(Boldface characters denote vectors or matrices. All others are scalars.)
* complex conjugate
a‘ convolution
denotes estimator
denotes estimator
denotes is distributed according to
~ denotes is asymptotically distributed according to
arg max g(0) denotes the value of 0 that maximizes g(9)
[Alli ijth element of A
[bli ith element of b
Bmse(d) Bayesian mean square error of d
Xi chi-squared distribution with n degrees of freedom
cov(x, y) covariance of a: and y
C, or C“ covariance matrix of x
C1,! covariance matrix of x and y
C,” covariance matrix of y with respect to PDF of y conditioned on x
CA/(ﬂ, a2) complex normal distribution with mean [i and variance a2
CA/(ﬁ, C) multivariate complex normal distribution with mean [i and covari-
ance C
(M172)
A/(u, C)
Dirac delta function
discrete-time impulse sequence
Kronecker delta
time sampling interval
determinant of matrix A
diagonal matrix with elements ~ - - on main diagonal
natural unit vector in ith direction
expected value
expected value with respect to PDF of x
conditional expected value with respect to PDF of z conditioned on 6
energy
signal-to-noise-ratio
discrete-time frequency
continuous-time frequency
Fourier transform
inverse Fourier transform
conjugate transpose
observation matrix
inner product of z and y
Fisher information for single data sample and scalar 6
Fisher information for scalar 6
identity matrix
Fisher information matrix for vector 6
periodogram
imaginary part of
mean square error of 6 (classical)
mean square error matrix of 6
mean
sequence index
length of observed data set
normal distribution with mean p and variance a2
multivariate normal distribution with mean p. and covariance C
110v) or Mr)
P(XI9)
9 (9)
Ufa» bl
var(z)
var(:c|6)
vector of all ones
probability density function of z
probability density function of x with 6 as a parameter
conditional probability density function of x conditioned on 6
projection matrix
orthogonal projection matrix
gradient vector with respect to x
Hessian matrix with respect to x
probability
power spectral density of discrete-time process z[n]
cross-spectral density of discrete-time processes z[n] and y[n]
power spectral density of continuous-time process z(t)
correlation coefficient
autocorrelation function of discrete-time process 
autocorrelation function of continuous-time process z(t)
cross-correlation function of discrete-time processes z[n] and g/[n]
cross-correlation function of continuous-time processes z(t) and y(t)
autocorrelation matrix of x
real part
variance
discrete-time signal
vector of signal samples
continuous-time signal
continuous time
trace of matrix A
unknown parameter (vector)
estimator of 6 (6)
transpose
uniform distribution over the interval [11, b]
variance of z
variance of conditional PDF or of p(z\6)
observation noise sequence
w vector of noise samples
w(t) continuous-time noise
 observed discrete-time data
x vector of data samples
:c(t) observed continuous-time waveform
i‘ sample mean of a:
Z z transform
Z” inverse z transform
0 vector or matrix of all zeros
Abbreviations
ACF autocorrelation function
ANC adaptive noise canceler
AR autoregressive
AR(p) autoregressive process of order p
ARMA autoregressive moving average
BLUE best linear unbiased estimator
CCF cross-correlation function
CRLB Cramer-Rao lower bound
CWGN complex white Gaussian noise
DC constant level (direct current)
DFT discrete Fourier transform
EM expectation-maximization
FFT fast Fourier transform
FIR ﬁnite impulse response
IID independent and identically distributed
IIR inﬁnite impulse response
LMMSE linear minimum mean square error
LPC linear predictive coding
LS least squares
LSE least squares estimator
LSI linear shift invariant
moving average
maximum a posteriori
maximum likelihood estimator
minimum mean square error
mean square error
minimum variance distortionless response
minimum variance unbiased
on-off keyed
probability density function
pseudorandom noise
power spectral density
Rao-Blackwell-Lehmann-Scheffe
signal-to-noise ratio
tapped delay line (same as FIR)
time difference of arrival
white Gaussian noise
wide sense stationary
Index
ACF (see Autocorrelation)
Adaptive beamforming, 544-48
Adaptive ﬁlters: (see also Least squares,
sequential)
Kalman, 439
noise canceler, 268-73
Analytic signal, 497, 551
AR (see Autoregressive)
ARMA (see Autoregressive moving average)
Asymptotic:
Cramer-Rao lower bound, 51, 77-81
efﬁciency, 38-39, 160, 164
Gaussian PDF:
real, 8O
complex, 535
mean and variance, 295, 301-2, 305-6
probability density function of MLE, 164
unbiasedness, 38, 160
Autocorrelation:
deﬁnition, 575
estimator, 197, 204, 267
Autocorrelation method of linear prediction,
Autoregressive: (see also Linear predictive
coding)
deﬁnition, 59-60, 578
power spectral density, complex process,
prediction, 414
Autoregressive moving average:
deﬁnition, 266
dynamic model, 468
estimation, 266-68
Beamforming, conventional, 547
Bearing estimation, 3, 57-59, 195-96
Bernoulli trial, 123, 200
Best linear unbiased estimator:
complex data, 523-24
covariance errors, 150
deﬁnition, 134, 137, 139-40
derivation. 151-55
linear model, 141
transformations, 135, 147, 149-50
Bias error, 18
Biomedical signal processing, 23
BLUE (see Best linear unbiased estimator)
CCF (see Cross-correlation)
Chirp rate estimator, 553
Communications:
channel equalization, 365
coherent demodulation, 273
on-off keying, 148
Complete sufﬁcient statistic, 109-12, 119
Complex envelope, 494
Conditional mean estimator (see Minimum
mean square error estimator, Bayesian)
Consistency, estimator, 24, 161, 200
Correlation coefficient:
conditional Gaussian PDF, 323
Correlation coefficient (Contd.):
deﬁnition, 64
least squares, 241
Correlation time, 50, 77-78, 535
Correlator, signal, 192
Cost function, 342
Covariance matrix, complex:
deﬁnition, 501
properties. 505-6, 555-57
Cramer-Rao lower bound:
asymptotic, 51, 77-81
complex Gaussian, 525
deﬁnition, 22, 30, 39-40, 44
Gaussian PDF, 47-48
signals in WGN, 36, 48
transformed parameters, 37, 45
CRLB (see Cramer-Rao lower bound)
Cross-correlation, 514, 575
Cross-power spectral density, 576-77
Curve ﬁtting:
least squares, 232-35
linear model. 86-88
CWGN (see White Gaussian noise, complex)
Cyclical data (see Sinusoidal estimation)
DC level in noise: (see Examples)
deﬁnition, 31
Deconvolution. 365-70
Derivative, complex, 499-500, 517, 519-21
Detection:
jump in level, 278
sinusoidal, 98-99, 148-49, 554
DFT (see Discrete Fourier transform)
Digital ﬁlter design:
equation error, 261-65
least squares, 280-81
Discrete Fourier transform:
normalization of, 511
orthogonality, 89, 569-70
PDF for WGN, 509-11, 537
Dispersive channel, 452
Efficiency, estimator, 34, 38-39, 84-86, 160,
Eigenanalysis of covariance matrix, 147-48,
Eigenvalue/eigenvector, 573
EM (see Expectation-maximization)
Entropy, 336
Equation error modeling, 266
Error ellipse, 364
Estimators:
classical vs. Bayesian. 8, 309, 312
combining, 17
deﬁnition, 9
performance, 9-12, 24, 295 (see also Monte
Carlo method and Asymptotic, mean
and variance)
selection, rationale for, 489-90
summary:
classical, 480-83
Bayesian, 484-85
Examples:
adaptive beamformer, 544-48
adaptive noise canceler, 268-73
autoregressive parameters, CRLB, 59-62
autoregressive parameters. MLE, 196-98
autoregressive parameters in ARMA, LSE.
bandpass Gaussian noise, 515-17
bearing, CRLB, 57-59
bearing, MLE, 195-96
channel estimation, 452-56
covariance matrix scale factor, Bayesian
estimation, 329-30
curve ﬁtting, MVU estimator, 86-88
DC level in colored noise, complex BLUE,
DC level in colored noise, MVU estimator.
DC level and exponential in WGN, MVU
estimator, 96-97
DC level in noise, LSE, 221
DC level in non-Gaussian noise, 172-73
DC level in uncorrelated noise, BLUE,
DC level in WGN, amplitude and variance,
MAP estimator, 355-58
DC level in WGN, amplitude/variance,
DC level in WGN, amplitude and variance
sufficient statistics, 118
DC level in WGN, biased estimator, 17
DC level in WGN, CRLB for amplitude,
Examples (Contd):
DC level in WGN. CRLB for amplitude
and variance, 40-41
DC level in WGN, CRLB for random
amplitude variance, 49-50
DC level in WGN, Gaussian prior, MMSE
estimator, 317-21, 326-28. 360-61
DC level in WGN, method of moments,
DC level in WGN. NILE for amplitude,
DC level in WGN, MLE for amplitude and
variance. 183
DC level in WGN, MLE Monte Carlo
performance, 164-66
DC level in WGN, MVU amplitude estima-
tor from sufficient statistic, 107-109
DC level in WGN, MVU amplitude and
variance estimator from sufficient
statistic, 119-22
DC level in YVGN. sequential LMMSE
estimator, 392-93
DC level in WGN, sequential LSE, 243-48
DC level in WGN, sufficient statistic, 105
DC level in WGN, transformed parameter
DC level in WGN, unbiased estimator, 16
DC level in WGN, uniform prior, LMMSE
estimator, 383
DC level in WGN, uniform prior, MAP
estimator. 352-53
DC level in \VGN. uniform prior, MMSE
estimator, 315
DC level in white noise, BLUE, 137-38
digital ﬁlter design, LSE, 261-65
discrete Fourier transform, PDF ofCWGN,
discrete Fourier transform, PDF of WGN,
exponential PDF parameter, MAP estima-
tor, 351-52
exponential PDF parameter, method of
moments, 292, 295-97
exponential PDF parameter transforma-
tion, MAP estimator, 358-59
exponential signal, LSE, 257-58
exponential signal in WGN, MLE, 178-82
exponential signal in white noise, ad-hoc
estimator, 298-99
Examples (Contd):
Fourier analysis, Bayesian, 347-49, 362-64.
Fourier analysis. LSE, 226-27, 230-31
Fourier analysis, MVU estimator, 88-90
Fourier analysis. sequential LSE, 250-51
frequencies of sinusoids, EM estimator.
frequency of sinusoid. CRLB. 36
frequency of sinusoid, method of moments.
frequency of VVSS process, center, CRLB,
Gaussian mixture parameters, 290-91.
Gauss-Markov model, 427-28
Hermitian form, mean and variance, 512-13
Hermitian function. minimization. 521-23
identiﬁcation of FIR system, MVU estima-
tor, 90-94
Kalman ﬁlter. 436-38, 443-45
linear model, classical complex, 529-30
line ﬁtting, CRLB. 41-43
line ﬁtting, order-recursive LSE, 237-40
localization, source, BLUE, 142-46
mean of uniform noise, MVU estimator.
moving average, MLE,. 19(P91
MVU estimator, possible nonexistence of.
orthogonal random variables. LMMSE
estimator. 388-89
PDF parameter dependence, 28-31
periodogram spectral estimation, 538-39
phase-locked loop, 273-75
phase of complex sinusoid, MLE, 531-32
phase of sinusoid, CRLB, 33-34
phase of sinusoid, MLE, 167-72
phase of sinusoid, sufficient statistic, 106-7
power of noise, CRLB, 49
power of noise, sufficient statistic, 105
range, CRLB, 53-56
range, MLE, 192
signal, constrained LSE, 252-54
signal amplitude estimation, complex LSE.
signal in non-Gaussian noise, MLE, 184-85
signal in \VGN, CRLB, 48
signal-to-noise ratio. CRLB, 46
Examples (ContzL):
sinusoidal amplitude, LSE, 255-56
sinusoidal complex amplitude, MMSE es-
timator, 534-35
sinusoidal modeling, complex, 496-97
sinusoidal parameters, complex MLE,
sinusoidal parameters, CRLB, 56-57
sinusoidal parameters, LSE, 222-23
sinusoidal parameters, MLE, 193-95
sinusoidal parameters, suﬁicient statistics.
sinusoidal power. complex MVU estimator.
suﬁicient statistic, completeness of, 110-11
suﬁicient statistic, incompleteness of,
suﬁicient statistic veriﬁcation, 103-4
vehicle tracking, 456-66
Wiener ﬁltering, 365-70, 400-409, 443-45
Expectation-maximization, 182, 187-89
Exponential PDF family:
deﬁnition, see Probability density func-
tions
Exponential signals:
estimation, 257-58, 298-99
Fading signal, 100, 452
Finite impulse response ﬁlter, 90-94
FIR (see Finite impulse response ﬁlter)
Fisher information:
decoupled matrix, 41, 65
deﬁnition, 34, 4O
properties, 35, 65
Fourier analysis, 88-90, 226-27, 250-51,
Frequency estimation (see Sinusoidal estima-
tion and Examples)
Gaussian random process, 467, 513, 577-78
Gauss-Markov process:
deﬁnition, 421, 426, 430-31
properties, 424, 429
Gauss-Markov theorem, 141, 143, 552
Gauss-Newton iteration, 260
Gradient formulas, 73-74, 84, 519-21
Gram-Schmidt orthogonalization, 236, 396,
Grid search, 177
Hermitian form:
deﬁnition, 502
minimization, 521-23
moments, 502-3, 513
Histogram, 10, 165. 206-7. 209
Image signal processing, 365
Innovations, 396, 433. 441
In-phase signal, 495-96
Interference suppression, 270
Interpolation, 412
Kalman ﬁlter:
deﬁnition, 436, 446-49, 455
derivation, 471-75
extended, 451-52, 462, 476-77
gain, 436, 447
information form, 449
steady state, 443
Least squares:
BLUE, relationship with, 225
constrained, 252
deﬁnition, 220-21
estimator, 225
modiﬁed Yule-Walker equations, 268
nonlinear, 222, 254
numerical determination, 259-60
order-recursive, 237, 282-84
separable, 222-23, 256-57
sequential, 249, 279, 286-88
weighted, 150, 225-26, 244-48, 270
Levinson recursion, 198, 403
Likelihood function:
deﬁnition, 29
modiﬁed, 175, 185
Linear minimum mean square error estima-
deﬁnition, 380-82, 389
properties, 390
sequential, 393, 398, 415-18
vector space interpretation, 386
Linear model (Bayesian):
deﬁnition, 325
Kalman ﬁlter modeling, 447
MMSE estimator, 364-65, 533-34
properties, 487-89
Linear model (classical):
deﬁnition, 84, 94-95, 97, 529-30
eﬁiciency, 85-86
estimator and properties, 85, 486-88
line ﬁtting, 45
reduced. 99, 254
suﬁicient statistics, 126
Linear predictive coding, 5, 59, 198, 407
Linear random process, 77
Line arrays, 58, 145
Line ﬁtting, 41, 83-84, 237-40, 373
LMMSE (see Linear minimum mean square
error estimator)
Localization, source, 142-46, 456-66
LPC (see Linear predictive coding)
LS, LSE (see Least squares)
Lyapunov equation, 430
MA (see Moving average)
MAP (see Nlaximum a posteriori estimator)
Matrix:
autocorrelation, 62, 93
determinant, 567
diagonal. 568-69
eigenanalysis, 573
Hermitian. 501
idempotent, 194, 570
ill-conditioned, 85, 98, 240-41
inversion:
deﬁnition, 567
lemma, 571
Woodbury's identity, 571
orthogonal, 569
partitioned, 571-72
positive deﬁnite (semideﬁnite), 568, 572
projection, 231, 242, 277, 285
square, 567
symmetric, 567
Toeplitz, 62, 93, 570
trace, 568
transpose, 567
Maximum a posteriori estimator:
deﬁnition, 344, 351, 354, 372
properties, 358, 372
Maximum likelihood estimator:
asymptotic, 190
Bayesian, 352
Maximum likelihood estimator (Contzi):
complex data, 530-31, 563-65
deﬁnition, 162, 182
eﬁiciency, 164, 187
Gaussian PDF, 185
invariance, 174-76, 185
numerical determination, 177-82, 187-89
probability density function, asymptotic,
properties, asymptotic, 172, 201-2
Mean square bandwidth, 55
Mean square error:
Bayesian, 311, 320, 347, 533
classical, 19
Mean square error matrix, 361-62, 390
Minimal suﬁicient statistic, 102, 117
Minimum mean square error estimator:
Bayesian:
deﬁnition, 313, 316, 346
performance, 360, 364-65, 534
properties, 349-50
classical, 19, 311
Minimum variance distortionless response,
Minimum variance unbiased estimator:
deﬁnition, 20
determination of, 109, 112-13
linear model, 85-86
MLE (see Maximum likelihood estimator)
MMSE (see Minimum mean square error
estimator)
Modeling:
dynamical signal, 421
identiﬁability, 85
least squares, 232-34
linearization, 143, 259, 273, 451, 461
speech spectrum, 5 (see also Autoregres-
sive and Linear predictive coding)
Moments, method of:
deﬁnition, 293
exponential parameter, estimator, 292,
Gaussian mixture, 290-91, 293-94
Monte Carlo method, 10, 164-167, 205-10
Moving average:
asymptotic MLE, 190-91
deﬁnition, 580
MSE (see Mean square error)
.\IVU (see Minimum variance unbiased esti-
mater)
Narrowband representation, 495
Newton~Raphson iteration, 179-82, 187, 259
Neyman-Fisher factorization, 104-5, 117,
Normal equations, 225. 387
Notational conventions, 13 (see also Ap-
pendix 2)
Nuisance parameters, 329
Observation equation, 446
Observation matrix, 84, 100, 140, 224
Order statistics, 114
Orthogonality. 89, 385 (see also Projection
theorem, orthogonal)
Outliers, 170
PDF (see Probability density functions)
Periodogram, 80, 190, 195, 197, 204 (see also
Spectral estimation)
Phase-locked loop, 273-75
Posterior PDF:
Bayesian linear model, 326, 533
deﬁnition, 313, 317
Power estimation, random process, 66, 203,
Power spectral density, 576-77
Prediction:
Kalman, 440-41. 469-70
\Viener. 400
Prior PDF:
conjugate, 335 (see also Reproducing
deﬁnition. 313
noninformative, 332, 336
Probability density functions:
chi-squared. 122, 575
complex Gaussian:
conditional, 508-9, 562
deﬁnition, 503-4, 507
properties, 508-9, 550, 558-62
exponential, 122
exponential family, 110, 124
gamma, inverted, 329-30, 355
Gaussian, 574
Gaussian, conditional, 323-25, 337-39
Gaussian mixture, 150
Probability density functions (Conttl):
Laplacian. 63
lognormal. 147
Rayleigh, 122, 371
Processing gain, 554
Projection theorem, orthogonal, 228-29, 386
Prony method, 264
PSD (see Power spectral density)
Pseudorandom noise, 92, 165, 206
Pythagorean theorem. least squares, 276
Quadratic form:
deﬁnition. 568
moments, 76
Quadrature signal, 495-96
Radar signal processing, 1 >
Random number generator (see Pseudoran-
dom noise)
Random variable. complex, 500-501
Range estimation. 1, 14, 53-56, 192
Ra0-Blackwell-Lehmann-Scheffe theorem.
Rayleigh fading, 347
RBLS (see Rae-Blackwell-Lehmann-Scheffe
theorem)
Regression, nonlinear, 254
Regularity conditions, 30, 44, 63, 67, 70
Reproducing PDF, 321. 334-35
Ricatti equation, 443
Risk, Bayes. 342
Sample mean estimator, 115, 121, 164 (see
also DC level in noise)
Sample variance estimator, 121, 164
Scoring, 180, 187
Seismic signal processing, 365
Separability, least squares, 222-23, 256
Signal amplitude estimator, 136, 498-500
Sinusoidal estimation:
amplitudes, 88-90
complex data. 525-27, 531-32, 534-35, 543
CRLB for frequency, 36
CRLB for parameters, 56-57, 542
CRLB for phase, 33
EM for frequency, 187-89
least squares for parameters, 255-56
method of moments for frequency, 300, 306
MLE for parameters, 193-95, 203-4
Sinusoidal estimation (Contd. )1
phase estimator, 123. 167-72
sufficient statistics, 117-18
Sinusoidal modeling, complex, 496
Slutsky’s theorem, 201
Smoothing, Wiener, 400
Sonar signal processing, 2
Spatial frequency, 58, 195
Spectral estimation:
autoregressive, 60
Fourier analysis, 88-90
periodogram, 204, 538-39, 543, 552
Speech recognition, 4
State transition matrix, 426
State vector, 424
Statistical linearization, 39, 200 (see also
Modeling)
Sufficient statistic, 22, 102-3, 107, 116
System identiﬁcation:
nonrandom FIR, 90-94, 99
random FIR, 452-55
Tapped delay line (see FIR)
Threshold effect, 170
Time delay estimation, 53-56, 142-46
Time difference of arrival, 142
Time series, 6
Tracking:
frequency, 470 (see also Phase-locked loop)
vehicle position, 456-66
Unbiased estimator, 16, 22
Vector spaces:
least squares. 227-30
random variables, 384
Wavenumber (see Spatial frequency)
WGN (see White Gaussian noise)
White Gaussian noise:
real, 7
complex, 517
Whitening:
Kalman, 441, 444
matrix transformation, 94-96
White noise, 576
Wide sense stationary, 575
Wiener ﬁltering, 365-70, 373-74, 379, 400-409,
Wiener-Hopf equations:
ﬁltering, 403
prediction, 406-7
WSS (see Wide sense stationary)
Yule-Walker equations:
